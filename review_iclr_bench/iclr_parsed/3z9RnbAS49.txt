# A THEORETICAL AND EMPIRICAL MODEL
## OF THE GENERALIZATION ERROR UNDER TIME-VARYING LEARNING RATE

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Stochastic gradient descent is commonly employed as the most principled optimization algorithm for deep learning, and the dependence of the generalization
error of neural networks on the given hyperparameters is crucial. However, the
case in which the batch size and learning rate vary with time has not yet been
analyzed, nor the dependence of them on the generalization error as a functional
form for both the constant and time-varying cases has been expressed. In this
study, we analyze the generalization bound for the time-varying case by applying
PAC-Bayes and experimentally show that the theoretical functional form for the
batch size and learning rate approximates the generalization error well for both
cases. We also experimentally show that hyperparameter optimization based on
the proposed model outperforms the existing libraries.

1 INTRODUCTION

While deep learning addresses a variety of practical tasks, the reason deep learning generalizes well
is not fully understood. It is hoped that the performance of deep learning methods can be improved
by understanding deep learning. Consequently, deep learning will become a cornerstone for the
next generation of technology. Stochastic gradient descent (SGD) is a principled learning algorithm
for deep learning, and because most deep learning algorithms are developed based on SGD, its
theoretical analysis is crucial. He et al. (2019) modeled the generalization bounds when both the
batch size and learning rate are constant in SGD; however, they did not model the generalization
error. Furthermore, it is crucial to decrease the learning rate as the training progresses to make
SGD converge (Robbins & Monro, 1951), which results in good performance (You et al., 2020),
but He et al. (2019) did not consider the case in which these parameters are time-varying. We
analyze the time-varying case in SGD and propose a generalization error model for both constant
and time-varying cases. Based on these models, we also propose a novel hyperparameter search
method. In this study, we utilized a PAC-Bayes-based generalization bound to generate a model of
the generalization error. To validate the proposed model, we trained approximately 300 networks on
both practical networks (VGG16 (Simonyan & Zisserman, 2015), Wide-ResNet 28-10 (Zagoruyko
& Komodakis, 2016)) and data (CIFAR10, CIFAR100 (Krizhevsky et al., 2009)). We also showed
that the proposed model can approximate the experimental results well. We also show that the
proposed model can be used for hyperparameter search via Bayesian optimization and can yield
better results, compared with the previous method for searching in the uniform distribution, and
comparable results for searching in the logarithmic uniform distribution.

The contributions of the present study are summarized as follows:

-  We analyzed the generalization bound using PAC-Bayes with the timevarying batch size and learning rate.

-  We directly modeled the generalization error on the batch size and learning
rate and proposed its functional form.

-  We experimentally showed that it is possible to model the generalization error
with high accuracy for practical data.

-  Our experimental results demonstrated that the hyperparameter search
method built on the proposed model can outperform current libraries when


-----

searching over uniform distributions and has comparable performance when
searching over logarithmic uniform distributions.

2 RELATED WORK

In this section, we present related work on SGD modeling, generalization bounds, prediction of
model performance, and hyperparameter optimization.

**Theoretical analysis of SGD:** SGD is often treated as a continuous-time stochastic differential
equation (SDE) by modeling the noise of the gradient and the loss function and applying the Euler–
Maruyama approximation (Lin et al., 2018; Mandt et al., 2017; Li et al., 2017; Jastrz˛ebski et al.,
2017). This approximation can be solved analytically as an Ornstein–Uhlenbeck process (Uhlenbeck
& Ornstein, 1930). In this study, we also analyzed SGD using the same process as the previously
mentioned studies. However, in most previous studies, the batch size and learning rate in SGD
were treated as constants. Hence, in this study, we treat them as time-varying and seek an analytic
solution.

**Theoretical error bounds:** There is a large body of research on theoretical explanations for the
generalization capability of deep neural networks (Negrea et al., 2019; Arora et al., 2018; Cao & Gu,
2019; Deng et al., 2021). He et al. (2019), building on their analysis of SGD, provided a theoretical
explanation of the effects of the batch size and learning rate on the generalization error. However,
this research only handles the case of constant batch size and learning rate and does not model the
generalization error in the function form. In this study, we derive a generalization bound based on
an analytic solution for the time-varying case and derive a model of the generalization error.

**Predicting model performance:** Most studies are based on the results from training networks,
assuming hyperparameters as inputs, and searching for the best parameters without training the
networks. In neural architecture search, the performance of a model was predicted for a given
model on a given dataset (Istrate et al., 2019), small data (Klein et al., 2017), and model size (Real
et al., 2019), respectively. Several studies predict the performance of a model in a functional form
by treating both the dataset and model sizes as input (Rosenfeld et al., 2020), or the pruning scale as
input (Rosenfeld et al., 2021). In this study, we exploit the findings of the above studies to predict
the performance of a model in a functional form, based on a theoretically derived generalization
bound to the batch size and learning rate.

**Hyperparameter optimization:** Hyperparameter optimization algorithms are mainly categorized into two types: nonadaptive and adaptive. Nonadaptive algorithms search for the best hyperparameters without using any past search results and, thus, have the advantage of being easily
parallelizable. Grid search and random search fall into this category, and many studies have shown
that random search is more useful (Bergstra & Bengio, 2012; Li & Talwalkar, 2020). Adaptive algorithms search for the best hyperparameters by exploiting past search results and can obtain better
hyperparameters with fewer searches. Adaptive algorithms include specific algorithms, such as the
evolutionary algorithm (Young et al., 2015) and Bayesian optimization (Wu et al., 2019), and practical tools, such as Hyperopt (Bergstra et al., 2015) and Optuna (Akiba et al., 2019). These methods
are flexible enough to be optimized for arbitrary functions. However, there is still room for more
efficient search when searching only for specific hyperparameters. In this study, we aim to further
improve the search efficiency of both the batch size and learning rate by employing a hyperparameter
search based on the proposed generalization error model.

In the following sections, we will analyze the influence of the batch size and learning rate on the
generalization error in SGD, based on assumptions about the distribution of the gradients and the
shape of the risk function near minima, an approximation to SDE, and the PAC-Bayes method.

3 PRELIMINARIES

In this section, we describe the preliminary knowledge used to model the learning process of SGD.

**Generalization error of stochastic algorithms. In general, machine learning algorithms are de-**
signed to select a hypothesis function Fθ from a hypothesis class {Fθ|θ ∈ Θ ⊂ R[d]} such that the
expected value of the risk function R computed from the loss function l is minimized. In a proba

-----

bilistic algorithm such as SGD, the parameter θ after learning is considered to lie on the distribution
_Q. The risk function for parameter θ is defined as follows:_

(θ) = E(X,Y ) [l(Fθ(X), Y )] . (1)
_R_ _∼D_

Therefore, (Q) = Eθ _Q [_ (θ)]. However, one cannot compute the risk function, and in practice,
_R_ _∼_ _R_
the empirical risk _R[ˆ](θ) from the data is approximated as the risk function R(θ)._


ˆ(θ) = [1]
_R_ _N_


_l(Fθ(Xi), Yi),_ (2)
_i=1_

X


where (Xi, Yi) is the N training data consisting of the input data and label pairs. Thus, [ˆ](Q) =
_R_
Eθ _Q[ [ˆ](θ)]._
_∼_ _R_

**Stochastic gradient descent. Gradient descent is an algorithm that computes the gradient of the risk**
function defined in Eq.1 and updates the parameter θ based on the gradient. The gradient g(θ(t))
is defined as g(θ(t)) ≜ _∇θ(t)R(θ(t)) = ∇θ(t)E(X,Y )_ _l(Fθ(t)(X), Y )_ . We update the parameter
_θ by θ(t + 1) = θ(t)_ _ηg(θ(t)), where η is the learning rate. In practice, the true gradient of_
_−_  
_g(θ(t)) is approximated by the gradient of ˆgS(θ(t)) computed from the empirical risk_ [ˆ](θ) defined
_R_
in Eq.2, and the parameter θ is updated with ˆgS(θ(t)). Thus, the gradient ˆgS(θ(t)) is defined as
_gˆS(θ(t)) =_ _θ(t)_ [ˆ](θ(t)) = _S1_ _i_ _S_
_∇_ _R_ _|_ _|_ _∈_ _[∇][θ][(][t][)][l][(][F][θ][(][t][)][(][X][i][)][, Y][i][)][, and the parameter][ θ][ is updated by]_

_θ(t_ +1) = θ(t) _ηgˆS(θ(t)). We use a mini-batch S to train the model in parallel, and_ _S_ represents
_−_ P _|_ _|_
its size. Then, we treat li(θ) = l(Fθ(t)(Xi), Yi).

**Analysis of the learning process. The loss function li(θ) and its mean** _R[ˆ](θ) is an unbiased estima-_
tor of the risk function (θ), and the gradient of the loss function _θli(θ) and its mean ˆgS(θ) is an_
_R_ _∇_
unbiased estimator of the gradient g(θ) = ∇θR(θ). Hence, we have E[li(θ)] = E[ R[ˆ](θ)] = R(θ)
and E[∇θli(θ)] = E[ˆgS(θ)] = g(θ) = ∇θR(θ), where the expectations are computed from the
data distribution (X, Y ). Here, as in Mandt et al. (2017), we assume that _li(θ)_ is independently
_{∇_ _}_
and identically sampled from a Gaussian distribution with mean g(θ) = _θ_ (θ). We can express
_∇_ _R_
_θli(θ)_ (g(θ), C), where C is the covariance matrix, which is assumed to be a constant matrix
_∇_ _∼N_
for all θ. Then the gradient of ˆgS(θ) computed for a mini-batch follows the following Gaussian
distribution:

_gˆS(θ) = [1]_ _θli(θ)_ _g(θ),_ [1] _._ (3)

_S_ _∇_ _∼N_ _S_
_|_ _|_ Xi∈S  _|_ _|_ _[C]_

Because the covariance matrix is symmetric and (semi) positive definite, we suppose that C can be
factorized as C = BB[⊤]. In an actual SGD algorithm, the parameters are iteratively updated using
Eq.3 to minimize the risk function R(θ):


_η_

_|S|_ _B∆W,_ ∆W ∼N (0, I). (4)


∆θ(t) = θ(t + 1) _θ(t) =_ _ηgˆS(θ(t)) =_ _ηg(θ) +_
_−_ _−_ _−_


**Analytical solution of the stochastic differential equation. When the learning rate η is small,**
Eq.4 can be approximated by SDE, which is known as Ornstein–Uhlenbeck process (Uhlenbeck &
Ornstein, 1930). Here, we assume that the risk function is convex and second-order differentiable
in the neighborhood of the minimum (He et al., 2019). In addition, when the risk function R(θ) is
minimized at θ = θ[∗], it is shifted so that it is minimized at θ = 0. Thus, the risk function R(θ) can
be approximated by second-order statistic on the distribution Q as follows:

(θ) = [1] (5)
_R_ 2 _[θ][⊤][Aθ,]_


where A is the Hessian matrix around the minimum of the risk function and is assumed to be positive
definite. This assumption was experimentally demonstrated by Li et al. (2018), and other studies
have made similar assumptions (Jastrz˛ebski et al., 2017; Poggio et al., 2017). Because E[ R[ˆ](θ)] =
_R(θ), we further assume that the empirical risk_ _R[ˆ](θ) is in the following quadratic form:_

ˆ(θ) = [1] _A(θ_ _θb)_ (6)
_R_ 2 [(][θ][ −] _[θ][b][)][⊤]_ [ˆ] _−_


-----

where θb is the bias term, _A[ˆ] is the Hessian matrix, and E[θb] = 0, E[ A[ˆ]] = A._

It is well known that Eq.4 has an analytic solution (Gardiner, 2004),


_η_
_θ(t) = e[−][At]θ(0) +_

_S_

r _|_ _|_


_t_

0

Z


_e[−][A][(][t][−][t][′][)]BdW_ (t[′]), (7)


where W (t[′]) is white noise and follows N (0, I) and Q is the distribution of θ in Eq.7 and Gaussian.

4 THEORETICAL GENERALIZATION BOUND

In this section, we derive the generalization bounds for both constant and time-varying cases.

4.1 CONSTANT CASE

Using Eq.7 and PAC-Bayes, we obtain a generalization bound for SGD as follows (see Appendix
B.1 for the derivation).

**Theorem 1 (extension of He et al. (2019), Theorem 2). We treat A as the Hessian of the risk function**
_around the local minimum, C as the covariance matrix of the gradients calculated by single sample_
_points, Q as the distribution of the output hypothesis function of SGD, Σ is the covariance matrix of_
_the distribution Q, d is the dimension of the parameter θ (network size), and R is the search radius_
_of the parameter θ._

_When A and Σ commute, for any positive real δ ∈_ (0, 1), with probability 1 − _δ over a training_
_sample set of size N, we have the following inequality for the distribution Q:_

_R(Q) ≤_ _R[ˆ](Q)_


_η_ 2|S|

2|S| [tr(][CA][−][1][) +][ d][ log] _η_



_−_ log (det (CA[−][1])) + R[2] _−_ _d + 2 log (_ [1]δ [) + 2 log][ N][ + 4]

_._
4N − 2

(8)


This generalization bound incorporates a new scale factor for the parameter search, R[2], into Theorem 2 of He et al. (2019). Although similar results can be derived without assuming that A and Σ
commute, we used the above assumption for simplicity. R represents the radius of the hypersphere
in the parameter search, and _θ_ _θ0_ _R[2]. We also show that this generalization bound also has_
_∥_ _−_ _∥[2]_ _≤_
a network initialization factor. A detailed description is provided in Appendix B.2.

4.2 TIME-VARYING CASE

It should be noted that, when solving Eq.4, the hyperparameters in SGD such as the batch size
_|S| and the learning rate η can be time-varying. While He et al. (2019) derived a generalization_
bound for the case in which these are constant, we further analyze the time-varying case. In the
time-varying case, the solution takes the form of a hyperparameter term in Eq.7 entering the integral
(Gardiner, 2004).

_t_
_θ(t) = e[−][At]θ(0) +_ _n(t[′])e[−][A][(][t][−][t][′][)]BdW_ (t[′]), (9)

0

Z

p

where n(t) = _|Sη((tt))|_ [. Then, we treat the time-varying batch size and learning rate as a univariate]

function n(t). This allows us to treat changes in the learning rate (Robbins & Monro, 1951; You
et al., 2020) and batch size (Smith et al., 2018) in a unified manner. In general, if n(t) is allowed to be
an arbitrary function, this integral cannot be solved analytically. However, because the learning rate
generally decreases and the batch size generally increases as the learning progresses, if we assume
_n(t) to be a monotonically decreasing function and apply the mean value theorem for integrals_
_t1_ _t1_
_t0_ _[n][(][t][)][F]_ [(][t][)d][t][ =][ n][(][ξ][)] _t0_ _[F]_ [(][t][)d][t, ξ][ ∈] [[][t][0][, t][1][]][, then we obtain a generalization bound for the time-]
varying case as follows (see Appendix B.3 for the detailed derivation).
R R


-----

_t_
**Theorem 2. We treat D =** 0 _[e][−][A][(][t][−][t][′][)][Ce][−][A][(][t][−][t][′][)][d][t][′][. Using the same settings as theorem 1 and]_
_when A and Σ commute and A and D commute, we have the following inequality for the time-_
R
_varying case:_

_R(Q) ≤_ _R[ˆ](Q)_


_−_ log (det (CA[−][1])) + R[2] _−_ _d + 2 log (_ [1]δ [) + 2 log][ N][ + 4]

_,_
4N − 2

(10)


_ninter_


tr(CA[−][1]) + d log


_ninter_


_and_
_ninter = (1 −_ _λ) min (n(t)) + λ max (n(t)),_ _λ ∈_ [0, 1]. (11)

5 FUNCTIONAL APPROXIMATION OF THE GENERALIZATION ERROR

Based on the theoretical analysis thus far, we consider approximating R(Q) in a functional form.
From Eq.8 and Eq.10, we have already modeled the generalization bound (R(Q) − _R[ˆ](Q)). We_
can also design a empirical functional form based on He et al. (2019)’s experimental results on the
effect of ratio of the batch size to learning rate on the generalization error. Thus, if we can model
the training error _R[ˆ](Q), we can derive the functional form of the generalization error. We model the_
training error by substituting Eqs.7 and 9 into Eq.6. Subsequently, we show the specific functional
form of the generalized error model for constant and time-varying cases.

5.1 CONSTANT CASE

First, we organize Eq.7 with respect to the batch size |S| and learning rate η and the learned parameter θ(t) is as follows:

_η_
_θ(t) = a0 +_ (12)

_S_

r _|_ _|_ **_[a][1][,]_**

where a0, a1 are vectors that do not depend on |S|, η. By substituting this into Eq.6, the functional
form of the training error _R[ˆ](Q) is as follows. The detailed derivation is in Appendix B.4.1._

1 1 _η_ _⊤_ _η_

ˆ(Q) Eθ _Q_ _A(θ_ _θb)_ = E **_a[′]0_** [+] _Aˆ_ **_a[′]0_** [+]
_R_ _≃_ _∼_  2 [(][θ][ −] _[θ][b][)][⊤]_ [ˆ] _−_  " 2  r _|S|_ **_[a][1]_**  r _|S|_ **_[a][1][#]_**

_η_ _η_
= a0 + a1 (13)

_S_ [+][ a][2] _S_
_|_ _|_ r _|_ _|_ _[,]_

where a[′]0 [=][ a][0] [are constants. Although we can model][ R][(][Q][)][ directly by]
substituting Eq.7 into Eq.5, this cannot be treated as a practical functional form as it does not model[−] _[θ][b][, and][ a][0][, a][1][, a][2]_
the experimental increase of the generalization bound with respect to the decrease of _Sη_ [(He et al.,]

_|_ _|_

2019). Thus, we address this problem by modeling the training error _R[ˆ](Q) and the generalization_
bound (R(Q) − _R[ˆ](Q)) separately. To model the generalization bound, we propose the following_
functional form based on the model derived in Eq.8 and He et al. (2019)’s experimental results. The
detailed derivation is given in Appendix B.5.1.


_|S|_


_R(Q) −_ _R[ˆ](Q) ≃_ _a3 + a4_


(14)


Here, a3, a4 are constants. By combining Eq.13 and Eq.14, we propose the following functional
form as a model of the generalization error R(Q).

_η_ _η_ _S_
(Q) = ˜ϵ(S, η) = _c0_ + _c2_ _|_ _|_ + c3 (15)
_R_  _|S|_ [+][ c][1]r _|S|_  s _η_ !


-----

**(a) CIFAR10** **(b) CIFAR100** **(c) Estimated vs. actual error.**

**(d) CIFAR10** **(e) CIFAR100** **(f) Estimated vs. actual error.**

**Figure 1: Experimental results of the proposed model for both (a, b, c) constant and (d, e, f) time-varying**
cases. (a, b) For the constant case, we plotted points with the ratio of the learning rate to the batch
size on the horizontal axis of a log scale and the generalization error on the vertical axis. (d, e)
For the time-varying case, we plotted points with the ratio of the initial learning rate to the batch
size and the ratio of the final learning rate to the batch size on the horizontal axis of the log scale,
and the generalization error on the vertical axis. (a, b, d, e) For all cases, we also show the results
fitted by the proposed model as green lines and surfaces, respectively. (c, f) The true generalization
error and estimation error are plotted, and the mean and standard deviation of the relative error δ are
represented by µ and σ, respectively.

where c0 ∼ _c3 are constants. The first term models the training error_ _R[ˆ](Q), and the second term_
models the generalization bound ( (Q) (Q)). Thus, the first term denotes that the training error
_R_ _−_ _R[ˆ]_ _η_
monotonically increases with respect to the hyperparameter _|S|_ [, and the second term denotes that]

the generalization bound inversely decreases.

5.2 TIME-VARYING CASE

As discussed in Section 4.2, we approximate Eq.9 to a simple form similar to Eq.12, using an approximation of the mean value theorem for integrals. Based on the functional form obtained by substituting Eq.9 into Eq.6, the model of generalization bounds derived in Eq.10, and He et al. (2019)’s
experimental results, we propose the following functional form as the model of the generalization
error ϵ(S, η). A detailed derivation is given in Appendix B.4.2 and B.5.2.


1

_ϵ˜(S, η) =_ _c0n[2]1_ [+][ c][1][n][1] + _c2_ + c3 (16)

_n2_

 r 
  

_n1 = (1 −_ _λ1) min_ _n(t)_ + λ1 max _n(t)_ (17)

_n2 = (1 −_ _λ2) min (p_ _n(t)) +_ _λ2 max (pn(t))_  (18)


whereintermediate value is computed after taking the square root of c0 ∼ _c3, λ1, λ2 are constants and λ1, λ2 ∈_ [0, 1]. Note that, for n(t), and for n n1 in the first term, the2 in the second term,
the linear interpolation is computed.

5.3 PRACTICAL FUNCTIONAL FORM

In the discussion above, we constructed a rough model of the generalization error. However, the
models in Eqs.15 and 16 have no upper bounds, and they don’t model the fact that the loss function


-----

**(a) Constant case** **(b) Time-varying case** **(c) Constant case** **(d) Time-varying case**

**Figure 2: Experimental results on the stability of the generalization error model for (a, b) CIFAR10 and (c, d)**
CIFAR100, for the (a, c) constant case and (b, d) time-varying case. The horizontal axis shows the
number of networks randomly selected to estimate the generalization error, whereas the vertical axis
shows the mean value µ and standard deviation σ of the relative error δ. The shaded areas represent
one standard deviation from the mean in each direction.

hits its head at the random guess level. Therefore, it is necessary to model the transition from the
initial random guess level to the proposed model ˜ϵ(S, η). Thus, we propose the model ˆϵ(S, η) using
a soft-min function based on the LogSumExp function.

_ϵ(S,η)_ _c4ϵ0_ []

_ϵˆ(S, η) = softmin(˜ϵ(S, η), ϵ0; c4) = [1]_ log _e[−][c][4][˜]_ + e− (19)

_c4_ _−_

 

where c4 is a constant that controls the shape (sharpness) of the soft-min function (softmin(x, y) =
_−_ log (exp (−x) + exp (−y))). ϵ0 is the value of the loss function at the random guess level, which
is statistically determined and not explored. (e.g., for a balanced dataset in image classification, we
can treat it as ϵ0 = 1 − (1/Nclasses).)

6 EXPERIMENT

To validate the proposed model ˆϵ(S, η), we trained a practical network (VGG16 (Simonyan & Zisserman, 2015), Wide-ResNet 28-10 (Zagoruyko & Komodakis, 2016)) on the image classification
tasks (CIFAR10, CIFAR100 (Krizhevsky et al., 2009)).

6.1 EXPERIMENTAL SETTINGS AND RESULT

In the constant case, the batch size and learning rate were fixed throughout the training process. To evaluate the generalization error, we trained 60 VGG models with a batch size |S| ∈
_{16, 32, 64, 128, 256, 512} and a learning rate η ∈{1/2[i]|i = 3, ..., 12} for CIFAR10, as well as_
66 Wide-ResNet models with the same batch size |S| and learning rate η ∈{1/2[i]|i = 2, ..., 12}
for CIFAR100 over a total of 200 epochs. The experimental results for the constant case are shown
in Figures.1a and 1b. We plotted the log scale of _Sη_ [on the horizontal axis and the generalization]

_|_ _|_
error on the vertical axis. We found both a decreasing and an increasing property of the generalization error before and afterη _|Sη_ _|_ _[≃]_ [10][−][4][, which we discussed in Eq.15. Although He et al. (2019)]

claimed that increasing _S_ [decreases the generalization error, the experimental results show that the]

generalization error increases when| _|_ _Sη_ [exceeds a certain value. This implies that, in addition to the]

_|_ _|_

generalization bound, the training error _R[ˆ](Q) must be modeled._

In the time-varying case, only the learning rate is set to be time-varying, and we use an exponential
step function. We chose ηinit and ηfinal and trained over 200 epochs, starting with a learning rate
of ηinit, decaying with a decay rate of γ at 60, 120, and 160 epochs, and finally reached a learning
rate ηfinal = γ[3]ηinit at 160 epochs.

In addition to the constant case, we trained a case in which the batch size |S| _∈_
16, 32, 64, 128, 256, the learning rate of ηinit 1/2[i] _i = 3, ..., 4_ and ηfinal 0.1/2[i] _i =_
_{_ _}_ _∈{_ _|_ _}_ _∈{_ _|_
0, ..., 4 for CIFAR10, the same batch size _S_, the learning rate of ηinit 1/2[i] _i = 2, ..., 4_ and
the same} _ηfinal for CIFAR100, as well as a case in which the batch size |_ _|_ _∈{ |S| ∈{|_ 128, 256}}, the
learning rate of ηinit 1/2[i] _i = 5, ..., 9_ and ηfinal 0.1/2[i] _i = 5, ..., 9_ for both datasets. In
_∈{_ _|_ _}_ _∈{_ _|_ _}_


-----

**(a) CIFAR10, uniform** **(b) CIFAR10, log** **(c) CIFAR100, uniform** **(d) CIFAR100, log**

**(e) CIFAR10, uniform** **(f) CIFAR10, log** **(g) CIFAR100, uniform** **(h) CIFAR100, log**

**Figure 3: Hyperparameter optimization results of several methods. The number of trained models is plotted**
on the horizontal axis, and the maximum accuracy achieved among the trained models is plotted on
the vertical axis for the (a, b, c, d) constant and (e, f, g, h) time-varying cases. The 95% confidence intervals are shaded. The proposed method always searches for a uniform range, whereas the
comparison method searches the range written in the caption (uniform or log-uniform).

total, we measured the generalization error for the 153 models for CIFAR10 and 184 models for CIFAR100. More implementation details can be found in Appendix C.1. The experimental results for
the time-varying case are shown in Figures.1d and 1e. _[η][init]S_ [and][ η][final]S are plotted on the horizontal

_|_ _|_ _|_ _|_

axis, whereas the generalization error ϵ(S, η) is plotted on the vertical axis. We also plot the results
of projecting the generalization error onto _[η][init]S_ [and][ η][final]S [, respectively. As in the constant case, we]

_|_ _|_ _|_ _|_

can observe that, when the value of n(t) = _Sη((tt))_ [is not in the appropriate range (either quite large]

_|_ _|_
or quite small), the generalization error becomes large.


6.2 ERROR LANDSCAPE ESTIMATION

We estimate the obtained generalization error ϵ(S, η) using a parametric family, ˆϵ(S, η; φ). Here,
we treat the parameter φ as φ = _c0, c1, c2, c3, c4_ for the constant case, and then _λ1, λ2_ is added
_{_ _}_ _{_ _}_
to the time-varying case. As in Rosenfeld et al. (2021), we search for φ such that the squared error
of δ(S, η; φ) = _ϵ[ˆ](S,ηϵ;φ(S,η)−ϵ)(S,η)_ is minimized. Implementation details are provided in Appendix C.2.

Figure.1c plots our results for the constant case. The mean µ and standard deviation σ of the relative
error δ are |µ| < 2% and σ < 12%, respectively. Considering the small number of parameters used
to estimate the generalization error (|φ| = 5), the proposed functional model can accurately estimate
the complex generalization error landscape (60 and 66 models, respectively). In Figure.1f, we plot
the results for the time-varying case. We achieved |µ| < 3% and σ < 17%, respectively. Although
the relative error is larger than the constant case, we can still declare that the proposed functional
model can estimate the generalization error sufficiently, considering that the generalization error of
153 or 184 models is modeled with a small number of parameters (|φ| = 7). In Appendix C.2.3, we
provide a comparison between the proposed model, a model that analytically solves the time-varying
case of the step function, and a model that uses only the final learning rate.

In addition, we verified the stability of the proposed model with respect to the number of networks.
We randomly sampled a certain number of generalization errors obtained by training the networks,
estimated the parameter φ from them only, and computed the relative error δ. We performed this
procedure 100 times and computed µ and σ of the relative error. Details of the experimental setup
are presented in Appendix C.3.1.

In Figure.2 we show the experimental results. Figures.2a and 2c show that, for the constant case
of CIFAR10 and CIFAR100, we can accurately estimate the generalization error landscape by randomly training about 30 models. In contrast, Figures.2b and 2d show that, for the time-varying case,
we need to train about 75 models for the estimation.


-----

7 HYPERPARAMETER OPTIMIZATION

In this section, we show that the proposed model is useful for applications such as hyperparameter
optimization. In this study, we used Bayesian optimization.

Bayesian optimization can compute a function y = f (x) that can be fitted to any sequence of data
points (xi, yi) . Nogueira (2014) achieved this by using a Gaussian process with a Matern kernel,
_{_ _}_
and many Bayesian optimization methods use generic kernel functions which can be fitted to any
function. While this is practical for fitting unknown functions, ideally, the most accurate Bayesian
optimization can be achieved by employing a kernel function constructed from a feature extraction
function ψ(x) that can compute the unknown function with a linear combination of y = w[⊤]ψ(x).
Therefore, for the constant case, we propose a kernel function k(xi, xj) for the input x = _Sη_ [based]

_|_ _|_
on Eq.15 for hyperparameter optimization.

1
_k(xi, xj) = ψ(xi)[⊤]ψ(xj) =_ _xixj + xixj +_ + 1. (20)

_[√]_ _√xixj_

Details are in Appendix C.4.1. In contrast, the time-varying case cannot be represented by a linear
combination of y = w[⊤]ψ(x), because the product term of the parameters (c and λ) appears. Therefore, we employ a hyperparameter search by sequential model-based global optimization (SMBO)
(Bergstra et al., 2011) with the proposed model in Eq.19 using Eq.16. For SMBO, we use the
Levenberg–Marquardt algorithm (Moré, 1978) and Thompson Sampling (Thompson, 1933; Russo
et al., 2017). More details are provided in Appendix C.4.2.

To validate the proposed method, we explored _Sη_ [using the fitted generalization error model]

_|_ _|_
_ϵˆ(S, η; φ) as the training result of the network. The details are given in Appendix C.4.3. Figure.3_
shows the experimental results. The horizontal axis plots the number of training networks, and the
vertical axis plots the maximum test accuracy among the networks trained to date. A total of 100
experiments were conducted, and the 95% confidence interval was shaded. The methods used for
comparison were Bayesian optimization using the Matern kernel and RBF kernel (Nogueira, 2014),
Hyperopt (Bergstra et al., 2015), and Optuna (Akiba et al., 2019).

As shown in the Figure.3, in almost all cases, the proposed method achieves the highest accuracy
with the least number of models in the search method with uniform distribution. This indicates that
the proposed model represents a good feature extractor, and the proposed kernel function is useful
for hyperparameter optimization. In addition, in the constant case, the proposed method achieved
the highest accuracy with the same number of models as the comparison method, which searches
for logarithmic uniform distributions. It indicates that theoretical model-based hyperparameter optimization can be applied practically. In practice, it is generally believed that it is better to search for
hyperparameters S, η in the logarithmic uniform range (Rasmussen, 1997; Sundararajan & Keerthi,
2001). However, there is no theoretical foundation for this (this is confirmed by the fact that in Hyperopt, the results are not improved by exploring the logarithmic range). These results suggest that
the proposed model-based hyperparameter optimization is both theoretically sound and practically
useful when performing a hyperparameter search on the batch size and learning rate.

8 DISCUSSION AND CONCLUSION

In this paper, we presented a theoretical analysis and a model of the generalization error for the
batch size and learning rate in SGD. We analyzed the case in which these parameters are constant
during training and the more challenging and practical case when they vary with time. Through
experiments for both cases, we show that the proposed generalization error model is practical, and
we test the number of networks required to estimate the generalization error landscape (= stability
of the generalization error model). In addition, we demonstrate that a hyperparameter search based
on the proposed model outperform the conventional Bayesian optimization and hyperparameter optimization libraries, such as Hyperopt and Optuna. To the best of our knowledge, this is the first
study that explores the batch size and learning rate based on the generalization error model. From
the discussion, we conclude that this theoretical model is a practical and useful model for understanding the effects of the batch size and learning rate on the generalization error. We hope that
this work will provide further theoretical insights into deep learning and new research directions for
hyperparameter search based on theoretical models.


-----

REFERENCES

Hirotogu Akaike. Information theory and an extension of the maximum likelihood principle. In
_Selected papers of hirotugu akaike, pp. 199–213. Springer, 1998._

Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:
A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM
_SIGKDD international conference on knowledge discovery & data mining, pp. 2623–2631, 2019._

Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In International Conference on Machine Learning, pp. 242–252. PMLR, 2019.

Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In International Conference on Machine Learning, pp.
254–263. PMLR, 2018.

James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
_machine learning research, 13(2), 2012._

James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-parameter
optimization. Advances in neural information processing systems, 24, 2011.

James Bergstra, Brent Komer, Chris Eliasmith, Dan Yamins, and David D Cox. Hyperopt: a python
library for model selection and hyperparameter optimization. Computational Science Discovery,
[8(1):014008, 2015. URL http://stacks.iop.org/1749-4699/8/i=1/a=014008.](http://stacks.iop.org/1749-4699/8/i=1/a=014008)

Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns overparameterized networks that provably generalize on linearly separable data. In International
_[Conference on Learning Representations, 2018. URL https://openreview.net/forum?](https://openreview.net/forum?id=rJ33wwxRb)_
[id=rJ33wwxRb.](https://openreview.net/forum?id=rJ33wwxRb)

Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. Advances in Neural Information Processing Systems, 32:10836–10846,
2019.

Zhun Deng, Hangfeng He, and Weijie Su. Toward better generalization bounds with locally elastic
stability. In International Conference on Machine Learning, pp. 2590–2600. PMLR, 2021.

Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
[2019. URL https://openreview.net/forum?id=S1eK3i09YQ.](https://openreview.net/forum?id=S1eK3i09YQ)

C. W. Gardiner. Handbook of stochastic methods for physics, chemistry and the natural sciences,
volume 13 of Springer Series in Synergetics. Springer-Verlag, Berlin, third edition, 2004. ISBN
3-540-20882-8.

Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
_statistics, pp. 249–256. JMLR Workshop and Conference Proceedings, 2010._

Fengxiang He, Tongliang Liu, and Dacheng Tao. Control batch size and learning rate to generalize
well: Theoretical and empirical evidence. Advances in Neural Information Processing Systems,
32:1143–1152, 2019.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
_conference on computer vision, pp. 1026–1034, 2015._

Roxana Istrate, Florian Scheidegger, Giovanni Mariani, Dimitrios Nikolopoulos, Constantine Bekas,
and Adelmo Cristiano Innocenza Malossi. Tapas: Train-less accuracy predictor for architecture
search. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3927–
3934, 2019.


-----

Stanisław Jastrz˛ebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua
Bengio, and Amos Storkey. Three factors influencing minima in sgd. _arXiv preprint_
_arXiv:1711.04623, 2017._

Bumsoo Kim. wide-resnet.pytorch, 2018. [URL https://github.com/meliketoy/](https://github.com/meliketoy/wide-resnet.pytorch)
[wide-resnet.pytorch.](https://github.com/meliketoy/wide-resnet.pytorch)

Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast bayesian optimization of machine learning hyperparameters on large datasets. In Artificial Intelligence and
_Statistics, pp. 528–536. PMLR, 2017._

Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 and cifar-100 (canadian institute for
[advanced research). 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html.](http://www.cs.toronto.edu/~kriz/cifar.html)

Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran As[sociates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/](https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf)
[a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf.](https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf)

Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. In
_Uncertainty in artificial intelligence, pp. 367–377. PMLR, 2020._

Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and adaptive stochastic gradient algorithms. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
_Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp._
2101–2110. PMLR, 06–11 Aug 2017. [URL http://proceedings.mlr.press/v70/](http://proceedings.mlr.press/v70/li17f.html)
[li17f.html.](http://proceedings.mlr.press/v70/li17f.html)

Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
use local sgd. arXiv preprint arXiv:1808.07217, 2018.

Kangqiao Liu, Liu Ziyin, and Masahito Ueda. Noise and fluctuation of finite learning rate stochastic
gradient descent. In International Conference on Machine Learning, pp. 7045–7056. PMLR,
2021.

Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate
bayesian inference. arXiv preprint arXiv:1704.04289, 2017.

David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual confer_ence on Computational learning theory, pp. 164–170, 1999a._

David A McAllester. Some pac-bayesian theorems. Machine Learning, 37(3):355–363, 1999b.

Jorge J Moré. The levenberg-marquardt algorithm: implementation and theory. In Numerical anal_ysis, pp. 105–116. Springer, 1978._

Jeffrey Negrea, Mahdi Haghifam, Gintare Karolina Dziugaite, Ashish Khisti, and Daniel M Roy.
Information-theoretic generalization bounds for sgld via data-dependent estimates. arXiv preprint
_arXiv:1911.02151, 2019._

Fernando Nogueira. Bayesian Optimization: Open source constrained global optimization tool for
[Python, 2014. URL https://github.com/fmfn/BayesianOptimization.](https://github.com/fmfn/BayesianOptimization)

Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier Boix,
Jack Hidary, and Hrushikesh Mhaskar. Theory of deep learning iii: explaining the non-overfitting
puzzle. arXiv preprint arXiv:1801.00173, 2017.

Carl Edward Rasmussen. Evaluation of Gaussian processes and other methods for non-linear re_gression. PhD thesis, University of Toronto Toronto, Canada, 1997._

Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image
classifier architecture search. In Proceedings of the aaai conference on artificial intelligence,
volume 33, pp. 4780–4789, 2019.


-----

Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati_cal statistics, pp. 400–407, 1951._

Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction
of the generalization error across scales. In International Conference on Learning Representa_[tions, 2020. URL https://openreview.net/forum?id=ryenvpEKDr.](https://openreview.net/forum?id=ryenvpEKDr)_

Jonathan S Rosenfeld, Jonathan Frankle, Michael Carbin, and Nir Shavit. On the predictability of
pruning across scales. In International Conference on Machine Learning, pp. 9075–9083. PMLR,
2021.

Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on
thompson sampling. arXiv preprint arXiv:1707.02038, 2017.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning
_Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceed-_
_[ings, 2015. URL http://arxiv.org/abs/1409.1556.](http://arxiv.org/abs/1409.1556)_

Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don’t decay the learning rate, increase
[the batch size. In International Conference on Learning Representations, 2018. URL https:](https://openreview.net/forum?id=B1Yy1BxCZ)
[//openreview.net/forum?id=B1Yy1BxCZ.](https://openreview.net/forum?id=B1Yy1BxCZ)

Sellamanickam Sundararajan and S. Sathiya Keerthi. Predictive approaches for choosing hyperparameters in gaussian processes. Neural computation, 13(5):1103–1118, 2001.

William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285–294, 1933.

George E Uhlenbeck and Leonard S Ornstein. On the theory of the brownian motion. Physical
_review, 36(5):823, 1930._

Jia Wu, Xiu-Yun Chen, Hao Zhang, Li-Dong Xiong, Hang Lei, and Si-Hao Deng. Hyperparameter
optimization for machine learning models based on bayesian optimization. Journal of Electronic
_Science and Technology, 17(1):26–40, 2019._

Kaichao You, Mingsheng Long, Jianmin Wang, and Michael I. Jordan. How does learning rate
[decay help modern neural networks?, 2020. URL https://openreview.net/forum?](https://openreview.net/forum?id=r1eOnh4YPB)
[id=r1eOnh4YPB.](https://openreview.net/forum?id=r1eOnh4YPB)

Steven R Young, Derek C Rose, Thomas P Karnowski, Seung-Hwan Lim, and Robert M Patton.
Optimizing deep learning hyper-parameters through an evolutionary algorithm. In Proceedings of
_the workshop on machine learning in high-performance computing environments, pp. 1–5, 2015._

Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. _arXiv preprint_
_arXiv:1605.07146, 2016._


-----

A BACKGROUND

A.1 PAC-BAYESIAN FRAMEWORK

The PAC-Bayesian framework originates from McAllester (1999a;b)’s work. In the PAC-Bayesian
perspective, the hypothesis functions learned by probabilistic machine learning algorithms are drawn
at random from the hypothesis classes based on some “rules.” The generalization ability of an algorithm is negatively correlated with the distance (often measured by the Kullback–Leibler (KL) divergence) between the output hypothesis distribution and the a priori distribution (typically a Gaussian
or uniform distribution). The above results suggest that there is a tradeoff between minimizing the
empirical risk and exploring further regions of the hypothesis space from the beginning (a priori).

Let P be the initial priori distribution in the parameter space Θ, and let Q be the distribution in Θ
after learning. We can now define the expected value of the risk function under distribution Q as
follows:
(Q) = Eθ _Q[_ (θ)].
_R_ _∼_ _R_

Similarly, the empirical risk function under distribution Q can be defined as follows:

ˆ(Q) = Eθ _Q[ [ˆ](θ)]._
_R_ _∼_ _R_

Based on these, we obtain the classical result in that the expected value of the risk function R(Q) is
uniformly bounded by the expected value of the empirical risk function _R[ˆ](Q) and KL-divergence_
_D(Q∥P_ ).
**Lemma 1 (See McAllester (1999a), Theorem1). For any positive real δ ∈** (0, 1) with a probability
_of at least 1 −_ _δ over a sample of size N_ _, we have the following inequality for all distributions Q:_


_D(Q∥P_ ) + log ( [1]δ [) + log][ N][ + 2] _,_ (21)

2N − 1


_R(Q) ≤_ _R[ˆ](Q) +_


_where D(Q∥P_ ) is the KL divergence between the distributions Q and P and is defined as

(Q _P_ ) = Eθ _Q_ log _[Q][(][θ][)]_ _._
_D_ _∥_ _∼_ _P_ (θ)

 


(22)


B SUPPLEMENTAL PROOFS FOR THE PROPOSED METHOD

In this section, we describe the details of the proof, which we omitted in the proposed method.

B.1 GENERALIZATION BOUND FOR THE CONSTANT CASE

To prove Theorem 1, we use an existing proof method that approximates the SGD update of the
model to a stochastic differential equation (see, e.g., (Mandt et al., 2017)). With appropriate assumptions, the learning process of SGD can be approximated by the Ornstein–Uhlenbeck process
(Uhlenbeck & Ornstein, 1930). Because the Ornstein–Uhlenbeck process has an analytic solution,
we can use the distribution Q of the analytic solution θ to derive the KL-divergence, which is used
to derive a generalization error bound from Eq.21. Here, we modify and extend He et al. (2019)’s
Theorems 1 and 2 to derive the generalization error bound for the constant case.
**Lemma 2 (cf. Gardiner (2004), pp.109-110). Under second-order differentiable assumption (Eq.5),**
_if t is sufficiently large, the Ornstein–Uhlenbeck process (Eq.4)’s distribution_

_q(θ) = M exp_ _,_ (23)
_−_ 2[1] _[θ][⊤][Σ][−][1][θ]_
 


_has the following property:_


_AΣ + ΣA =_ _[η]_ (24)

_|S|_ _[C.]_


-----

This lemma is similar to that of Gardiner (2004).

_Proof of Lemma 2. From the analytical solution of the Ornstein–Uhlenbeck process (Uhlenbeck &_
Ornstein, 1930), the parameter θ can be expressed as follows:


_η_
_θ(t) = e[−][At]θ(0) +_

_S_

r _|_ _|_


_t_

0

Z


_e[−][A][(][t][−][t][′][)]BdW_ (t[′]), (25)


where W (t[′]) is a white noise and follows N (0, I). From Eq.23, we obtain

Σ = Eθ _Q_ _θθ[⊤][]_ _._ (26)
_∼_


Thus, we obtain the following equation:

_AΣ + ΣA = Ae[−][At]⟨θ(0), θ[⊤](0)⟩e[−][At]_ + e[−][At]⟨θ(0), θ[⊤](0)⟩e[−][At]A

_t_

+ _[η]_ _Ae[−][A][(][t][−][t][′][)]Ce[−][A][(][t][−][t][′][)]dt[′]_

_S_ 0
_|_ _|_ Z t

+ _[η]_ _e[−][A][(][t][−][t][′][)]Ce[−][A][(][t][−][t][′][)]Adt[′]_

_S_ 0
_|_ _|_ Z

= Ae[−][At]⟨θ(0), θ[⊤](0)⟩e[−][At] + e[−][At]⟨θ(0), θ[⊤](0)⟩e[−][At]A

_t_

d

+ _[η]_

_S_ 0 dt[′][ e][−][A][(][t][−][t][′][)][Ce][−][A][(][t][−][t][′][)][d][t][′]
_|_ _|_ Z

= Ae[−][At]⟨θ(0), θ[⊤](0)⟩e[−][At] + e[−][At]⟨θ(0), θ[⊤](0)⟩e[−][At]A

+ _[η]_

_|S|_ _[C][ −]_ _|S[η]_ _|_ _[e][−][At][Ce][−][At]_

(27)

_≃_ _S[η]_

_|_ _|_ _[C,]_


where A is the Hessian matrix and symmetric. The last transformation is guided by the assumption
that A is a positive definite matrix and all its eigenvalues are positive and by the approximation of
the term multiplied by e[−][At] twice with zero, because t is sufficiently large.

The proof is completed.

**Lemma 3 (extension of He et al. (2019), Appendix B.1). For any positive real δ ∈** (0, 1), with
_probability 1 −_ _δ over a training sample set of size N, we have the following inequality for the_
_distribution Q of the output hypothesis function of SGD:_


_η_ 1

_|S|_ [tr(][CA][−][1][)][ −] [2 log (det(Σ)) + 2][R][2][ −] [2][d][ + 4 log] _δ_ + 4 log N + 8 _, (28)_

8N 4

  

_−_


_R(Q) ≤_ _R[ˆ](Q)+_


_where A is the Hessian of the risk function around the local minimum, C is the covariance matrix of_
_the gradients calculated by single sample points, Σ is the covariance matrix of the distribution Q, d_
_is the dimension of the parameter θ(network size), and R is the search radius of the parameter θ._

_Proof of Lemma 3. In the PAC-Bayesian framework ((Lemma 1), it is crucial to compute the KL-_
divergence between the parameter distribution of the output hypothesis function and the prior parameter distribution in the hypothesis space. The prior distribution can be interpreted as an initial
parameter distribution, and in most cases, either a uniform distribution or a Gaussian distribution
is used. In this study, we assume that the prior distribution is Gaussian N (−θ[∗], I). θ[∗] is a parameter for which the risk function R(θ) is minimized. The mean value −θ[∗] of the prior distribution
corresponds to the shift in θ in Eq.5, which actually eliminates prior knowledge of the posterior
distribution. He et al. (2019) uses N (0, I) for the prior distribution. However, it is unnatural because the prior distribution has prior knowledge of the posterior distribution. Conversely, our setting


-----

becomes more natural. The posterior distribution Q and prior distribution P are represented by p(θ)
and q(θ) with respect to the parameter θ, given by the following equation:


1

exp (29)
(2π)[d] det(I) _−_ 2[1] [(][θ][ +][ θ][∗][)][⊤][I][(][θ][ +][ θ][∗][)]
 


_p(θ) =_


1

exp _._ (30)
(2π)[d] det(Σ) _−_ 2[1] _[θ][⊤][Σ][−][1][θ]_
 


_q(θ) =_


Here, d represents the dimension of the parameter θ, and Eq.30 computed the normalization term
_M in Eq.23. Thus, we obtain_

_q(θ)_
log

_p(θ)_

 

(2π)[d] det(I)

= log

[s] (2π)[d] det(Σ) [exp] − 2[1] [(][θ][ +][ θ][∗][)][⊤][I][(][θ][ +][ θ][∗][)][ −] [1]2 _[θ][⊤][Σ][−][1][θ][!]_


= [1]

2 [log]


+ [1]


(θ + θ[∗])[⊤]I(θ + θ[∗]) − _θ[⊤]Σ[−][1]θ_ _._ (31)



det(Σ)


Substituting Eq.31 into Eq.22, we can compute the KL divergence between Q and P as follows.
Here, we assume Θ = R[d].

_D(Q∥P_ )

_Q(θ)_

= Eθ _Q_
_∼_ _P_ (θ)

 

_q(θ)_

= log _q(θ)dθ_

_θ_ Θ _p(θ)_

Z _∈_  

1 1

= _θ_ Θ 2 [log] det(Σ) + 2[1] (θ + θ[∗])[⊤]I(θ + θ[∗]) − _θ[⊤]Σ[−][1]θ_ _q(θ)dθ_
Z _∈_   

= 2 [1] [log] det(Σ)1 + 2[1] _θ_ Θ _θ[⊤] _ _Iθq(θ)dθ + [1]2_ _θ_ Θ _θ[∗⊤]Iθ[∗]q(θ)d[]θ +_ _θ_ Θ _θ[∗⊤]Iθq(θ)dθ_

  Z _∈_ Z _∈_ Z _∈_

_θ[⊤]Σ[−][1]θq(θ)dθ_

_−_ 2[1] Zθ∈Θ

= [1] 1 + [1] _θ[⊤]Iθ_ + [1] _θ[∗⊤]Iθ[∗][]_ + Eθ (0,Σ) _θ[∗⊤]Iθ_

2 [log] det(Σ) 2 [E][θ][∼N][ (0][,][Σ)] 2 [E][θ][∼N][ (0][,][Σ)] _∼N_
 

    

_θ[⊤]Σ[−][1]θ_

_−_ 2[1] [E][θ][∼N][ (0][,][Σ)]  


= [1]

2 [log]

_≤_ 2 [1] [log]


+ [1]

2 [tr(Σ) + 1]2 _[∥][θ][∗][∥][2][ −]_ [1]2 [tr(][I][)]

+ [1] (32)

2 [tr(Σ) + 1]2 _[R][2][ −]_ [1]2 [tr(][I][)][.]


det(Σ)

1

det(Σ)


The last transformation is obtained because ∥θ[∗]∥[2] _≤_ _R[2]_ always holds when the search range of the
parameter θ is in a hypersphere of radius R. From Eq.24, we obtain

_AΣ + ΣA =_ _[η]_

_|S|_ _[C.]_


Therefore, we obtain
_AΣA[−][1]_ + Σ = _[η]_ (33)

_|S|_ _[CA][−][1][.]_

By computing the trace on both sides, the equation becomes

_η_
tr _AΣA[−][1]_ + Σ = tr _._ (34)

_S_

 _|_ _|_ _[CA][−][1]_
  


-----

The left-hand side (LHS) can be transformed as follows:

LHS = tr _AΣA[−][1]_ + Σ

= tr  AΣA[−][1][] + tr (Σ)

= tr  ΣA[−][1]A + tr (Σ)
  


= tr (Σ) + tr (Σ)
= 2tr (Σ) . (35)


Thus, we can derive the following:

_η_

tr (Σ) = [1] = [1]

2 [tr] _S_ 2
 _|_ _|_ _[CA][−][1]_


_η_

_CA[−][1][]_ _._ (36)
_|S|_ [tr]
 


We can also compute the following:
tr(I) = d, (37)

because d is the dimension of parameters θ and I ∈ R[d][×][d]. Therefore, by substituting Eq.36 and
Eq.37 into Eq.32, we obtain


(Q _P_ )
_D_ _∥_ _≤_ 4[1]


_η_

(38)

_S_ 2 [log (det(Σ)) + 1]2 _[R][2][ −]_ [1]2 _[d.]_
_|_ _|_ [tr(][CA][−][1][)][ −] [1]


Eq.38 is an upper bound on the distance between the distribution Q learned by SGD and the a
priori distribution P . Therefore, by substituting Eq.38 into Eq.21, we can obtain a PAC-Bayesian
generalization bound for SGD.

The proof is completed.

**Lemma 4 (extension of He et al. (2019), Appendix B.2). Assuming that A and Σ commute, the KL**
_divergence between the distribution Q of SGD and the prior distribution P satisfies the following_
_inequality:_


_η_ 2|S|

4 _S_ 2 _[d][ log]_ _η_
_|_ _|_ [tr(][CA][−][1][) + 1] 


_−_ 2 [1] [log]  det(CA[−][1]) + [1]2 _[R][2][ −]_ [1]2 _[d.]_ (39)


_D(Q∥P_ ) ≤


Lemma 4 provides the distance between the distribution Q obtained by SGD and the prior distribution of the parameters. The assumption that A and Σ commute implies that matrices A and Σ are
simultaneously diagonalizable, and similar assumptions have been used in the work of Jastrz˛ebski
et al. (2017) and Liu et al. (2021). Although similar results can be derived without assuming that A
and Σ commute, we used the above assumption for simplicity. Based on this assumption, we can
compute a generalization bound for a constant case.

_Proof of Lemma 4. Because A and Σ commute by assumption, the following equation can be ob-_
tained by transforming Eq.24:

_AΣ + ΣA =_ _[η]_

_|S|_ _[C]_

2ΣA = _[η]_

_|S|_ _[C]_

_η_
Σ = (40)

2|S| _[CA][−][1][.]_


Thus,


_η_ _η_
det(Σ) = det =

2 _S_ 2 _S_

 _|_ _|_ _[CA][−][1]_  _|_ _|_


_d_
det(CA[−][1]), (41)



-----

and we can compute


_d_

_η_

log (det(Σ)) = log det(CA[−][1])

2 _S_

" _|_ _|_  #

2 _S_
= _d log_ _|_ _|_ + log det(CA[−][1]) _._ (42)
_−_ _η_
 

  

Therefore, Eq.39 can be obtained by substituting Eq.42 into Eq.38.

The proof is completed.

Thus, we can easily prove Theorem 1.

_Proof of Theorem 1. It is obtained by substituting Eq.39 into Eq.21._

The proof is completed.

B.2 ADDITIONAL TWO FACTORS IN THE PROPOSED GENERALIZATION BOUND

In addition to the three factors discussed in He et al. (2019)’s Theorem 2, the local geometry around
minima, gradient fluctuations, and hyperparameters, the generalization bound derived in this study
incorporates a factor for the range of parameter search and a factor for network initialization.

**Parameter search space** The norm of θ after learning appears as the third term R[2] in Eq.32.
Hence, the parameter search range and weight decay are naturally incorporated into the generalization bound.

**Network initialization.** Moreover, this generalization bound naturally incorporates the network
initialization factor. Assuming that the prior distribution is N (−θ[∗], λI), 0 < λ, we obtain the
following:


1

exp _,_ 0 < λ. (43)
(2πλ)[d] det(I) _−_ 2[1]λ [(][θ][ +][ θ][∗][)][⊤][I][(][θ][ +][ θ][∗][)]
 


_p[′](θ) =_

Eq.32 is as follows.


_λ[d]_

_D(Q∥P_ _[′]) ≤_ [1]2 [log] det(Σ)




+ [1] (44)

2λ [tr(Σ) + 1]2λ _[R][2][ −]_ [1]2 [tr(][I][)][.]


By differentiating the right-hand side (RHS) by λ, we can observe that RHS is minimized by RHS =

_d2_ [log] tr(Σ)+Rd [2] 2 [log (det (Σ))][, when][ λ][ =][ tr(Σ)+R]d [2] . Assuming that d is sufficiently large and

_−_ [1]

tr(Σ) + R [2] _< d is valid (this property is also called overparameterization (Allen-Zhu et al., 2019;_
Brutzkus et al., 2018; Du et al., 2019)), we can theoretically guarantee that the generalization bound
will be tighter when we change the covariance matrix I of the prior distribution to a diagonal matrix
with even smaller eigenvalues, such as He’s initialization (He et al., 2015) or Xavier’s initialization
(Glorot & Bengio, 2010). Therefore, the generalization bound proposed in this study, which is an
extended version of He et al. (2019), also naturally incorporates factors related to the initialization
of the network.

B.3 GENERALIZATION BOUND FOR THE TIME-VARYING CASE


To prove Theorem 2, we only need to describe how the time-varying batch size and learning rate
change Eq.24 in Lemma 2. To do so, we solve following Lemma 5:
**Lemma 5 (extension of Gardiner (2004), pp.115-116). Under second-order differentiable assump-**
_tion (Eq.5), if t is sufficiently large and hyperparameter_ _Sη_ _[is time-varying, the Ornstein–Uhlenbeck]_

_|_ _|_
_process (Eq.4)’s distribution_

_q(θ) = M exp_ (45)
_−_ [1]2 _[θ][⊤][Σ][−][1][θ]_
 


-----

_has the following property:_
_AΣ + ΣA = n(ξ)C,_ _ξ ∈_ [0, t]. (46)


_Here,_


_n(t) =_ _[η][(][t][)]_ (47)

_|S(t)|_ _[.]_


_Proof of Lemma 5. From the analytical solution of the Ornstein–Uhlenbeck process (Uhlenbeck &_
Ornstein, 1930) in the time-varying case (Gardiner, 2004), the parameter θ can be expressed as
follows:

_t_
_θ(t) = e[−][At]θ(0) +_ _n(t[′])e[−][A][(][t][−][t][′][)]BdW_ (t[′]), (48)

0

Z

p

where W (t[′]) is a white noise and follows N (0, I). From Eq.45, we obtain

Σ = Eθ _Q_ _θθ[⊤][]_ _._ (49)
_∼_


Thus, we obtain the following equation:

_AΣ + ΣA = Ae[−][At]⟨θ(0), θ[⊤](0)⟩e[−][At]_ + e[−][At]⟨θ(0), θ[⊤](0)⟩e[−][At]A

_t_
+ _n(t[′])Ae[−][A][(][t][−][t][′][)]Ce[−][A][(][t][−][t][′][)]dt[′]_

0

Z

_t_
+ _n(t[′])e[−][A][(][t][−][t][′][)]Ce[−][A][(][t][−][t][′][)]Adt[′]_ (50)

0

Z


where A is the Hessian matrix and symmetric. We use the mean-value theorem of integrals to obtain
the following equation: For some ξ1, ξ2 [0, t],
_∈_
_t_ _t_

_n(t[′])Ae[−][A][(][t][−][t][′][)]Ce[−][A][(][t][−][t][′][)]dt[′]_ = n(ξ1) _Ae[−][A][(][t][−][t][′][)]Ce[−][A][(][t][−][t][′][)]dt[′]_ (51)
0 0

Z Z

_t_ _t_

_n(t[′])e[−][A][(][t][−][t][′][)]Ce[−][A][(][t][−][t][′][)]Adt[′]_ = n(ξ2) _e[−][A][(][t][−][t][′][)]Ce[−][A][(][t][−][t][′][)]Adt[′]_ (52)
0 0

Z Z


Here, we assume that the matrix D is

_t_
_D =_

0

Z


_e[−][A][(][t][−][t][′][)]Ce[−][A][(][t][−][t][′][)]dt[′]_ (53)


and that A and D commute. Then, because AD = DA,

_ξ = ξ1 = ξ2_ (54)

holds for Eq.51 and Eq.52. Thus, by substituting Eq.54 into Eqs.51 and 52, and then substituting
the result into Eq.50, we derive

_AΣ + ΣA = Ae[−][At]⟨θ(0), θ[⊤](0)⟩e[−][At]_ + e[−][At]⟨θ(0), θ[⊤](0)⟩e[−][At]A

_t_
+ n(ξ) _Ae[−][A][(][t][−][t][′][)]Ce[−][A][(][t][−][t][′][)]dt[′]_

0

Z

_t_
+ n(ξ) _e[−][A][(][t][−][t][′][)]Ce[−][A][(][t][−][t][′][)]Adt[′]_

0

Z

= Ae[−][At]⟨θ(0), θ[⊤](0)⟩e[−][At] + e[−][At]⟨θ(0), θ[⊤](0)⟩e[−][At]A

_t_

d

+ n(ξ)

0 dt[′][ e][−][A][(][t][−][t][′][)][Ce][−][A][(][t][−][t][′][)][d][t][′]

Z

= Ae[−][At]⟨θ(0), θ[⊤](0)⟩e[−][At] + e[−][At]⟨θ(0), θ[⊤](0)⟩e[−][At]A

+ n(ξ)C − _n(ξ)e[−][At]Ce[−][At]_

_≃_ _n(ξ)C._ (55)


-----

The last transformation is guided by the assumption that A is a positive definite matrix and all its
eigenvalues are positive and by the approximation of the term multiplied by e[−][At] twice with zero,
because t is sufficiently large.

The proof is completed.

_Proof of Theorem 2. Using Eq.55, rather than Eq.27 in the proof of Theorem 1, leads to Eq.10._
Furthermore, as exactly
min(n(t)) ≤ _n(ξ) ≤_ max(n(t)), (56)

by applying a certain λ ∈ [0, 1], we can express

_n(ξ) = (1 −_ _λ) min(n(t)) + λ max(n(t))._

Therefore, Eq.11 was derived simultaneously.

The proof is completed.

B.4 FUNCTIONAL MODEL OF THE TRAINING ERROR

B.4.1 CONSTANT CASE

We derive a training error model for the constant case. We approximate the training error _R[ˆ](θ)_
by substituting Eq.7 into Eq.6 and then approximate _R[ˆ](Q) by computing the expected value in the_
parameter distribution Q. In the constant case, we can compute


_η_ _t_ _⊤_

ˆ(θ) _A(θ_ _θb) = [1]_ _e[−][At]θ(0)_ _θb +_ _e[−][A][(][t][−][t][′][)]BdW_ (t[′])
_R_ _≃_ [1]2 [(][θ][ −] _[θ][b][)][⊤]_ [ˆ] _−_ 2 _−_ _S_ 0

 r _|_ _|_ Z 

_η_ _t_
_Aˆ_ _e[−][At]θ(0)_ _θb +_ _e[−][A][(][t][−][t][′][)]BdW_ (t[′])
_−_ _S_ 0
 r _|_ _|_ Z 

= [1] _b_ [) ˆ]A(e[−][At]θ(0) _θb)_

2 [(][θ][(0)][⊤][e][−][At][ −] _[θ][⊤]_ _−_

_t_ _⊤_ _t_ _η_

+ [1] _e[−][A][(][t][−][t][′][)]BdW_ (t[′]) _Aˆ_ _e[−][A][(][t][−][t][′][)]BdW_ (t[′])

2 0 0 _S_

Z  Z  _|_ _|_

_t_ _⊤_ _η_

+ _e[−][A][(][t][−][t][′][)]BdW_ (t[′]) _Aˆ(e[−][At]θ(0)_ _θb)_

0 _−_ _S_

Z  ! r _|_ _|_ _[.][ (57)]_


Therefore, by taking the expected value with the distribution Q, we obtain

ˆ(Q) = Eθ _Q_ ˆ(θ)
_R_ _∼_ _R_
h 1 i

Eθ _Q_ _A(θ_ _θb)_
_≃_ _∼_ 2 [(][θ][ −] _[θ][b][)][⊤]_ [ˆ] _−_

 

1
= E _b_ [) ˆ]A(e[−][At]θ(0) _θb)_

2 [(][θ][(0)][⊤][e][−][At][ −] _[θ][⊤]_ _−_

 

1 _t_ _⊤_ _t_ _η_

+ E _e[−][A][(][t][−][t][′][)]BdW_ (t[′]) _Aˆ_ _e[−][A][(][t][−][t][′][)]BdW_ (t[′])

" 2 Z0  Z0 [#] _|S|_

_t_ _⊤_ _η_

+ E _e[−][A][(][t][−][t][′][)]BdW_ (t[′]) _Aˆ(e[−][At]θ(0)_ _θb)_

0 _−_ _S_

" Z  !# r _|_ _|_

_η_ _η_
= a0 + a1 (58)

_S_ [+][ a][2] _S_
_|_ _|_ r _|_ _|_ _[.]_


This allows us to model _R[ˆ](Q), as shown in Eq.13._


-----

B.4.2 TIME-VARYING CASE

To compute the training error model for the time-varying case, we only need to consider how the
time-varying batch size and learning rate change Eq.6. Thus, we use the mean value theorem of
integrals in Eq.9 to obtain the following equation:

_t_
_θ(t) = θ(0)e[−][At]_ + _n(t[′])e[−][A][(][t][−][t][′][)]BdW_ (t[′])

0

Z

p _t_

= θ(0)e[−][At] + _n(ξ)_ _e[−][A][(][t][−][t][′][)]BdW_ (t[′]). (59)

0

Z

p

Here, ξ ∈ [0, t] and as exactly


_n(t)),_ (60)


min(

for some λ ∈ [0, 1], we obtain


_n(t)) ≤_


_n(t) ≤_ max(


_n(t))._ (61)


_n(ξ) = (1 −_ _λ) min(_


_n(t)) + λ max(_


Thus, we can compute the first term in Eq.16, which is the training error model for the time-varying
case, by substituting Eq.61 into Eq.59 and then performing the same transformation as in Appendix
B.4.1.

B.5 FUNCTIONAL MODEL OF THE GENERALIZATION BOUND

B.5.1 CONSTANT CASE

In this section, under appropriate assumptions, we show that Eq.8 monotonically decreases with
respect to _Sη_ [as a basis for deriving Eq.14 based on Eq.8. We recall He et al. (2019)’s proof to make]

_|_ _|_
our paper complete. We first make the following assumptions about dimension d of the parameter
space Θ.

**Assumption 1 (See He et al. (2019), Assumption 2). The network size is large enough:**

_d >_ [tr(][CA][−][1][)][η] _,_ (62)

2|S|

_where d is the number of parameters, C is the magnitude of the individual gradient noise, A is the_
_Hessian matrix around the global minima, η is the learning rate, and |S| is the batch size._

This assumption can be justified by the fact that network sizes of neural networks are often extremely
large. This property is also called overparameterization (Allen-Zhu et al., 2019; Brutzkus et al.,
2018; Du et al., 2019). We can obtain the following corollary by combining Eq.8 and Assumption
1.

**Corollary 1 (See He et al. (2019), Appendix B.3). When all conditions of Theorem 1 and Assump-**
_tion 1 hold, the generalization bound of the network is negatively correlated with the ratio of the_
_learning rate to the batch size._

_Proof of Corollary 1. We first define_


_η_ 2|S|

2 _S_ _η_
_|_ _|_ [tr(][CA][−][1][)+] _[d][ log]_ 


log (det (CA[−][1]))+ _R[2]_ _d_ +2 log ( [1]
_−_ _−_ _δ_ [)+2 log][ N][ +4][.][ (63)]


_I =_


Then, Eq.8 becomes


_R(Q) ≤_ _R[ˆ](Q) +_


_I_

(64)
4N 2 _[.]_
_−_


-----

We calculate the derivative of I with respect to the ratio _Sη_ [to check whether the generalization]

bound has a negative correlation with the ratio. For brevity, we set| _|_ _k =_ _|Sη_ _|_ [:]

_∂I_ _k_
_∂k_ [=][ ∂]∂k 2 [tr(][CA][−][1][)][ −] _[d][ log(2][k][)][ −]_ [log (det (][CA][−][1][)) +][ R][2][ −] _[d][ + 2 log (1]δ_ [) + 2 log][ N][ + 4]

 

= [tr(][CA][−][1][)] (65)

2 _−_ _k [d]_ _[.]_

Therefore, when Assumption 1 holds, we have


_d >_ [tr(][CA][−][1][)][η] = _[k]_ (66)

2 _S_ 2 [tr(][CA][−][1][)][.]
_|_ _|_

Thus,
_∂I_
_∂k [<][ 0][.]_ (67)

Then, I and the generalization bound have a negative correlation with the ratio of the learning rate
to the batch size.

The proof is competed.

B.5.2 TIME-VARYING CASE

In this section, the basis for deriving the second term in Eq.16 based on Eq.10, we will show that
Eq.10 monotonically decreases with respect to n2 in Eq.18 under appropriate assumptions. First,
we make the following assumptions about the dimension d of the parameter space Θ.

**Assumption 2 (cf. He et al. (2019), Assumption 2). The network size is large enough:**


_d >_ [tr(][CA][−][1][)]


_n2,_ (68)


_where d is the number of parameters, C is the magnitude of the individual gradient noise, A is the_
_Hessian matrix around the global minima, and n2 is the intermediate value of_ _Sη((tt))_ _[in Eq.18,][ η][(][t][)]_

_|_ _|_
_is the learning rate, and |S(t)| is the batch size._

This assumption can also be justified as Assumption 1 when the network sizes of neural networks are
often extremely large. We can obtain the following corollary by combining Eq.10 and Assumption
2.

**Corollary 2 (extension of He et al. (2019), Appendix B.3). When all conditions of Theorem 2 and**
_Assumption 2 hold, the generalization bound of the network is negatively correlated with the ratio_
_of the learning rate to the batch size._

_Proof of Corollary 2. Along the proof of Corollary 1, we use Eq.10, k = n2 and Assumption 2,_
rather than Eq.8, k = _Sη_ [and Assumption 1. Then, we can obtain]

_|_ _|_

_∂I_
_∂k [<][ 0][.]_ (69)

The proof is competed.


-----

C IMPLEMENTATION DETAIL

C.1 DATASETS AND MODELS

C.1.1 DATASETS

We tested the proposed model on two popular image datasets. CIFAR10 and CIFAR100 (Krizhevsky
et al., 2009): 60 K natural RGB images of 10 classes for CIFAR10 and 100 classes for CIFAR100
with a train/test split of 50K/10 K. For both datasets, we use PyTorch version[1].

C.1.2 MODELS

We tested the proposed model using two popular model networks. For CIFAR10, we used VGG16
(Simonyan & Zisserman, 2015), and for CIFAR100, we use WRN28-10 (Zagoruyko & Komodakis,
2016). For both model networks, we build on the code from the implementation of Kim (2018).

C.1.3 TRAINING

We modified the implementation of Kim (2018). Both datasets are normalized by mean and standard
deviation. In the main experiments, training was performed via SGD with a momentum of 0.9, and
weight decay of 5e-4 for 200 epochs. We began training with a learning rate of ηinit, run for
200 epochs, and reduced by a multiplicative factor of γ after 60, 120, and 160 epochs to make
_ηfinal = γ[3]ηinit._

C.2 ERROR ESTIMATION EXPERIMENT

C.2.1 EXPERIMENTAL DETAILS

As described in Section 6.2, we fit the parameters of φ to minimize δ(S, η; φ), using least squares
regression. In doing so, we used scipy.optimize.curve_fit[2]. The optimal parameter φ[∗]
is given by


**_φ[∗]_** = arg minφ _|δ(S, η; φ)|[2]_ _._ (70)

_S,η_

X


To measure the performance of the proposed model, the mean µ and standard deviation σ of the
relative error δ(S, η; φ[∗]) are computed based on the fitted parameter φ[∗]. To evaluate the stability of
the model, we fit a parameter φ based on a randomly sampled generalization error ϵ and compute
_µ and σ from the relative error δ(S, η; φ). The values of µ and σ obtained after 100 repetitions are_
shaded within one standard deviation, as shown in Figure.2.

C.2.2 FOUND PHI VALUES

**Table 1: The optimal value φ[∗]** is determined by the least squares regression of relative error.

**(a) Constant case**

_c0_ _c1_ _c2_ _c3_ _c4_

CIFAR10 81.92 _−7.87 · 10[−][1]_ 1.00 · 10[−][4] 8.20 · 10[−][2] 301
CIFAR100 8.97 6.45 7.66 · 10[−][4] 1.50 · 10[−][1] 7.69

**(b) Time-varying case**


_c0_ _c1_ _c2_ _c3_ _c4_ _λ1_ _λ2_

CIFAR10 57.68 1.17 1.56 · 10[−][4] 5.23 · 10[−][2] 709 8.35 · 10[−][1] 8.89 · 10[−][2]

CIFAR100 34.77 6.91 9.52 · 10[−][4] 1.21 · 10[−][1] 4.03 6.00 · 10[−][1] 1.63 · 10[−][1]

[1https://github.com/pytorch/vision](https://github.com/pytorch/vision)
[2https://github.com/scipy/scipy](https://github.com/scipy/scipy)


-----

C.2.3 COMPARISON MODELS FOR THE TIME-VARYING CASE

For the time-varying case in this study, we used a step decay for the learning rate. Thus, rather
than using the mean value theorem of integrals to make approximations, as in Eqs.55 and59, we can
obtain a more accurate model by treating the integral value as a constant for each interval as follows:
_t_ _t0_ _t_

_n(t[′])F_ (t[′])dt[′] = n(0) _F_ (t[′])dt[′] + n(t0) _F_ (t[′])dt[′] = n(0)I0 + n(t0)I1. (71)
0 0 _t0_

Z Z Z

We call this the analytical solution model. We call the model using only the final learning rate as the
stationary model and fit these two models to the generalization error. The comparison results can be
found in Table.2.

**Table 2: Fitting results of the proposed model and the comparison models for the time-varying case.**

**(a) CIFAR10**


_|φ|_ _µ (% ↓)_ _σ (% ↓)_ AIC (↓)

stationary model 5 -6.03 23.89 5.08
analytical solution model 13 **-2.66** **16.15** -98.79

proposed model 7 -2.71 16.28 **-108.21**

**(b) CIFAR100**

_|φ|_ _µ (% ↓)_ _σ (% ↓)_ AIC (↓)

stationary model 5 -4.88 21.88 -27.97
analytical solution model 13 **-0.86** **9.57** -316.23

proposed model 7 -0.88 9.69 **-323.80**

Here, the analytical solution model uses step decay and can divide the integration interval into four,
and then |φ| = 13. Meanwhile, the stationary model has the same number of parameters as the
constant case, |φ| = 5. As shown in these results, the analytical solution model obtains the smallest
relative error, and the proposed model has almost the same performance with a smaller number of
parameters. Assuming that the relative error δ follows a normal distribution N (µ, σ) for the mean
_µ and standard deviation σ of the relative error δ in the fitted model, we can observe that Akaike’s_
information criterion (AIC) (Akaike, 1998) is minimized by the proposed model. Thus, the integral
in the proposed model can be approximated sufficiently using the mean value theorem.

C.3 STABILITY EXPERIMENT

C.3.1 EXPERIMENTAL DETAILS

We use the same experimental setup as in Appendix C.2.1. However, there is instability in
scipy.optimize.curve_fit with respect to the initial value of the search parameter φ. Thus,
we sampled an initial value of φ from a uniform distribution [0, 1] and fitted it several times (10 and
5 times for the constant and time-varying cases, respectively). Then, we used the parameter that best
fit the sampled data for the stability experiment.


-----

C.4 HYPERPARAMETER OPTIMIZATION EXPERIMENT

C.4.1 BAYESIAN OPTIMIZATION MODEL FOR THE CONSTANT CASE

In this section, we describe the implementation details of the original kernel function proposed in
Eq.20. To construct the proposed kernel function, we implemented the following kernel function
using sklearn.gaussian_process.kernels[3]:

**from sklearn.gaussian_process.kernels import DotProduct, ConstantKernel**

sigma_0_bounds = (1e-10, 1e10)
equation:proposed_kernel_constant_case = (DotProduct(sigma_0_bounds=sigma_0_bounds) ** 0.5

+ DotProduct(sigma_0_bounds=sigma_0_bounds)
+ DotProduct(sigma_0_bounds=sigma_0_bounds) ** -0.5
+ ConstantKernel())


sigma_0_bounds decides the range of the σ, which controls the inhomogenity of the kernel.

C.4.2 ALGORITHM FOR THE TIME-VARYING CASE

The proposed model Eq.16 for the time-varying case cannot be represented by a linear combination
of y = w[⊤]ψ(x), as the product term of the parameters of c, λ appears. Thus, we employed a hyperparameter search by sequential model-based global optimization (Bergstra et al., 2011) combining the Levenberg–Marquardt algorithm (LM) (Moré, 1978) and Thompson Sampling (Thompson,
1933; Russo et al., 2017), directly using the proposed model. The LM is used to solve non-linear
least squares problems. It interpolates between the Gauss—Newton algorithm (GNA) and the GD
method. It is more robust than the GNA, which implies that, in many cases, it finds a solution even
if it starts very far from the final minimum. It can also be regarded as the GNA using a trust region approach. The distribution of the parameter φ is obtained by minimizing the relative error of
_δ(S, η; φ) =_ _ϵ[ˆ](S,ηϵ;φ(S,η)−ϵ)(S,η)_ with respect to the parameter φ by the LM. The algorithm is as follows.

**Algorithm 1 Hyperparameter search algorithm for the time-varying case**

_H ←_ ∅
sample φ0 from U (0, 1)
**for t** 1 to T do
_←η_ _η_

_∗_ _∗_
_|S|_ _max[,]_ _|S|_ _min_ _|S| max[,][ η]|S| min_ _[δ][(][S, η][;][ φ][t][−][1][)]_

EvaluateH ←H ∪ f (((|S[η]|S[η]| _∗max|_ _[←]∗max[,][arg min][,]|Sη|Sη|_ _∗min|_ _∗min[)][ η][)][, f]_ [(][ η]|S| _∗max[,]_ _|Sη_ _|_ _∗min[))]_ _▷_ Expensive step

**_µ, Σ ←_** LM(δ, H)
sample φt from trunc-norm(µ, diag(Σ), blower, bupper)

**end for**

p

**return H**


Here,the network for a hyperparameter H represents the history of observations, and ( _|S[η]_ _|_ _∗max[,]_ _|Sη_ _|_ _∗min[)][. To keep the parameter] f_ ( _|S[η]_ _|_ _∗max[,]_ _|Sη_ _|_ _∗min[)][ represents the evaluation of][ φ][ in this range, Thomp-]_

son sampling is performed for each parameter from the following truncated normal distribution using
the lower bound blower and upper bound bupper.


_ϕ(_ _[x][−]σ_ _[µ]_ [)]

Φ( _[b][upper]σ_ _[−][µ]_ ) Φ( _[b][lower]σ_ _[−][µ]_

_−_


_f_ (x; µ, σ, blower, bupper) = [1]


(72)


where ϕ(x) is the probability density function of the standard normal distribution as follows:


1

_,_ (73)

2π [exp] _−_ 2[1] _[x][2]_
 


_ϕ(x) =_

3https://github.com/scikit-learn/scikit-learn


-----

and Φ(x) is its cumulative distribution function:

_x_

Φ(x) = 2[1] 1 + erf _√_

 

C.4.3 EXPERIMENTAL DETAILS


_._ (74)



In this section, we present the implementation details of the hyperparameter optimization experiments for comparison. In both constant and time-varying cases, _Sη_ [was allowed to explore in the]

_|_ _|_
range of [10[−][7], 10[−][2]] for CIFAR10 and in the range of [10[−][6], 10[−][2]] for CIFAR100. These were
determined based on the experimental results for each dataset (Figures.1a and 1b). We computed
the maximum test accuracy obtained over 30 rounds of hyperparameter search. By performing this
for a total of 100 times, we computed a 95% confidence interval of the maximum accuracy. The
proposed method uses a uniform distribution search, whereas the comparison method uses a uniform distribution search and log-uniform search. For comparison, we used Bayesian optimization[4]
(Nogueira, 2014) using the Matern kernel and RBF kernel, Hyperopt[5] (Bergstra et al., 2015), and
Optuna[6] (Akiba et al., 2019).

[4https://github.com/fmfn/BayesianOptimization](https://github.com/fmfn/BayesianOptimization)
[5https://github.com/hyperopt/hyperopt](https://github.com/hyperopt/hyperopt)
[6https://github.com/optuna/optuna](https://github.com/optuna/optuna)


-----

