# CREATING TRAINING SETS VIA WEAK INDIRECT SUPERVISION

**Jieyu Zhang[1][,][2], Bohan Wang[1][,][3], Xiangchen Song[4], Yujing Wang[1], Yaming Yang[1], Jing Bai[1],**
**Alexander Ratner[2][,][5]**

1Microsoft Research Asia 2University of Washington
3University of Science and Technology of China 4Carnegie Mellon University 5Snorkel AI, Inc.
{jieyuz2, ajratner}@cs.washington.edu
{yujwang, yayaming, jbai}@microsoft.com
wbhfy@mail.ustc.edu.cn
xiangchensong@cmu.edu

ABSTRACT

Creating labeled training sets has become one of the major roadblocks in machine
learning. To address this, recent Weak Supervision (WS) frameworks synthesize
training labels from multiple potentially noisy supervision sources. However,
existing frameworks are restricted to supervision sources that share the same output
space as the target task. To extend the scope of usable sources, we formulate Weak
_Indirect Supervision (WIS), a new research problem for automatically synthesizing_
training labels based on indirect supervision sources that have different output label
spaces. To overcome the challenge of mismatched output spaces, we develop a
probabilistic modeling approach, PLRM, which uses user-provided label relations
to model and leverage indirect supervision sources. Moreover, we provide a
theoretically-principled test of the distinguishability of PLRM for unseen labels,
along with a generalization bound. On both image and text classification tasks as
well as an industrial advertising application, we demonstrate the advantages of
PLRM by outperforming baselines by a margin of 2%-9%.

1 INTRODUCTION

One of the greatest bottlenecks of using modern machine learning models is the need for substantial
amounts of manually-labeled training data. In real-world applications, such manual annotations are
typically time-consuming, labor-intensive and static. To reduce the efforts of annotation, researchers
have proposed Weak Supervision (WS) frameworks (Ratner et al., 2016; 2018; 2019; Fu et al., 2020)
for synthesizing labels from multiple weak supervision sources, e.g., heuristics, knowledge bases,
or pre-trained classifiers. These frameworks have been widely applied on various machine learning
tasks (Dunnmon et al., 2020; Fries et al., 2021; Safranchik et al., 2020; Lison et al., 2020; Zhou
et al., 2020; Hooper et al., 2021; Zhan et al., 2019; Varma et al., 2019) and industrial data (Bach
et al., 2019). Among them, data programming (Ratner et al., 2016), one representative example
that generalizes many approaches in the literature, represents weak supervision sources as labeling
_functions (LFs) and synthesizes training labels using Probabilistic Graphical Model (PGM)._

Given both the increasing popularity of WS and the general increase in open-source availability of
machine learning models and tools, there is a rising tide of available supervision sources that WS
frameworks and practitioners could potentially leverage, including pre-trained machine learning
models or prediction APIs (Chen et al., 2020; d’Andrea & Mintz, 2019; Yao et al., 2017). However,
existing WS frameworks only utilize weak supervision sources with the same label space as the
target task. This incompatibility largely limits the scope of usable sources, necessitating manual
effort from domain experts to provide supervision for unseen labels. For example, consider target
task of classifying {“dog”, “wolf ”, “cat”, “lion”} and a set of three weak supervision sources
(e.g. trained classifiers or expert heuristics) with disjoint output spaces {“caninae”, “felidae”},
{“domestic animals”, “wild animals”} and {“husky”, “bengal cat”} respectively. We call these types
of sources indirect supervision sources. For concreteness, we follow the general convention of data
_programming (Ratner et al., 2016) and refer to these sources as indirect labeling functions (ILFs)._


-----

Despite their apparent utility, existing weak supervision methods could not directly leverage such
ILFs, as their output spaces have no overlap with the target one.

In this paper, we formulate a novel research problem that aims to leverage such ILFs automatically,
minimizing the manual efforts to develop and deploy new models. We refer to this as the Weak
_Indirect Supervision (WIS) setting, a new Weak Supervision paradigm which leverages ILFs, along_
with the relational structures between individual labels, to automatically create training labels.

The key difficulty of leveraging ILFs is due to the mismatched label spaces. To overcome this, we
introduce pairwise relations between individual labels to the WIS setup, which are often available
in structured sources (e.g. off-the-shelf Knowledge Bases (Miller, 1995; Sinha et al., 2015; Dong
et al., 2020) or large scale label hierarchies (Murty et al., 2017; The Gene Ontology Consortium,
2018; Partalas et al., 2015) for various domains), or can be provided by subject matter experts in
far less time than generating entirely new sets of weak supervision sources. For example, in the
aforementioned example, we could rely on a biological species ontology to see that the unseen labels
“dog” and “cat” are both subsumed by the seen label “domestic animals”. Based on the label relations,
we can automatically leverage the supervision sources as ILFs. Notably, previous work (Qu et al.,
2020) also leveraged a label relation graph but was focused on relation extraction task in a few-shot
learning setting, while You et al. (2020) proposed to learn label relations given data for each label in
a transfer learning scenario. In contrast, we aim to solve the target task directly and without clean
labeled data.

The remaining questions are (1) how to synthesize labels based on pair-wise label relations and ILFs?
and (2) How can we know whether, given a set of ILFs and label relations, the unseen labels are
_distinguishable or not? To address the first question, we develop a probabilistic label relation model_
_(PLRM), the first PGM for WIS which aggregates the output of ILFs and models the label relations as_
dependencies between random variables. In turn, we use the learned PLRM to produce labels for
training an end model. Furthermore, we derive the generalization error bound of PLRM based on
assumptions similar to previous work (Ratner et al., 2016).

The second question presents an important stumbling block when dealing with unseen labels, as we
may not be able to distinguish the unseen labels given existing label relations and ILFs, resulting in
an unsatisfactory synthesized training set. To address this issue, we formally introduce the notion
of distinguishability in WIS setting and theoretically establish an equivalence between: (1) the
distinguishability of the label relation structure as well as the ILFs, and (2) the capability of PLRM
to distinguish unseen labels. This result then leads to a simple sanity test for preventing the model
from failing to distinguish unseen labels. In preliminary experiments, we observe a significant drop
in model performance when the condition is violated.

In experiments, we make non-trivial adaptations for baselines from related settings to the new WIS
problem. On both text and image classification tasks, we demonstrate the advantages of PLRM over
adapted baselines. Finally, in a commercial advertising system where developers need to collect
annotations for new ads tags, we illustrate how to formulate the training label collection as a WIS
problem and apply PLRM to achieve an effective performance.

**Summary of Contributions. Our contributions are summarized as follows:**

-  We formulate Weak Indirect Supervision (WIS), a new research problem which synthesizes
training labels based on indirect supervision sources and label relations, minimizing human efforts
of both data annotation and weak supervision sources construction;

-  We develop the first model for WIS, the Probabilistic Label Relation Model (PLRM) with
comparable statistical efficiency to previous WS frameworks and standard supervised learning;

-  We introduce a new notion of distinguishability in WIS setting, and provide a simple test of the
distinguishability of PLRM for unseen labels by theoretically establishing the connection between
the label relation structures and distinguishability;

-  We showcase the potential of the WIS formulation and the effectiveness of PLRM in a commercial
advertising system for synthesizing training labels of new ads tags. On academic image and
text classification tasks, we demonstrate the advantages of PLRM over baselines by quantitative
experiments. Overall, PLRM outperforms baselines by a margin of 2%-9%.


-----

2 RELATED WORK

Table 1: Comparisons between the proposed weak indirect supervision (WIS) and related machine
learning tasks. Compared to normal and weakly supervised learning, WIS handles mismatched train
and test label spaces. WIS is similar in spirit to indirect supervision (IS) and zero-shot learning
(ZSL), but distinct in that WIS only takes as input weak or noisy labels and a simple set of logical
label relations, and aims to output a training data set rather than a trained model, affording complete
modularity in which final model class is used.

**Task** **Label Type** _Ytrain = Ytest_ **Label Information** **When Label Info. is Required**

**Supervised Learning (SL)** Clean Labels ✓ – –
**Weak Supervision (WS)** Noisy Sources ✓ – –
**Indirect Supervision (IS)** Clean Labels Label Trans. Matrix Training
**Zero-Shot Learning (ZSL)** Clean Labels Label Embed. / Attribute Training & Test

**Weak Indirect Supervision (WIS)** Noisy Sources Label Relation Training

We briefly review related settings. The comparison between WIS and related tasks is in Table 1.

**Weak Supervision: We draw motivation from recent work which model and integrate weak supervi-**
sion sources using PGMs (Ratner et al., 2016; 2018; 2019; Fu et al., 2020) and other methods (Guan
et al., 2018; Khetan et al., 2018) to create training sets. While they assume supervision sources
share the same label space as the new tasks, we aim to leverage indirect supervision sources with
mismatched label spaces in a labor-free way.

**Indirect Supervision: Indirect supervision arises more generally in latent-variable models for various**
domains (Brown et al., 1993; Liang et al., 2013; Quattoni et al., 2004; Chang et al., 2010; Zhang
et al., 2019). Very recently, Raghunathan et al. (2016) proposed to use the linear moment method for
indirect supervision, wherein the transition between desired label space Y and indirect supervision
space O is known, as well as the ground truth of indirect supervisions for training. In contrast,
both are unavailable in WIS. Theoretically, Wang et al. (2020) developed a unified framework for
analyzing the learnability of indirect supervision with shared or superset label spaces, while we focus
on disjoint label spaces and the consequent unique challenge of distinguishability of unseen classes.

**Zero-Shot Learning: Zero-Shot Learning (ZSL) (Lampert et al., 2009; Wang et al., 2019) aims to**
learn a classifier that is able to generalize to unseen classes. The WIS problem differentiates from
ZSL by (1) in ZSL setting, the training and test data belong to seen and unseen classes, respectively,
and training data is labeled, while for WIS, both training and test data belong to unseen classes
and unlabeled; (2) ZSL tends to render a classifier that could predict unseen classes given certain
label information, e.g., label attributes (Romera-Paredes & Torr, 2015), label descriptions (Srivastava
et al., 2018) or label similarities (Frome et al., 2013), while WIS aims to provide training labels for
unlabeled training data, allowing users to train any machine learning models, and the label relations
are used only in synthesizing training labels.

3 PRELIMINARY: WEAK SUPERVISION

We first describe the Weak Supervision (WS) setting. A glossary of notations used is in App. A.

**Definitions and notations. We assume a k-way classification task, and have an unlabeled dataset D**
consisting of m data points. Denote by Xi the individual data point and Yi = _y1, . . ., yk_
the unobserved interested label of Xi. We also have ∈X _n sources, each represented by a labeling ∈Y_ _{_ _}_
function (LF) and denoted by λj. Each λj outputs a label _Y[ˆ]i[j]_ _j_ [=][ {]y[ˆ]1[j][, . . .,][ ˆ]yk[j] _λj_

_Yλj is the label space associated with λj and |Yλj_ _| = kλj_ . We denote the concatenation of LFs’[∈Y][λ] _[}][ on][ X][i][, where]_
output as _Y[ˆ]i = [ Y[ˆ]i[1][,][ ˆ]Yi[2][, . . .,][ ˆ]Yi[n][]][, and the union set of LFs’ label spaces as][ ˆ]Y with |Y|[ˆ]_ = k[ˆ]. Note
that _k[ˆ] is not necessarily equal to the sum over kλj_, since LFs may have overlapping label spaces. We
call ˆy ∈ _Y[ˆ] seen label and y ∈Y desired labels. In WS settings, we have Y ⊂_ _Y[ˆ]. Notably, we assume_
all the involved labels come from the same semantic space.

**The goal of WS. The goal is to infer the training labels for the dataset D based on LFs, and to use**
them to train an end discriminative classifier fW : X →Y, all without ground truth training labels.


-----

|nine Felidae|Domestic Animals|Wild Animals|Hus|ky Bengal Cat|
|---|---|---|---|---|

|Col1|def labeling rule: return label|
|---|---|


Bengal Cat Cat Canine Felidae Domestic Animals Wild Animals Husky Bengal Cat

Domestic Animal

Felidae

Lion Wild Animal Dog **def labeling rule:** **def labeling rule:** … **def labeling rule:**

Canine **return label** **return label** **return label**

Wolf Husky _λTrvB5Gbud5+o0iyWD2aUF/gkWQRI9hY6XHAbTEQ29Yrbl1NwdaJV5BalCgNax+DcKYpIJKQzjWu+5ifEzrAwjnM4qg1TBJMJHtG+pRILqv0sX3iGzqwSoihW9kmDcvX3RIaF1lMR2KTAZqyXvbn4n9dPTXTtZ0wmqaGSLD6KUo5MjObXo5ApSgyfWoKJYnZXRMZYWJsRxVbgrd8irpNOreZb1xf1Fr3hV1lOETuEcPLiCJtxC9pAQMAzvMKbo5wX5935WERLTjFzDH/gfP4AbLWQNA=</latexit>_ 1 _λTrvB5Gbud5+o0iyWD2aUF/gkWQRI9hY6XHAbTEw8awWnPrbg60SryC1KBAa1j9GoQxSQWVhnCsd9zE+NnWBlGOJ1VBqmCSYTPKJ9SyUWVPtZvAMnVklRFGs7JMG5erviQwLracisEmBzVgve3PxP6+fmujaz5hMUkMlWXwUpRyZGM2vRyFTlBg+tQTxeyuiIyxwsTYjiq2BG/5FXSadS9y3rj/qLWvCvqKMJnMI5eHAFTbiFrSBgIBneIU3RzkvzrvzsYiWnGLmGP7A+fwBbjmQNQ=</latexit>_ 2 _λTrvB5Gbud5+o0iyWD2aUF/gkWQRI9hY6XHAbTEQzms1ty6mwOtEq8gNSjQGla/BmFMUkGlIRxr3fcxPgZVoYRTmeVQapgskEj2jfUokF1X6WLzxDZ1YJURQr+6RBufp7IsNC6kIbFJgM9bL3lz8z+unJr2MyaT1FBJFh9FKUcmRvPrUcgUJYZPLcFEMbsrImOsMDG2o4otwVs+eZV0GnXvst64v6g174o6ynACp3AOHlxBE26hBW0gIOAZXuHNUc6L8+58LKIlp5g5hj9wPn8AySmQcQ=</latexit>_ _n_

**Label Graph** **Indirect Labeling Functions**

=</latexit> Cat
_Y_

_YtB1MbjO/USVZpF8MNOY+gKPJAsZwcZK3X6AVdqdPXqDcsWtunOgVeLlpAI5GoPyV38YkURQaQjHWvc8NzZ+ipVhNZqZ9oGmMywSPas1RiQbWfzg+eoTOrDFEYKVvSoLn6eyLFQupCGynwGasl71M/M/rJSa89lMm48RQSRaLwoQjE6HsezRkihLDp5Zgopi9FZExVpgYm1HJhuAtv7xKWhdV7Jau69V6jd5HEU4gVM4Bw+uoA530IAmEBDwDK/w5ijnxXl3PhatBSefOY/cD5/AKCakE4=</latexit>_ ¯ [1] _YaTuY3M79hNVmkXywUxj6gs8kixkBsrdfsBVml39lgZFEtu2V0ArRMvIyXI0BgUv/rDiCSCSkM41rnubHxU6wMI5zOCv1E0xiTCR7RnqUSC6r9dHwDF1YZYjCSNmSBi3U3xMpFlpPRWA7BTZjverNxf+8XmLCaz9lMk4MlWS5KEw4MhGaf4+GTFi+NQSTBSztyIyxgoTYzMq2BC81ZfXSatS9mrl6n21VL/J4sjDGZzDJXhwBXW4gwY0gYCAZ3iFN0c5L86787FszTnZzCn8gfP5A6IekE8=</latexit>_ ¯ [2] Lion

Dog

Input _YN4Px7cxvPnFtRKQecBJzP6RDJQaCUbRSuzuimLanj16vWHL7hxklXgZKUGWq/41e1HLAm5QiapMR3PjdFPqUbBJ8WuonhMWVjOuQdSxUNufHT+cFTcmaVPhlE2pZCMld/T6Q0NGYSBrYzpDgy95M/M/rJDi49lOh4gS5YotFg0QSjMjse9IXmjOUE0so08LeStiIasrQZlSwIXjL6+SxkXZuyxX7iul6k0WRx5O4BTOwYMrqMId1KAODEJ4hld4c7Tz4rw7H4vWnJPNHMfOJ8/rO6QVg=</latexit>_ ˆ [1] _YzXB8O/ObT0wbHqkHnMQskGSo+IBTglZqd0cE0/b0sdwrFL2SN4e7SvyMFCFDrVf46vYjmkimkApiTMf3YgxSopFTwab5bmJYTOiYDFnHUkUkM0E6P3jqnlul7w4ibUuhO1d/T6REGjORoe2UBEdm2ZuJ/3mdBAfXQcpVnCBTdLFokAgXI3f2vdvnmlEUE0sI1dze6tIR0YSizShvQ/CX14ljXLJvyxV7ivF6k0WRw5O4QwuwIcrqMId1KAOFCQ8wyu8Odp5cd6dj0XrmpPNnMAfOJ8/rnKQVw=</latexit>_ ˆ [2] _Ym/74ZuY3H7k2IgrvcRLznqLDUASCUbRSuzuimLanD+f9Ysktu3OQv8TLSAky1PrFz+4gYoniITJjel4boy9lGoUTPJpoZsYHlM2pkPesTSkipteOj94Sk6sMiBpG2FSObqz4mUKmMmyrediuLILHsz8T+vk2Bw1UtFGCfIQ7ZYFCSYERm35OB0JyhnFhCmRb2VsJGVFOGNqOCDcFbfvkvaZyVvYty5a5Sql5nceThCI7hFDy4hCrcQg3qwEDBE7zAq6OdZ+fNeV+05pxs5hB+wfn4Bq/2kFg=</latexit>_ ˆ [3] Wolf

**Unlabeled Data** **Label Model** **Probabilistic Labels** **End Model**

**An example of WIS problem: the input consists of an unlabeled dataset, a label graph, and**
indirect labeling functions (ILFs). The ILFs represent weak supervision sources such as pretrained
classifiers, knowledge bases, heuristic rules, etc. We can see that the ILFs cannot predict desired

labels i.e., {“dog”, “wolf ”, “cat”, “lion”}. To address this, a label graph is given; here we only
visualize the subsuming relation. Finally, a label model, instantiated as a PGM, takes the ILF’s
outputs and produces probabilistic labels in the target output space, which are in turn used to train an
end machine learning model that can generalize beyond them.

4 WEAK INDIRECT SUPERVISION

Now, we introduce the new Weak Indirect Supervision (WIS) problem. Unlike the standard WS
setting, we only have indirect labeling functions (ILFs) instead of LFs, and an additional label graph
is given. The goal of WIS remains the same as WS. An example of WIS problem is in Fig. 1.

**Indirect Labeling Function. In WIS, we only have indirect labeling functions (ILFs), which cannot**
directly predict any desired labels, i.e., _Y ∩Y[ˆ]_ = ∅. Therefore, we refer to the desired labels as unseen
labels. To make it possible to leverage the ILFs, a label graph is given, which encodes pair-wise label
relations between different seen and unseen labels.

**Label Graph. Concretely, a label graph G = (V, E) consists of (1) a set of all the labels as nodes, i.e.,**
= [ˆ], and (2) a set of pair-wise label relations as typed edges, i.e., = (yi, yj, tyiyj ) _tyiyj_
_V, i < j,Y ∪Yyi, yj_ . Here, is the set of label relation types and, similar to E _{_ Deng et al.| (2014 ∈),
_Tthere are four types of label relations: ∀_ _∈V}_ _T_ _exclusive, overlapping, subsuming, subsumed, notated by_
_t[o], t[e], t[sg], t[sd], respectively. Notably, for any ordered pair of labels (yi, yj), their label relation should_
fall into one of the four types. The rationale behind these label relations is that when treating each
label as a set, there are four unique set relations and each corresponds to one defined label relation
respectively as shown in Fig. 2. For convenience, we denote the set of non-exclusive neighbors of a
given label y in [ˆ] as (y, [ˆ]), i.e., (y, [ˆ]) = _yˆ_ _tyyˆ_
_Y_ _N_ _Y_ _N_ _Y_ _{_ _∈_ _Y|[ˆ]_ _[̸][=][ t][e][}][.]_


Label relation:

Set relation


_AEeM91oGaoQHzQTcG4w+G9MAoLdoOlakA6Vj9PTFgvtZ93zOdPsOunvVG4n9eLcZ2vjEQRQjBHyqB1LiEdxUFbQgFH2TeEcSXMrZR3mWIcTWhpE4I7+/I8KR/n3JOc2vSuCETpMge2SeHxCWnpECuSZGUCcP5Im8kFfr0Xq23qz3SWvCms7skD+wPn4ABruW7A=</latexit>Exclusive \ B = ;_ SubsumingA % B SubsumedA $ B _AMPDPr0AbEalT7MVQD1lbiZbgDK3UyJRyubLPWUwrvoJLH8IYewYwT8u+itA3SWArWskP/J+6nMs1Mlm34A5BJ4n3TbL72c/Hm+p98biRefObEU9CUMglM6bmuTHW+0yj4BKu035iIGa8y9pQs1SxEy9P7zwm5bpUlbkbZPIR2qvyf6LDSmFwa2M2TYMePeQPzPqyXYKtX7QsUJguKjRa1EUozoIC7aFBo4yp4ljGth/0p5h2nG0YatiF4ydPkrPdglcsuCc2jSMyQopski2yQzyR/bJITkmVcLJLXkgz+TFuXOenFfnbdQ65XzPbJA/cD6+ADiPpg8=</latexit>_ _\ B 6= ;Overlap, A 6⇢_ _B, B 6⇢_ _A_


A B


A B


B A


A B


Figure 2: The one-to-one mapping between label relations and set relations.

5 PROBABILISTIC LABEL RELATION MODEL

One of the key difficulties in both WS and WIS is that we do not observe the true label Yi. Following
prior work (Ratner et al., 2016; 2019; Fu et al., 2020), we use a latent variable Probabilistic Graphical
Model (PGM) for estimating Yi based on the _Y[ˆ]i output by ILFs. Specifically, the PGM is instantiated_


-----

as a factor graph model. This standard technique lets us describe the family of generative distributions
in terms of M known dependencies/factor functions {ϕ}, and an unknown parameter Θ ∈ R[M] as
_PΘ(_ ) exp(Θ[⊤]Φ( )), where Φ is the concatenation of _ϕ_ . However, the unique challenge for

_·_ _∝_ _·_ _{_ _}_
WIS is that the dependencies {ϕ} between Yi and _Y[ˆ]i are unknown due to the mismatches of label_
spaces. We overcome these by leveraging the label graph G to build the dependencies for the PGM.

5.1 A BASELINE PGM FOR WIS

In prior work (Ratner et al., 2016; Bach et al., 2017), the PGM for WS is governed by accuracy
dependencies:
_ϕ[Acc]y,j[(][Y,][ ˆ]Y_ _[j]) := 1{Y = Y[ˆ]_ _[j]_ = y}
which is defined for eachylabel relation to build a corresponding "accuracy" factor. Specifically, for an ILF ∈Y. As a simple baseline approach to start, we leverage the coarse-grained exclusive/non-exclusive λj and y ∈Yλj ∩Y. However, in WIS, the ILFs cannot predict desired label λj and one label
_yˆ_ _λj_, given a desired label y, if ˆy and y have non-exclusive label relation, i.e., ˆy (y, _λj_ )
_∈Y_ _∈Y_ _∈N_ _Y_
we expect a certain portion of data assigned ˆy should be labeled as y. Thus, we treat _Y[ˆ]_ _[j]_ = ˆy as a
pseudo indicator of Y = y and add a pseudo accuracy dependency between them:

_ϕ[Acc]y,y,jˆ_ [(][Y,][ ˆ]Y _[j]) := 1{Y = y ∧_ _Y[ˆ]_ _[j]_ = ˆy}

We call the PGM governed by pseudo accuracy dependencies Weak Supervision with Label Graph
(WS-LG). Notably, it can be treated as a simple adaptation of PGM for WS (Ratner et al., 2016; 2019;
Fu et al., 2020) to the WIS problem. However, such a naïve adaptation might have two drawbacks:

1. It does not model specific dependencies ILFs with different undesired labels. For example, two
ILFs outputting “Husky” and “bulldog” respectively would be naively modeled the same as if they
both output “Dog”.

2. It can only directly model exclusive/non-exclusive label relations, ignoring the prior knowledge
encoded in other relation types, i.e., subsuming, subsumed, or overlapping. For example, given
an unseen label “Dog” and some ILFs outputting “Husky” or “Domestic Animals”, WS-LG
would treat all ILFs as indicators of “Dog”. However, we know a “Husky” is of course a “Dog”
(subsumed relation) while a “Domestic Animals” is not necessarily a “Dog” (subsuming relation).

5.2 PROBABILISTIC LABEL RELATION MODEL

To more directly model the full range and nuance of label relations, we propose a new probabilistic
_label relation model (PLRM). In PLRM, we explicitly model both (1) the dependency between ILF_
outputs and the true labels in their output spaces, i.e. their direct accuracy, and (2) the dependencies
between these labels and the target unseen labels, as separate dependency types, thus explicitly
incorporating the full label relation graph into our model and learning its corresponding weights.

Concretely, we augment the WS-LG model with (1) latent variables representing the assignment
of the data to each seen label, and (2) label relation dependencies which capture fine-grained label
relations between these output labels and desired labels. To model seen label ina binary latent random vector _Y[¯] = [ Y[¯]_ [1], . . ., _Y[¯]_ _k[ˆ]], where ¯Y_ _[i]_ indicating whether the data should beY[ˆ], we introduce
assigned ˆyi. Then, for ILF λj that could predict ˆyi, we have accuracy dependency:

_ϕ[Acc]yˆi,j[( ¯]Y_ _[i],_ _Y[ˆ]_ _[j]) := 1_ _Y[¯]_ _[i]_ = 1 _Y_ _[j]_ = ˆyi
_{_ _∧_ [ˆ] _}_

To model fine-grained label relations, for a desired label y and seen label ˆyi, we add label
_relation dependencies. We enumerate the label relation dependencies corresponding to the four label ∈Y_ _∈_ _Y[ˆ]_
relation types, i.e., exclusive, overlapping, subsuming, subsumed, as follows:

_ϕ[e]y,yˆi_ [(][Y,][ ¯]Y _[i]) := −_ 1{Y = y ∧ _Y[¯]_ _[i]_ = 1}

_ϕ[o]y,yˆi_ [(][Y,][ ¯]Y _[i]) := 1{Y = y ∧_ _Y[¯]_ _[i]_ = 1}

_ϕ[sg]y,yˆi_ [(][Y,][ ¯]Y _[i]) := −_ 1{Y ̸= y ∧ _Y[¯]_ _[i]_ = 1}

_ϕ[sd]y,yˆi_ [(][Y,][ ¯]Y _[i]) := −_ 1{Y = y ∧ _Y[¯]_ _[i]_ = 0}


-----

The above dependencies encode the prior knowledge of the label relations, but also allow the model
to learn corresponding parameters. For example, an exclusive label relation dependency ϕ[e] outputs
-1 when two exclusive labels are activated at the same time for the same data, otherwise 0, which
reflects our prior knowledge of the exclusive label relation; and the corresponding parameter can be
treated as the strength of the label relation. Likewise, for any pair of seen labels, we add label relation
dependency following the same convention. Finally, we specify the model as:

_PΘ(Y,_ _Y,[¯]_ _Y[ˆ] ) ∝_ exp Θ[⊤]Φ(Y, _Y,[¯]_ _Y[ˆ] )_ _._ (1)
 

Recall that Y is the unobserved true label, _Y[¯] is the binary random vector, each of whose binary value_
_Y¯_ _[i]_ reflects whether the data should be assigned seen label ˆyi, and _Y[ˆ] is the concatenated outputs_
of ILFs. _∈_ _Y[ˆ]_

**Learning Objective. We estimate the parameters** Θ[ˆ] by minimizing the negative log marginal
likelihood PΘ( Y[ˆ] ) for observed ILF outputs _Y[ˆ]1:m:_


_PΘ(Y,_ _Y,[¯]_ _Y[ˆ]i) ._ (2)
_Y,Y[¯]_

X


Θ = arg minˆ


log

_i=1_

X


We follow Ratner et al. (2016) to optimize the objective using stochastic gradient descent.

**Training an End Model. Let p ˆΘ[(][Y][ |][ ˆ]Y ) be the probabilistic label (i.e. distribution) predicted by**
learned PLRM. We then train an end model fW : X →Y parameterized by W, by minimizing the
empirical noise-aware loss (Ratner et al., 2019) with respect to Θ[ˆ] over m unlabeled data points:


EY ∼p ˆΘ[(][Y][ |][ ˆ]Yi)[ℓ][(][Y, f][W][ (][X][i][))][,] (3)
_i=1_

X


_Wˆ_ = arg min


where ℓ(Y, fW (Xi)) is a standard cross entropy loss.

**Generalization Error Bound. We extend previous results from (Ratner et al., 2016) to bound both**
the expected error of learned parameter Θ[ˆ] and the expected risk for _W[ˆ]_ . All the proof details and
description of assumptions can be found in Appendix.

**Theorem 1. Suppose that we run stochastic gradient descent to produce** Θ[ˆ] _and_ _W[ˆ]_ _based on Eqs. (2)_
_and (3), respectively, and that our setup satisfies certain assumptions (App D.2). Let |D| be the size_
_of the unlabeled dataset. Then we have_


2
Θ Θ[∗] _O_ _M_ [log][ |][D][|]
_−_ _≤_ _D_
 _|_ _|_

[ˆ]


log |D|

_|D|_


_ℓ( W[ˆ]_ ) − _ℓ(W_ _[∗])_ _≤_ _χ + O_
i


**Interpreting the Bound.** By Theorem 1, the two errors decrease by the rate _O[˜](1/|D|) and_
_O˜(1/|D|[1][/][2]) respectively as |D| increases. This shows that although we trade computational effi-_
ciency for the reduction of human efforts by using complex dependencies and more latent variables,
we maintain comparable statistical efficiency as previous WS frameworks and supervised learning
theoretically.

6 DISTINGUISHABILITY OF UNSEEN LABELS


One unique challenge of WIS is that there may exist pairs of unseen
labels which cannot be distinguished by the learned model. For
example, as shown in Fig. 3, where “Dog” is a seen label for which
LFs could predict for and “Husky” and “Bulldog” are unseen labels
for which we want to generate training labels; however, we could
not distinguish between “Husky” and “Bulldog” even though the LFs
make correct predictions of seen label “Dog”, because both “Husky”
and “Bulldog” share the same label relation to “Dog”.

To tackle this issue, we theoretically connect the distinguishability of
unseen labels to the label relation structures and provide a testable


Exclusive

Dog Subsuming

Husky Bulldog


Figure 3: Example of indistinguishable unseen labels
“Husky” and “Bulldog”.


-----

condition for the distinguishability. Intuitively, same label relation
structures could lead to indistinguishable unseen labels as shown in Fig. 3; however, it turns out to be
_challenging to prove that different label relation structures could guarantee the distinguishability_
_with respect to the model. To illustrate, we formally define the distinguishability as below._

**Definition 1 (Distinguishability). For any model PΘ(Y,** _Y,[¯]_ _Y[ˆ] ) with parameters Θ, any pair of unseen_
_labels yi, yj_ _are distinguishable w.r.t. the model, if for a.e. Θ > 0 (element-wisely), there does_
_∈Y_
_NOT exist such a_ Θ[˜] _> 0 that, for ∀Y,[¯]_ _Y[ˆ], the following equations hold_

_PΘ(Y = yi|Y,[¯]_ _Y[ˆ] ) = P ˜Θ[(][Y][ =][ y][j][|][ ¯]Y,_ _Y[ˆ] ), PΘ(Y = yj|Y,[¯]_ _Y[ˆ] ) = P ˜Θ[(][Y][ =][ y][i][|][ ¯]Y,_ _Y[ˆ] ),_ (4)

_PΘ(Y = y|Y,[¯]_ _Y[ˆ] ) = P ˜Θ[(][Y][ =][ y][|][ ¯]Y,_ _Y[ˆ] ), ∀y ∈Y/{yi, yj},_ (5)

_PΘ( Y[ˆ] ) = P ˜Θ[( ˆ]Y )._ (6)


From the definition, we can see that the opposite of distinguishability, i.e., indistinguishability,
describes an undesired model: for any learned parameter Θ > 0, we can always find another Θ[˜]
which optimizes the loss equally well (Eq. (6)), but Eqs. (4-5) implies whenever PΘ predict yi,
_P ˜Θ_ _[will predict][ y][j][ instead][, which reflects that the model cannot distinguish the two unseen labels.]_
Note that the notion of distinguishability is different from the identifiability in PGMs: the generic
_identifiability (Allman et al., 2015), the strongest notion of identifiability, requires the model to be_
identifiable up to label swapping, while the distinguishability aims to avoid the label swapping.

However, distinguishability is hard to verify since Eqs. (4-5) and (6) need to hold for any possible
configuration of _Y,[¯]_ _Y[ˆ], and any pair of unseen labels. Fortunately, for the proposed PLRM, we_
prove that distinguishability is equivalent to the asymmetry of the label relation structures when two
conditions hold. To state the required conditions, we first introduce the notations of consistency and
informativeness to characterize the label graph and ILFs.

**Consistency. We discuss the consistency of a label graph to avoid an ambiguous or unrealistic label**
graph. We interpret semantic labels ya, yb as sets A, B, and then connect the label relations to the set
relations (Fig. 2). Given the set interpretations, we define the consistency of label graph as:

**Definition 2 (Consistent Label Graph). A label graph G = (Y, E) is consistent if the induced set**
_relations are consistent._

For example, assume Y = {ya, yb, yc}, and tab = tbc = tca = t[sg]. From tab, tbc, we can observe
that A ⫌ _B ⫌_ _C, which contradicts to C ⫌_ _A implied by tca = t[sg]. Thus, G is inconsistent._

**Informativeness. In addition, we try to describe what kind of ILF is desired. Intuitively, an ILF is**
uninformative if it always "votes" for one of the desired labels. For example, if the desired label
space Y is {“Dog”, “Bird”}, then for an ILF λ1 outputting {“Husky”, “Bulldog”}, we know “Dog” is
non-exclusive to “Husky” and “Bulldog”, while “Bird” exclusive to both. In such case, λ1 can hardly
provide information to help distinguish “Dog” from “Bird”, because it always votes for “Dog”. On
the other hand, a binary classifier of “Husky”, i.e., λ2, is favorable since it could output “Not a Husky”
to avoid consistently voting for “Dog”. We can see an undesired ILF always votes for a single desired
label. To formally describe this, we define an informative ILF as:

**Definition 3 (Informative ILF). An ILF λj is informative if, for ∀y ∈Y, there exists Xi ∈D s.t. the**
_output of λj on Xi is not in_ (y, _λj_ ), i.e., _Y[ˆ]i[j]_ _j_ [)][.]
_N_ _Y_ _[̸∈N]_ [(][y,][ Y][λ]

**Testable Conditions for Distinguishability. Based on the introduced notations, we prove the**
_necessary and sufficient condition for learned PLRM being able to distinguish unseen labels:_

**Theorem 2. For PLRM induced from a consistent label graph, as well as informative ILFs, for any**
_pair of yi, yj ∈Y, they are indistinguishable, if and only if tik = tjk for ∀yk ∈_ _Y[ˆ]._

Theorem 2 provides users with a testable condition: for any pair of unseen labels yi, yj, there should
_exist at least one seen labelso that PLRM is able to distinguish yk such that yi and yk y has different label relations toj. In preliminary experiments, we observe the violation yi and yj, i.e., tik ̸= tjk,_
of this condition causes a dramatic drop in overall performance (about 10 points). Notably, based on
Theorem 2, users could theoretically guarantee the distinguishability of a pair of unseen labels by
adding only one seen label and corresponding ILFs to break the symmetry.


-----

7 EXPERIMENTS

We demonstrate the applicability and performance of our method on image classification tasks
derived from ILSVRC2012 (Russakovsky et al., 2015) and text classification tasks derived from
LSHTC-3 (Partalas et al., 2015). Both datasets have off-the-shelf label relation structure (Deng et al.,
2014; Partalas et al., 2015), which are directed acyclic graphs (DAGS) and from which we could
query pairwise label relations. Indeed, there is a one-to-one mapping between a DAG structure of
labels and a consistent label graph (See App. E.1 for an example). The ILSVRC2012 dataset consists
of 1.2M training images from 1,000 leave classes; for non-leave classes, we follow Deng et al. (2014)
to aggregate images belonging to its descendent classes as its data points. The LSHTC-3 dataset
consists of 456,886 documents and 36,504 labels organized in a DAG.

7.1 SETUP

For each dataset, we randomly sample 100 different label graphs, each of which consists of 8 classes,
and use each label graph to construct a WIS task. For each label graph, we treat 3 of the sampled
classes as unseen classes and the other 5 as seen classes. The distinguishable condition in Sec. 6
is ensured for all the WIS tasks, and the performance drop when it is violated can be found in
App. G.1. We sample data belonging to unseen classes for our experiments and split them into
train and test set. For image classification tasks, we follow Mazzetto et al. (2021b;a) to train a
branch of image classifiers as supervision sources of seen classes. For text classification tasks, we
made keyword-based labeling functions as supervision sources of seen classes following Zhang et al.
(2021); each of the labeling functions returns its associated label when a certain keyword exists in
the text, otherwise abstains. Notably, all the involved supervision sources are "weak" because they
cannot predict the desired unseen classes. Experimental details and additional results are in App. F.

7.2 COMPARED METHODS AND RESULTS

In addition to the WS-LG baseline, which is an adaptation of Data Programming (Ratner et al., 2019)
to WIS task, and PLRM, we also include the following baselines. Note that all compared methods
input the same data, ILFs, and label relations throughout our experiments for fair comparisons.

**Label Relation Majority Voting (LR-MV).** We modify the majority voting method based on the
label’s non-exclusive neighbors: we replace ˆy predicted by any ILF with the set of desired labels
_N_ (ˆy, Y), i.e., the desired labels with non-exclusive relation to ˆy, then aggregate the modified votes.

**Weighted Label Relation Majority Voting (W-LR-MV).** LR-MV only leverages exclusive/nonexclusive label relations. To leverage fine-grained label relations, W-LR-MV attaches a weight to
each replaced label. Specifically, if the ILF’s output ˆy is replaced with its ancestor label y (subsumed
relation), then the weight of y equals 1, while for the other relations, the weight is _|Y_ _[∗]1(ˆy)|_ [, where]

(ˆy) = _y_ (ˆy) _tyyˆ_
_Y_ _[∗]_ _{_ _∈Y_ _|_ _[̸][=][ t][sd][}][.]_

For the above methods, we compare the performance of (1) directly applying included models on the
test set and (2) the end models (classifiers) trained with inferred training labels.

**Zero-Shot Learning (ZSL).** It is non-trivial to apply ZSL methods, because ZSL assumes label
attributes for all classes and a labeled training set of seen classes, while WIS input an unlabeled
dataset of unseen classes, label relations and ILFs. Fortunately, the Direct Attribute Prediction
(DAP) (Lampert et al., 2013) method is able to make predictions solely based on attributes without
labeled data, by training attribute classifier p(ai _x) for each attribute ai. Therefore we include it in_
_|_
our experiments. The details of applying DAP can be found in App. F.2.

**Evaluation Results.** For a fair comparison, we fix the network architecture of the classifiers for all
the methods. For image classification, we use ResNet-32 (He et al., 2016) and for text classification,
we use logistic regression with pre-trained text embedding (Reimers & Gurevych, 2019). The overall
results for both datasets can be found in Table 2. From the results, we can see that PLRM consistently
outperforms baselines. The advantages of PLRM show the effect of not just leveraging the label
graph, as the baselines do, but modeling the accuracy of ILFs and the strengths of label relations


-----

Table 2: Averaged evaluation results over 100 WIS tasks derived from LSHTC-3 and ILSVRC2012.

**LSHTC-3** **ILSVRC2012**
**Method**

**Accuracy** **F1-score** **Accuracy** **F1-score**


DAP 42.90 ± 13.53 35.98 ± 15.73 33.25 ± 3.68 29.13 ± 4.63

W-LR-MVLR-MV 58.8659.28 ± 10.50 10.47 54.3354.55 ± 11.10 11.36 46.8841.39 ± 10.66 10.80 40.1130.19 ± 16.44 16.94

Label Model WS-LG 62.60 ± ± 10.12 57.50 ± ± 11.19 53.68 ± ± 7.62 52.15 ± ± 7.94

PLRM **64.65 ± 11.30** **60.01 ± 13.39** **56.18 ± 7.35** **54.94 ± 7.44**

W-LR-MVLR-MV 67.1766.57 ± 12.25 11.73 62.4961.80 ± 13.95 13.24 49.6042.61 ± 12.80 12.46 42.8331.34 ± 18.17 18.20

End Model WS-LG 70.69 ± ± 13.05 67.36 ± ± 14.24 56.56 ± ± 9.68 54.57 ± ± 11.17

PLRM **72.32 ± 13.18** **69.37 ± 14.41** **58.38 ± 8.27** **56.83 ± 8.49**

as PLRM does. The reported results have high variance, which actually indicates the 100 different
WIS tasks are diverse and have varying difficulty. Also, we can see the end models are much better
than directly applying the label models on the test set; this shows that the end models are able to
generalize beyond the training labels produced by label models.

7.3 REAL-WORLD APPLICATION

In this section, on a commercial advertising system (CAS), we showcase how to reduce human
annotation efforts of new labeling tasks by formulating them as WIS problems. In a CAS, ads tagging
(classification) is a critical application for understanding the semantics of ads copy. When new ads
and tags are added to the system, manual annotations need to be collected for training a new classifier.
As tags are commonly organized as taxonomies, the label relations between existing and new tags are
readily available or can be trivially figured out by humans; Existing classifiers and the heuristic rules
previously used for annotating existing tags could serve as ILFs. Therefore, given (1) an unlabeled
dataset of new tags, (2) the label relations, and (3) ILFs, we formulate it as a WIS problem.

On such WIS formulation, we apply our method and baselines, to synthesize training labels of new
tags. Specifically, we have two WIS tasks where the tags are under the “Car Accessories” and
“Furniture” categories respectively. For both tasks, we have 3 new tags and leverage 5 existing
tags related to the new ones with given relations. On a test set, we evaluate the performance of
DAP and the quality of labels produced by label models, as shown in Table 3. Note that since we
re-use the existing labeling sources tailored for existing tags as ILFs and obtain label relations from
an existing taxonomy, we achieve these results without any manual annotation or creation of new
labeling functions. This demonstrates the potential of the proposed WIS task in real-world scenarios.

Table 3: Evaluation on product tagging with new tags.

**Category** **Metric** **DAP** **LR-MV** **W-LR-MV** **WS-LG** **PLRM**

**F1** 50.62 68.68 68.06 66.85 **76.37**
**Car Accessories**
**Accuracy** 52.83 68.17 67.67 66.33 **75.83**

**F1** 30.81 64.70 61.45 70.59 **80.57**
**Furniture**
**Accuracy** 33.60 72.53 72.13 74.51 **82.02**

8 CONCLUSION

We propose Weak Indirect Supervision (WIS), a new research problem which leverages indirect
supervision sources and label relations to synthesize training labels for training machine learning
models. We develop the first method for WIS called Probabilistic Label Relation Model (PLRM) with
the generalization error bound of both PLRM and end model. We provide a theoretically-principled
sanity test to ensure the distinguishability of unseen labels. Finally, we provide experiments to
demonstrate the effectiveness of PLRM and its advantages over baselines on both academic datasets
and industrial scenario.


-----

**Reproducibility Statement.** All the assumptions and proofs of our theory can be found in App. C

& D. Examples and illustrations of label graph are in App. E. Experimental details can be found in
App. F. Additional experiments are in App. G.

REFERENCES

Elizabeth S Allman, John A Rhodes, Elena Stanghellini, and Marco Valtorta. Parameter identifiability
of discrete bayesian networks with hidden variables. Journal of Causal Inference, 3(2):189–205,
2015.

Stephen H. Bach, Bryan He, Alexander J. Ratner, and Christopher Ré. Learning the structure of
generative models without labeled data. In Proceedings of the 34th International Conference on
_Machine Learning (ICML 2017), Sydney, Australia, 2017._

Stephen H. Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik
Sen, Alex Ratner, Braden Hancock, Houman Alborzi, Rahul Kuchhal, Chris Ré, and Rob Malkin.
Snorkel drybell: A case study in deploying weak supervision at industrial scale. In Proceedings
_of the 2019 International Conference on Management of Data, SIGMOD ’19, pp. 362–375, New_
York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450356435. doi:
[10.1145/3299869.3314036. URL https://doi.org/10.1145/3299869.3314036.](https://doi.org/10.1145/3299869.3314036)

Peter F Brown, Stephen A Della Pietra, Vincent J Della Pietra, and Robert L Mercer. The mathematics
of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263–311,
1993.

Ming-Wei Chang, Vivek Srikumar, Dan Goldwasser, and Dan Roth. Structured output learning with
indirect supervision. In ICML, pp. 199–206, 2010.

Lingjiao Chen, Matei Zaharia, and James Zou. Frugalml: How to use ml prediction apis more
accurately and cheaply. In Advances in Neural Information Processing Systems (NeurIPS), 2020.

Carlos d’Andrea and André Mintz. Studying the live cross-platform circulation of images with
computer vision api: An experiment based on a sports media event. International Journal of
_Communication, 13(0), 2019. ISSN 1932-8036._

Jia Deng, Nan Ding, Yangqing Jia, Andrea Frome, Kevin Murphy, Samy Bengio, Yuan Li, Hartmut
Neven, and Hartwig Adam. Large-scale object classification using label relation graphs. In
_European conference on computer vision, pp. 48–64. Springer, 2014._

Xin Luna Dong, Xiang He, Andrey Kan, Xian Li, Yan Liang, Jun Ma, Yifan Ethan Xu, Chenwei
Zhang, Tong Zhao, Gabriel Blanco Saldana, et al. Autoknow: Self-driving knowledge collection
for products of thousands of types. In Proceedings of the 26th ACM SIGKDD International
_Conference on Knowledge Discovery & Data Mining, pp. 2724–2734, 2020._

Jared A. Dunnmon, Alexander J. Ratner, Khaled Saab, Nishith Khandwala, Matthew Markert,
Hersh Sagreiya, Roger Goldman, Christopher Lee-Messer, Matthew P. Lungren, Daniel L.
Rubin, and Christopher Ré. Cross-modal data programming enables rapid medical machine
learning. Patterns, 1(2):100019, 2020. ISSN 2666-3899. doi: https://doi.org/10.1016/j.patter.
2020.100019. [URL https://www.sciencedirect.com/science/article/pii/](https://www.sciencedirect.com/science/article/pii/S2666389920300192)
[S2666389920300192.](https://www.sciencedirect.com/science/article/pii/S2666389920300192)

Jason A Fries, Ethan Steinberg, Saelig Khattar, Scott L Fleming, Jose Posada, Alison Callahan, and
Nigam H Shah. Ontology-driven weak supervision for clinical entity classification in electronic
health records. Nature Communications, 12(1), 2021.

Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc' Aurelio Ranzato, and Tomas Mikolov. Devise: A deep visual-semantic embedding model. In
C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Ad_vances in Neural Information Processing Systems, volume 26, pp. 2121–2129. Curran Asso-_
[ciates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/](https://proceedings.neurips.cc/paper/2013/file/7cce53cf90577442771720a370c3c723-Paper.pdf)
[7cce53cf90577442771720a370c3c723-Paper.pdf.](https://proceedings.neurips.cc/paper/2013/file/7cce53cf90577442771720a370c3c723-Paper.pdf)


-----

Daniel Y. Fu, Mayee F. Chen, Frederic Sala, Sarah M. Hooper, Kayvon Fatahalian, and Christopher
Ré. Fast and three-rious: Speeding up weak supervision with triplet methods. In Proceedings of
_the 37th International Conference on Machine Learning (ICML 2020), 2020._

Melody Y. Guan, Varun Gulshan, Andrew M. Dai, and Geoffrey E. Hinton. Who said what: Modeling
individual labelers improves classification. In AAAI, 2018.

K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016
_IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016. doi:_
10.1109/CVPR.2016.90.

Sarah Hooper, Michael Wornow, Ying Hang Seah, Peter Kellman, Hui Xue, Frederic Sala, Curtis
Langlotz, and Christopher Re. Cut out the annotator, keep the cutout: better segmentation
with weak supervision. In International Conference on Learning Representations, 2021. URL
[https://openreview.net/forum?id=bjkX6Kzb5H.](https://openreview.net/forum?id=bjkX6Kzb5H)

Ashish Khetan, Zachary C. Lipton, and Anima Anandkumar. Learning from noisy singly-labeled data.
[In International Conference on Learning Representations, 2018. URL https://openreview.](https://openreview.net/forum?id=H1sUHgb0Z)
[net/forum?id=H1sUHgb0Z.](https://openreview.net/forum?id=H1sUHgb0Z)

Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object
classes by between-class attribute transfer. In 2009 IEEE Conference on Computer Vision and
_Pattern Recognition, pp. 951–958. IEEE, 2009._

Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Attribute-based classification for zeroshot visual object categorization. IEEE transactions on pattern analysis and machine intelligence,
36(3):453–465, 2013.

Percy Liang, Michael I Jordan, and Dan Klein. Learning dependency-based compositional semantics.
_Computational Linguistics, 39(2):389–446, 2013._

Pierre Lison, Jeremy Barnes, Aliaksandr Hubin, and Samia Touileb. Named entity recognition
without labelled data: A weak supervision approach. In Proceedings of the 58th Annual Meeting
_of the Association for Computational Linguistics, pp. 1518–1533, Online, July 2020. Association_
[for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.139. URL https://www.](https://www.aclweb.org/anthology/2020.acl-main.139)
[aclweb.org/anthology/2020.acl-main.139.](https://www.aclweb.org/anthology/2020.acl-main.139)

Alessio Mazzetto, Cyrus Cousins, Dylan Sam, Stephen H Bach, and Eli Upfal. Adversarial multi
class learning under weak supervision with performance guarantees. In Marina Meila and Tong
Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139
of Proceedings of Machine Learning Research, pp. 7534–7543. PMLR, 18–24 Jul 2021a. URL
[https://proceedings.mlr.press/v139/mazzetto21a.html.](https://proceedings.mlr.press/v139/mazzetto21a.html)

Alessio Mazzetto, Dylan Sam, Andrew Park, Eli Upfal, and Stephen Bach. Semi-supervised aggregation of dependent weak supervision sources with performance guarantees. In Arindam
Banerjee and Kenji Fukumizu (eds.), Proceedings of The 24th International Conference on Artifi_cial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pp._
[3196–3204. PMLR, 13–15 Apr 2021b. URL https://proceedings.mlr.press/v130/](https://proceedings.mlr.press/v130/mazzetto21a.html)
[mazzetto21a.html.](https://proceedings.mlr.press/v130/mazzetto21a.html)

George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):
39–41, 1995.

Shikhar Murty, Pat Verga, L. Vilnis, and A. McCallum. Finer grained entity typing with typenet.
_AKBC Workshop, 2017._

Ioannis Partalas, Aris Kosmopoulos, Nicolas Baskiotis, Thierry Artières, George Paliouras, Éric
Gaussier, Ion Androutsopoulos, Massih-Reza Amini, and Patrick Gallinari. LSHTC: A benchmark
for large-scale text classification. CoRR, abs/1503.08581, 2015.

Meng Qu, Tianyu Gao, Louis-Pascal Xhonneux, and Jian Tang. Few-shot relation extraction via
bayesian meta-learning on relation graphs. In International Conference on Machine Learning, pp.
7867–7876. PMLR, 2020.


-----

Ariadna Quattoni, Michael Collins, and Trevor Darrell. Conditional random fields for object recognition. Advances in neural information processing systems, 17:1097–1104, 2004.

Aditi Raghunathan, Roy Frostig, John Duchi, and Percy Liang. Estimation from indirect supervision
with linear moments. In International Conference on Machine Learning (ICML), 2016.

A. J. Ratner, Christopher M. De Sa, Sen Wu, Daniel Selsam, and C. Ré. Data programming:
Creating large training sets, quickly. In Proceedings of the 29th Conference on Neural Information
_Processing Systems (NIPS 2016), Barcelona, Spain, 2016._

A. J. Ratner, B. Hancock, J. Dunnmon, F. Sala, S. Pandey, and C. Ré. Training complex models with
multi-task weak supervision. In Proceedings of the AAAI Conference on Artificial Intelligence,
Honolulu, Hawaii, 2019.

Alexander Ratner, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher
Ré. Snorkel: Rapid training data creation with weak supervision. In Proceedings of the 44th
_International Conference on Very Large Data Bases (VLDB), Rio de Janeiro, Brazil, 2018._

Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.
[Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.](https://arxiv.org/abs/1908.10084)
[10084.](https://arxiv.org/abs/1908.10084)

Bernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot
learning. In International conference on machine learning, pp. 2152–2161. PMLR, 2015.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet
large scale visual recognition challenge. Int. J. Comput. Vision, 115(3):211–252, December 2015.
[ISSN 0920-5691. doi: 10.1007/s11263-015-0816-y. URL https://doi.org/10.1007/](https://doi.org/10.1007/s11263-015-0816-y)
[s11263-015-0816-y.](https://doi.org/10.1007/s11263-015-0816-y)

Esteban Safranchik, Shiying Luo, and Stephen Bach. Weakly supervised sequence tagging from
noisy rules. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp.
5570–5578, 2020.

Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Paul Hsu, and Kuansan
Wang. An overview of microsoft academic service (mas) and applications. In WWW, 2015.

Shashank Srivastava, Igor Labutov, and Tom Mitchell. Zero-shot learning of classifiers from natural
language quantification. In Proceedings of the 56th Annual Meeting of the Association for
_Computational Linguistics (Volume 1: Long Papers), pp. 306–316, Melbourne, Australia, July_
[2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1029. URL https:](https://www.aclweb.org/anthology/P18-1029)
[//www.aclweb.org/anthology/P18-1029.](https://www.aclweb.org/anthology/P18-1029)

The Gene Ontology Consortium. The Gene Ontology Resource: 20 years and still GOing strong.
_Nucleic Acids Research, 47(D1):D330–D338, 11 2018. ISSN 0305-1048. doi: 10.1093/nar/_
[gky1055. URL https://doi.org/10.1093/nar/gky1055.](https://doi.org/10.1093/nar/gky1055)

Paroma Varma, Frederic Sala, Shiori Sagawa, Jason Alan Fries, Daniel Y. Fu, Saelig Khattar, Ashwini Ramamoorthy, Ke Xiao, Kayvon Fatahalian, James Priest, and Christopher Ré. Multiresolution weak supervision for sequential data. In Hanna M. Wallach, Hugo Larochelle,
Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Ad_vances in Neural Information Processing Systems 32: Annual Conference on Neural Informa-_
_tion Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,_
[pp. 192–203, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/](https://proceedings.neurips.cc/paper/2019/hash/93db85ed909c13838ff95ccfa94cebd9-Abstract.html)
[93db85ed909c13838ff95ccfa94cebd9-Abstract.html.](https://proceedings.neurips.cc/paper/2019/hash/93db85ed909c13838ff95ccfa94cebd9-Abstract.html)

Kaifu Wang, Qiang Ning, and Dan Roth. Learnability with indirect supervision signals. Advances in
_Neural Information Processing Systems 32, 2020._

Wei Wang, Vincent W Zheng, Han Yu, and Chunyan Miao. A survey of zero-shot learning: Settings,
methods, and applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10
(2):1–37, 2019.


-----

Yuanshun Yao, Zhujun Xiao, Bolun Wang, Bimal Viswanath, Haitao Zheng, and Ben Y. Zhao.
Complexity vs. performance: Empirical analysis of machine learning as a service. In Proceedings
_of the 2017 Internet Measurement Conference, IMC ’17, pp. 384–397, New York, NY, USA, 2017._
Association for Computing Machinery. ISBN 9781450351188. doi: 10.1145/3131365.3131372.
[URL https://doi.org/10.1145/3131365.3131372.](https://doi.org/10.1145/3131365.3131372)

Kaichao You, Zhi Kou, Mingsheng Long, and Jianmin Wang. Co-tuning for transfer learning.
_Advances in Neural Information Processing Systems, 33, 2020._

Eric Zhan, Stephan Zheng, Yisong Yue, Long Sha, and Patrick Lucey. Generating multi-agent
trajectories using programmatic weak supervision. In International Conference on Learning
_[Representations, 2019. URL https://openreview.net/forum?id=rkxw-hAcFQ.](https://openreview.net/forum?id=rkxw-hAcFQ)_

Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and Alexander Ratner.
Wrench: A comprehensive benchmark for weak supervision. In Thirty-fifth Conference on Neural
_Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021._

Yivan Zhang, Nontawat Charoenphakdee, and Masashi Sugiyama. Learning from indirect observations, 2019.

Wenxuan Zhou, Hongtao Lin, Bill Yuchen Lin, Ziqi Wang, Junyi Du, Leonardo Neves, and Xiang
Ren. Nero: A neural rule grounding framework for label-efficient relation extraction. The Web
_Conference, 2020._


-----

## SUPPLEMENTARY MATERIALS FOR “CREATING TRAINING SETS VIA WEAK INDIRECT SUPERVISION”

The supplementary materials are organized as follows. In Appendix A, we provide a glossary of
variables and symbols used in this paper. In Appendix B, we provide the details of PLRM model.
In Appendix C and D, we provide the detailed proofs of Theorem 2 and Theorem 1 respectively. In
Appendix E, we provide the detailed examples and illustrations of label graph in WIS. In Appendix F
and G, we provide experimental details and additional experiment resulst respectively.

A GLOSSARY OF SYMBOLS

Table 4: Glossary of variables and symbols used in this paper.

Symbol Simplified Used for

_Xi_ The i-th data point, Xi
_m_ Number of data points _∈X_
_Yi_ The true desired label of the i-th data point, Yi
_y_ A semantic label, e.g., "dog" _∈Y_
The set of desired labels, = _y1, y2, . . ., yk_
_Y_ _Y_ _{_ _}_
_k_ Cardinality of Y, i.e., k = |Y|

_λj_ The j-th Indirect labeling function (ILF)
_n_ Number of ILF
_Yˆi[j]_ The output label of j-th ILF on i-th data point, _Y[ˆ]i[j]_ _j_
_Yˆi_ The concatenation of ILFs’ output, _Y[ˆ]i = [ Y[ˆ]i[1][,][ ˆ]Yi[2][, . . .,][∈Y][ ˆ]Y[λ]i[n][]]_
_yˆ[j]_ A semantic label in the label space of λj
_kYλλjj_ _kYjj_ Cardinality of the output space of ILFLabel label space of ILF λj, Yλj = {yˆ λ1[j][,],[ ˆ] i.e.y2[j][, . . .,], kλj[ ˆ]y =k[j] _λj |Y[}]_ _λj_ _|_
_Yˆ_ Union set of all the Yλj, _Y[ˆ] = {yˆ1, ˆy2, . . ., ˆykˆ[}]_
_kˆ_ Cardinality of the _Y[ˆ], i.e.,_ _k[ˆ] = |Y|[ˆ]_
_K_ Total number of labels, i.e., K = k[ˆ] + k

_Y¯_ _[i]_ Latent binary variable indicating whether the data should be assigned ˆyi .
_Y¯_ Concatenation of all latent binary variable, _Y[¯] = [ Y[¯]_ [1], . . ., _Y[¯]_ _k[ˆ]]_ _∈_ _Y[ˆ]_


_G_ Label graph, G = ( Y ∪Y[ˆ] _, E)_
The set of label relations, = (yi, yj, tyiyj ) _tyiyj_ _, i < j,_ _yi, yj_
_TE_ The set of label relation types, E _{ T = {t[e], t[o], t[sd]|_ _, t[sg] ∈T}_ _∀_ _∈V}_
_t[e]_ Exclusive label relation
_t[o]_ Overlap label relation
_t[sg]_ Subsuming label relation
_t[sd]_ Subsumed label relation
_N_ (y, _Y[ˆ])_ the set of non-exclusive neighbors of a given label y in _Y[ˆ]_

_ϕ_ A single dependency, or, factor function
Φ Concatenation of all individual dependency
_M_ Number of total dependencies
_θ_ A single parameter of the PGM
Θ Concatenation of all parameters of the PGM, Θ ∈ R[M]
Θˆ The learned parameters
Θ[∗] The golden parameters
_W_ The parameter of an end model
_Wˆ_ The learned parameters
_W_ _[∗]_ The golden parameters


-----

B DETAILS OF THE PLRM

We use Y, _Y[¯], and_ _Y[ˆ] to represent random vector. Then, we give the formal form of the PLRM as:_

_PΘ(Y,_ _Y,[¯]_ _Y[ˆ] )_ exp Θ[⊤]Φ(Y, _Y,[¯]_ _Y[ˆ] )_ _._ (7)
_∝_

Recall that Y is the unobserved true label, _Y[¯] is the binary random vector, each of whose binary value_ 
_Y¯_ _[i]_ reflects whether the data should be assigned seen label ˆyi, and _Y[ˆ] is the concatenated outputs_
of ILFs. Specifically, we enumerate Φ as below: _∈_ _Y[ˆ]_

1. (Pseudo accuracy dependency): _j_ [n], y _/_ _unknown_ _, ˆy_ _λj_, we have
_∀_ _∈_ _∈Y_ _{_ _}_ _∈Y_

_ϕ[Acc]y,y,jˆ_ [(][Y,][ ˆ]Y _[j]) := 1{Y = y ∧_ _Y[ˆ]_ _[j]_ = ˆy ∧ _yˆ ∈N_ (y, Yλj )}[1]

2. (Accuracy dependency): ∀j ∈ [n], ˆyi ∈ _Y ∩Y[ˆ]_ _j we have_

_ϕ[Acc]yˆi,j[( ¯]Y_ _[i],_ _Y[ˆ]_ _[j]) := 1_ _Y[¯]_ _[i]_ = 1 _Y_ _[j]_ = ˆyi
_{_ _∧_ [ˆ] _}_

3. (Label relation dependency between seen labels): _yˆi, ˆyj_ _, i < j_
_∀_ _∈_ _Y[ˆ]_

(a) if tyˆi ˆyj [=][ t][e][, we have]

_ϕ[e]yˆi,yˆj_ [( ¯]Y _[i],_ _Y[¯]_ _[j]) := −_ 1{Y[¯] _[i]_ = 1 ∧ _Y[¯]_ _[j]_ = 1}

(b) if tyˆi ˆyj [=][ t][o][, we have]

_ϕ[o]yˆi,yˆj_ [( ¯]Y _[i],_ _Y[¯]_ _[j]) := 1{Y[¯]_ _[i]_ = 1 ∧ _Y[¯]_ _[j]_ = 1}

(c) if tyˆi ˆyj [=][ t][sg][, we have]

_ϕ[sg]yˆi,yˆj_ [( ¯]Y _[i],_ _Y[¯]_ _[j]) := −_ 1{Y[¯] _[i]_ = 0 ∧ _Y[¯]_ _[j]_ = 1}

(d) if tyˆi ˆyj [=][ t][sd][, we have]

_ϕ[sd]yˆi,yˆj_ [( ¯]Y _[i],_ _Y[¯]_ _[j]) := −_ 1{Y[¯] _[i]_ = 1 ∧ _Y[¯]_ _[j]_ = 0}

4. (Label relation dependency between desired and seen labels): _y_ _/_ _unknown_ _, ˆyi_
_∀_ _∈Y_ _{_ _}_ _∈_ _Y[ˆ]_

(a) if tyyˆi [=][ t][e][, we have]

_ϕ[e]y,yˆi_ [(][Y,][ ¯]Y _[i]) := −_ 1{Y = y ∧ _Y[¯]_ _[i]_ = 1}

(b) if tyyˆi [=][ t][o][, we have]

_ϕ[o]y,yˆi_ [(][Y,][ ¯]Y _[i]) := 1{Y = y ∧_ _Y[¯]_ _[i]_ = 1}

(c) if tyyˆi [=][ t][sg][, we have]

_ϕ[sg]y,yˆi_ [(][Y,][ ¯]Y _[i]) := −_ 1{Y ̸= y ∧ _Y[¯]_ _[i]_ = 1}

(d) if tyyˆi [=][ t][sd][, we have]

_ϕ[sd]y,yˆi_ [(][Y,][ ¯]Y _[i]) := −_ 1{Y = y ∧ _Y[¯]_ _[i]_ = 0}

And example of our PLRM is shown in Fig. 4, where square with difference colors corresond to
different dependency/factor functions in PLRM.

_YPnM8ftCeM0g=</latexit>_

_YEYKVvSoLn6eyLFQupCGynwGasl71M/M/rJSa89lMm48RQSRaLwoQjE6HsezRkihLDp5Zgopi9FZExVpgYm1HJhuAtv7xKWhdV7Jau69V6jd5HEU4gVM4Bw+uoA530IAmEBDwDK/w5ijnxXl3PhatBSefOY/cD5/AKCakE4=</latexit>_ ¯ [1] _YjCSNmSBi3U3xMpFlpPRWA7BTZjverNxf+8XmLCaz9lMk4MlWS5KEw4MhGaf4+GTFi+NQSTBSztyIyxgoTYzMq2BC81ZfXSatS9mrl6n21VL/J4sjDGZzDJXhwBXW4gwY0gYCAZ3iFN0c5L86787FszTnZzCn8gfP5A6IekE8=</latexit>_ ¯ [2]

_YlE2pZCMld/T6Q0NGYSBrYzpDgy95M/M/rJDi49lOh4gS5YotFg0QSjMjse9IXmjOUE0so08LeStiIasrQZlSwIXjL6+SxkXZuyxX7iul6k0WRx5O4BTOwYMrqMId1KAODEJ4hld4c7Tz4rw7H4vWnJPNHMfOJ8/rO6QVg=</latexit>_ ˆ [1] _Y4ibUuhO1d/T6REGjORoe2UBEdm2ZuJ/3mdBAfXQcpVnCBTdLFokAgXI3f2vdvnmlEUE0sI1dze6tIR0YSizShvQ/CX14ljXLJvyxV7ivF6k0WRw5O4QwuwIcrqMId1KAOFCQ8wyu8Odp5cd6dj0XrmpPNnMAfOJ8/rnKQVw=</latexit>_ ˆ [2] _YBpG2FSObqz4mUKmMmyrediuLILHsz8T+vk2Bw1UtFGCfIQ7ZYFCSYERm35OB0JyhnFhCmRb2VsJGVFOGNqOCDcFbfvkvaZyVvYty5a5Sql5nceThCI7hFDy4hCrcQg3qwEDBE7zAq6OdZ+fNeV+05pxs5hB+wfn4Bq/2kFg=</latexit>_ ˆ [3]

Label Relation Dependency
True Accuracy Dependency
Pseudo Accuracy Dependency


Figure 4: PLRM.

1When ˆy /∈N (y, Yλj ), ϕ[Acc]y,y,jˆ [is always zero and will not occur in the model. Here we use this form for the]
sack of rigorous representation.


-----

C PROOF OF THEOREM 2

C.1 SIMPLIFYING THE NOTATION

To simplify the indexing of dependencies, we use Φ[1] to represent the concatenation of ϕ which
involves both Y and _Y[¯], Φ[2]_ to represent the concatenation of ϕ which involve both Y and **Λ[ˆ]**, and Φ[3]
to represent the concatenation of remaining ϕ which do not involve Y.

Specifically, Φ[1] consists of k × _k[ˆ] components corresponding to the dependency between the k desired_
labels and the _k[ˆ] seen labels. We use the subscript i, j to denote the dependency function between the_
desired label yi and seen label ˆyj, i.e.,

Φ[1]i,j [=][ ϕ]y[?]i,yˆj _[,]_

where ? is the corresponding relation.

Similarly, Φ[2] consists of k × ([P][n]j=1 _[k][j][)][ components corresponding to the dependency between the]_
_k desired labels and the kj seen labels output by the ILF λj (j ∈_ [n]), and we use Φ[2]i,j,l [to denote the]

dependency of yi and ˆyl[j][, and][ Φ]i,j[2] [= (Φ]i,j,l[2] [)][k]l=1[j] [to denote the dependency of][ y][i][ and][ ˆ]y[j].

According to Φ[1], Φ[2], and Φ[3], we also divide the parameter Θ into Θ[1] (with elements being Θ[1]i,j
correspondingly), Θ[2] (with elements being Θ[2]i,j [= (Θ]i,j,l[2] [)][k]l=1[j] [correspondingly), and][ Θ][3][, and the]
joint probability is then given as:

exp (Θ[1])[T]Φ[1](Y, _Y[¯] ) + (Θ[2])[⊤]Φ[2](Y,_ _Y[ˆ] ) + (Θ[3])[T]Φ[3]( Y,[¯]_ _Y[ˆ] )_
PΘ(Y, _Y,[¯]_ _Y[ˆ] ) =_ (8)
 

_Y_ _[′],Y[¯]_ _[′],Y[ˆ]_ _[′][ exp]_ (Θ[1])[⊤]Φ[1](Y _[′],_ _Y[¯]_ _[′]) + (Θ[2])[⊤]Φ[2](Y_ _[′],_ _Y[ˆ]_ _[′]) + (Θ[3])[T]Φ[3]( Y[¯]_ _[′],_ _Y[ˆ]_ _[′])_
 

P

Also, for notation convenience, we adopt following simplifications:

1. _yi_ _i_ [k] since = k, similarly, _yˆi_ _j_ _i_ [kj] and _yˆi_ _i_
_∀_ _∈Y →∀_ _∈_ _|Y|_ _∀_ _∈Y_ _→∀_ _∈_ _∀_ _∈_ _Y →∀[ˆ]_ _∈_

[k[ˆ]];

2. _λj_ _j_ [n] since we have n ILFs in total;
_∀_ _→∀_ _∈_

3. ϕytyiyji,yj _yi,yj_ [where][ t][ =][ t][y]i[y]j [and can be seen from the subscript of the dependency.]

_[→]_ _[ϕ][t]_

C.2 PROPOSITIONS AND LEMMAS

First, we state some propositions and lemmas that will be useful in the proof to come.


**Proposition 1 (Multi-class classification). For a multi-class classification task, ∀yi, yj ∈Y, we have**
_tyiyj = t[e]. Similarly, ∀yˆa, ˆyb ∈_ _Y[ˆ], we have tyˆa ˆyb_ [=][ t][e][.]

**Lemma 1.tyj ˆyl** _For a consistent label graph G and ∀yˆl ∈_ _Y[ˆ], ∀yi, yj ∈Y, if tyi ˆyl_ [=][ t][o][, we have]

_[̸][=][ t][sg][ .]_

_Proof. For ∀yi, yj ∈Y, based on Proposition 1, we know tyiyj = t[e], which implies (1) the_
intersection of the sets labeled bythe intersection of the sets labeled by yi and yi and yj is empty. For ˆyl is not empty. If ∀yˆl ∈ tyY[ˆ]j ˆy, ifl [=] t[ t]yi ˆ[sg]yl [, which implies (3)][=][ t][o][, we have (2)]
_ytyji ⫌ ˆyl_ [=]yˆl[ t]. Based on (2)(3), we have[o][, t]yj ˆyl _[̸][=][ t][sg][.]_ _yi ∩_ _yj ̸= ∅, which is contradictory to (1). Thus, we prove when_

**Lemma 2. For an informative ILF λj and given any yd ∈Y, there exists some ˆyl ∈Yj, such that,**
Φ[2]d,j,l[(][y][d][,][ ˆ]y) = 0, ∀l ∈ [kj].

_Proof. Because ILF λj is informative, we know there exists one ˆya ∈Yj such that ˆya is exclusive to_
_yd, i.e., ˆya /∈N_ (yd, Yj). Therefore, for any ˆyl ∈ _Y[ˆ], either ˆya ̸= ˆyl, or ˆyl = ˆya /∈Yj, which leads to_
the conclusion by the definition of Φ[2]d,j,l [=][ ϕ]y[Acc]d,yˆl,j[.]


-----

C.3 DEFINITIONS

Before the main proof, we connect the indistinguishablity of label relation structure with the dependency structure of PLRM by introducing the concept of symmetry as follows:

**Definition 4the following equation holds: (Symmetry). For yi, yj ∈Y, we say yi and yj have symmetric dependency structure if**

Φ[1]i,l [= Φ]j,l[1] _[,][ ∀][l][ ∈]_ _k[ˆ];_

Φ[2]i,a,b [= Φ]j,a,b[2] _[,][ ∀][a][ ∈]_ [[][n][]][, b][ ∈] [[][k][a][]][.] (9)

Based on the construction of PLRM, we know that forstatement in Theorem 2) is equivalent to yi and yj have symmetric dependency structure. ∀yi, yj ∈Y, ∀yˆb ∈ _Y[ˆ], tyi ˆyb_ [=][ t]yj ˆyb [(the]

C.4 EQUIVALENT STATEMENT OF THEOREM 2

Our main result states that asymmetric is equivalent to distinguishable as in the following theorem,
which can readily be seen to be identical to Theorem 2 in the main body of the paper:

**Theorem 3. For a probability model defined as Eq. (8) induced from a consistent label graph and**
_informative ILFs, for any pair ofasymmetric dependency structure. yi, yj ∈Y, yi and yj are distinguishable if and only if they have_

C.5 PROOF OF THE NECESSITY IN THEOREM 3: NECESSARY CONDITION

We first prove that for anynecessary condition of that they are distinguishable. yi, yj ∈Y, yi and yj have asymmetric dependency structure is the

_Proof of Theorem 3. We prove this theorem by reduction to absurdity. Suppose yi and yj are sym-_
metric. Then, by Eq. (8), the distribution of Y condition on any _Y[¯] and_ _Y[ˆ] can be calculated as_
follows:

_Y,_ _Y[ˆ] )_
PΘ(Y = yi _Y,[¯]_ _Y[ˆ] ) =_ [P][Θ][(][y][i][,][ ¯] _._
_|_ PΘ( Y,[¯] _Y[ˆ] )_

On the other hand, applying Y = yi in the definition of Φ[2] leads to

Φ[2]r,a,l[(][y][i][,][ ·][) = 0][,][ ∀][r][ ∈] [[][k][]][, r][ ̸][=][ i,][ ∀][a][ ∈] [[][n][]][,][ ∀][l][ ∈] [[][k][a][]][.]

We further separate Φ[1] into (Φ[1]i [)]i[k]=1[, where][ Φ][1]i [collects all the dependency in][ Φ][1][ with][ y][i][ involved,]
i.e.,

Φ[1]i [= (Φ]i,j[1] [)]kjˆ=1[,]

with the corresponding parameters respectively denoted as Θ[1]i [with][ Θ][1][ = (Θ]i[1][)]i[k]=1[. Similarly,][ Φ][2][ is]
also divided into (Φ[2]i [)]i[k]=1 [following the same routine and][ Θ][2][ is respectively divided into][ (Θ]i[2][)]i[k]=1[.]
Specifically, if yi and yj are symmetric, we further have

Φ[1]i [= Φ]j[1][,][ Φ][2]i [= Φ]j[2][.]

Based on the notation, PΘ(Y = yi|Y,[¯] _Y[ˆ] ) can then be represented as_

PΘ(Y = yi|Y,[¯] _Y[ˆ] )_ exp (Θ[1])[⊤]Φ[1](Y _[′],_ _Y[¯] ) + (Θ[2])[⊤]Φ[2](Y_ _[′],_ _Y[ˆ] ) + (Θ[3])[T]Φ[3]( Y,[¯]_ _Y[ˆ] )_

XY _[′]_  [!]


(Θ[1]l [)][T][Φ]l[1][(][y][i][,][ ¯]Y ) +
_l=1_

X


(Θ[2]l [)][T][Φ]l[2][(][y][i][,][ ˆ]Y ) + (Θ[3])[T]Φ[3]( Y,[¯] _Y[ˆ] )_
_l=1_

X


= exp


-----

which further leads to

PΘ(Y = yi|Y,[¯] _Y[ˆ] )_


exp (Θ[1])[⊤]Φ[1](Y _[′],_ _Y[¯] ) + (Θ[2])[⊤]Φ[2](Y_ _[′],_ _Y[ˆ] )_

XY _[′]_  [!]


(Θ[1]l [)][T][Φ]l[1][(][y][i][,][ ¯]Y ) + (Θ[2]i [)][T][Φ]i[2][(][y][i][,][ ˆ]Y )
_l=1_

X


(10)

(11)

(12)


= exp


which is independent of Θ[3]. Similarly,

PΘ(Y = yj|Y,[¯] _Y[ˆ] )_ exp (Θ[1])[⊤]Φ[1](Y _[′],_ _Y[¯] ) + (Θ[2])[⊤]Φ[2](Y_ _[′],_ _Y[ˆ] )_

XY _[′]_  [!]


(Θ[1]l [)][T][Φ]l[1][(][y][i][,][ ¯]Y ) + (Θ[2]j [)][T][Φ][2]j [(][y][j][,][ ˆ]Y )
_l=1_

X


= exp

and ∀l ∈ [k]/{i, j},


exp (Θ[1])[⊤]Φ[1](Y _[′],_ _Y[¯] ) + (Θ[2])[⊤]Φ[2](Y_ _[′],_ _Y[ˆ] )_

XY _[′]_  [!]


PΘ(Y = yl|Y,[¯] _Y[ˆ] )_


(Θ[1]l [)][T][Φ]l[1][(][y][i][,][ ¯]Y ) + (Θ[2]l [)][T][Φ]l[2][(][y][l][,][ ˆ]Y )
_l=1_

X


= exp


Let Θ[˜] be defined as follows:
Θ˜ [1]i [= Θ]j[1][,][ ˜]Θ[1]j [= Θ]i[1][,][ ˜]Θ[1]l [= Θ]l[1][,][ ∀][l /]∈{i, j},

Θ˜ [2]i [= Θ]j[2][,][ ˜]Θ[2]j [= Θ]i[2][,][ ˜]Θ[2]l [= Θ]l[2][,][ ∀][l /]∈{i, j},
and
Θ˜ [3] = Θ[3].
We then have

PΘ(Y = yi|Y,[¯] _Y[ˆ] )_

P ˜Θ[(][Y][ =][ y][j][|][ ¯]Y, _Y[ˆ] )_

_Y_ _[′][ exp]_ (Θ[˜] [1])[⊤]Φ[1](Y _[′],_ _Y[¯] ) + (Θ[˜]_ [2])[⊤]Φ[2](Y _[′],_ _Y[ˆ] )_

=

PY _[′][ exp]_ (Θ[1])[⊤]Φ[1](Y _[′],_ _Y[¯] ) + (Θ[2])[⊤]Φ[2](Y_ _[′],_ _Y[ˆ] )_

_· exp ((ΘP_ [1]i [)][T][(Φ]i[1][(][y][i][,][ ¯]Y ) − Φ[1]j [(][y][j][,][ ¯]Y )) + (Θ[1]j [)][T][(Φ][1]j [(][y][i][,][ ¯]Y ) − Φ[1]i [(][y][j][,][ ¯]Y )) + (Θ[2]i [)][T][(Φ]i[2][(][y][i][,][ ˆ]Y ) − Φ[2]j [(][y][j][,][ ˆ]Y )))

_Y_ _[′][ exp]_ (Θ[˜] [1])[⊤]Φ[1](Y _[′],_ _Y[¯] ) + (Θ[˜]_ [2])[⊤]Φ[2](Y _[′],_ _Y[ˆ] )_

= _._

PY _[′][ exp]_ (Θ[1])[⊤]Φ[1](Y _[′],_ _Y[¯] ) + (Θ[2])[⊤]Φ[2](Y_ _[′],_ _Y[ˆ] )_

Similarly,P  


PΘ(Y = yj|Y,[¯] _Y[ˆ] )_

P ˜Θ[(][Y][ =][ y][i][|][ ¯]Y, _Y[ˆ] )_

_Y_ _[′][ exp]_ (Θ[˜] [1])[⊤]Φ[1](Y _[′],_ _Y[¯] ) + (Θ[˜]_ [2])[⊤]Φ[2](Y _[′],_ _Y[ˆ] )_

PY _[′][ exp]_ (Θ[1])[⊤]Φ[1](Y _[′],_ _Y[¯] ) + (Θ[2])[⊤]Φ[2](Y_ _[′],_ _Y[ˆ] )_

_· exp ((ΘP_ [1]j [)][T][(Φ][1]j [(][y][j][,][ ¯]Y ) − Φ[1]i [(][y][i][,][ ¯]Y )) + (Θ[1]i [)][T][(Φ]i[1][(][y][j][,][ ¯]Y ) − Φ[1]j [(][y][i][,][ ¯]Y )) + (Θ[2]j [)][T][(Φ][2]j [(][y][j][,][ ˆ]Y ) − Φ[2]i [(][y][i][,][ ˆ]Y )))

_Y_ _[′][ exp]_ (Θ[˜] [1])[⊤]Φ[1](Y _[′],_ _Y[¯] ) + (Θ[˜]_ [2])[⊤]Φ[2](Y _[′],_ _Y[ˆ] )_

_._

PY _[′][ exp]_ (Θ[1])[⊤]Φ[1](Y _[′],_ _Y[¯] ) + (Θ[2])[⊤]Φ[2](Y_ _[′],_ _Y[ˆ] )_

P  


-----

and ∀l ∈ [k]/{i, j},

PΘ(Y = yl|Y,[¯] _Y[ˆ] )_

P ˜Θ[(][Y][ =][ y][l][|][ ¯]Y, _Y[ˆ] )_

Similarly, we have


_Y_ _[′][ exp]_ (Θ[˜] [1])[⊤]Φ[1](Y _[′],_ _Y[¯] ) + (Θ[˜]_ [2])[⊤]Φ[2](Y _[′],_ _Y[ˆ] )_

_._

PY _[′][ exp]_ (Θ[1])[⊤]Φ[1](Y _[′],_ _Y[¯] ) + (Θ[2])[⊤]Φ[2](Y_ _[′],_ _Y[ˆ] )_

P  


_Y_ _[′][ exp]_ (Θ[˜] [1])[⊤]Φ[1](Y _[′],_ _Y[¯] ) + (Θ[˜]_ [2])[⊤]Φ[2](Y _[′],_ _Y[ˆ] )_

_._

PY _[′][ exp]_ (Θ[1])[⊤]Φ[1](Y _[′],_ _Y[¯] ) + (Θ[2])[⊤]Φ[2](Y_ _[′],_ _Y[ˆ] )_

P  


PΘ(Y = unknown|Y,[¯] _Y[ˆ] )_

P ˜Θ[(][Y][ =][ unknown][|][ ¯]Y, _Y[ˆ] )_

Therefore, we have


PΘ(Y = yi _Y,[¯]_ _Y[ˆ] )_ _Y,_ _Y[ˆ] )_ _Y,_ _Y[ˆ] )_
_|_ = [P][Θ][(][Y][ =][ y][j][|][ ¯] = [P][Θ][(][Y][ =][ y][|][ ¯] _,_ _y_ _/_ _yi, yj_ _._

P ˜Θ[(][Y][ =][ y][j][|][ ¯]Y, _Y[ˆ] )_ P ˜Θ[(][Y][ =][ y][i][|][ ¯]Y, _Y[ˆ] )_ P ˜Θ[(][Y][ =][ y][|][ ¯]Y, _Y[ˆ] )_ _∀_ _∈Y_ _{_ _}_

Since
PΘ(Y = yi|Y,[¯] _Y[ˆ] ) + PΘ(Y = yj|Y,[¯]_ _Y[ˆ] ) +_ PΘ(Y = yl|Y,[¯] _Y[ˆ] ) = 1,_

_lX≠_ _i,j_

and
P ˜Θ[(][Y][ =][ y][j][|][ ¯]Y, _Y[ˆ] ) + P ˜Θ[(][Y][ =][ y][i][|][ ¯]Y,_ _Y[ˆ] ) +_ P ˜Θ[(][Y][ =][ y][l][|][ ¯]Y, _Y[ˆ] ) = 1,_

_lX≠_ _i,j_

we obtain that
PΘ(Y = yi|Y,[¯] _Y[ˆ] ) = P ˜Θ[(][Y][ =][ y][j][|][ ¯]Y,_ _Y[ˆ] )_

PΘ(Y = yj|Y,[¯] _Y[ˆ] ) = P ˜Θ[(][Y][ =][ y][i][|][ ¯]Y,_ _Y[ˆ] )_

PΘ(Y = yl|Y,[¯] _Y[ˆ] ) = P ˜Θ[(][Y][ =][ y][l][|][ ¯]Y,_ _Y[ˆ] ),_
which indicates yi and yj indistinguishable, and leads to a contradictory.

The proof is completed.

C.6 PROOF OF THEOREM 3: SUFFICIENT CONDITION

We then prove that for anysufficient condition of that they are distinguishable. yi, yj ∈Y, yi and yj have asymmetric dependency structure is the

_Proof. We use the same notations (Θ[1]i_ [)]i[k]=1[,][ (Θ]i[2][)]i[k]=1[, and][ Θ][3][ in Appendix][ C.5][ to denote the separation]
of the parameter Θ. Let Θ be any parameter satisfying that there exists a parameter Θ[˜], such that Eq.
(4-5) holds. By Eqs. (10), (11), and Eq. (12) together with Eqs. (4-5), we have ∀r ∈ [k], r ̸= i, j,

exp ((Θ[1]i [)][T][Φ]i[1][(][y][i][,][ ¯]Y ) + (Θ[1]j [)][T][Φ][1]j [(][y][i][,][ ¯]Y ) + (Θ[2]i [)][T][Φ]i[2][(][y][i][,][ ˆ]Y ))

exp ((Θ[˜] [1]i [)][T][Φ]i[1][(][y][j][,][ ¯]Y ) + (Θ[˜] [1]j [)][T][Φ][1]j [(][y][j][,][ ¯]Y ) + (Θ[˜] [2]j [)][T][Φ][2]j [(][y][j][,][ ˆ]Y ))


_i_ [)][T][Φ]i[1][(][y][j][,][ ¯]Y ) + (Θ[1]j [)][T][Φ][1]j [(][y][j][,][ ¯]Y ) + (Θ[2]j [)][T][Φ][2]j [(][y][j][,][ ˆ]Y ))
= [exp ((Θ][1]

exp ((Θ[˜] [1]i [)][T][Φ]i[1][(][y][i][,][ ¯]Y ) + (Θ[˜] [1]j [)][T][Φ][1]j [(][y][i][,][ ¯]Y ) + (Θ[˜] [2]i [)][T][Φ]i[2][(][y][i][,][ ˆ]Y ))

_i_ [)][T][Φ]i[1][(][y][r][,][ ¯]Y ) + (Θ[1]j [)][T][Φ][1]j [(][y][r][,][ ¯]Y )) _i_ [)][T][Φ]i[1][(][y][j][,][ ¯]Y ) + (Θ[1]j [)][T][Φ][1]j [(][y][i][,][ ¯]Y ))
= [exp ((Θ][1] = [exp ((Θ][1]

exp ((Θ[˜] [1]i [)][T][Φ]i[1][(][y][r][,][ ¯]Y ) + (Θ[˜] [1]j [)][T][Φ][1]j [(][y][r][,][ ¯]Y )) exp ((Θ[˜] [1]i [)][T][Φ]i[1][(][y][j][,][ ¯]Y ) + (Θ[˜] [1]j [)][T][Φ][1]j [(][y][i][,][ ¯]Y ))


By simple rearranging, we have

((Θ[1]i [)][T][Φ]i[1][(][y][i][,][ ¯]Y ) + (Θ[1]j [)][T][Φ][1]j [(][y][i][,][ ¯]Y ) + (Θ[2]i [)][T][Φ]i[2][(][y][i][,][ ˆ]Y ) + (Θ[2]j [)][T][Φ][2]j [(][y][i][,][ ˆ]Y ))

_−_ ((Θ[˜] [1]i [)][T][Φ]i[1][(][y][j][,][ ¯]Y ) + (Θ[˜] [1]j [)][T][Φ][1]j [(][y][j][,][ ¯]Y ) + (Θ[˜] [2]i [)][T][Φ]i[2][(][y][j][,][ ˆ]Y ) + (Θ[˜] [2]j [)][T][Φ][2]j [(][y][j][,][ ˆ]Y ))

=((Θ[1]i [)][T][Φ]i[1][(][y][j][,][ ¯]Y ) + (Θ[1]j [)][T][Φ][1]j [(][y][j][,][ ¯]Y ) + (Θ[2]i [)][T][Φ]i[2][(][y][j][,][ ˆ]Y ) + (Θ[2]j [)][T][Φ][2]j [(][y][j][,][ ˆ]Y ))


_−_ ((Θ[˜] [1]i [)][T][Φ]i[1][(][y][i][,][ ¯]Y ) + (Θ[˜] [1]j [)][T][Φ][1]j [(][y][i][,][ ¯]Y ) + (Θ[˜] [2]i [)][T][Φ]i[2][(][y][i][,][ ˆ]Y ) + (Θ[˜] [2]j [)][T][Φ][2]j [(][y][i][,][ ˆ]Y ))

=((Θ[1]i [)][T][Φ]i[1][(][y][j][,][ ¯]Y ) + (Θ[1]j [)][T][Φ][1]j [(][y][i][,][ ¯]Y )) − ((Θ[˜] [1]i [)][T][Φ]i[1][(][y][j][,][ ¯]Y ) + (Θ[˜] [1]j [)][T][Φ][1]j [(][y][i][,][ ¯]Y )). (13)


-----

By the equality between the second term and the third term in Eq. (13), we obtain that

(Θ[1]j [)][T][Φ][1]j [(][y][i][,][ ¯]Y ) − (Θ[˜] [1]i [)][T][Φ]i[1][(][y][j][,][ ¯]Y )

=((Θ[1]j [)][T][Φ][1]j [(][y][j][,][ ¯]Y ) + (Θ[2]j [)][T][Φ][2]j [(][y][j][,][ ˆ]Y )) − ((Θ[˜] [1]i [)][T][Φ]i[1][(][y][i][,][ ¯]Y ) + (Θ[˜] [2]i [)][T][Φ]i[2][(][y][i][,][ ˆ]Y )). (14)

We further set _Y[¯] in Eq. (14) respectively to el (the one hot vector with its l-th position being 1) and_
**0 for any fixed l ∈** [k[ˆ]], i.e.,

((Θ[1]j [)][T][Φ][1]j [(][y][i][,][ e][l][)][ −] [(Θ][1]j [)][T][Φ][1]j [(][y][i][,][ 0][))][ −] [((˜]Θ[1]i [)][T][Φ]i[1][(][y][j][,][ e][l][)][ −] [(˜]Θ[1]i [)][T][Φ]i[1][(][y][j][,][ 0][))]

=((Θ[1]j [)][T][Φ][1]j [(][y][j][,][ e][l][)][ −] [(Θ][1]j [)][T][Φ][1]j [(][y][j][,][ 0][))][ −] [((˜]Θ[1]i [)][T][Φ]i[1][(][y][i][,][ e][l][)][ −] [(˜]Θ[1]i [)][T][Φ]i[1][(][y][i][,][ 0][))][,]

which by simple rearranging further leads to


Θ[1]j,l[(Φ][1]j,l[(][y][j][,][ 1)][ −] [Φ][1]j,l[(][y][j][,][ 0)][ −] [Φ][1]j,l[(][y][i][,][ 1)) = ˜]Θ[1]i,l[(Φ][1]i,l[(][y][i][,][ 1)][ −] [Φ][1]i,l[(][y][i][,][ 0)][ −] [Φ][1]i,l[(][y][j][,][ 1))][.]

Since Θ[1]j,l[,][ ˜]Θ[1]i,l _[>][ 0][, and by definition we have]_

_|Φ[1]j,l[(][y][j][,][ 1)][ −]_ [Φ][1]j,l[(][y][j][,][ 0)][ −] [Φ][1]j,l[(][y][i][,][ 1)][|][ = 1][,]

and
_|Φ[1]i,l[(][y][i][,][ 1)][ −]_ [Φ]i,l[1] [(][y][i][,][ 0)][ −] [Φ]i,l[1] [(][y][j][,][ 1)][|][ = 1][,]

we obtain Θ[1]j,l [= ˜]Θ[1]i,l[, and]


Φ[1]j,l[(][y][j][,][ 1)][ −] [Φ]j,l[1] [(][y][j][,][ 0)][ −] [Φ]j,l[1] [(][y][i][,][ 1) = Φ]i,l[1] [(][y][i][,][ 1)][ −] [Φ]i,l[1] [(][y][i][,][ 0)][ −] [Φ]i,l[1] [(][y][j][,][ 1)][.] (15)

Therefore, either tyj ˆyl _[∈{][t][o][, t][sd][, t][sg][}][ and][ t]yi ˆyl_ _[∈{][t][o][, t][sd][, t][sg][}][, or][ t]yj ˆyl_ [=][ t][e][ and][ t]yi ˆyl [=][ t][e][, which]
by definition further indicates that Φ[2]i [= Φ]j[2] [(recall the way we build dependency between][ Y][ and][ ˆ]Y ).

As l is arbitrarily picked, we then have Θ[1]j [is equal to][ ˜]Θ[1]i [component-wisely.]

By the equality between the first term and the third term in Eq. (13) and following exact the same
routine, we also have Θ[˜] [1]j [= Θ]i[1][.]

On the other hand, for any r ∈ [k[ˆ]], fixing _Y[¯] and_ _Y[ˆ]_ _[s]_ (∀s ̸= r), and setting _Y[ˆ]r = ˆyl[r]_ [(][l][ ∈] _[k][r][,]_
_yˆl[r]_

_[∈N]_ [(][y][j][,][ Y][l][)][) in Eq. (][14][), we have]

(Θ[1]j [)][T][Φ]j[1][(][y][j][,][ ¯]Y ) + (Θ[˜] [1]i [)][T][Φ]i[1][(][y][j][,][ ¯]Y ) + Θ[2]j,r,l[Φ]j,r,l[2] [(][y][j][,][ ˆ]yl[r][) +] Θ[2]j,s[Φ]j,s[2] [(][y][j][, Y][ s][)]
Xs≠ _r_

=(Θ[1]j [)][T][Φ]j[1][(][y][i][,][ ¯]Y ) + (Θ[˜] [1]i [)][T][Φ]i[1][(][y][i][,][ ¯]Y ) + Θ[˜] [2]i,r,l[Φ]i,r,l[2] [(][y][i][,][ ˆ]yl[r][) +] Θ˜ [2]i,s[Φ]i,s[2] [(][y][i][,][ ˆ]Y _[s])._
Xs≠ _r_

On the other hand, by Lemma 2, there exists some p, s.t., ˆyp[r] _∈N[/]_ (yj, Yr) (which by Φ[2]i [= Φ]j[2] [further]
leads to ˆyp[r] _∈N[/]_ (yi, Yr)). Setting _Y[ˆ]r = ˆyl[r]_ [leads to]

(Θ[1]j [)][T][Φ][1]j [(][y][j][,][ ¯]Y ) + (Θ[˜] [1]i [)][T][Φ]i[1][(][y][j][,][ ¯]Y ) + Θ[2]j,s[Φ][2]j,s[(][y][j][, Y][ s][)]
Xs≠ _r_

=(Θ[1]j [)][T][Φ][1]j [(][y][i][,][ ¯]Y ) + (Θ[˜] [1]i [)][T][Φ]i[1][(][y][i][,][ ¯]Y ) + Θ˜ [2]i,s[Φ][2]i,s[(][y][i][,][ ˆ]Y _[s])._
Xs≠ _r_


Subtracting the above two equations leads to Θ[2]j,a,l [= ˜]Θ[2]i,a,l[. Since][ a][ and][ l][ are arbitrarily picked, we]
conclude that Θ[2]j [= ˜]Θ[2]i [. Following the same routine, we also have][ Θ]i[2] [= ˜]Θ[2]j [.]

Therefore, by applying Θ[1]j [= ˜]Θ[1]i [,][ Θ]i[1] [= ˜]Θ[1]j [,][ Θ][2]j [= ˜]Θ[2]i [, and][ Θ]i[2] [= ˜]Θ[2]j [in Eq. (][13][), we have]

(Θ[1]i [)][T][Φ]i[1][(][y][i][,][ ¯]Y ) − (Θ[1]i [)][T][Φ]j[1][(][y][j][,][ ¯]Y ) = (Θ[1]i [)][T][Φ]i[1][(][y][j][,][ ¯]Y ) − (Θ[1]i [)][T][Φ]j[1][(][y][i][,][ ¯]Y ),

(Θ[1]j [)][T][Φ][1]j [(][y][j][,][ ¯]Y ) − (Θ[1]j [)][T][Φ][1]i [(][y][i][,][ ¯]Y ) = (Θ[1]j [)][T][Φ][1]j [(][y][i][,][ ¯]Y ) − (Θ[1]j [)][T][Φ][1]i [(][y][j][,][ ¯]Y ).


-----

Let _Y[¯] = 1kˆ_ [(i.e., the][ ˆ]k-dimension all 1 vector), we have

(Θ[1]i [)][T][((Φ]i[1][(][y][i][,][ 1]k[ˆ][)][ −] [Φ]i[1][(][y][j][,][ 1]k[ˆ][))][ −] [((Φ]j[1][(][y][j][,][ 1]k[ˆ][)][ −] [Φ]j[1][(][y][i][,][ 1]k[ˆ][)))) = 0][,] (16)

(Θ[1]j [)][T][((Φ][1]i [(][y][i][,][ 1]k[ˆ][)][ −] [Φ]i[1][(][y][j][,][ 1]k[ˆ][))][ −] [((Φ]j[1][(][y][j][,][ 1]k[ˆ][)][ −] [Φ]j[1][(][y][i][,][ 1]k[ˆ][)))) = 0][.] (17)


Since yi and yj are asymmetric, we have that there exists l, such that tyi ˆyl _[̸][=][ t]yj ˆyl[. Concretely, by]_
Eq. (15), we have tyi ˆyl _[∈{][t][o][, t][sd][, t][sg][}][,][ t]yj ˆyl_ [=][ {][t][o][, t][sd][, t][sg][}][, and][ t]yi ˆyl _[̸][=][ t]yj ˆyl_ [. On the other hand,]

Φ[1]i,l[(][y][i][,][ 1)][ −] [Φ][1]i,l[(][y][j][,][ 1)) = Φ][1]j,l[(][y][j][,][ 1)][ −] [Φ][1]j,l[(][y][i][,][ 1)][,]

if and only if tyi ˆyl [=][ t][o][,][ t]yj ˆyl [=][ t][sg][, or][ t]yj ˆyl [=][ t][o][,][ t]yi ˆyl [=][ t][sg][, which contradicts Lemma][ 1][.]

Therefore,
Φ[1]i,l[(][y][i][,][ 1)][ −] [Φ][1]i,l[(][y][j][,][ 1))][ ̸][= Φ][1]j,l[(][y][j][,][ 1)][ −] [Φ][1]j,l[(][y][i][,][ 1)][.]

In this case, solutions of Θ[1]i _[,][ Θ]j[1]_ [subject to respectively Eqs. (][16][) and (][17][) lie along a zero-measure]
set.

The proof is completed.

D PROOF OF THEOREM 1

D.1 LEARNING ALGORITHM

We first present the algorithm for producing Θ[ˆ] and _W[ˆ]_ in Algorithm 1.

**Algorithm 1 WIS**


**Require:Θˆ** Θ Step size0. _η, dataset D ⊂X_, and initial parameter Θ0.

_→_
**for all X ∈** _D do_

Independently sample (Y, _Y,[¯]_ _Y[ˆ] ) from π ˆΘ[, and][ (][Y][ ′][,][ ¯]Y_ _[′],_ _Y[ˆ]_ _[′]) from π ˆΘ_ [conditionally given][ ˆ]Y _[′]_ =
_Yˆ (X)._

Θˆ _←_ Θ +[ˆ] _η(Φ(Y,_ _Y,[¯]_ _Y[ˆ] ) −_ Φ(Y _[′],_ _Y[¯]_ _[′],_ _Y[ˆ]_ _[′]))._

Compute _W[ˆ]_ as described in (3) using Θ[ˆ] .

**output (Θ[ˆ]** _,_ _W[ˆ]_ )

D.2 ASSUMPTIONS

First, the problem distribution π[∗] needs to be accurately modeled by some distribution Θ[∗] in the
family that we are trying to learn:

_∃Θ[∗]_ s.t. ∀(Y, _Y[ˆ] ), p(X,Y )∼π∗_ (Y, _Y[ˆ] ) = pθ[∗]_ (Y, _Y[ˆ] )._ (18)

Secondly, given an example (X, Y ) ∼ _π[∗], we assume Y is independent of X given_ _Y[ˆ] (X):_

(X, Y ) ∼ _π[∗]_ _⇒_ _Y ⊥_ _X |_ _Y[ˆ] (X)._ (19)
This assumption encodes the idea that while the ILFs can be arbitrarily dependent on the features,
they provide sufficient information to accurately identify the true label vector. Then, for any Θ,
accurately learning Θ from data distribution is possible. That is, there exists an unbiased estimator
Θ(ˆ _D) which is a function of the dataset D of i.i.d from πΘ, such that, for any Θ and some c > 0,_

_I_
**Cov(Θ([ˆ]** _D))_ (20)
_⪯_ 2c _D_

_|_ _|_ _[.]_

And we are reasonably certain in our guess of latent variables, i.e., Y and _Y[¯] . That is, for any Θ, Θ[∗],_


_kˆ_ 12

_i=1(mi + K −_ 1)Var(Y, ¯Y,Y[ˆ] )∼πΘ [( ¯]Y _[i]|Y[ˆ] = Y[ˆ]_ _[∗])[2]#_

X


(ni + k[ˆ])Var(Y, ¯Y,Y[ˆ] )∼πΘ [(][1][Y][ =][y]i _[|][ ˆ]Y = Y[ˆ]_ _[∗])[2]_ +
_i=1_

X


E ˆY _[∗]∼Θ[∗]_


_c_

_._ (21)
2M


-----

We also assume that the output of the last layer of end model hW has bounded ℓ norm, that is, for
_∞_
any possible parameter W,
_hW_ _H._ (22)
_∥_ _∥∞_ _≤_

Finally, we assume that solving Eq. (3) has bounded generalization risk such that for some χ > 0,
solution _W[ˆ]_ satisfies
E ˆW _ℓΘˆ_ [( ˆ]W ) min Θ[(][W] [)] _χ._ (23)
_−_ _W_ _[ℓ]_ [ˆ] _≤_
h i

D.3 PROOF OF THEOREM 1

To begin with, we state two basic lemmas needed for proofs throughout this section:
**Lemma D.1. Let x1, x2 be two binary random variable. Then we have variance of product of x1**
_and x2 can be bounded as_

**Var [x1x2]** **Var [x1] + Var [x2] .**
_≤_

**Lemma D.2. Let Y be a random vector and ∥·∥s be the spectral norm. Then we have**


**Cov(Y, Y )** _s_
_∥_ _∥_ _≤_


**Var(Yi).**


Then, we borrow two lemmas from (Ratner et al., 2016), which are slightly different from the original
ones but can be easily proved following the same derivations:
**Lemma D.3. [Lemma D.1 in (Ratner et al., 2016)] Given a family of maximum-entropy distributions**


_πΘ(Y,_ _Y,[¯]_ _Y[ˆ] ) =_


1

exp (Θ[T]Φ(Y, _Y,[¯]_ _Y[ˆ] ))._
_ZΘ_


_If we let J be the maximum expected log-likelihood objective, under another distribution π[∗], for the_
_event associated with the observed labeling function values_ _Y[ˆ],_

_J(Θ) = E(Y ∗, ¯Y_ _[∗],Y[ˆ]_ _[∗])∼π[∗]_ log P(Y, ¯Y,Y[ˆ] )∼πΘ _Yˆ = Y[ˆ]_ _[∗][i]_ _,_
h 

_then its Hessian can be calculated as_

_∇[2]J(Θ) = E(Y ∗, ¯Y_ _[∗],Y[ˆ]_ _[∗])∼π[∗]_ **Cov(Y,Y, ¯** _Y[ˆ] )∼πΘ_ _ϕ(Y,_ _Y,[¯]_ _Y[ˆ] ) |_ _Y[ˆ] = Y[ˆ]_ _[∗][i]−Cov(Y,Y, ¯_ _Y[ˆ] )∼πΘ_ [(][ϕ][(][Y,][ ¯]Y, _Y[ˆ] ))._
h 

**Lemma D.4. [Lemma D.4 in (Ratner et al., 2016)] Suppose that we are looking at a WIS maximum**
_likelihood estimation problem and the objective function J(Θ) is strongly concave with concavity_
_parameter c > 0. If we run stochastic gradient descent using unbiased samples from a true_
_distribution πΘ∗_ _, then if we set step size as_

_η =_ _[cϵ][2]_

4 _[,]_


_and run (using a fresh sample at each iteration) for T steps, where_


2 Θ0 Θ[∗]
_∥_ _−_ _∥[2]_


_T =_


_c[2]ϵ[2][ log]_


_We can bound the expected parameter estimation error with_

2
E Θ − Θ[∗] _≤_ _Mϵ[2],_ (24)

_where M is the dimension of Θ._

[ˆ]

Based on Lemma D.4, in order to obtain the optimization error with respect to the estimated Θ[ˆ]
produced by Algorithm 1, we only need to show that the WIS object function J(Θ)[2] is strongly
concave. We prove this through the following lemma, which is a non-trivial extension of Lemma D.3
in (Ratner et al., 2016) given the fact that we have multiple latent variables and relatively complex
dependency structures with comparison to (Ratner et al., 2016):

2Note that, in the Eq. (2) of the main body of the paper, we are minimizing −J(Θ), which is equivalent to
maximizing J(Θ) as discussed here.


-----

**Lemma D.5. [Extension of Lemma D.3 in (Ratner et al., 2016)] With conditions (20) and (21), the**
_WIS objective function J(Θ) is strongly concave with strong convexity c._

We then come to bound the generalization error of _W[ˆ]_ produced by Algorithm 1, using the following
non-trivial extension of Lemma D.5 in (Ratner et al., 2016):

**Lemma D.6. [Extension of Lemma D.5 in (Ratner et al., 2016)] Suppose that conditions (18)-(23)**
_hold. Let_ _W[ˆ]_ _be the learned parameters of the end model produced by Algorithm 1, and ℓ(W_ _[∗]) be the_
_minimum of cross entropy loss function ℓ. Then, we can bound the expected risk with_

E _ℓ( W[ˆ]_ ) − _ℓ(W_ _[∗])_ _≤_ _χ + 4cHϵ._
h i

Finally, we conclude Lemmas (D.4), (D.5) and (D.6) as the following theorem, which is identical to
the Theorem 1 in the main body of the paper:

**Theorem 4 (Extension of Theorem 2 in (Ratner et al., 2016)). Suppose that we run Algoirthm 1 on a**
_WIS specification to produce_ Θ[ˆ] _and_ _W[ˆ]_ _, and all conditions of Lemmas (D.5) and (D.6) are satisfied._
_Then, for any ϵ > 0, if we set the step size to be_

_η =_ _[cϵ][2]_

4

_and the input dataset D is large enough such that_


2 Θ0 Θ[∗]
_∥_ _−_ _∥[2]_


_|D| >_


_c[2]ϵ[2][ log]_


_then we can bound the expected parameter error and the expected risk as:_

2
E Θ − Θ[∗] _≤_ _Mϵ[2],_ E _ℓ( W[ˆ]_ ) − _ℓ(W_ _[∗])_ _≤_ _χ + 4cHϵ._
h i

D.4 PROOFS OF LEMMAS[ˆ]

**Lemma D.1. Let x1, x2 be two binary random variable. Then we have variance of product of x1**
_and x2 can be bounded as_

**Var [x1x2]** **Var [x1] + Var [x2] .**
_≤_

_Proof. Joint distribution of x1 and x2 can be listed as the following table: (where p1 +_ _p2 +_ _p3 +_ _p4 =_
1)

**x1/x2** 0 1
0 _p1_ _p2_
1 _p3_ _p4_

Then we have
**Var [x1x2] = p4** _p[2]4_ [=][ p][4][(][p][1] [+][ p][2] [+][ p][3][)][,]
_−_

while

**Var [X1] + Var [X2] = (p2 + p4)(p1 + p3) + (p3 + p4)(p1 + p2) ≥** _p4(p1 + p2 + p3)._

The proof is completed.

**Lemma D.2. Let Y be a random vector and ∥·∥s be the spectral norm. Then we have**


**Cov(Y, Y )** _s_
_∥_ _∥_ _≤_


**Var(Yi).**


-----

_Proof. By definition of spectral norm, we have_

**Cov(Y, Y )** _s = max_
_∥_ _∥_ **_x_** 2 1 **_[x][T][Cov][(][Y, Y][ )][x]_**
_∥_ _∥_ _≤_

Where x is a constant vector. And by Cauchy-Schwarz inequality,

**_x[T]Cov(Y, Y )x = E_** x[T](Y − E [Y ])(Y − E [Y ])[T]x _≤_ E h∥x∥[2] _∥Y −_ E [Y ]∥[2][i] _._

Because x is a constant vector and ∥x∥≤ 1,

max **_x_** _Y_ E [Y ]
_∥x∥2≤1_ [E] _∥_ _∥[2]_ _∥_ _−_ _∥[2][i]_
h

= max _Y_ E [Y ]
**_x_** 2 1 _∥_ _−_ _∥[2][i]_
_∥_ _∥_ _≤_ _[∥][x][∥][2][ E]_ h

= max **Var(Yi)**
**_x_** 2 1
_∥_ _∥_ _≤_ _[∥][x][∥][2]_ _i_ #
"X

= **Var(Yi).**

_i_

X

The proof is completed.

**Lemma D.5. [Extension of Lemma D.3 in (Ratner et al., 2016)] With conditions (20) and (21), the**
_WIS objective function J(Θ) is strongly concave with strong convexity c._

_Proof. By Lemma D.3, hessian matrix of J can be decomposed as follows:_


Φ(Y, _Y,[¯]_ _Y[ˆ] ) |_ _Y[ˆ] = Y[ˆ]_ _[∗][i]−Cov(Y,Y, ¯_ _Y[ˆ] )∼πΘ_ [(Φ(][Y,][ ¯]Y, _Y[ˆ] ))._


_∇[2]J(Θ) = E ˆY_ _[∗]∼πΘ∗_ **Cov(Y,Y, ¯** _Y[ˆ] )∼πΘ_
h


Basically, to prove that J(Θ) is strongly concave with strong convexity c, we need to show for a real
number c > 0,

_∇[2]J(Θ) ⪯_ _cI._

We calculate each term separately: for the first term

_A = E ˆY_ _[∗]∼πΘ∗_ **Cov(Y,Y, ¯** _Y[ˆ] )∼πΘ_ Φ(Y, _Y,[¯]_ _Y[ˆ] ) |_ _Y[ˆ] = Y[ˆ]_ _[∗][i]_ _,_
h 

since A is symmetric, for any real number c, A _cI, if and only if its spectral norm_ _A_ _s_ _c,_
where ∥A∥s equals to the eigenvalue of A with largest absolute value. ⪯ _∥_ _∥_ _≤_

Since by definition, vector function Φ(Y, _Y,[¯]_ _Y[ˆ] ) can be represented as:_


_ϕ[Acc]_ _Y_ _[j])_
_yi,yˆl[j]_ _[,j][(][Y,][ ˆ]_ i∈[k],j∈[n],yˆl[j] _[∈N][ (][y][i][,][Y][j]_ [)]

_ϕ[Acc]yˆi,j[( ¯]Y_ _[i],_ _Y[ˆ]_ _[j])_ _j∈[n],yˆi∈Yj_
 

_ϕ[t]yˆi,yˆj_ [( ¯]Y _[i],_ _Y[¯]_ _[j])_

_i,j∈[k[ˆ]]_

 

_ϕ[t]yi,yˆj_ [(][Y,][ ¯]Y _[j])_

_i∈[k],j∈[k[ˆ]]_

 


Φ(Y, _Y,[¯]_ _Y[ˆ] ) =_


-----

by Lemma D.2, we have A can be further bounded by


_k_ _n_

 **Var(Y,Y, ¯** _Y[ˆ] )∼πΘ_ _ϕ[Acc]yi,yˆl[j]_ _[,j][(][Y,][ ˆ]Y_ _[j]) |_ _Y[ˆ] = Y[ˆ]_ _[∗][]_

Xi=1 Xj=1 _yˆl[j]_ _[∈N]X[ (][y][i][,][Y][j]_ [)] 


_n_

 **Var(Y,Y, ¯** _Y[ˆ] )∼πΘ_ _ϕ[Acc]yˆi,j[( ¯]Y_ _[i],_ _Y[ˆ]_ _[j]) |_ _Y[ˆ] = Y[ˆ]_ _[∗][]_

_j=1_ _yˆi_ _j_

X X∈Y 
 

 **Var(Y, ¯Y,Y[ˆ] )∼πΘ** _ϕ[t]yˆi,yˆj_ [( ¯]Y _[i],_ _Y[¯]_ _[j]) |_ _Y[ˆ] = Y[ˆ]_ _[∗][]_

1≤i,j≤k[ˆ] 

 _k[X]kˆ_ 

 **Var(Y,Y, ¯** _Y[ˆ] )∼πΘ_ _ϕ[t]yi,yˆj_ [(][Y,][ ¯]Y _[j]) |_ _Y[ˆ] = Y[ˆ]_ _[∗][]_

_i=1_ _j=1_

X X 
 


E ˆY _[∗]∼πΘ∗_



+ E ˆY _[∗]∼πΘ∗_

+ E ˆY _[∗]∼πΘ∗_

+ E ˆY _[∗]∼πΘ∗_


_A ≤_


=A1 + A2 + A3 + A4,


where

_k_ _n_

_A1 =E ˆY_ _[∗]∼πΘ∗_  **Var(Y, ¯Y,Y[ˆ] )∼πΘ** _ϕ[Acc]yi,yˆl[j]_ _[,j][(][Y,][ ˆ]Y_ _[j]) |_ _Y[ˆ] = Y[ˆ]_ _[∗][]_ ;

Xi=1 Xj=1 _yˆl[j]_ _[∈N]X[ (][y][i][,][Y][j]_ [)] 

 

_n_

_A2 =E ˆY_ _[∗]∼πΘ∗_  **Var(Y,Y, ¯** _Y[ˆ] )∼πΘ_ _ϕ[Acc]yˆl,j[(][Y,][ ˆ]Y_ _[j]) |_ _Y[ˆ] = Y[ˆ]_ _[∗][]_ ;

_j=1_ _yˆl_ _j_

X X∈Y 

 

_A3 =E ˆY_ _[∗]∼πΘ∗_  **Var(Y,Y, ¯** _Y[ˆ] )∼πΘ_ _ϕ[t]yˆi,yˆj_ [( ¯]Y _[i],_ _Y[¯]_ _[j]) |_ _Y[ˆ] = Y[ˆ]_ _[∗][]_ ;

1≤i,j≤k[ˆ] 

 _k[X]kˆ_ 

_A4 =E ˆY_ _[∗]∼πΘ∗_  **Var(Y,Y, ¯** _Y[ˆ] )∼πΘ_ _ϕ[t]yi,yˆj_ [(][Y,][ ¯]Y _[j]) |_ _Y[ˆ] = Y[ˆ]_ _[∗][]_ _._

_i=1_ _j=1_

X X 

 

We then bound the four terms respectively. As for A1, for fixed _Y[ˆ]_, we have

_[∗]_

_k_ _n_

**Var(Y,Y, ¯** _Y[ˆ] )∼πΘ_ _ϕ[Acc]yi,yˆl[j]_ _[,j][(][Y,][ ˆ]Y_ _[j]) |_ _Y[ˆ] = Y[ˆ]_ _[∗][]_

Xi=1 Xj=1 _yˆl[j]_ _[∈N]X[ (][y][i][,][Y][j]_ [)] 

_k_ _n_

= **Var(Y,Y, ¯** _Y[ˆ] )∼πΘ_ 1Y =yi∧Yˆ _[j]_ =ˆyl[j] _Y = Y[ˆ]_ _[∗][]_

Xi=1 Xj=1 _yˆl[j]_ _[∈N]X[ (][y][i][,][Y][j]_ [)]  _[|][ ˆ]_

_k_

= Xi=1 j∈[n],yˆl[j] _[∈N][ (]X[y][i][,][Y][j]_ [)][,][( ˆ]Y _[∗])[j]_ =ˆyl[j] **Var(Y,Y, ¯** _Y[ˆ] )∼πΘ_ 1Y =yi | _Y[ˆ] = Y[ˆ]_ _[∗][]_

 

_k_

= Xi=1 j∈[n],yˆl[j] _[∈N][ (]X[y][i][,][Y][j]_ [)][,][( ˆ]Y _[∗])[j]_ =ˆyl[j] **Var(Y,Y, ¯** _Y[ˆ] )∼πΘ_ 1Y =yi | _Y[ˆ] = Y[ˆ]_ _[∗][]_

_k_  

_≤_ _i=1_ _niVar(Y,Y, ¯_ _Y[ˆ] )∼πΘ_ 1Y =yi | _Y[ˆ] = Y[ˆ]_ _[∗][]_ _,_

X 

where ni is the number of ILFs whose label space contains label that is non-exclusive to label yi, i.e.,
_ni = |{j ∈_ [n]|N (yi, Yj) ̸= ∅}|.

Therefore, we have


1Y =yi _Y[ˆ] = Y[ˆ]_ _[∗][]_
_|_


_niE ˆY_ _[∗]∼πΘ∗_ **[Var](Y,Y,[¯]** _Y[ˆ] )∼πΘ_
_i=1_

X


_A1_
_≤_


-----

Similarly, for A2, we have

_A2_
_≤_


_Y¯_ _[i]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_


_miE ˆY_ _[∗]∼πΘ∗_ **[Var](Y,Y,[¯]** _Y[ˆ] )∼πΘ_
_i=1_

X


where mi is the number of ILFs whose label space contains the label ˆyi.

As forwhich is simplified as A3, for fixed _Y[ˆ] t:[∗]_ and any ˆyi, ˆyj ∈ _Y[ˆ], we further separate the proof into subcases by tyˆi ˆyj_

(1). t = t[o]. In this case,


**Var(Y,Y, ¯** _Y[ˆ] )∼πΘ_ _ϕ[t]yˆi,yˆj_ [( ¯]Y _[i],_ _Y[¯]_ _[j]) |_ _Y[ˆ] = Y[ˆ]_ _[∗][]_



=Var(Y,Y, ¯ _Y[ˆ] )_ _πΘ_ **1Y ¯** _[i]= Y[¯]_ _[j][ |][ ˆ]Y = Y[ˆ]_ _[∗][]_
_∼_



=Var(Y,Y, ¯ _Y[ˆ] )∼πΘ_ _Y¯_ _[i][ ¯]Y_ _[j]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_

(≤∗)Var(Y, ¯Y,Y[ˆ] )∼πΘ Y¯ _[i]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_ + Var(Y,Y, ¯ _Y[ˆ] )∼πΘ_



where Eq. (∗) is due to Lemma D.1.

(2). t = t[e]. Similarly,


_Y¯_ _[j]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_

_Y¯_ _[j]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_


_ϕ[t]yˆi,yˆj_ [( ¯]Y _[i],_ _Y[¯]_ _[j]) |_ _Y[ˆ] = Y[ˆ]_ _[∗][]_

_−1Y ¯_ _[i]= Y[¯]_ _[j]_ =1 _[|][ ˆ]Y = Y[ˆ]_ _[∗][]_

**1 ¯Y** _[i]= Y[¯]_ _[j]_ =1 _Y = Y[ˆ]_ _[∗][]_

_[|][ ˆ]_

_Y¯_ _[i][ ¯]Y_ _[j]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_

_Y¯_ _[i]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_ + Var(Y, ¯Y,Y[ˆ] )∼πΘ


**Var(Y,Y, ¯** _Y[ˆ] )∼πΘ_

=Var(Y,Y, ¯ _Y[ˆ] )∼πΘ_

=Var(Y,Y, ¯ _Y[ˆ] )∼πΘ_

=Var(Y,Y, ¯ _Y[ˆ] )∼πΘ_

**Var(Y,Y, ¯** _Y[ˆ] )_ _πΘ_
_≤_ _∼_

(3). t = t[sg]. In this case,

**Var(Y,Y, ¯** _Y[ˆ] )∼πΘ_



=Var(Y,Y, ¯ _Y[ˆ] )∼πΘ_



=Var(Y, ¯Y,Y[ˆ] )∼πΘ



**Var(Y, ¯Y,Y[ˆ] )** _πΘ_
_≤_ _∼_



=Var(Y,Y, ¯ _Y[ˆ] )∼πΘ_



(4). t = t[sd]. Similar to (3).,

**Var(Y,Y, ¯** _Y[ˆ] )∼πΘ_



=Var(Y,Y, ¯ _Y[ˆ] )∼πΘ_



=Var(Y, ¯Y,Y[ˆ] )∼πΘ



**Var(Y, ¯Y,Y[ˆ] )** _πΘ_
_≤_ _∼_



=Var(Y,Y, ¯ _Y[ˆ] )∼πΘ_




_ϕ[t]yˆi,yˆj_ [( ¯]Y _[i],_ _Y[¯]_ _[j]) |_ _Y[ˆ] = Y[ˆ]_ _[∗][]_

_−1Y ¯_ _[i]=1,Y[¯]_ _[j]_ =0 _[|][ ˆ]Y = Y[ˆ]_ _[∗][]_

(1 − _Y[¯]_ _[i]) Y[¯]_ _[j]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_

1 − _Y[¯]_ _[i]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_ + Var(Y,Y, ¯ _Y[ˆ] )∼πΘ_ _Y¯_ _[j]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_



_Y¯_ _[i]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_ + Var(Y, ¯Y,Y[ˆ] )∼πΘ _Y¯_ _[j]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_ _,_



_ϕ[t]yˆi,yˆj_ [( ¯]Y _[i],_ _Y[¯]_ _[j]) |_ _Y[ˆ] = Y[ˆ]_ _[∗][]_

_−1Y ¯_ _[i]=0,Y[¯]_ _[j]_ =1 _[|][ ˆ]Y = Y[ˆ]_ _[∗][]_

(1 − _Y[¯]_ _[j]) Y[¯]_ _[i]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_

1 − _Y[¯]_ _[j]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_ + Var(Y,Y, ¯ _Y[ˆ] )∼πΘ_ _Y¯_ _[i]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_



_Y¯_ _[i]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_ + Var(Y, ¯Y,Y[ˆ] )∼πΘ _Y¯_ _[j]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_ _,_




-----

Combining (1), (2), (3), and (4), we have


_Y¯_ _[i]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_


(k[ˆ] − 1)E ˆY _[∗]∼πΘ∗_ **[Var](Y,Y,[¯]** _Y[ˆ] )∼πΘ_
_i=1_

X


_A3_
_≤_


As for A4, by similar discussion of A3,


_Y¯_ _[i]_ _|_ _Y[ˆ] = Y[ˆ]_ _[∗][]_


1Y =yi _Y[ˆ] = Y[ˆ]_ _[∗][]_
_|_


_A4 ≤_ _i=1_ _kE ˆY_ _[∗]∼πΘ∗_ **[Var](Y,Y,[¯]** _Y[ˆ] )∼πΘ_

X


_kˆE ˆY_ _[∗]∼πΘ∗_ **[Var](Y,Y,[¯]** _Y[ˆ] )∼πΘ_
_i=1_

X


Combining estimation of A1, A2, A3, A4, and by condition (21) we have

_A_ _s_
_∥_ _∥_
_≤A1 + A2 + A3 + A4_


(ni + k[ˆ])VarY,Y ˆ [(][1][Y][ =][y][i] _[|][ ˆ]Y = Y[ˆ]_ _[∗]) +_
_i=1_

X


(mi + K − 1)VarY,Y ˆ [( ¯]Y _[i]|Y[ˆ] = Y[ˆ]_ _[∗])_
_i=1_

X


_≤E ˆY_ _[∗]∼πΘ∗_

_≤E ˆY_ _[∗]∼πΘ∗_


(ni + k[ˆ])Var[2]Y,Y[ˆ] [(][Y][ |][ ˆ]Y = Y[ˆ] _[∗]) +_
_i=1_

X


(mi + K − 1)Var[2]Y,Y[ˆ] [( ¯]Y _[i]|Y[ˆ] = Y[ˆ]_ _[∗])_
_i=1_

X



[1]

_K_ 2

_i=1(mi + K −_ 1)!

X


(ni + k[ˆ]) +
_i=1_

X


2M ≤ _c,_


2M


which further leads to
_A ⪯_ _cI._

For the second term B = Cov(Y, ¯Y,Y[ˆ] ) _πΘ_ [(Φ(][Y,][ ¯]Y, _Y[ˆ] )),_
_∼_

_B = E(Y,Y, ¯_ _Y[ˆ] )∼πΘ_ (Φ(Y, _Y,[¯]_ _Y[ˆ] ) −_ E(Y,Y, ¯ _Y[ˆ] )∼πΘ_ [[Φ(][Y,][ ¯]Y, _Y[ˆ] )])[2][i]_

h 2[]

_Y_ _[′],Y[¯]_ _[′],Y[ˆ]_ _[′][ Φ(][Y][ ′][,][ ¯]Y_ _[′],_ _Y[ˆ]_ _[′]) exp_ Θ[T] Φ(Y _[′],_ _Y[¯]_ _[′],_ _Y[ˆ]_ _[′])_

= EY,Y, ¯ _Y[ˆ] ∼πΘ_ Φ(Y, ¯Y, _Y[ˆ] ) −_ P _Y_ _[′],Y[¯]_ _[′],Y[ˆ]_ _[′][ exp]_ Θ[T] Φ(Y _[′],_ _Y[¯]_ _[′],_ _Y[ˆ]_ _[′])_  

 
 P    

= EY,Y, ¯ _Y[ˆ] ∼πΘ_ ∇Θ log exp Θ[T] Φ(Y, _Y,[¯]_ _Y[ˆ] )_ _−∇Θ log_  exp Θ[T] Φ(Y _[′],_ _Y[¯]_ _[′],_ _Y[ˆ]_ _[′])_

   _Y_ _[′]X,Y[¯]_ _[′],Y[ˆ]_ _[′]_ 


 2[] 

= E(Y,Y, ¯ _Y[ˆ] )_ _πΘ_ Θ log πΘ(Y, _Y,[¯]_ _Y[ˆ] )_ _,_
_∼_ _∇_

 

2[]

where E(Y,Y, ¯ _Y[ˆ] )_ _πΘ_ Θ log πΘ(Y, _Y,[¯]_ _Y[ˆ] )_ is the Fisher Information of Θ. By the Cramér-Rao
_∼_ _∇_

bound and the condition ( 20), 


2[]




2[][−][1]

_I_

2c _D_ Θ) ⪰ _DE(Y,Y ˆ )∼πΘ_ _∇Θ log πΘ(Y,_ _Y[ˆ] )_
_|_ _| [⪰]_ **[Cov][(ˆ]**   

which further leads to

2[]

_B = E(Y,Y, ¯_ _Y[ˆ] )_ _πΘ_ Θ log πΘ(Y, _Y,[¯]_ _Y[ˆ] )_ 2cI.
_∼_ _∇_ _⪰_

 

The proof is completed by putting estimation of terms A and B together.


-----

**Lemma D.6. [Extension of Lemma D.5 in (Ratner et al., 2016)] Suppose that conditions (18)-(23)**
_hold. Let_ _W[ˆ]_ _be the learned parameters of the end model produced by Algorithm 1, and ℓ(W_ _[∗]) be the_
_minimum of cross entropy loss function ℓ. Then, we can bound the expected risk with_

E _ℓ( W[ˆ]_ ) − _ℓ(W_ _[∗])_ _≤_ _χ + 4cHϵ._
h i

_Proof. We begin by rewriting objective of expected loss minimization problem using law of total_
expectation as follows:

_ℓ(W_ ) =E(X,Y )∼π∗ []E(X,Y )∼π∗ [H(Y, σ(h(X, W )))|X]

=E(X _′,Y ′)∼π∗_ []E(X,Y )∼π∗ [H(Y, σ(h(X, W )))|X = X _[′]]_

=E(X _′,Y ′)∼π∗_ []E(X,Y )∼π∗ [H(Y, σ(h(X _[′], W_ )))|X = X _[′]]_

and by our conditional independence assumption (condition (19)), we have 


P(Y |X = X _[′]) = P(Y |Y[ˆ] (X) = Y[ˆ] (X_ _[′])),_

which further leads to

_ℓ(W_ ) =E(X _′,Y ′)∼π∗_ E(X,Y )∼π∗ _H(Y, σ(h(X_ _[′], W_ ))) _Y[ˆ] (X) = Y[ˆ] (X_ _[′])_
h h ii

=E(X _′,Y ′)∼π∗_ E(Y,Y ˆ )∼πΘ∗ _H(Y, σ(h(X_ _[′], W_ ))) _Y[ˆ] = Y[ˆ] (X_ _[′])_
h h ii


On the other hand, if we are minimizing the model with learned parameter Θ[ˆ], we will be actually
minimizing



[)][∼][π][∗] E(Y,Y ˆ )∼π ˆΘ _H(Y, σ(h(X_ _[′], W_ ))) _Y[ˆ] = Y[ˆ] (X_ _[′])_ _,_
h h ii

_H(Y, σ(h(X_ _[′], W_ ))) _Y[ˆ] = Y[ˆ] (X_ _[′])_ can be further calculated as
i


_ℓΘˆ_ [(][W] [) =][ E][(][X] _[′][,Y][ ′][)][∼][π][∗]_ E(Y,Y ˆ ) _π ˆΘ_
_∼_
h


where for any X _[′], E(Y,Y ˆ )_ _π ˆΘ_
_∼_


_H(Y, σ(h(X_ _[′], W_ ))) _Y[ˆ] = Y[ˆ] (X_ _[′])_


E(Y,Y ˆ ) _π ˆΘ_
_∼_


= log (σ(h(X _[′], W_ ))l) P(Y, ˆY ) _π ˆΘ_ [(][Y][ =][ y][l][|][ ˆ]Y = Y[ˆ] (X _[′]))._

_∼_
_l=1_

X

For simplification, we rewrite P(Y,Y ˆ ) _π ˆΘ_ [(][Y][ =][ y][l][|][ ˆ]Y = Y[ˆ] (X _[′])) as follows with slight abuse of_
_∼_
notations:

P(Y,Y ˆ ) _π ˆΘ_ [(][Y][ =][ y][l][|][ ˆ]Y = Y[ˆ] (X _[′])) = Pπ ˆΘ_ [(][y][l][|][ ˆ]Y (X _[′])),_
_∼_

and similarly

E(X _′,Y ′)∼π∗_ = Eπ[∗] _,_

Let lX _′_ = arg min△ _l log (σ(h(X_ _[′], W_ ))l). The difference between the loss functions will be

_k_

_|ℓΘˆ_ [(][W] [)][ −] _[ℓ][(][W]_ [)][|][ =] " _l=1_ log _σ(h(X_ _[′], W_ ))l PπΘ∗ (yl|Y[ˆ] (X _[′])) −_ Pπ ˆΘ [(][y][l][|][ ˆ]Y (X _[′]))_

X    [] [#]

= [E]E[π]π[∗][∗] log _σ(h(X_ _[′], W_ ))lX′ PπΘ∗ (ylX′ |Y[ˆ] (X _[′])) −_ Pπ ˆΘ [(][y][l]X[′][ |][ ˆ]Y (X _[′]))_
h    [] i

+ Eπ[∗]  log _σ(h(X_ _[′], W_ ))l PπΘ∗ (yl|Y[ˆ] (X _[′])) −_ Pπ ˆΘ [(][y][l][|][ ˆ]Y (X _[′]))_  _._

_l≠_ _lX′_    [] 

 [X] 


-----

Furthermore,

Eπ[∗] log _σ(h(X_ _[′], W_ ))lX′ PπΘ∗ (ylX′ _Y[ˆ] (X_ _[′]))_ Pπ ˆΘ [(][y][l]X[′][ |][ ˆ]Y (X _[′]))_
_|_ _−_
h    [] i

+ Eπ∗ log (σ(h(X _[′], W_ ))l) PπΘ∗ (yl _Y[ˆ] (X_ _[′]))_ Pπ ˆΘ [(][y][l][|][ ˆ]Y (X _[′]))_

 _|_ _−_ 

_l≠_ _lX′_  

 [X] 

= Eπ∗ log _σ(h(X_ _[′], W_ ))lX′ − PπΘ∗ (yl|Y[ˆ] (X _[′])) +_ Pπ ˆΘ [(][y][l][|][ ˆ]Y (X _[′]))_

   _l≠XlX′_ _jX≠_ _lX′_
  

+ Eπ∗ log (σ(h(X _[′], W_ ))l) PπΘ∗ (yl _Y[ˆ] (X_ _[′]))_ Pπ ˆΘ [(][y][l][|][ ˆ]Y (X _[′]))_

 _|_ _−_ 

_l≠_ _lX′_  

 [X] 

= Eπ∗  log (σ(h(X _[′], W_ ))l) − log _σ(h(X_ _[′], W_ ))lX′ PπΘ∗ (yl|Y[ˆ] (X _[′])) −_ PπΘ ˆ [(][y][l][|][ ˆ]Y (X _[′]))_

_l≠_ _lX′_      []

 [X]

= Eπ∗ l≠ _lX′_  h(X _[′], W_ )l − _h(X_ _[′], W_ )lX′  []PπΘ∗ (yl|Y[ˆ] (X _[′])). −_ Pπ ˆΘ [(][y][l][|][ ˆ]Y (X _[′]))_ _._ (25)

 [X] 

Let
_h¯(l1, l2) = h(X_ _[′], W_ )l1 _h(X_ _[′], W_ )l2 _._
_−_

By Eq. (22), we have for any l ∈ [k],

0 _h(l, lX_ _[′]_ ) 2H.
_≤_ [¯] _≤_

For any fixed X _[′], define gX_ _′_ (Θ) as follows:

_gX_ _′_ (Θ) = _h¯(l, lX_ _′_ )PπΘ (yl _Y[ˆ] (X_ _[′])),_ (26)

_|_

_l≠XlX′_


based on which we have
_ℓΘˆ_ [(][W] [)][ −] _[ℓ][(][W]_ [)] _≤_ Eπ[∗] _gX_ _′_ (Θ)[ˆ] _−_ _gX_ _′_ (Θ[∗])
 

By First Mean Value Theorem,

_gX_ _′_ (Θ)[ˆ] _gX_ (Θ[∗]) = _gX_ _′_ (ξ), Θ[ˆ] Θ[∗] Θ Θ[∗] _gX_ _′_ (ξ) _._
_−_ _⟨∇_ _−_ _⟩≤∥_ [ˆ] _−_ _∥∥∇_ _∥_


We then bound _gX_ (ξ) element-wisely:
_∇_

(1). For any i ∈ [k], j ∈ [n], ˆyl[j] _[∈N]_ [(][y][i][,][ Y][j][)][, if][ i][ =][ l][X] _[′]_ [,][ ˆ]Y _[j](X_ _[′]) = ˆyl[j][,]_


_Y (X_ _[′]))_
_h¯(l, lX_ _[′]_ ) _[∂][P][π][ξ]_ [(][y][l][|][ ˆ]

_∂θ[Acc]_
_yi,yˆl[j]_ _[,j]_


_∂gX_ _′_ (ξ)

_∂θ[Acc]_
_yi,yˆl[j]_ _[,j]_


_l≠_ _lX′_


_h¯(l, lX_ _′_ )Pπξ (yi _Y[ˆ] (X_ _[′]))Pπξ_ (yl _Y[ˆ] (X_ _[′]))_
_|_ _|_


_l≠_ _lX′_


= _h¯(l, lX_ _[′]_ )Pπξ (yi|Y[ˆ] (X _[′]))Pπξ_ (yl|Y[ˆ] (X _[′]))_

_l≠XlX′_

_≤2HPπξ_ (yi|Y[ˆ] (X _[′]))(1 −_ Pπξ (yi|Y[ˆ] (X _[′])))_

=2HVar 1Y =yi _Y[ˆ] (X_ _[′])_ _._
_|_
h i


-----

If i ̸= lX _′_, _Y[ˆ]_ _[j](X_ _[′]) = ˆyl[j][,]_

_∂gX_ _′_ (ξ)

=

_∂θ[Acc]_
_yi,yˆl[j]_ _[,j]_


_Y (X_ _[′]))_
_h¯(l, lX_ _′_ ) _[∂][P][π][ξ]_ [(][y][l][|][ ˆ]

_∂θ[Acc]_
_yi,yˆl[j]_ _[,j]_


_l≠_ _lX′_


_h¯(l, lX_ _′_ )Pπξ (yi _Y[ˆ] (X_ _[′]))Pπξ_ (yl _Y[ˆ] (X_ _[′]))_
_|_ _|_


_l/∈{i,lX′_ _}_


+ h[¯](i, lX _′_ ) Pπξ (yi _Y[ˆ] (X_ _[′]))_ Pπξ (yi _Y[ˆ] (X_ _[′]))Pπξ_ (yi _Y[ˆ] (X_ _[′]))_
_|_ _−_ _|_ _|_



max _h¯(l, lX_ _′_ )Pπξ (yi _Y[ˆ] (X_ _[′]))Pπξ_ (yl _Y[ˆ] (X_ _[′])),_
_≤_  _|_ _|_

l/∈{Xi,lX′ _}_

_h¯(i, lX_ _′_ ) Pπξ (yi _Y[ˆ] (X_ _[′]))_ Pπξ (yi _Y[ˆ] (X_ _[′]))Pπξ_ (yi _Y[ˆ] (X_ _[′]))_
_|_ _−_ _|_ _|_
 o

_≤2HPπξ_ (yi|Y[ˆ] (X _[′]))(1 −_ Pπξ (yi|Y[ˆ] (X _[′])))_

=2HVar 1Y =yi _Y[ˆ] (X_ _[′])_ _._
_|_
h i


If _Y[ˆ]_ _[j](X_ _[′]) ̸= ˆyl[j][,]_


_Y (X_ _[′]))_
_h¯(l, lX_ _[′]_ ) _[∂][P][π][ξ]_ [(][y][l][|][ ˆ]

_∂θ[Acc]_
_yi,yˆl[j]_ _[,j]_


_∂gX_ _′_ (ξ)

_∂θ[Acc]_
_yi,yˆl[j]_ _[,j]_


= 0.


_l≠_ _lX′_


(2). For j [n], ˆyr _j, if_ _Y[ˆ]_ _[j](X_ _[′]) = ˆyr,_
_∈_ _∈Y_


_Y (X_ _[′]))_
_h¯(l, lX_ _′_ ) _[∂][P][π][ξ]_ [(][y][l][|][ ˆ]

_∂θy[Acc]ˆr,j_

_h¯(l, lX_ _′_ ) Pπξ (Y = yl, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))_ Pπξ (Y = yl _Y[ˆ] (X_ _[′]))Pπξ_ ( Y[¯] _[r]_ = 1 _Y[ˆ] (X_ _[′]))_
_|_ _−_ _|_ _|_


_f1(l) = Pπξ_ (Y = yl, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′]))_

_f2(l) = Pπξ_ (Y = yl|Y[ˆ] (X _[′]))Pπξ_ ( Y[¯] _[r]_ = 1|Y[ˆ] (X _[′])),_


_∂gX_ _′_ (ξ)

_∂θy[Acc]ˆr,j_

Let

and


_l≠_ _lX′_

_l≠XlX′_


= _l : f1(l)_ _f2(l), l_ = lX _′_ _,_
_B[1]_ _{_ _≥_ _̸_ _}_

= _l : f1(l) < f2(l), l_ = lX _′_ _._
_B[2]_ _{_ _̸_ _}_


-----

Therefore,

_h¯(l, lX_ _′_ ) Pπξ (Y = yl, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))_ Pπξ (Y = yl _Y[ˆ] (X_ _[′]))Pπξ_ ( Y[¯] _[r]_ = 1 _Y[ˆ] (X_ _[′]))_
_|_ _−_ _|_ _|_

_l≠XlX′_ 

= _h¯(l, lX_ _′_ ) (f1(l) _f2(l))_

_−_

_l≠XlX′_


_h¯(l, lX_ _′_ ) (f1(l) _f2(l)) +_
_−_
_lX∈B[1]_


_h¯(l, lX_ _′_ ) (f1(l) _f2(l))_
_−_
_lX∈B[2]_


max
_≤_ _t=1,2_

= max


_h¯(l, lX_ _′_ ) (f1(l) _f2(l))_
_−_
_lX∈B[T]_

_h¯(l, lX_ _′_ ) (f1(l) _f2(l)),_
_−_

(Xl∈B[1]


_h¯(l, lX_ _′_ ) (f2(l) _f1(l))_
_−_
_lX∈B[2]_


On the other hand,

_h¯(l, lX_ _′_ ) (f1(l) _f2(l))_
_−_
_lX∈B[1]_

= _h¯(l, lX_ _′_ ) Pπξ (Y = yl, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))_ Pπξ (Y = yl _Y[ˆ] (X_ _[′]))Pπξ_ ( Y[¯] _[r]_ = 1 _Y[ˆ] (X_ _[′]))_

_|_ _−_ _|_ _|_
_l_

X∈B[1]  

_≤2H_ Pπξ (Y = yl, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′])) −_ Pπξ (Y = yl|Y[ˆ] (X _[′]))Pπξ_ ( Y[¯] _[r]_ = 1|Y[ˆ] (X _[′]))_

_l_

X∈B[1]  

=2H Pπξ (Y = yl, ∃l ∈B[1], _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′])) −_ Pπξ (Y = yl, ∃l ∈B[1]|Y[ˆ] (X _[′]))Pπξ_ ( Y[¯] _[r]_ = 1|Y[ˆ] (X _[′]))_



=2H Pπξ (Y = yl, ∃l ∈B[1], _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′]))Pπξ_ ( Y[¯] _[r]_ = 0|Y[ˆ] (X _[′]))_


_−_ Pπξ (Y = yl, ∃l ∈B[1], _Y[¯]_ _[r]_ = 0|Y[ˆ] (X _[′]))Pπξ_ ( Y[¯] _[r]_ = 1|Y[ˆ] (X _[′]))_


_≤2H_ Pπξ ( Y[¯] _[r]_ = 1|Y[ˆ] (X _[′]))Pπξ_ ( Y[¯] _[r]_ = 0|Y[ˆ] (X _[′]))_
 

=2HVar _Y¯_ _[r]|Y[ˆ] (X_ _[′])_ _._
h i

Similarly, we have


_h¯(l, lX′_ ) Pπξ (Y = yl, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′])) + Pπξ_ (Y = yl|Y[ˆ] (X _[′]))Pπξ_ ( Y[¯] _[r]_ = 1|Y[ˆ] (X _[′]))_
_l_

X∈B[2] 


_h¯(l, lX′_ ) (f2(l) − _f1(l)) −_
_lX∈B[1]_

_≤2HVar_ _Y¯_ _[r]|Y[ˆ] (X_ _[′])_ _._
h i


Conclusively, we have
_∂g∂θXy[Acc]ˆ′r(,jξ)_ _Y¯_ _[r]|Y[ˆ] (X_ _[′])_

_[≤]_ [2][H][Var] h

If _Y[ˆ]_ _[j]_ = ˆyr, similar to (1), we have

_∂gX_ _′_ (ξ)

_∂θy[Acc]ˆr,j_ [= 0][.]


-----

(3). For any ˆyi, ˆyj ∈ _Y[ˆ], by the definition of ϕ[t]yˆi,yˆj_ [, there exists][ (][a, b][)][ ∈{][0][,][ 1][}][2][, such that]
_ϕ[t]yˆi,yˆj_ [(][a, b][)][ ̸][= 0][. Similar to][ (2)][, let]

_f3(l) = Pπξ_ (Y = yl, _Y[¯]_ _[i]_ = a, _Y[¯]_ _[j]_ = b|Y[ˆ] (X _[′]))_

_f4(l) = Pπξ_ (Y = yl|Y[ˆ] (X _[′]))Pπξ_ ( Y[¯] _[i]_ = a, _Y[¯]_ _[j]_ = b|Y[ˆ] (X _[′]))_


and

we have

_∂gX_ _′_ (ξ)

_∂θy[t]ˆi,yˆj_


= _l : f3(l)_ _f4(l), l_ = lX _′_ _,_
_B[3]_ _{_ _≥_ _̸_ _}_

= _l : f3(l) < f4(l), l_ = lX _′_
_B[4]_ _{_ _̸_ _}_


_h¯(l, lX_ _′_ ) (f3(l) _f4(l)),_ _h¯(l, lX_ _′_ ) (f4(l) _f3(l))_

[= max] _−_ _−_

(Xl∈B[3] _lX∈B[4]_

_≤2HVar_ _ϕ[t]yˆi,yˆj_ [( ¯]Y _[i],_ _Y[¯]_ _[j])|Y[ˆ] (X_ _[′])_

(≤∗)2H **Varh** _Y¯_ _[i]|Y[ˆ] (X_ _[′])_ + Var iY¯ _[j]|Y[ˆ] (X_ _[′])_ _,_
 h i h i


where inequality (∗) comes from Lemma D.1.

(4). For any yi ∈Y, ˆyr ∈ _Y[ˆ], by the definition of ϕ[t]yi,yˆr_ [, there exists][ a][ ∈{][0][,][ 1][}][,][ y][j][ ∈Y][, s.t.,]
_ϕ[t]yi,yˆr_ [(][y][j][, a][)][ ̸][= 0][. We further divide the proof into two cases:][ ϕ]y[t] _i,yˆr_ [(][y][i][, a][) = 0][, and][ ϕ]y[t] _i,yˆr_ [(][y][i][, a][)][ ̸][=]
0.

(4a). If ϕ[t]yi,yˆr [(][y][i][, a][) = 0][, we have][ t][y]iy[ˆ]r [=][ t][sg][ and consequently][ a][ = 1][. Similar to (1-3)., we have]


_Y (X_ _[′]))_
_h¯(l, lX_ _[′]_ ) _[∂][P][π][ξ]_ [(][y][l][|][ ˆ]

_∂θy[t]_ _i,yˆr_


_k_ _Y (X_ _[′]))_

_h¯(l, lX_ _′_ ) _[∂][P][π][ξ]_ [(][y][l][|][ ˆ]
_l=1_ _∂θy[t]_ _i,yˆr_

X


_∂gX_ _′_ (ξ)

_∂θy[t]_ _i,yˆr_


(=•)


_l≠_ _lX′_


= _h¯(l, lX_ _′_ ) Pπξ (Y = yl, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))_ Pπξ (Y = yl _Y[ˆ] (X_ _[′]))Pπξ_ (Y = yi, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))_

_|_ _−_ _|_ _̸_ _|_
_l=i_

X̸ 

_h(i, lX_ _′_ )Pπξ (Y = yi _Y[ˆ] (X_ _[′]))Pπξ_ (Y = yi, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))_ _,_
_−_ [¯] _|_ _̸_ _|_

where Eq. (•) is due to Let

_f5(l) = Pπξ_ (Y = yl, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′]))_

_f6(l) = Pπξ_ (Y = yl|Y[ˆ] (X _[′]))Pπξ_ (Y ̸= yi, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′]))_


and


= _l : f5(l)_ _f6(l),_ _l_ = i _,_
_B[5]_ _{_ _≥_ _̸_ _}_

= _l : f5(l) < f6(l),_ _l_ = i
_B[6]_ _{_ _̸_ _}_


Then we have

_∂gX_ _′_ (ξ) _h¯(l, lX_ _′_ ) (f5(l) _f6(l)),_

_∂θy[t]_ _i,yˆr_ _[≤]_ [max] (Xl∈B[5] _−_


_h¯(l, lX_ _′_ ) (f6(l) _f5(l)) + h[¯](i, lX_ _′_ )f6(i)
_−_
_lX∈B[6]_


-----

On one hand,

_h¯(l, lX_ _′_ ) (f5(l) _f6(l))_
_−_
_lX∈B[5]_

= _h¯(l, lX_ _′_ ) Pπξ (Y = yl, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))_ Pπξ (Y = yl _Y[ˆ] (X_ _[′]))Pπξ_ (Y = yi, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))_

_|_ _−_ _|_ _̸_ _|_
_l_

X∈B[5]  

_≤_ 2H Pπξ (Y = yl, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′])) −_ Pπξ (Y = yl|Y[ˆ] (X _[′]))Pπξ_ (Y ̸= yi, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′]))_

_l_

X∈B[5]  

=2H Pπξ (Y = yl, ∃l ∈B[5], _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′])) −_ Pπξ (Y = yl, ∃l ∈B[5]|Y[ˆ] (X _[′]))Pπξ_ (Y ̸= yi, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′]))_



=2H Pπξ (Y = yl, ∃l ∈B[5], _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′]))(1 −_ Pπξ (Y ̸= yi, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′])))_


_−_ Pπξ (Y = yl, ∃l ∈B[5], _Y[¯]_ _[r]_ = 0|Y[ˆ] (X _[′]))Pπξ_ (Y ̸= yi, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′]))_


_≤2HPπξ_ (Y ̸= yi, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′]))(1 −_ Pπξ (Y ̸= yi, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′])))_

=2HVarπξ _ϕ[t]yi,yˆl_ [(][Y,][ ¯]Y _[r])|Y[ˆ] (X_ _[′])_
h i

2HVarπξ 1Y =yi _Y[ˆ] (X_ _[′])_ + 2HVarπξ _Y¯_ _[r]_ _Y[ˆ] (X_ _[′])_ _._
_≤_ _|_ _|_
h i h i

On the other hand,

_h¯(l, lX_ _′_ ) (f6(l) _f5(l)) + h[¯](i, lX_ _′_ )f6(i)
_−_
_lX∈B[6]_

= _h¯(l, lX_ _′_ ) Pπξ (Y = yl, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′])) + Pπξ_ (Y = yl _Y[ˆ] (X_ _[′]))Pπξ_ (Y = yi, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))_
_−_ _|_ _|_ _̸_ _|_

_l_

X∈B[6] 

+ h[¯](i, lX _′_ )Pπξ (Y = yi _Y[ˆ] (X_ _[′]))Pπξ_ (Y = yi, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))_
_|_ _̸_ _|_

_≤2H_ _−Pπξ_ (Y = yl, ∃l ∈B[6], _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′]))(1 −_ Pπξ (Y ̸= yi, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′])))_


_−_ Pπξ (Y = yl, ∃l ∈B[6], _Y[¯]_ _[r]_ = 0|Y[ˆ] (X _[′]))Pπξ_ (Y ̸= yi, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′]))_

+ Pπξ (Y = yi|Y[ˆ] (X _[′]))Pπξ_ (Y ̸= yi, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′]))_


_≤2H_ Pπξ (Y = yl, ∃l ∈B[6], _Y[¯]_ _[r]_ = 0|Y[ˆ] (X _[′]))Pπξ_ (Y ̸= yi, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′]))_


+ Pπξ (Y = yi|Y[ˆ] (X _[′]))Pπξ_ (Y ̸= yi, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′]))_


_≤2H_ Pπξ ( Y[¯] _[r]_ = 0|Y[ˆ] (X _[′]))Pπξ_ ( Y[¯] _[r]_ = 1|Y[ˆ] (X _[′])) + Pπξ_ (Y = yi|Y[ˆ] (X _[′]))Pπξ_ (Y ̸= yi, _Y[¯]_ _[r]_ = 1|Y[ˆ] (X _[′]))_


2HVarπξ 1Y =yi _Y[ˆ] (X_ _[′])_ + 2HVarπξ _Y¯_ _[r]_ _Y[ˆ] (X_ _[′])_ _._
_≤_ _|_ _|_
h i h i


Therefore, in this case, we have
_∂g∂θXy[t]_ _i′,(yˆξr)_ 1Y =yi _|Y[ˆ] (X_ _[′])_ + 2HVarπξ _Y¯_ _[r]|Y[ˆ] (X_ _[′])_

_[≤]_ [2][H][Var][π][ξ] h i h

(4b). If ϕ[t]yi,yˆr [(][y][i][, a][)][ ̸][= 0][, similar to][ (4][a][)][.][, we have]


_∂gX_ _′_ (ξ)

_∂θy[t]_ _i,yˆr_


_h¯(l, lX_ _′_ )Pπξ (Y = yl _Y[ˆ] (X_ _[′]))Pπξ_ (Y = yi, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))_

_−_ _|_ _|_
Xl≠ _i_

+ h[¯](i, lX _′_ ) Pπξ (Y = yi, _Y[¯]_ _[l]_ = 1 _Y[ˆ] (X_ _[′]))_ Pπξ (Y = yi _Y[ˆ] (X_ _[′]))Pπξ_ (Y = yi, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))_
_|_ _−_ _|_ _|_



-----

Since

_h¯(i, lX_ _′_ ) Pπξ (Y = yi, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))_ Pπξ (Y = yi _Y[ˆ] (X_ _[′]))Pπξ_ (Y = yi, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))_
_|_ _−_ _|_ _|_


=h[¯](i, lX _′_ )Pπξ (Y = yi, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))Pπξ_ (Y = yi _Y[ˆ] (X_ _[′]))_
_|_ _̸_ _|_

_≥0,_

we have
_∂gX_ _′_ (ξ) _h¯(i, lX_ _′_ )Pπξ (Y = yi, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))Pπξ_ (Y = yi _Y[ˆ] (X_ _[′])),_

_∂θy[t]_ _i,yˆl_ _|_ _̸_ _|_

_[≤]_ [max] n

_h¯(l, lX_ _′_ )Pπξ (Y = yl _Y[ˆ] (X_ _[′]))Pπξ_ (Y = yi, _Y[¯]_ _[r]_ = 1 _Y[ˆ] (X_ _[′]))_
_|_ _|_ 
_l=i_

X̸ 




2HVar 1Y =yi _Y[ˆ] (X_ _[′])_ _._
_≤_ _|_
h i

Combining (4a). and (4b)., we have that
_∂g∂θXy[t]_ _i′_ _,(yˆξl)_ **Varπξ** 1Y =yi _|Y[ˆ] (X_ _[′])_ + Varπξ _Y¯_ _[l]|Y[ˆ] (X_ _[′])_

_[≤]_ [2][H]  h i h i

Combining (1-4)., we then have


_gX_ _′_ (ξ)
_∥∇_ _∥[2]_

_k_ _n_

2

4H [2] ( (yi, _j)_ 1) Varπξ 1Y =yi _Y[ˆ] (X_ _[′])_ (27)
_≤_ _|N_ _Y_ _| −_ _|_

_i=1_ _j=1_

X X h i

2

+ 4H [2] **Varπξ** _Y¯_ _[r]_ _Y[ˆ] (X_ _[′])_

_|_
_j_ [n],yˆr _j_
_∈_ X∈Y h i

2

+ 4H [2][ X] **Varπξ** _Y¯_ _[i]_ _Y[ˆ] (X_ _[′])_ + Varπξ _Y¯_ _[j]_ _Y[ˆ] (X_ _[′])_

_|_ _|_

_i,j∈[k[ˆ]]_  h i h i

2

+ 4H [2] **Varπξ** 1Y =yi _Y[ˆ] (X_ _[′])_ + Varπξ _Y¯_ _[l]_ _Y[ˆ] (X_ _[′])_

_|_ _|_

_i∈[kX],j∈[k[ˆ]]_  h i h i


(ni + k[ˆ])Varπξ (1Y =yi _|Y[ˆ] = Y[ˆ]_ _[∗])[2]_ +
_i=1_

X


_≤8H_ [2]


(mi + K − 1)Varπξ ( Y[¯] _[i]|Y[ˆ] = Y[ˆ]_ _[∗])[2]_
_i=1_

X


_._



(28)


Therefore, by Eqs. (25), (26), and (28), and Assumption Eq. (21), we have

_ℓ(W_ ) _ℓΘˆ_ [(][W] [)][|][ =] Eπ∗ _h¯(l, lX_ _′_ ) PπΘ∗ (Y = yl _Y[ˆ] (X_ _[′]))_ Pπ ˆΘ [(][Y][ =][ y][l][|][ ˆ]Y (X _[′]))_
_|_ _−_  _|_ _−_

_l≠_ _lX′_ 

 [X]

= Eπ[∗] _gX_ _′_ (Θ[∗]) _gX_ _′_ (Θ)[ˆ]
_−_
h i

_≤_ Eπ[∗] _∥∇gX_ _[′]_ (ξ)∥∥Θ[∗] _−_ Θ[ˆ] _∥_

_≤_ _√[2][cH]M_ _∥Θ[∗]_ _−_ Θ[ˆ] _∥._


Now, we apply the assumption that we are able to solve the empirical problem, producing an estimate
_Wˆ_ that satisfies
E _ℓΘˆ_ [( ˆ]W ) − _ℓΘˆ_ [(][W][ ∗]Θ[)] _≤_ _χ,_
h i


-----

where WΘˆ[∗] [is the true solution to]

Therefore,


_WΘˆ[∗]_ [= arg min]W _[ℓ][Θ][(][W]_ [)][.]


_ℓ( W[ˆ]_ ) − _ℓ(W_ _[∗])_ = E _ℓΘˆ_ [( ˆ]W ) − _ℓΘˆ_ [(][W][ ∗]Θˆ [) +][ ℓ]Θ[ˆ] [(][W][ ∗]Θˆ [)][ −] _[ℓ]Θ[ˆ]_ [( ˆ]W ) + ℓ( W[ˆ] ) − _ℓ(W_ _[∗])_
i (∗) h

_≤_ _χ + E_ _ℓΘˆ_ [(][W][ ∗]Θˆ [)][ −] _[ℓ]Θ[ˆ]_ [( ˆ]W ) + ℓ( W[ˆ] ) − _ℓ(W_ _[∗])_
h i


E Θ Θ[∗] + E _ℓΘˆ_ [(][W][ ∗]Θˆ [)][ −] _[ℓ]Θ[ˆ]_ [( ˆ]W ) + ℓΘˆ [( ˆ]W ) _ℓΘˆ_ [(][W][ ∗][)]
_∥_ [ˆ] _−_ _∥_ _−_
h

E∥Θ[ˆ] _−_ Θ[∗]∥,


_≤_ _χ + 4cH_

_≤_ _χ + 4cH_


where Eq. (∗) comes from condition (23).

With Eqs. (20) and (21), we have Eq. (24) by Lemma D.5, i.e.,

2
E∥Θ[ˆ] _−_ Θ[∗]∥ _≤_ E∥Θ[ˆ] _−_ Θ[∗]∥[2] _≤_ _ε[2]M._
 

We can now bound this using the result of Lemma D.6, which results in

E _ℓ( W[ˆ]_ ) − _ℓ(W_ _[∗])_ _≤_ _χ + 4cHϵ._
h i

The proof is completed.

E EXAMPLES AND ILLUSTRATIONS

E.1 LABEL GRAPH AND LABEL HIERARCHY

Fig 5 shows the mapping between a label hierarchy and the corresponding label graph. Indeed, given
the order of labels, any label structure represented as a (directed acyclic graph) DAG can be converted
to exact one consistent label graph based on the four types of label relations.

Canidae DomesticAnimal Cat

Exclusive
Overlap
Subsuming
Subsumed

Cat

Domestic

Animal Dog

Canidae Husky

Canidae DomesticAnimal

Dog Cat

Husky


Figure 5: The illustration of mapping between a DAG of labels and a label graph.

E.2 AN EXAMPLE OF INCONSISTENT LABEL GRAPH

Fig. 6 shows an example of an inconsistent label graph. We can see that the label graph is unrealistic
and ambiguous because “Husky” subsumes “Canidae”, but (1) “Canidae” subsumes “Dog” and
(2) “Dog” subsuems “Husky” combined imply that “Husky” should be subsumed by “Canidae”.
Also, from the example, we can see that label graph induced from cyclic label hierarchy must be
inconsistent.


-----

Exclusive

Overlap
Subsuming

Subsumed


Canidae

Dog Husky


Canidae

Dog Husky


Figure 6: The illustration of inconsistent label graph.

E.3 ENUMERATION OF INCONSISTENT TRIANGLE LABEL GRAPH

For a triangle label graph G, we list all inconsistent label relation structures. The consistency of
larger label graph with more labels can be verified by checking the consistency of every triangle
inside. One example proof of {Exclusive, Overlap, Subsuming} can be found in Lemma 1.

Table 5: Enumeration of Inconsistent Label Relation Triplets.

**label relation Triplets**

|t ab|t bc|t ac|
|---|---|---|


Overlap Subsumed Subsuming
Overlap Subsumed Exclusive
Overlap Subsuming Subsumed
Overlap Exclusive Subsumed
Exclusive Subsumed Subsuming
Exclusive Overlap Subsuming
Exclusive Subsuming Subsuming
Exclusive Subsuming Subsumed
Exclusive Subsuming Overlap
Subsuming Exclusive Subsumed
Subsuming Subsumed Exclusive
Subsuming Overlap Subsumed
Subsuming Overlap Exclusive
Subsuming Subsuming Exclusive
Subsuming Subsuming Subsumed
Subsuming Subsuming Overlap
Subsumed Overlap Subsuming
Subsumed Subsumed Exclusive
Subsumed Subsumed Subsuming
Subsumed Subsumed Overlap
Subsumed Exclusive Subsuming
Subsumed Exclusive Subsumed
Subsumed Exclusive Overlap


E.4 AN EXAMPLE OF INDISTINGUISHABLE LABEL GRAPH

Fig. 7 shows an example label graph with indistinguishable label relation structure. Again, red
labels represent desired unseen labels, while gray labels are undesired and seen. We can see that
unseen label “Husky” and “Bulldog” have indistinguishable label relation structures because for all
seen labels, their label relations are equal. For example, seen label “Dog” subsumes both “Husky”
and “Bulldog”. In contrast, for “Husky” and “Bengal Cat”, seen label “Cat” subsumes the latter
but exclusive to the former, which indicates that “Husky” and “Bengal Cat” have distinguishable
label relation structure. Note that “Bengal Cat” and “Persian Cat” also have indistinguishable label
relation structure, but the former is unseen desired label while the latter is seen and can be predicted
by some ILF(s). We are only interested in the distinguishablity of a pair of unseen labels.


-----

In practice, users could "break the symmetry" by adding new ILFs with new labels. For example, if
we add an ILF that could predict “Arctic Animals”, then the new seen label “Arctic Animals” will be
added into label graph as shown in Fig. 8. We know that “Arctic Animals” subsumes “Husky” but
not “Bulldog”, so we break the indistinguishable label relation structure of “Husky” and “Bulldog”
successfully.

Animal Husky …

Dog Cat Bulldog …

Exclusive
Overlap
Subsuming

Subsumed

Domestic

Animal

Dog Cat

Husky Bulldog BengalCat PersianCat

Dog

Husky …

Domestic

Animal

Bulldog …

Cat

Bengal

Cat …

Persian

Cat


Figure 7: An example of an indistinguishable label relation structure (“Husky” and “Bulldog”).

DomesticAnimal Husky DomesticAnimal

AnimalsArctic Dog Cat Bulldog Cat


Exclusive

Overlap

Subsuming

Subsumed

Domestic

Animal

AnimalsArctic Dog Cat

Husky Bulldog BengalCat PersianCat

Dog

…

Husky DomesticAnimal

…

Bulldog Cat

…

Bengal Persian

Cat Cat

…

Arctic

Animals

Figure 8: An example of fixing an indistinguishable label relation structure (“Husky” and “Bulldog”)
by adding a new label (“Arctic Animals”).

F EXPERIMENTAL DETAILS

F.1 DATASET

**Large scale Text Classification Dataset[3]: LSHTC-3 (Partalas et al., 2015), a large scale hierarchical**
text classification dataset, which consists of 456,886 documents and 36,504 categories organized
in a label hierarchy. We filter out the documents with multiple labels, and preserve categories with
more than 500 documents. We use a pre-trained sentence transformer (Reimers & Gurevych, 2019)
to obtain document embeddings for classification. We follow Zhang et al. (2021) to generate 5
keyword-based labeling functions for each seen label as ILFs.

**Large scale Image Classification Dataset[4]: ILSVRC2012 (Russakovsky et al., 2015), a large scale**
image classification dataset, which consists of 1.2M training images from 1000 object classes based

[3http://lshtc.iit.demokritos.gr/](http://lshtc.iit.demokritos.gr/)
[4http://image-net.org/challenges/LSVRC/2012/index#data](http://image-net.org/challenges/LSVRC/2012/index#data)


-----

on ImageNet. Following Deng et al. (2014) we use WordNet as the label hierarchy, and because all
the images are assigned to leave labels in WordNet, for each non-leave label, we aggregate images
belonging to its descendants as its data points (Deng et al., 2014). For weak supervision sources
creation, we follow Mazzetto et al. (2021b;a) to train 10 image classifiers as ILFs. We randomly
sampling 2 or 3 exclusive seen labels from the label graph as well as 500 images for each label to
train a ResNet-32 classifier.

F.2 DESCRIPTION OF APPLYING DAP

To apply DAP, we use both label relations and ILFs to construct attributes for both unseen classes
and unlabeled data points. Then, we train the attribute classifiers, which in turn are used to predict
unseen labels on the test set as in Lampert et al. (2013). To construct attributes for unseen labels and
data points, we leverage the outputs of ILFs and label relations.

First, based on the label relations and basic logistic rules, we enumerate all the possible assignments
of seen labels given a data point. For example, if label A is subsumed by label B, then for a data
point, when it belongs to label A, it must also belong to B; And if label A and B are exclusive, then
one data cannot belong to both at the same time. Let s ∈ _S denote one possible label assignment and_
_S is the set of all possible s. Then we define the attribute as a vector of |S| dimension where each_
dimension corresponds to one s.

Second, we define the attribute of unseen labels. For an unseen label A and a label assignment s, if A
is not exclusive to any label in s then we set the corresponding attribute as = 1 for label A, other
wise 0. The intuition is that, if A is not exclusive to labels in s, it’s likely that when a data belongs
to assignment s, it also belongs to label A. For each data point, we use the labels assigned by ILFs
to build their attributes. If a data belongs to assignment s then its corresponding attribute as = 1,
otherwise 0.

Then, we can train attribute classifier p(a|x) for each attribute based on data point attributes. During
inference, we use unseen label attribute as well as attribute classifier as in Lampert et al. (2013):


_|S|_

_m=1_

Y


_p(a[c]m[|][x][)]_

(29)
_p(a[c]m[|][x][)]_


_f_ (x) = arg max


F.3 HYPER-PARAMETERS

For the training of PGMs, we set the learning rate to be _n[1]_ [where][ n][ is the number of training data. For]

training logistic regression model, we use the default parameters in scikit-learn library. For training
ResNet model, we set batch size as 256 and use Adam optimizer with learning rate being 1e-3 and
weight decay being 5e-5.

F.4 HARDWARE AND IMPLEMENTATION DETAILS

All experiments ran on a machine with an Intel(R) Xeon(R) CPU E5-2678 v3 with a 512G memory
and a GeForce GTX 1080Ti-11GB GPU.

All the code was implemented in Python. We use the standard implementation of the logistic
regression model from Python scikit-learn library[5] and the ResNet model from torchvision library[6].

Our code will be released upon the acceptance.

F.5 DATASET DETAILS OF REAL-WORLD APPLICATIONS

We list the tags we used in the real-world application (Sec. 7.3) and examples of label relations we
query from the existing product category taxonomy.

[5https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
[LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)

[6https://pytorch.org/docs/stable/torchvision/models.html](https://pytorch.org/docs/stable/torchvision/models.html)


-----

Table 6: The tags and examples of label relations of “Car Accessories” category.

**new unseen tags:** “Performance Modifying Parts”, “Vehicle Tires & Tire Parts”, “Car Engines & Engine Parts”

“Car Modification Parts”, “Car Parts & Accessories”
**existing tags:**
“Car & Truck Tires”, “Replacement Car Parts”, “Car & Truck Wheels”

“Replacement Car Parts” subsumes “Car Engines & Engine Parts”
**label relation examples:**
“Car & Truck Tires” is subsumed by “Vehicle Tires & Tire Parts”

Table 7: The tags and examples of label relations of “Furniture Accessories” category.

**new unseen tags:** “Clothing & Shoe Storage”, “Living Room Furniture”, “Beds & Headboards”

“Coffee Tables & End Tables”, “Entertainment & Media Centers”
**existing tags:**
“Bedroom Furniture”, “Sofas & Chairs”, “Mattresses”

“Bedroom Furniture” subsumes “Beds & Headboards”
**label relation examples:**
“Sofas & Chairs” is subsumed by “Living Room Furniture”

G ADDITIONAL EXPERIMENTS

G.1 PERFORMANCE DROP WHEN THE DISTINGUISHABLE CONDITION IS VIOLATED

To validate the effectiveness of the distinguishable condition, we drive another 100 WIS tasks from
LSHTC-3 dataset where each task has at least one pair of unseen labels sharing exactly the same
label relation structure. In Table 8, we report the performance drop on the averaged evaluation results
over the 100 WIS tasks with comparison to the numbers in Table 2. Although the two sets of WIS
tasks are different and therefore are not individually comparable, the averaged performance drop does
indicates that the violation of the distinguishable condition results in undesirable synthesized training
labels, which implicitly demonstrates the effectiveness of the distinguishable condition.

Table 8: Performance drop on averaged evaluation results over 100 WIS tasks derived from LSHTC-3
when the distinguishable condition is violated.

**Method** **Accuracy** **F1-score**


LR-MV -11.49 -13.83
W-LR-MV -11.51 -13.47
WS-LG -9.28 -8.63

PLRM -9.66 -9.63

LR-MV -16.14 -17.08
W-LR-MV -15.27 -15.97
WS-LG -13.13 -13.78

PLRM -13.39 -14.09


Label Model

End Model


-----

