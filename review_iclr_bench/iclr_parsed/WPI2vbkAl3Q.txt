# LEARNING CURVES FOR STOCHASTIC GRADIENT DE## SCENT ON STRUCTURED FEATURES

**Blake Bordelon & Cengiz Pehlevan**
John A. Paulson School of Engineering and Applied Sciences
Center for Brain Science
Harvard University
Cambridge, MA 02138, USA
_{blake bordelon,cpehlevan}@g.harvard.edu_

ABSTRACT

The generalization performance of a machine learning algorithm such as a neural network depends in an intricate way on the structure of the data distribution.
To analyze the influence of data structure on test loss dynamics, we study an exactly solveable model of stochastic gradient descent (SGD) which predicts test
loss when training on features with arbitrary covariance structure. We solve the
theory exactly for both Gaussian features and arbitrary features and we show that
the simpler Gaussian model accurately predicts test loss of nonlinear randomfeature models and deep neural networks trained with SGD on real datasets such
as MNIST and CIFAR-10. We show that the optimal batch size at a fixed compute budget is typically small and depends on the feature correlation structure,
demonstrating the computational benefits of SGD with small batch sizes. Lastly,
we extend our theory to the more usual setting of stochastic gradient descent on
a fixed subsampled training set, showing that both training and test error can be
accurately predicted in our framework on real data.

1 INTRODUCTION

Understanding the dynamics of SGD on realistic learning problems is fundamental to learning theory. Due to the challenge of modeling the structure of realistic data, theoretical studies of generalization often attempt to derive data-agnostic generalization bounds or study the typical performance
of the algorithm on high-dimensional, factorized data distributions (Engel & Van den Broeck, 2001).
Realistic datasets, however, often lie on low dimensional structures embedded in high dimensional
ambient spaces (Pope et al., 2021). For example, MNIST and CIFAR-10 lie on surfaces with intrinsic dimension of ∼ 14 and ∼ 35 respectively (Spigler et al., 2020). To understand the average-case
performance of SGD in more realistic learning problems and its dependence on data, model and
hyperparameters, incorporating structural information about the learning problem is necessary.

In this paper, we calculate the average case performance of SGD on models of the form f (x) =
**w** _·ψ(x) for nonlinear feature map ψ trained with MSE loss. We express test loss dynamics in terms_
of the induced second and fourth moments of ψ. Under a regularity condition on the fourth moments,
we show that the test error can be accurately predicted in terms of second moments alone. We
demonstrate the accuracy of our theory on random feature models and wide neural networks trained
on MNIST and CIFAR-10 and accurately predict test loss scalings on these datasets. We explore in
detail the effect of minibatch size, m, on learning dynamics. By varying m, we can interpolate our
theory between single sample SGD (m = 1) and gradient descent on the population loss (m →∞).
To explore the computational advantages SGD compared to standard full batch gradient descent
we analyze the loss achieved at a fixed compute budget C = tm for different minibatch size m and
number of steps t, trading off the number of parameter update steps for denoising through averaging.
We show that generally, the optimal batch size is small, with the precise optimum dependent on the
learning rate and structure of the features. Overall, our theory shows how learning rate, minibatch
size and data structure interact with the structure of the learning problem to determine generalization
dynamics. It provides a predictive account of training dynamics in wide neural networks.


-----

1.1 OUR CONTRIBUTIONS

The novel contributions of this work are described below.

-  We calculate the exact expected test error for SGD on MSE loss for arbitrary feature structure in terms of second and fourth moments as we discuss in Section 4.2. We show how
structured gradient noise induced by sampling alters the loss curve compared to vanilla GD.

-  For Gaussian features (or those with regular fourth moments), we compute the test error in
Section 1. This theory is shown to be accurate in experiments with random feature models
and wide networks in the kernel regime trained on MNIST and CIFAR-10.

-  We show that for fixed compute/sample budgets and structured features with power law
spectral decays, optimal batch sizes are small. We study how optimal batch size depends
on the structure of the feature correlations and learning rate.

-  We extend our exact theory to study multi-pass SGD on a fixed finite training set. Both test
and training error can be accurately predicted for random feature models on MNIST.

2 RELATED WORK

The analysis of stochastic gradient descent has a long history dating back to seminal works of Polyak
& Juditsky (1992) and Ruppert (1988), who analyzed time-averaged iterates in a noisy problem.
Many more works have examined a similar setting, identifying how averaged and accelerated versions of SGD perform asymptotically when the target function is noisy (not a deterministic function
of the input) (Flammarion & Bach, 2015; 2017; Shapiro, 1989; Robbins & Monro, 1951; Chung,
1954; Duchi & Ruan, 2021; Yu et al., 2020; Anastasiou et al., 2019; Gurbuzbalaban et al., 2021).

Recent studies have also analyzed the asymptotics of noise-free MSE problems with arbitrary feature structure to see what stochasticity arises from sampling. Prior works have found exponential loss curves problems as an upper bound (Jain et al., 2018) or as typical case behavior for
SGD on unstructured data (Werfel et al., 2004). A series of more recent works have considered
the over-parameterized (possibly infinite dimension) setting for SGD, deriving power law test loss
curves emerge with exponents which are better than the O(t[−][1]) rates which arise in the noisy problem (Berthier et al., 2020; Pillaud-Vivien et al., 2018; Dieuleveut et al., 2016; Varre et al., 2021;
Dieuleveut & Bach, 2016; Ying & Pontil, 2008; Fischer & Steinwart, 2020; Zou et al., 2021). These
works provide bounds of the form O(t[−][β]) for exponents β which depend on the task and feature
distribution.

Several works have analyzed average case online learning in shallow and two-layer neural networks.
Classical works often analyzed unstructured data (Heskes & Kappen, 1991; Biehl & Riegler, 1994;
Mace & Coolen, 1998; Saad & Solla, 1999; LeCun et al., 1991; Goldt et al., 2019), but recently
the hidden manifold model enabled characterization of learning dynamics in continuous time when
trained on structured data, providing an equivalence with a Gaussian covariates model (Goldt et al.,
2020; 2021). In the continuous time limit considered in these works, SGD converges to gradient
flow on the population loss, where fluctuations due to sampling disappear and order parameters
obey deterministic dynamics. Other recent works, however, have provided dynamical mean field
frameworks which allow for fluctuations due to random sampling of data during a continuous time
limit of SGD, though only on simple generative data models (Mignacco et al., 2020; 2021).

Studies of fully trained linear (in trainable parameters) models also reveal striking dependence on
data and feature structure. Analysis for models trained on MSE (Bartlett et al., 2020; Tsigler &
Bartlett, 2020; Bordelon et al., 2020; Canatar et al., 2020), hinge loss (Chatterji & Long, 2021; Cao
et al., 2021; Cao & Gu, 2019) and general convex loss functions (Loureiro et al., 2021) have now
been performed, demonstrating the importance of data structure for offline generalization.

Other works have studied the computational advantages of SGD at different batch sizes m. Ma et al.
(2018) study the tradeoff between taking many steps of SGD at small m and taking a small number
of steps at large m. After a critical m, they observe a saturation effect where increasing m provides
diminishing returns. Zhang et al. (2019) explore how this critical batch size depends on SGD and
momentum hyperparameters in a noisy quadratic model. Since they stipulate constant gradient noise


-----

induced by sampling, their analysis results in steady state error rather than convergence at late times,
which may not reflect the true noise structure induced by sampling.

3 PROBLEM DEFINITION AND SETUP

We study stochastic gradient descent on a linear model with parameters w and feature map
**_ψ(x) ∈_** R[N] (with N possibly infinite). Some interesting examples of linear models are random
feature models, where ψ(x) = φ(Gx) for random matrix G and point-wise nonlinearity φ (Rahimi
& Recht, 2008; Mei & Montanari, 2020). Another interesting linearized setting is wide neural networks with neural tangent kernel (NTK) parameterization (Jacot et al., 2020; Lee et al., 2020). Here
the features are parameter gradients of the neural network function ψ(x) = ∇θf (x, θ)|θ0 at initialization. We will study both of these special cases in experiments.

We optimize the set of parameters w by SGD to minimize a population loss of the form

_L(w) =_ (w **_ψ(x)_** _y(x))[2][E]_ (1)
_·_ _−_ **x** _p(x)_ _[,]_

_∼_

D

where x are input data vectors associated with a probability distribution p(x), ψ(x) is a nonlinear
feature map and y(x) is a target function which we can evaluate on training samples. We assume
that the target function is square integrable _y(x)[2]_ **x** _[<][ ∞]_ [over][ p][(][x][)][. Our aim is to elucidate how]

this population loss evolves during stochastic gradient descent on w. We derive a formula in terms
of the eigendecomposition of the feature correlation matrix and the target function

_N_

**Σ =** **_ψ(x)ψ(x)[⊤]_** **x** [=] _λkuku[⊤]k_ _[,]_ _y(x) =_ _vku[⊤]k_ **_[ψ][(][x][) +][ y][⊥][(][x][)][,]_** (2)

_k=1_ _k_

X X

where _y_ (x)ψ(x) = 0. We justify this decomposition of y(x) in the Appendix A using an
_⟨_ _⊥_ _⟩_
eigendecomposition and show that it is general for target functions and features with finite variance.

During learning, parameters w are updated to estimate a target function y which, as discussed above,
can generally be expressed as a linear combination of features y = w[∗] **_ψ + y_** . At each time step

_·_ _⊥_
_t, the weights are updated by taking a stochastic gradient step on a fresh mini-batch of m examples_


**wt+1 = wt**
_−_ _m[η]_


_µ=1_ **_ψt,µ (wt · ψt,µ −_** _yt,µ),_ (3)

X


where each of the vectors ψt,µ are sampled independently and yt,µ = w[∗] _· ψt,µ. The learning rate_
_η controls the gradient descent step size while the batch size m gives a empirical estimate of the_
gradient at timestep t. At each timestep, the test-loss, or generalization error, has the form

_Lt =_ (wt **_ψ(x)_** **w[∗]** **_ψ(x)_** _y_ (x))[2][E] _y_ (x)[2] _,_ (4)
_·_ _−_ _·_ _−_ _⊥_ **x** [= (][w][t][ −] **[w][∗][)][⊤][Σ][(][w][t][ −]** **[w][∗][) +]** _⊥_

which quantifies exactly the test error of the vectorD **wt. Note, however, that Lt is a random variable**
since wt depends on the precise history of sampled feature vectors Dt = {ψt,µ}. Our theory, which
generalizes the recursive method of (Werfel et al., 2004) allows us to compute the expected test
loss by averaging over all possible sequences to obtain _Lt_ _t_ . Our calculated learning curves are
_⟨_ _⟩D_
not limited to the one-pass setting, but rather can accommodate sampling minibatches from a finite
training set with replacement and testing on a separate test set which we address in Section 4.4.

In summary, we will develop a theory that predicts the expected test loss ⟨Lt⟩Dt averaged over
training sample sequences Dt in terms of the quantities {λk, vk, _y⊥(x)[2]_ **x[}][. This will reveal how]**

the structure in the data and the learning problem influence test error dynamics during SGD. This
theory is a quite general analysis of linear models on square loss, analyzing the performance of
linearized models on arbitrary data distributions, feature maps ψ, and target functions y(x).

4 ANALYTIC FORMULAE FOR LEARNING CURVES

4.1 LEARNABLE AND NOISE FREE PROBLEMS

Before studying the general case, we first analyze the setting where the target function is learnable,
meaning that there exist weights w[∗] such that y(x) = w[∗] _· ψ(x). For many cases of interest, this_


-----

is a reasonable assumption, especially when applying our theory to real datasets by fitting an atomic
measure on P points _P[1]_ _µ_ _[δ][(][x][ −]_ **[x][µ][)][. We will further assume that the induced feature distribution]**

is Gaussian so that all moments of ψ can be written in terms of the covariance Σ. We will remove

P

these assumptions in later sections.
**Theorem 1. Suppose the features ψ follow a Gaussian distribution ψ ∼N** (0, Σ) and the target
_function is learnable in these features y = w[∗]_ _· ψ. After t steps of SGD with minibatch size m and_
_learning rate η, the expected (over possible sample sequences Dt) test loss ⟨Lt⟩Dt has the form_

_⟨Lt⟩Dt = λ[⊤]A[t]v[2]_ _, A = (I −_ _η diag(λ))[2]_ + _[η]m[2]_ _[diag]_ **_λ[2][]_** + _[η]m[2]_ **_[λλ][⊤]_** (5)

_where λ is a vector containing the eigenvalues of Σ and v[2]_ _is a vector containing elements _ (v[2])k =
_vk[2]_ [= (][u][k][ ·][ w][∗][)][2][ for eigenvectors][ u][k][ of][ Σ][. The function diag][(][·][)][ constructs a diagonal matrix with]
_the argument vector placed along the diagonal._

_Proof. See Appendix B for the full derivation._ We will provide a brief sketch of the proof
here. The strategy of the proof relies on the fact that ⟨Lt⟩ = Tr Σ Ct where Ct =
(wt **w[∗]) (wt** **w[∗])[⊤]** _t_ [. We derive the following recursion relation for this error matrix]
_−_ _−_ _D_

**Ct+1 = (I** _ηΣ)Ct(I_ _ηΣ) +_ _[η][2]_ (6)
_−_ _−_ _m_ [[][ΣC][t][Σ][ +][ Σ][Tr][ (][ΣC][t][)]]

The loss only depends on ck,t = u[⊤]k **[C][t][u][k][. Solving the recurrence,][ c][t][ =][ A][t][v][2][ and using][ ⟨][L][t][⟩]** [=]

_k_ _[λ][k][u]k[⊤][C][t][u][k][ =][ P]k_ _[c][k,t][λ][k][ =][ λ][⊤][A][t][v][2][, we obtain the desired result.]_

P

Below we provide some immediate interpretations of this result.

-  The matrix A contains two components; a matrix (I − _η diag(λ))[2]_ which represents
the time-evolution of the loss under average gradient updates. The remaining matrix
_η[2]_

_m_ diag(λ[2]) + λλ[⊤][] arises due to fluctuations in the gradients, a consequence of the stochastic sampling process.
 

-  The test loss obtained when training directly on the population loss can be obtained by taking the
minibatch size m →∞. In this case, A → (I − _η diag(λ))[2]_ and one obtains the population loss
_L[pop]t_ = _k_ _[v]k[2][λ][k][(1][ −]_ _[ηλ][k][)][2][t][. This population loss can also be obtained by considering small]_

learning rates, i.e. the η → 0 limit, where A = (I − _η diag(λ))[2]_ + O(η[2]).

-  For general[P] λ and η[2]/m > 0, A is non-diagonal, indicating that the components **u1, ..., uk** are
_{_ _}_
not learned independently as t increases like for L[pop]t, but rather interact during learning due to
non-trivial coupling across eigenmodes at large η[2]/m. This is unlike offline theory for learning
in feature spaces (kernel regression), (Bordelon et al., 2020; Canatar et al., 2020), this observation
of mixing across covariance eigenspaces agrees with a recent analysis of SGD, which introduced
recursively defined “mixing terms” that couple each mode’s evolution (Varre et al., 2021).

-  Though increasing m always improves generalization at fixed time t (proof given in Appendix D),
learning with a fixed compute budget (number of gradient evaluations) C = tm, can favor smaller
batch sizes. We provide an example of this in the next sections and Figure 1 (d)-(f).

-  The lower bound _Lt_ **_λ[⊤]v[2][ h](1_** _η)[2]_ + _[η]m[2]_ can be used to find necessary stability con_⟨_ _⟩≥_ _−_

_[|][λ][|][2][i][t]_ _η_

ditions on m, η. This bound implies that ⟨Lt⟩ will diverge if m < 2−η _[|][λ][|][2][. The learning rate must]_

be sufficiently small and the batch size sufficiently large to guarantee convergence. This stability
condition depends on the features through |λ|[2] = _k_ _[λ]k[2][. One can derive heuristic optimal batch]_

sizes and optimal learning rates through this lower bound. See Figure 2 and Appendix C.

[P]

4.1.1 SPECIAL CASE 1: UNSTRUCTURED ISOTROPIC FEATURES

This special case was previously analyzed by Werfel et al. (2004) which takes Σ = I ∈ R[N] _[×][N]_ and
_m = 1. We extend their result for arbitrary m, giving the following learning curve_


_t_
_η[2]_ _||w[∗]||[2]_ _,_ _⟨L[∗]t_ _[⟩]Dt_ [=] 1 −
 


_t_
_||w[∗]||[2],_ (7)



_⟨Lt⟩Dt =_ (1 − _η)[2]_ + [1 +]m[ N]



_m + N + 1_


-----

10[0] 10[1] 10[2] 10[3] 10[4]

|100 Lt 101|MNIST 8-9 Gauss. Theory et/N|
|---|---|


MNIST 8-9
Gauss. Theory
e t/N

t

(c) MNIST Random ReLU Features

10[0] 10[0]

= 0.1 4 × 10 1

L/mC6 × 10 1 = 0.1= 0.5= 1.0 *** L/mC10 1 = 0.3= 0.5 L/mC3 × 102 × 10 11 = 0.1

= 0.3

4 × 10 1 = 0.5

10 2 10 1

10[0] 10[1] 10[2] 10[3] 10[0] 10[1] 10[0] 10[1]

m m m


(f) Fixed Compute ReLU MNIST


10[0] 10[0]

10 2

Lt 1010 46 N = 100 Lt10 1 N = 100

10 8 NN = 200 = 500 N = 200N = 500

10 10 N = 1000 N = 1000

0 500 1000 1500 2000 2500 10[0] 10[1] 10[2] 10[3]

t t


(a) Isotropic Features

10[0]

= 0.1 - 

6 × 10 1 = 0.5 - 

= 1.0 - 

4 × 10 1

10[0] 10[1] 10[2]

m

(d) Fixed Compute Isotropic


(b) Power Law Features

10[0]

= 0.1

= 0.3

= 0.5

L/mC10 1

10 2

10[0] 10[1]

m

(e) Fixed Compute Power Law


Figure 1: Isotropic features generated as ψ ∼N (0, I) have qualitatively different learning curves
than power-law features observed in real data. Black dashed lines are theory. (a) Online learning
with N -dimensional isotropic features gives a test loss which scales like Lt _e[−][t/N]_ for any target
_function, indicating that learning requires t_ _N steps of SGD, using the optimal learning rates ∼_
_m_ _∼_
_η[∗]_ = _N_ +m+1 [. (b) Power-law features][ ψ][ ∼N] [(0][,][ Λ][)][ with][ Λ][kl][ =][ δ][k,l][k][−][2][ have non-extensive give a]

_power-law scaling Lt_ _t[−][β]_ with exponent β = ON (1). (c) Learning to discrimninate MNIST 8’s
and 9’s with N = 4000 ∼ dimensional random ReLU features (Rahimi & Recht, 2008), generates a
power law scaling at large t, which is both quantitatively and qualitatively different than the scaling
predicted by isotropic features e[−][t/N] . (d)-(f) The loss at a fixed compute budget C = tm = 100 for
(d) isotropic features, (e) power law features and (f) MNIST ReLU random features with simulations
(dots average and standard deviation for 30 runs). Intermediate batch sizes are preferable on real
data.

where the second expression has optimal η. First, we note the strong dependence on the ambient
dimension N : as N _m, learning happens at a rate_ _Lt_ _e[−][tm/N]_ . Increasing the minibatch size
_≫_ _⟨_ _⟩∼_
_m improves the exponential rate by reducing the gradient noise variance. Second, we note that this_
feature model has the same rate of convergence for every learnable target function y. At small m,
the convergence at any learning rate η is much slower than the convergence of the m →∞ limit,
_Lpop = (1 −_ _η)[2][t]||w[∗]||[2]_ which does not suffer from a dimensionality dependence due to gradient
noise. Lastly, for a fixed compute budget C = tm, the optimal batch size is m[∗] = 1; see Figure 1
(d). This can be shown by differentiating _LC/m_ with respect to m (see Appendix E). In Figure 1
(a) we show theoretical and simulated learning curves for this model for varying values of N at the
optimal learning rate and in Figure 1 (d), we show the loss as a function of minibatch size for a fixed
compute budget C = tm = 100. While fixed C represents fixed sample complexity, we stress that
it may not represent wall-clock run time when data parallelism is available (Shallue et al., 2018).

4.1.2 SPECIAL CASE 2: POWER LAWS AND EFFECTIVE DIMENSIONALITY

Realistic datasets such as natural images or audio tend to exhibit nontrivial correlation structure,
which often results in power-law spectra when the data is projected into a feature space, such as a
randomly intialized neural network (Spigler et al., 2020; Canatar et al., 2020; Bahri et al., 2021).
In the _[η]m[2]_

_λkvk[2]_ _[≪]_ [1][ limit, if the feature spectra and task specra follow power laws,][ λ][k][ ∼] _[k][−][b][ and]_
law: _[∼]Lt[k][−][a][ with]Ct[−][ a, b >][β],_ _β[ 1] =[, then Theorem 1 implies that generalization error also falls with a power]a−b_ 1 where C is a constant. See Appendix G for a derivation with
_⟨_ _⟩∼_

saddle point integration. Notably, these predicted exponents we recovered as a special case of our
theory agree with prior work on SGD with power law spectra, which give exponents in terms of the


-----

10[0]

10 2

10 4

10 6

10 8


|Va b = 0.6 b = 0.8|rying b|
|---|---|
|b = 1.1 b = 1.4 b = 1.7 b = 2.0 100|101|


30 Optimal m Scales with | |[2] 2.0 log C/m 0.90

True Optimum

25 | |[2] 0.45

20 k k 0.000.45

m 15 0.90

10 1.35

5 1.80

10 1 2.25

2 4 6 8 10 10[0] 10[2]

| |[2] m


(a) Power Law Exponents b


(b) Optimal Batchsize vs λ


(c) Hyper-parameter Dependence


Figure 2: Optimal batch size depends on feature structure and noise level. (a) For power law features
_λk_ _k[−][b], λkvk[2]_
exponent ∼ _b. Each color is a different[∼]_ _[k][−][a][, the][ m][ dependence of the loss] b value, evenly spaced in[ L][C/m] [0[ depends strongly on the feature].6, 2.5] with a = 2.5, C = 500._
Solid lines show exact theory while dashed lines show the error predicted by approximating the mode
coupling term _[η]m[2]_ **_[λλ][⊤]_** [with decoupled][ η]m[2] [diag][(][λ][2][)][. Mode coupling is thus necessary to accurately]

predict optimal m. (b) The optimal m scales proportionally with |λ|[2] _≈_ 2b1−1 [. We plot the lower]

bound mmin (black), the heuristic optimum (m which optimizes a lower bound for L, green) and
2η

2 _η_ _[|][λ][|][2][ (red). (c) The loss at fixed compute][ C][ = 150][,][ a][ = 2][,][ b][ = 0][.][85][, optimal batchsize][ m][ for]_
_−_
each η shown in dashed black. For sufficiently small η, the optimal batchsize is m = 1. For large η,
it is better to trade off update steps for denoised gradients resulting in m[∗] _> 1._

feature correlation structure (Berthier et al., 2020; Dieuleveut et al., 2016; Velikanov & Yarotsky,
2021; Varre et al., 2021). Further, our power law scaling appears to accurately match the qualitative
behavior of wide neural networks trained on realistic data (Hestness et al., 2017; Bahri et al., 2021),
which we study in Section 5.

We show an example of such a power law scaling with synthetic features in Figure 1 (b). Since the
total variance approaches a finite value as N →∞, the learning curves are relatively insensitive to
_N_, and are rather sensitive to the eigenspectrum through terms like |λ|[2] and 1[⊤]λ, etc. In Figure
1 (c), we see that the scaling of the loss is more similar to the power law setting than the isotropic
features setting in a random features model of MNIST, agreeing excellently with our theory. For this
model, we find that there can exist optimal batch sizes when the compute budget C = tm is fixed
(Figure 1 (e) and (f)). In Appendix C.1, we heuristically argue that the optimal batch size for power
law features should scale as, m[∗] _≈_ (2b1−1) [. Figure 2 tests this result.]

We provide further evidence of the existence of power law structure on realistic data in Figure 3
(a)-(c), where we provide spectra and test loss learning curves for MNIST and CIFAR-10 on ReLU
random features. The eigenvaluesboth follow power laws, generating power law test loss curves. These learning curves are contrasted λk ∼ _k[−][b]_ and the task power tail sums _n=k_ _[λ][n][v]n[2]_ _[∼]_ _[k][−][a][+1]_
with isotropically distributed data in R[784] passed through the same ReLU random feature model and

[P][∞]
we see that structured data distributions allow much faster learning than the unstructured data. Our
theory is predictive across variations in learning rate, batch size and noise (Figure 3).


4.2 ARBITRARY INDUCED FEATURE DISTRIBUTIONS: THE GENERAL SOLUTION

The result in the previous section was proven exactly for Gaussian vectors (see Appendix B). For
arbitrary distributions, we obtain a slightly more involved result (see Appendix F).

**Theorem 2. Let ψ(x)** _∈_ R[N] _be an arbitrary feature map with covariance matrix Σ_ =

_k_ _[λ][k][u][k][u]k[⊤][. After diagonalizing the features][ φ][k][(][x][) =][ u]k[⊤][ψ][(][x][)][, introduce the fourth moment]_
_tensor κ[4]ijkl_ [=][ ⟨][φ][i][φ][j][φ][k][φ][l][⟩][. The expected loss is exactly][ ⟨][L][t][⟩] [=][ P]k _[λ][k][c][k][(][λ][,][ κ][,][ v][, t][)][.]_

P

We provide an exact formula for ck in the Appendix F We see that the test loss dynamics depends
_only on the second and fourth moments of the features through quantities λk and κijkℓ_ respectively.
We recover the Gaussian result as a special case when κijkl is a simple weighted sum of these three
products of Kronecker tensors κ[Gauss]ijkl = λiλjδikδjl + _λiλkδijδkl +_ _λiλjδilδjk. As an alternative to_


-----

|100 Lt 101|MNIST ReLU CIFAR ReLU Iso. ReLU Gauss. Theory|
|---|---|


10[0] 10[1] 10[2] 10[3] 10[4]

MNIST ReLU
CIFAR ReLU
Iso. ReLU
Gauss. Theory

t

(c) Learning Curves m = 5

|100 Lt|= 0.00 = 0.50 = 1.00|
|---|---|



10[0] 10[1] 10[2] 10[3]

= 0.00
= 0.50
= 1.00

t

(f) Vary σ (20 Trials)


10[0]

10[0]

k10 1 2vnn

MNIST ReLU = kn10 1 MNIST ReLU

10 2 CIFAR ReLU CIFAR ReLU

Iso. ReLU Iso. ReLU

10 2

10[0] 10[1] 10[2] 10[3] 10[0] 10[1] 10[2] 10[3]

k k


(a) Feature Spectra

|100 6 × 101 4 × 101 Lt 3 × 101 2 × 101|= 0.05 = 0.10 = 0.20 Gauss. Theory|
|---|---|



10[0] 10[1] 10[2] 10[3]

= 0.05

= 0.10

= 0.20

Gauss. Theory

t

(d) Vary η (20 trials)


(b) Task Power Tail Sum


|100 Lt 1|Col2|
|---|---|


10[0] 10[1] 10[2] 10[3]

m = 1
m = 2
m = 5

t

(e) Vary m (20 Trials)


Figure 3: Structure in the data distribution, nonlinearity, batchsize and learning rate all influence
learning curves. (a) ReLU random feature embedding in N = 4000 dimensions of MNIST and
CIFAR images have very different eigenvalue scalings than spherically isotropic vectors in 784
dimensions. (b) The task power spectrum decays much faster for MNIST than for random isotropic
vectors. (c) Learning curves reveal the data-structure dependence of test error dynamics. Dashed
lines are theory curves derived from equation. (d) Increasing the learning rate increases the initial
speed of learning but induces large fluctuations in the loss and can be worse at large t. Experiment
curves averaged over 20 random trajectories of SGD. (e) Increasing the batch size alters both the
average test loss Lt and the variance. (f) Noise in the target values during training produces an
asymptotic error L which persists even as t .
_∞_ _→∞_

the above closed form expression for _Lt_, a recursive formula which tracks N mixing coefficients
_⟨_ _⟩_
has also been used to analyze the test loss dynamics for arbitrary distributions (Varre et al., 2021).

Next we show that a regularity condition, similar to those assumed in other recent works (Jain et al.,
2018; Berthier et al., 2020; Varre et al., 2021), on the fourth moment structure of the features allows
derivation of an upper bound which is qualitatively similar to the Gaussian theory.
**Theorem 3. If the fourth moments satisfy** **_ψψ[⊤]Gψψ[⊤]_** _⪯_ (α + 1)ΣGΣ + αΣTrΣG for any
_positive-semidefinite G, then_

_Lt_ **_λ[⊤]A[t]v[2]_** _, A = (I_ _η diag(λ))[2]_ + _[αη][2]_ _diag(λ[2]) + λλ[⊤][]_ _._ (8)
_≤_ _−_ _m_



We provide this proof in Appendix F.1. We note that the assumed bound on the fourth moments is
tight for Gaussian features with α = 1, recovering our previous theory. Thus, if this condition on
the fourth moments is satisfied, then the loss for the non-Gaussian features is upper bounded by the
Gaussian test loss theory with the batch size effectively altered ˜m = m/α.

The question remains whether the Gaussian approximation will provide an accurate model on re_alistic data. We do not provide a proof of this conjecture, but verify its accuracy in empirical_
experiments on MNIST and CIFAR-10 as shown in Figure 3. In Appendix Figure F.1, we show that
the fourth moment matrix for a ReLU random feature model and its projection along the eigenbasis
of the feature covariance is accurately approximated by the equivalent Gaussian model.

4.3 UNLEARNABLE OR NOISE CORRUPTED PROBLEMS

In general, the target function y(x) may depend on features which cannot be expressed as linear
combinations of features ψ(x), y(x) = w[∗] _· ψ(x) + y⊥(x). Let_ _y⊥(x)[2]_ **x** [=][ σ][2][. Note that][ y][⊥]

need not be deterministic, but can also be a stochastic process which is uncorrelated with ψ(x).


-----

10[0] M = 25 10[0]

M = 50
M = 100
M = 250

Ltrain Ltest

M = 25

10 1 10 1 M = 50

M = 100
M = 250

10[0] 10[1] 10[2] 10[3] 10[0] 10[1] 10[2] 10[3]

t t


(a) MNIST Training Error


(b) MNIST Test Error


Figure 4: Training and test errors of a model trained on a training set of size M can be computed
with the Ct matrix. Dashed black lines are theory. (a) The training error for MNIST random feature
model approaches zero asymptotically. (b) The test error saturates to a quantity dependent on M .

**Theorem 4. For a target function with unlearnable variance** _y[2]_ = σ[2] _trained on Gaussian ψ,_
_⊥_
_the expected test loss has the form_

_Lt_ _σ[2]_ = λ[⊤]A[t]v[2] + [1] (9)
_⟨_ _⟩−_ _m_ _[η][2][σ][2][λ][⊤][(][I][ −]_ **[A][)][−][1][(][I][ −]** **[A][t][)][λ]**


_which has an asymptotic, irreducible error ⟨L∞⟩_ = σ[2] + _m[1]_ _[η][2][σ][2][λ][⊤][(][I][ −]_ **[A][)][−][1][λ][ as][ t][ →∞][.]**

See Appendix H for the proof. The convergence to the asymptotic error takes the form ⟨Lt − _L∞⟩_ =
**_λ[⊤]A[t][  ]v[2]_** _−_ _m[1]_ _[η][2][σ][2][(][I][ −]_ **[A][)][−][1][λ]** . We note that this quantity is not necessarily monotonic in t and

can exhibit local maxima for sufficiently large σ[2], as in Figure 3 (f).


4.4 TEST/TRAIN SPLITS

Rather than interpreting our theory as a description of the average test loss during SGD in a one-pass
setting, where data points are sampled from the a distribution at each step of SGD, our theory can be
suitably modified to accommodate multiple random passes over a finite training set. To accomplish
this, one must first recognize that the training and test distributions are different.
**Theorem 5. Let ˆp(x) =** _M1_ _µ_ _[δ][(][x][ −]_ **[x][µ][)][ be the empirical distribution on the][ M][ training data]**

_points and let_ **Σ[ˆ]** = **_ψ(x)ψ(Px)[⊤]_** **x** _pˆ(x)_ [=][ P]k _λ[ˆ]kuku[⊤]k_ _[be the feature correlation matrix on this]_

_∼_

_training set. Let p(x) be the test distribution Σ its corresponding feature correlation. Then we have_

_⟨Ltrain,t⟩_ = Tr **ΣCˆ** _t_ _,_ _⟨Ltest,t⟩_ = Tr [ΣCt]
h i

**Ct+1 = (I −** _ηΣ[ˆ]_ )Ct(I − _ηΣ[ˆ]_ ) + _[η]m[2]_ **_ψ(x)ψ(x)[⊤]Ctψ(x)ψ(x)[⊤]_** **x∼pˆ(x)** _[−]_ **ΣC[ˆ]** _tΣ[ˆ]_ (10)

h i

We provide the proof of this theorem in Appendix I. The interpretation of this result is that it provides the expected training and test loss if, at each step of SGD, m points from the training set
_{x[1], ..., x[M]_ _} are sampled uniformly with replacement and used to calculate a stochastic gradient._
Note that while Σ can be full rank, the rank of **Σ[ˆ]** has rank upper bounded by M, the number of
training samples. The recurrence for Ct can again be more easily solved under a Gaussian approximation which we employ in Figure 4. Since learning will only occur along the M dimensional
subspace spanned by the data, the test error will have an irreducible component at large time, as
evidenced in Figure 4. While the training errors continue to go to zero, the test errors saturate at a
_M_ -dependent final loss. This result can also allow one to predict errors on other test distributions.


5 COMPARING NEURAL NETWORK FEATURE MAPS

We can utilize our theory to compare how wide neural networks of different depths generalize when
trained with SGD on a real dataset. With a certain parameterization, large width NNs are approximately linear in their parameters (Lee et al., 2020). To predict test loss dynamics with our theory,
it therefore suffices to characterize the geometry of the gradient features ψ(x) = **_θf_** (x, θ). In
_∇_


-----

10[0] Depth = 1 Depth = 1 Depth = 2

10 1 Depth = 2Depth = 5 10[0] Depth = 2Depth = 5 10[0] Depth = 3Depth = 6

k1010 23 2vnn k Lt

10 4 =n 10 1

10 5 10 1

10[0] 10[1] 10[2] 10[3] 10[4] 10[0] 10[1] 10[2] 10[3] 10[4] 10[0] 10[1] 10[2] 10[3] 10[4]

k k t


(a) MNIST NTK Spectra

1010[0]1 CONVMLPFit 9 × 108 × 10 11 CONVMLPFit 9 × 1010[0]1 CONVMLPTheory

k10101010 2345 2vnn = kn7 × 106 × 10 11 Lt8 × 107 × 106 × 10 111

10 6 5 × 10 1 5 × 10 1

10[0] 10[1] 10[2] 10[3] 10[0] 10[1] 10[2] 10[3] 10[1] 10[2] 10[3] 10[4]

k k t


(d) CIFAR-10 NTK Spectra


(b) MNIST Task Spectra

9 × 10 1

8 × 10 1 Fit

n
n 7 × 10 1

k
=n6 × 10 1

5 × 10 1

10[0] 10[1] 10[2] 10[3]

k

(e) CIFAR-10 Task Spectra


(c) Test Loss Scaling Laws

10[0] MLP

9 × 10 1

8 × 10 1

7 × 10 1

6 × 10 1

5 × 10 1

10[1] 10[2] 10[3]

t

(f) Test Loss Scalings


Figure 5: ReLU neural networks of depth D and width 500 are trained with SGD on full MNIST. (a)(b) Feature and spectra are estimated by diagonalizing the infinite width NTK matrix on the training
data. We fit a simple power law to each of the curves λk _k[−][b]_ and vk[2]
loss during SGD (color) compared to theoretical power-law scalings ∼ _t[−][∼][a][−]b[k][1][−][a](dashed black). Deeper[. (c) Experimental test]_

networks train faster due to their slower decay in their feature eigenspectra λk, though they have
similar task spectra. (d)-(f) The spectra and test loss for convolutional and fully connected networks
on CIFAR-10. The CNN obtains a better convergence exponent due to its faster decaying task
spectra. The predicted test loss scalings (dashed black) match experiments (color).

Figure 5, we show the Neural Tangent Kernel (NTK) eigenspectra and task-power spectra for fully
connected neural networks of varying depth, calculated with the Neural Tangents API (Novak et al.,
2020). We compute the kernel on a subset of 10, 000 randomly sampled MNIST images and estimate the power law exponents for the kernel and task spectra λk and vk[2][. Across architectures,]
the task spectra vk[2] [are highly similar, but that the kernel eigenvalues][ λ][k][ decay more slowly for]
deeper models, corresponding to a smaller exponent b. As a consequence, deeper neural network
models train more quickly during stochastic gradient descent as we show in Figure 5 (c). After
fitting power laws to the spectra λk _k[−][b]_ and the task power vk[2]
test loss dynamics (color) for a width-500 neural network model with the predicted power-law scal- ∼ _[∼]_ _[k][−][a][, we compared the true]_
ings β = _[a][−]b_ [1] from the fit exponents a, b. The predicted scalings from NTK regression accurately

describe trained width-500 networks. On CIFAR-10, we compare the scalings of the CNN model
and a standard MLP and find that the CNN obtains a better exponent due to its faster decaying tail
sum _n=k_ _[λ][n][v]n[2]_ [. We stress that the exponents][ β][ were estimated from our one-pass theory, but were]
utilized experiments on a finite training set. This approximate and convenient version of our theory
is quite accurate across these varying models, in line with recent conjectures about early training

[P][∞]
dynamics (Nakkiran et al., 2021).


6 CONCLUSION

Studying a simple model of SGD, we were able to uncover how the feature geometry governs the
dynamics of the test loss. We derived average learning curves _Lt_ for both Gaussian and general
_⟨_ _⟩_
features and showed conditions under which the Gaussian approximation is accurate. The proposed
model allowed us to explore the role of the data distribution and neural network architecture on
the learning curves, and choice of hyperparameters on realistic learning problems. While our theory
accurately describes networks in the lazy training regime, average case learning curves in the feature
learning regime would be interesting future extension. Further extensions of this work could be used
to calculate the expected loss throughout curriculum learning where the data distribution evolves
over time as well as alternative optimization strategies such as SGD with momentum.


-----

REPRODUCIBILITY STATEMENT

[The code to reproduce the experimental components of this paper can be found here https://](https://github.com/Pehlevan-Group/sgd_structured_features)
[github.com/Pehlevan-Group/sgd_structured_features, which contains jupyter](https://github.com/Pehlevan-Group/sgd_structured_features)
notebook files which we ran in Google Colab. More details about the experiments can be found in
Appendix J. Generally, detailed derivations of our theoretical results are provided in the Appendix.

ACKNOWLEDGEMENTS

We thank the Harvard Data Science Initiative and Harvard Dean’s Competitive Fund for Promising Scholarship for their support. We also thank Jacob Zavatone-Veth for useful discussions and
comments on this manuscript.

REFERENCES

Andreas Anastasiou, Krishnakumar Balasubramanian, and Murat A. Erdogdu. Normal approximation for stochastic gradient descent via non-asymptotic rates of martingale clt. In Alina Beygelzimer and Daniel Hsu (eds.), Proceedings of the Thirty-Second Conference on Learning Theory,
volume 99 of Proceedings of Machine Learning Research, pp. 115–137, Phoenix, USA, 25–28
[Jun 2019. PMLR. URL http://proceedings.mlr.press/v99/anastasiou19a.](http://proceedings.mlr.press/v99/anastasiou19a.html)
[html.](http://proceedings.mlr.press/v99/anastasiou19a.html)

Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural
scaling laws. arXiv preprint arXiv:2102.06701, 2021.

Peter L Bartlett, Philip M Long, G´abor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070, 2020.

Carl Bender and Steven Orszag. Advanced Mathematical Methods for Scientists and Engineers:
_Asymptotic Methods and Perturbation Theory, volume 1. 01 1999. ISBN 978-1-4419-3187-0._
doi: 10.1007/978-1-4757-3069-2.

Rapha¨el Berthier, Francis Bach, and Pierre Gaillard. Tight nonparametric convergence rates for
stochastic gradient descent under the noiseless linear model, 2020.

M Biehl and P Riegler. On-line learning with a perceptron. Europhysics Letters (EPL), 28(7):525–
530, dec 1994. doi: 10.1209/0295-5075/28/7/012. [URL https://doi.org/10.1209/](https://doi.org/10.1209/0295-5075/28/7/012)
[0295-5075/28/7/012.](https://doi.org/10.1209/0295-5075/28/7/012)

Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in
kernel regression and wide neural networks. In International Conference on Machine Learning,
pp. 1024–1034. PMLR, 2020.

Abdulkadir Canatar, B. Bordelon, and C. Pehlevan. Spectral bias and task-model alignment explain
generalization in kernel regression and infinitely wide neural networks. Nature Communications,
12:1–12, 2020.

Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. Advances in Neural Information Processing Systems, 32:10836–10846,
2019.

Yuan Cao, Quanquan Gu, and Misha Belkin. Risk bounds for over-parameterized maximum margin
classification on sub-gaussian mixtures. In Thirty-Fifth Conference on Neural Information Pro_[cessing Systems, 2021. URL https://openreview.net/forum?id=ChWy1anEuow.](https://openreview.net/forum?id=ChWy1anEuow)_

Niladri S Chatterji and Philip M Long. Finite-sample analysis of interpolating linear classifiers in
the overparameterized regime. Journal of Machine Learning Research, 22(129):1–30, 2021.

K. L. Chung. On a Stochastic Approximation Method. The Annals of Mathematical Statistics, 25
[(3):463 – 483, 1954. doi: 10.1214/aoms/1177728716. URL https://doi.org/10.1214/](https://doi.org/10.1214/aoms/1177728716)
[aoms/1177728716.](https://doi.org/10.1214/aoms/1177728716)


-----

Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large stepsizes. The Annals of Statistics, 44(4):1363 – 1399, 2016. doi: 10.1214/15-AOS1391. URL
[https://doi.org/10.1214/15-AOS1391.](https://doi.org/10.1214/15-AOS1391)

Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger convergence rates for least-squares regression. Journal of Machine Learning Research, 18, 02 2016.

John C. Duchi and Feng Ruan. Asymptotic optimality in stochastic optimization. The Annals of
_[Statistics, 49(1):21 – 48, 2021. doi: 10.1214/19-AOS1831. URL https://doi.org/10.](https://doi.org/10.1214/19-AOS1831)_
[1214/19-AOS1831.](https://doi.org/10.1214/19-AOS1831)

A. Engel and C. Van den Broeck. Statistical Mechanics of Learning. Cambridge University Press,
2001. doi: 10.1017/CBO9781139164542.

Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares al[gorithms. Journal of Machine Learning Research, 21(205):1–38, 2020. URL http://jmlr.](http://jmlr.org/papers/v21/19-734.html)
[org/papers/v21/19-734.html.](http://jmlr.org/papers/v21/19-734.html)

Nicolas Flammarion and Francis Bach. From averaging to acceleration, there is only a step-size. In
_Conference on Learning Theory, pp. 658–695. PMLR, 2015._

Nicolas Flammarion and Francis Bach. Stochastic composite least-squares regression with convergence rate o(1/n). In Satyen Kale and Ohad Shamir (eds.), Proceedings of the 2017
_Conference on Learning Theory, volume 65 of Proceedings of Machine Learning Research,_
[pp. 831–875. PMLR, 07–10 Jul 2017. URL https://proceedings.mlr.press/v65/](https://proceedings.mlr.press/v65/flammarion17a.html)
[flammarion17a.html.](https://proceedings.mlr.press/v65/flammarion17a.html)

Sebastian Goldt, Madhu Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborov´a. Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student
setup. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso[ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/cab070d53bd0d200746fb852a922064a-Paper.pdf)
[cab070d53bd0d200746fb852a922064a-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/cab070d53bd0d200746fb852a922064a-Paper.pdf)

Sebastian Goldt, Marc M´ezard, Florent Krzakala, and Lenka Zdeborov´a. Modeling the influence
of data structure on learning in neural networks: The hidden manifold model. Phys. Rev. X, 10:
[041044, Dec 2020. doi: 10.1103/PhysRevX.10.041044. URL https://link.aps.org/](https://link.aps.org/doi/10.1103/PhysRevX.10.041044)
[doi/10.1103/PhysRevX.10.041044.](https://link.aps.org/doi/10.1103/PhysRevX.10.041044)

Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc M´ezard, and Lenka Zdeborov´a. The gaussian equivalence of generative models for learning with shallow neural networks.
_Proceedings of Machine Learning Research, 145:1–46, 2021._

Mert Gurbuzbalaban, Umut Simsekli, and Lingjiong Zhu. The heavy-tail phenomenon in sgd. In
_International Conference on Machine Learning, pp. 3964–3975. PMLR, 2021._

Tom M. Heskes and Bert Kappen. Learning processes in neural networks. Phys. Rev. A, 44:2718–
[2726, Aug 1991. doi: 10.1103/PhysRevA.44.2718. URL https://link.aps.org/doi/](https://link.aps.org/doi/10.1103/PhysRevA.44.2718)
[10.1103/PhysRevA.44.2718.](https://link.aps.org/doi/10.1103/PhysRevA.44.2718)

Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad,
Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically, 2017.

Arthur Jacot, Berfin Simsek, Francesco Spadaro, Cl´ement Hongler, and Franck Gabriel.
Kernel alignment risk estimator: Risk prediction from training data. In NeurIPS,
2020. URL [https://proceedings.neurips.cc/paper/2020/hash/](https://proceedings.neurips.cc/paper/2020/hash/b367e525a7e574817c19ad24b7b35607-Abstract.html)
[b367e525a7e574817c19ad24b7b35607-Abstract.html.](https://proceedings.neurips.cc/paper/2020/hash/b367e525a7e574817c19ad24b7b35607-Abstract.html)

Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating stochastic gradient descent for least squares regression. In Conference On Learning Theory,
pp. 545–604. PMLR, 2018.


-----

Yann LeCun, Ido Kanter, and Sara A. Solla. Eigenvalues of covariance matrices: Application to
neural-network learning. Phys. Rev. Lett., 66:2396–2399, May 1991. doi: 10.1103/PhysRevLett.
[66.2396. URL https://link.aps.org/doi/10.1103/PhysRevLett.66.2396.](https://link.aps.org/doi/10.1103/PhysRevLett.66.2396)

Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Journal of Statistical Mechanics: Theory and Experiment, 2020(12):
[124002, Dec 2020. ISSN 1742-5468. doi: 10.1088/1742-5468/abc62b. URL http://dx.](http://dx.doi.org/10.1088/1742-5468/abc62b)
[doi.org/10.1088/1742-5468/abc62b.](http://dx.doi.org/10.1088/1742-5468/abc62b)

Bruno Loureiro, C´edric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc M´ezard, and
Lenka Zdeborov´a. Capturing the learning curves of generic features maps for realistic data sets
with a teacher-student model, 2021.

Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the
effectiveness of sgd in modern over-parametrized learning. In ICML, pp. 3331–3340, 2018. URL
[http://proceedings.mlr.press/v80/ma18a.html.](http://proceedings.mlr.press/v80/ma18a.html)

C. Mace and A. Coolen. Statistical mechanical analysis of the dynamics of learning in perceptrons.
_Statistics and Computing, 8:55–88, 1998._

Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve, 2020.

Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborov´a. Dynamical
mean-field theory for stochastic gradient descent in gaussian mixture classification, 2020.

Francesca Mignacco, Pierfrancesco Urbani, and Lenka Zdeborov´a. Stochasticity helps to navigate
rough landscapes: comparing gradient-descent-based algorithms in the phase retrieval problem,
2021.

Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good
online learners are good offline generalizers. In International Conference on Learning Represen_[tations, 2021. URL https://openreview.net/forum?id=guetrIHLFGI.](https://openreview.net/forum?id=guetrIHLFGI)_

Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural tangents: Fast and easy infinite neural networks in python. In
_[International Conference on Learning Representations, 2020. URL https://github.com/](https://github.com/google/neural-tangents)_
[google/neural-tangents.](https://github.com/google/neural-tangents)

Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes, 2018.

B. Polyak and A. Juditsky. Acceleration of stochastic approximation by averaging. Siam Journal on
_Control and Optimization, 30:838–855, 1992._

Phil Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic
dimension of images and its impact on learning. In International Conference on Learning Repre_[sentations, 2021. URL https://openreview.net/forum?id=XJk19XzGq2J.](https://openreview.net/forum?id=XJk19XzGq2J)_

Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt,
D. Koller, Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems,
[volume 20. Curran Associates, Inc., 2008. URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf)
[paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf.](https://proceedings.neurips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf)

Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning
_(Adaptive Computation and Machine Learning). The MIT Press, 2005._

Herbert Robbins and Sutton Monro. A Stochastic Approximation Method. The Annals of Math_ematical Statistics, 22(3):400 – 407, 1951._ doi: 10.1214/aoms/1177729586. [URL https:](https://doi.org/10.1214/aoms/1177729586)
[//doi.org/10.1214/aoms/1177729586.](https://doi.org/10.1214/aoms/1177729586)

David Ruppert. Efficient estimations from a slowly convergent robbins-monro process. 02 1988.


-----

David Saad and Sara Solla. Dynamics of on-line gradient descent learning for multilayer neural
networks. 04 1999.

Christopher J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and
George E Dahl. Measuring the effects of data parallelism on neural network training. arXiv
_preprint arXiv:1811.03600, 2018._

Alexander Shapiro. Asymptotic Properties of Statistical Estimators in Stochastic Programming.
_[The Annals of Statistics, 17(2):841 – 858, 1989. doi: 10.1214/aos/1176347146. URL https:](https://doi.org/10.1214/aos/1176347146)_
[//doi.org/10.1214/aos/1176347146.](https://doi.org/10.1214/aos/1176347146)

Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods:
empirical data versus teacher–student paradigm. Journal of Statistical Mechanics: Theory and
_Experiment, 2020(12):124001, Dec 2020. ISSN 1742-5468. doi: 10.1088/1742-5468/abc61d._
[URL http://dx.doi.org/10.1088/1742-5468/abc61d.](http://dx.doi.org/10.1088/1742-5468/abc61d)

Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. _arXiv preprint_
_arXiv:2009.14286, 2020._

Aditya Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. Last iterate convergence of sgd for
least-squares in the interpolation regime, 2021.

Maksim Velikanov and Dmitry Yarotsky. Universal scaling laws in the gradient descent training of
neural networks, 2021.

Justin Werfel, Xiaohui Xie, and H. Seung. Learning curves for stochastic gradient
descent in linear feedforward networks. In S. Thrun, L. Saul, and B. Sch¨olkopf
(eds.), _Advances in Neural Information Processing Systems,_ volume 16. MIT Press,
2004. URL [https://proceedings.neurips.cc/paper/2003/file/](https://proceedings.neurips.cc/paper/2003/file/f8b932c70d0b2e6bf071729a4fa68dfc-Paper.pdf)
[f8b932c70d0b2e6bf071729a4fa68dfc-Paper.pdf.](https://proceedings.neurips.cc/paper/2003/file/f8b932c70d0b2e6bf071729a4fa68dfc-Paper.pdf)

Yiming Ying and Massimiliano Pontil. Online gradient descent learning algorithms. Found. Comput.
_Math., 8(5):561–596, October 2008. ISSN 1615-3375._

Lu Yu, Krishnakumar Balasubramanian, Stanislav Volgushev, and Murat A. Erdogdu. An analysis
of constant step size sgd in the non-convex regime: Asymptotic normality and bias, 2020.

Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl, Chris
Shallue, and Roger B Grosse. Which algorithmic choices matter at which batch sizes? insights
from a noisy quadratic model. Advances in neural information processing systems, 32:8196–8207,
2019.

Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham M Kakade. Benign overfitting of constant-stepsize sgd for linear regression. arXiv preprint arXiv:2103.12692, 2021.


-----

A DECOMPOSITION OF THE FEATURES AND TARGET FUNCTION

Let y(x) be a square integrable target function with _y(x)[2]_ _< ∞. Define the following integral_
operator TK for kernel K(x, x[′]) = ψ(x) · ψ(x[′]):

_TK[φ](x[′]) =_ _p(x)K(x, x[′])φ(x)dx_ (11)
Z

We are interested in eigenfunctions of this operator, function φk for which TK[φk] = λkφk. For
kernels with finite trace _K(x, x)p(x)dx < ∞, Mercer’s theorem (Rasmussen & Williams, 2005)_
guarantees the existence of a set of orthonormal eigenfunctions. Since ψ(x) spans an N dimensional
function space, only N of the kernel eigenfunctions will have non-zero eigenvalue. Since the basisR
of kernel eigenfunctions (including the zero eigenvalue functions) is complete over the space of
square integrable functions. After ordering the eigenvalues λ1 > λ2 > ... > λN with λN +ℓ = 0,
we obtain the expansion


_y(x) =_


_⟨y(x)φk(x)⟩x φk(x) =_


_vkφk(x) + y⊥(x), y⊥(x) =_
_kX≤N_


_y(x)φk(x)_ _φk(x)_
_⟨_ _⟩_
_k>N_

X

(12)


Further, we can decompose the feature map in this basis ψ(x) = _k=1_ _√λkukφk(x)._ We
recognize through these decompositions the coefficients vk can be computed uniquely as vk =
_λk[−][1][/][2]u[⊤]k_ [P][N]
our theory. We see that the feature map’s decomposition above reveals that[⟨][ψ][(][x][)][y][(][x][)][⟩][. This provides a recipe for determining the necessary spectral quantities for] λk are also the eigenvalues of the feature correlation matrix Σ since

**Σ =** **_ψ(x)ψ(x)[⊤]_** = _λkλℓuku[⊤]ℓ_ _[⟨][φ][k][(][x][)][φ][ℓ][(][x][)][⟩]_ [=] _λkuku[⊤]k_ _[.]_ (13)

_kℓ_ _k_

X p X

A.1 FINITE SAMPLE SPACES

When we discuss experiments on MNIST or CIFAR, we use this technology for an atomic data
distribution p(x) = _M1_ _Mµ=1_ _[δ][(][x][ −]_ **[x][′][)][. Plugging this into the integral operator gives]**

P

_TK[φ](x) = [1]_ _K(x, x[µ])φ(x[µ])_ (14)

_M_

_µ_

X

We see that, restricting the domain to the set of points **x1, ..., xM**, this amounnts to solving a
1 _{_ _}_
matrix eigenvalue problem _M_ **_[Kφ][k][ =][ λ][k][φ][k][ where][ K][ ∈]_** [R][M] _[×][M][ is the kernel gram matrix with]_

entries Kµν = K(x[µ], x[ν]) and φk has entries φk,µ = φk(x[µ]).


B PROOF OF THEOREM 1

correlation matrix for this differenceLet ∆t = wt − **w[∗]** represent the difference between the current and optimal weights and define the


**Ct =** **∆t∆[⊤]t**


(15)
_Dt−1_ _[.]_


Using stochastic gradient descent,the Ct matrix satisfies the recursion wt+1 = wt − _ηgt with gradient vector gt =_ _m[1]_ _mi=1_ **_[ψ][i][ψ]i[⊤][∆][t][,]_**

P

**Ct+1 =** (∆t − _ηgt)(∆t −_ _ηgt)[⊤]_ _Dt_ [=][ C][t][ −] _[η]_ **gt∆[⊤]t** _−_ _η_ **∆tgt[⊤]** + η[2] **gtgt[⊤]** _._ (16)

First, note that since ψi are all independently sampled at timestep t, we can break up the average
into the fresh batch of m samples and an average over Dt−1


_⟨gt∆t⟩Dt = m[1]_


**_ψiψi[⊤]_**


**_ψi_** **∆∆[⊤]t**


(17)
_Dt−1_ [=][ ΣC][t][.]


_i=1_


-----

The last term requires computation of fourth moments

1
**gtgt[⊤]** = _m[2]_ **_ψiψi[⊤]_** **∆t∆[⊤]t**

_i,j_

X D

1
= **_ψiψi[⊤][C][t][ψ][j][ψ]j[⊤]_**

_m[2]_

_i,j_

X


_Dt−1_ **_[ψ][j][ψ]j[⊤]_**


(18)
**_ψi,ψj_**


(19)
**_ψi,ψj_** _[.]_


First, consider the case where i = j. Letting ψ = ψi, we need to compute terms of the form

_Ck,ℓ_ _ψjψkψℓψn_ _._ (20)
_⟨_ _⟩_
_k,ℓ_

X

For Gaussian random vectors, we resort to Wick-Isserlis theorem for the fourth moment

_ψjψkψlψn_ = _ψjψk_ _ψℓψn_ + _ψjψℓ_ _ψkψn_ + _ψjψn_ _ψℓψk_ (21)
_⟨_ _⟩_ _⟨_ _⟩⟨_ _⟩_ _⟨_ _⟩⟨_ _⟩_ _⟨_ _⟩⟨_ _⟩_

giving
**gtgt[⊤]** = _[m]m[ + 1]_ **ΣCtΣ + m[1]** **[Σ][ Tr][ (][ΣC][t][)][ .]** (22)

This correlation structure for gt implies that its covariance has the form

_⟨Covψ(gt)⟩Dt = m[1]_ **[ΣC][t][Σ][ + 1]m** **[Σ][Tr][(][ΣC][t][)][.]** (23)


Using the formula for **gtgt[⊤]**, we arrive at the following recursion relation for Ct


**Ct+1 = Ct** _ηCtΣ_ _ηΣCt + η[2][ m][ + 1]_ **ΣCtΣ + [1]** (24)
_−_ _−_ _m_ _m_ _[η][2][Σ][ Tr][ (][ΣC][t][)][ .]_


Since we are ultimately interested in the generalization error _Lt_ = **∆[⊤]t** **[Σ∆][t]** = TrΣCt =
_⟨_ _⟩_
_k_ _[λ][k][u]k[⊤][C][t][u][k][, it suffices to track the evolution of][ c][t,k][ =][ u]k[⊤][C][t][u][k]_
P


_ct+1,k =_ 1 2ηλk + η[2][ m][ + 1]
_−_ _m_



_ct,k + [1]_

_m_ _[η][2][λ][k]_


_λ[2]k_


_λjct,j._ (25)


Vectorizing this equation for c generates the following solution

**ct = A[t]c0, A = I** 2η diag(λ) + _[m][ + 1]_ _η[2]_ diag(λ[2]) + _[η][2]_ (26)
_−_ _m_ _m_ **_[λλ][⊤][.]_**

The coefficient c0,k = vk[2] [=] **u[⊤]k** **[w][∗][][2][. To get the generalization error, we merely compute][ ⟨][L][t][⟩]** [=]
**_λ[⊤]at = λ[⊤]A[t]v[2]_** as desired.
 

C PROOF OF STABILITY CONDITIONS

We will first establish the following lower bound on the loss, where without loss of generality we
assumed the maximum correlation is 1:

_t_

_Lt_ **_λ[⊤]v[2]_** (1 _η)[2]_ + _[η][2]_ _._ (27)
_≥_ _−_ _m_
 _[|][λ][|][2]_

We will then use this lower bound to provide necessary conditions on the learning rate and batch
size for stability of the loss evolution. First, note that the following inequality holds elementwise


**Aλ =** (I _η diag (λ))[2]_ + _[η][2]_ **_λ_** (1 _η)[2]_ + _[η][2]_ **_λ_** (28)
_−_ _m_ [diag][(][λ][2][) +][ η]m[2] **_[λ]_** _≥_ _−_ _m_
   _[|][λ][|][2]_

Repeating this inequality t times gives A[t]λ (1 _η)[2]_ + _[η]m[2]_ **_λ. Using the fact that Lt =_**
_≥_ _−_ _[|][λ][|][2][i][t]_

**_λ[⊤]A[t]v[2]_** gives the desired inequality. Note that this inequality is very close to the true result forh


-----

_t_

isotropic features λ = 1 which gives Lt (1 _η)[2]_ + _[η]m[2]_ [(][|][λ][|][2][ + 1)] . For anisotropic features
_∝_ _−_

with small learning rate, this bound becomes less tight. For the loss to converge to zero at large time,h i
the quantity in brackets must necessarily be less than one. This implies the following necessary
condition on the batchsize and learning rate

2m
_η <_ _m > mmin =_ _[η][|][λ][|][2]_ (29)

_m +_ **_λ_** _⇒_ 2 _η_
_|_ _|[2][ ⇐]_ _−_

where mmin is the minimal batch size for learning rate η and feature covariance eigenvalues λ.

C.1 HEURISTIC BATCH SIZE AND LEARNING RATE

We can derive heuristic optimal choices of the learning rate and batch size hyperparameters η, m at
a fixed compute budget which optimize the lower bound derived above.

C.1.1 FIXED LEARNING RATE

We will first consider optimizing only the batch size at a fixed learning rate η before discussing
the optimal m when η is chosen optimally. The loss at a fixed compute budget C = tm is lower
bounded by

_C/m_

_LC/m ≥_ **_λ[⊤]v[2]_** (1 − _η)[2]_ + _[η]m[2]_ (30)
 _[|][λ][|][2]_

For the purposes of optimization, we introduce x = 1/m and consider optimizing

_f_ (x) = x ln [A + Bx], A = (1 − _η)[2], B = η[2]|λ|[2]_ (31)

The first order optimality condition f _[′](x) = 0 implies that ln(A + Bx) +_ _A+BxBx_ [= 0][. Letting][ z][ =]

_A + Bx, this is equivalent to z ln z + z −_ _A = 0. This equation has solutions for all valid A ∈_ (0, 1)
giving solutions z ∈ (e[−][1], 1). Letting z(η), represent the solution to z + z ln z − (1 − _η)[2]_ = 0, the
optimal batchsize has the form

_B_ _η[2]_ **_λ_**
_m[∗](η) =_ _|_ _|[2]_ (32)

_z(A) −_ _A_ [=] _z(η) −_ (1 − _η)[2]_

We can gain intuition for this result by considering the limit of η → 0 and η → 1. First, in the
_η_ 0 limit, we find that z 2 so m[∗] 2 _η_ = 2mmin, making contact with the stability
_→_ _∼_ _[A][+1]_ _∼_ [2][η]−[|][λ][|][2]

bound derived in Appendix Section C. Thus for small learning rates, this heuristic optimum suggests
doubling the minimal stable batchsize for optimal convergence. At large learning rates, η ∼ 1 with
_A ∼_ 0, we find z ∼ _e[−][1]_ so m[∗](η) ∼ _eη[2]|λ|[2]. Thus for small η, we expect m[∗]_ to scale linearly with
_η while for large η, we expect a scaling of the form η[2]. In either case, the optimal batchsize scales_
with feature eigenvalues through the sum of the squares |λ|[2] = _k_ _[λ]k[2][.]_

C.1.2 HEURISTIC OPTIMAL LEARNING RATE AND BATCH SIZE

[P]

We will now examine what happens when one first optimizes loss bounmd with respect to the the
learning rate at any value of m and then subsequently optimizes over the batch size m. We can
easily find the η which minimizes the lower bound.
_∂_ _m_

(1 _η)[2]_ + _[η][2][|][λ][|][2]_ = 0 = _η[∗]_ = (33)

_∂η_ _−_ _m_ _⇒_ _m +_ **_λ_**

  _|_ _|[2]_

Note that this heuristic optimal learning rate is very close to the true optimum in the isotropic data
settingcompute η Ctrue = = tmm, the loss scales like+mN +1 _[≈]_ _mm+N_ [. Plugging this back into the loss bound, we find that at fixed]

_C/m_

**_λ_**
_LC/m ≥_ **_λ[⊤]v[2]_** _m +|_ _|[2]λ_ (34)
 _|_ _|[2]_ 

With the optimal choice of learning rate, the loss at fixed compute monotonically increases with
batch size, giving an optimal batchsize of m = 1. This shows that if the learning rate is chosen
optimally then small batch sizes give the best performance per unit of computation. This corresponds
to the hyperparameter choices (η, m) = 1+1|λ|[2][,][ 1] .
 


(1 _η)[2]_ + _[η][2][|][λ][|][2]_
_−_ _m_


= 0 =⇒ _η[∗]_ =


_∂η_


-----

D INCREASING m REDUCES THE LOSS AT FIXED t

We will show that for a fixed number of steps t, increasing the minibatch size m can only decrease
the expected error. To do this, we will simply show that the derivative of the expected loss with
respect to m,
_∂_ _Lt_
_⟨_ _⟩_ = λ[⊤] _[∂][A][t]_ (35)

_∂m_ _∂m_ **[v][2][,]**

is always non-positive. The derivative of the t-th power of A can be identified with the chain rule
_∂A[t]_

(36)

_∂m_ [=][ ∂]∂m[A] **[A][t][−][1][ +][ A]** _∂m[∂][A]_ **[A][t][−][2][ +][ A][2][ ∂]∂m[A]** **[A][t][−][3][ +][ ...][ +][ A][t][−][1][ ∂]∂m[A]** _[.]_

Note that the matrix
_∂A_

diag(λ[2]) + λλ[⊤][] (37)

_∂m_ [=][ −] _m[η][2][2]_

has all non-positive entries. Thus we find that 

_∂_ _Lt_ _t−1_
_⟨_ _⟩_ = **_λ[⊤]A[n][ ∂][A]_** (38)

_∂m_ _∂m_ **[A][t][−][n][−][1][v][2][.]**

_n=0_

X

Note that since all entries in vk[2] [and][ A][t][−][n][−][1][ are non-negative, the vector][ z][n][ =][ A][t][−][n][−][1][v][2][ has]
non-negative entries. By the same argument, the vector qn = A[n]λ is also non-negative in each
entry. Therefore, each of the terms in _[∂]∂m[⟨][L][t][⟩]_ [above must be non-positive]


_t−1_ _∂A_

**_zn[⊤]_** _∂m_ **_[q][n][ =][ −]_** _[η]m[2]_
_n=0_

X


_∂_ _Lt_
_⟨_ _⟩_

_∂m_


_zn,k_ _δk,ℓλ[2]k_ [+][ λ][k][λ][ℓ] _qn,ℓ_ 0. (39)
_≤_
_k,ℓ_

X  


Thus we find _[∂]∂m[⟨][L][t][⟩]_ 0, implying that optimal _Lt_ is always obtained (possibly non-uniquely) at

_≤_ _⟨_ _⟩_
_m →∞._

E INCREASING m INCREASES THE LOSS AT FIXED C = tm ON ISOTROPIC
FEATURES


Unlike the previous section, which considered fixed t and varying m, in this section we consider
fixing the total number of samples (or gradient evaluations) which we call the compute budget
_C = tm. For a fixed compute budget C = tm, and unstructured N dimensional Gaussian features_
and optimal learning rate η[∗] = _m+mN_ +1 [, we have]

_C/m_

_N + 1_
_LC/m_ = _m + N + 1_ _||w[∗]||[2]._ (40)
 

Taking a derivative with respect to the batch size we get
_∂_ _∂_ _C_ _N + 1_

_∂m_ [log] _LC/m_ = _∂m_ _m_ [log] _m + N + 1_

 

_m + N + 1_ _C_

= _[C]_ + (41)

_m[2][ log]_ _N + 1_ _m(m + N + 1)_ _[>][ 0][.]_
 

This exercise demonstrates that, for the isotropic features, smaller batch-sizes are preferred at a fixed
compute budget C. This result does not hold for arbitrary spectra λk. In the general case, optimal
minibatch sizes can exist as we show in Figure 1 (e)-(f) for power law and MNIST spectra.

F PROOF OF THEOREM 2


Let Vec(·) denote a flattening of an N ×N matrix into a vector of length N [2] and let Mat(·) represent
a flattening of a 4D tensor into a N [2] _× N_ [2] two-dimensional matrix. We will show that the expected
loss (over _t) is_
_D_ _t_

_Lt_ = _λkct,kk, ct =_ **G +** _[η][2]_ Vec(vv[⊤]) R[N][ 2] (42)
_⟨_ _⟩_ _m_ [Mat][(][κ][)] _∈_

_k_  

X


-----

where [G]ij,kl = δikδjl 1 − _η(λi + λj) +_ _[η][2][(][m]m[−][1)]_ _λiλj_ and [v]k = uk · w[∗].
 

We rotate all of the feature vectors into the eigenbasis of the covariance, generating diagonalized
features φk = u[⊤]k **_[ψ][ and introduce the following fourth moment tensor]_**

_κijkl = ⟨φiφjφkφℓ⟩_ _._ (43)

We redefine Ct in the appropriate (rotated) basis by projecting onto the eigenvectors of the covariance
**Ct = U[⊤]** **∆t∆[⊤]t** **U,** (44)
where U = [u1, u2, ..., uN ]. With this definition, C’s dynamics take the form

**Ct+1 = Ct** **ΛCt** **CtΛ +** _[η][2][(][m][ −]_ [1)] **ΛCtΛ +** **_φφ[⊤]Ctφφ[⊤]_** _._ (45)
_−_ _−_ _m_

The elements of the matrix can be expressed with the fourth moment tensor


_κijklCk,ℓ._ (46)

_kℓ_

X


_φiφjφkφℓ_ _Ckℓ_ =
_⟨_ _⟩_
_kℓ_

X

We thus generate the following dynamics for Cij[t]

_Cij[t][+1]_ = 1 − _η(λi + λj) +_ _[η][2][(][m]m[ −]_ [1)]


Let ct = Vec(Ct), then we have

**ct+1 =** **G0 +** _[η][2]_ **ct, [G0]ij,kℓ** = δikδjℓ

_m_ [Mat][(][κ][)]

 


_Cij[t]_ [+][ η][2]

_m_


_κijklCkl[t]_ _[.]_ (47)
_kl_

X


_λiλj_


1 _η(λi + λj) +_ _[η][2][(][m][ −]_ [1)]
_−_ _m_


(48)


_λiλj_


Solving these dynamics for c, recognizing that c0 = Vec(vv[⊤]), and computing ⟨Lt⟩ = TrΣCt =

_k_ _[C][kk][λ][k][ gives the desired result.]_

P

F.1 PROOF OF THEOREM 3


Suppose that the features satisfy the regularity condition
**_ψψ[⊤]Gψψ[⊤]_** _⪯_ (α + 1)ΣGΣ + αΣTr (ΣG) (49)

Recalling the recursion relation for Ct

**Ct+1 = Ct** _ηΣCt_ _ηCtΣ + η[2][ m][2][ −]_ _[m]_ **ΣCtΣ +** _[η][2]_ **_ψψ[⊤]Ctψψ[⊤]_**
_−_ _−_ _m[2]_ _m_

**_Ct_** _ηΣCt_ _ηCtΣ + η[2][ m][2][ −]_ _[m]_ **ΣCtΣ +** _[η][2]_ (50)
_⪯_ _−_ _−_ _m[2]_ _m_ [[(][α][ + 1)][ΣC][t][Σ][ +][ α][Σ][Tr][C][t][Σ][]]

= (I _ηΣ)Ct(I_ _ηΣ) +_ _[αη][2]_ (51)
_−_ _−_ _m_ [[][ΣC][t][Σ][ +][ Σ][Tr][ΣC][t][]]

Defining that ck,t = u[⊤]k **[C][t][u][k][, we note][ c][t][+1][ ≤]** (I − _η diag(λ))[2]_ + _[η]m[2][α]_ diag(λ[2]) + λλ[⊤][] **_ct._**

Using the fact that Lt = c[⊤]t **_[λ][, we find]_**  

_Lt_ **_λ[⊤]_** (I _η diag(λ))[2]_ + _[η][2][α]_ diag(λ[2]) + λλ[⊤][][t] **_v[2]_** (52)
_≤_ _−_ _m_




which proves the desired bound.

G POWER LAW SCALINGS IN SMALL LEARNING RATE LIMIT

By either taking a small learning rate η or a large batch size, the test loss dynamics reduce to the test
loss obtained from gradient descent on the population loss. In this section, we consider the small
learning rate limit η → 0, where the average test loss follows


_λkvk[2][(1][ −]_ _[ηλ][k][)][2][t][.]_ (53)
_k=1_

X


_Lt_
_⟨_ _⟩∼_


-----

Gauss 1.0 4 1.0 | 4 Gauss[|] Gauss 1.0 4 1.0 | 4 Gauss[|] 0.04

0.8 0.8 0.04 0.8 0.8 0.03

0.6 0.6 0.6 0.6

0.02

0.4 0.4 0.02 0.4 0.4

0.2 0.2 0.2 0.2 0.01

0.0 0.0 0.00 0.0 0.0


(a) Non-Gaussian Effects on MNIST


(b) Non-Gaussian Effects on CIFAR


Figure F.1: Non-Gaussian effects are small on random feature models. (a)-(b) The first 20dimensions of the summed fourth moment matrix κ[4]ij [=][ u]i[⊤] **_ψψ[⊤]ψψ[⊤]_** **uj are plotted for the**
Gaussian approximation and the empirical fourth moment. Differences between the Gaussian approximation and true fourth moment matrices on this example are visible, but are only on the order
of 5% of the size of the entries in κ4.
_∼_

Under the assumption that the eigenvalue and target function power spectra both follow power laws
_λk ∼_ _k[−][b]_ and vk[2][λ][k][ ∼] _[k][−][a][, the loss can be approximated by an integral over all modes][ k]_

_∞_

_Lt_ = _k[−][a](1_ _ηk[−][b])[2][t]_ exp 2η ln(1 _ηk[−][b])t_ _a ln k_ _dk_ (54)
_⟨_ _⟩_ _k_ _−_ _∼_ Z1 _−_ _−_
X   

_∞_
_∼_ 1 exp _−2ηηk[−][b]t −_ _a ln k_ _dk, η →_ 0 (55)
Z

  

We identify the function f (k) = 2ηk[−][b] + [1]t [ln][ k][ and proceed with Laplace’s method (Bender &]

Orszag, 1999). This consists of Taylor expanding f (k) around its minimum to second order and
computing a Gaussian integral

2π

exp( _tf_ (k))dk exp _tf_ (k[∗]) exp( _tf_ (k[∗]))
Z _−_ _∼_ Z − _−_ 2[t] _[f][ ′′][(][k][∗][)(][k][ −]_ _[k][∗][)][2]_ _∼_ _−_ s _tf_ _[′′](k[∗])_ _[.]_

(56)
We must identify the k[∗] which minimizes f (k). The interpretation of this value is that it indexes the
mode which dominates the error at a large time t. The first order condition gives

_f_ _[′](k) = −2bηk[−][b][−][1]_ + _tk[a]_ [= 0 =]⇒ _k[∗]_ = (2bηt/a)[1][/b] _._ (57)

The second derivative has the form

_f_ _[′′](k[∗]) = 2b[2]ηk[−][b][−][2]_ _−_ _tk[1][2][ |][k][∗]_ [= 2][b][2][η][ (2][bηt/a][)][−][(][b][+2)][/b][ −] [1]t [(2][bηt/a][)][−][2][/b][ ∼] _[t][−][1][−][2][/b][.]_ (58)

Thus we are left with a scaling of the form

_Lt_ exp( _a/b ln t)t[1][/b]_ _t[−]_ _[a][−]b .[1]_ (59)
_⟨_ _⟩∼_ _−_ _∼_


H PROOF OF THEOREM 4

Let _y[2]_ = σ[2] and _y_ = 0, _y_ **_ψ_** = 0. The gradient descent updates take the following form
_⊥_ _⟨_ _⊥⟩_ _⟨_ _⊥_ _⟩_
**∆t+1 = ∆t** _ηgt with_
_−_ _m_

**gt = [1]** **_ψi_** **_ψi[⊤][∆][t]_** [+][ y][⊥][,i] _._ (60)

_m_

_i=1_

X  

Again, defining Ct = **∆t∆[⊤]t** we perform the average over each of the ψi vectors to obtain the
following recursion relation

**Ct+1 =** **∆t∆[⊤]t** _η_ **∆tgt[⊤]** _η_ **gt∆[⊤]t** + η[2] **gtgt[⊤]**
_−_ _−_

= Ct _ηΣCt_ _ηCtΣ +_ _[m][ + 1]_ **ΣCtΣ + [1]** (61)
_−_ _−_ _m_ _m_ **[Σ][Tr][ (][C][t][Σ][) +][ η][2][σ][2][Σ][.]**

Again, analyzing ct,k = u[⊤]k **[C][t][u][k][ we find]**

_ct+1,k =_ 1 − 2ηλk + η[2][ m]m[ + 1] _λ[2]k_ _ct,k + m[1]_ _λℓct,ℓ_ + η[2]σ[2]λk. (62)
 

X


-----

The vector ct follows the linear evolution

**_ct+1 = Act + η[2]σ[2]λ._** (63)

Let b = η[2]σ[2]λ. Writing out the first few steps, we identify a pattern

**_c1 = Ac0 + b_**

**_c2 = Ac1 + b = A[2]c0 + Ab + b_**

**_c3 = Ac2 + b = A[3]c0 + A[2]b + Ab + b_**
_..._

_t−1_

**_ct = A[t]c0 +_** **A[n]** **_b._** (64)

_n=0_ !

X

The geometric sum _tn−=01_ **[A][n][]** can be computed exactly under the assumption that (I **A) is**
_−_
invertible which holds provided all of A’s eigenvalues are less than unity, which necessarily holds
P
provided the system is stable. The geometric sum yields

_t−1_

_n=0_ **A[n]!** = (I − **A)[−][1][  ]I −** **A[t][]** _._ (65)

X

Recalling the definition of b = σ[2]η[2]λ and the definition of the average loss _Lt_ = λ[⊤]ct, we have
_⟨_ _⟩_

_⟨Lt⟩_ = σ[2] + λ[⊤]A[t]c0 + η[2]σ[2]λ[⊤](I − **A)[−][1][  ]I −** **A[t][]** **_λ._** (66)

Recognizing c0 = v[2] gives the desired result.

I PROOF OF THEOREM 5

We will now prove that if the training ˆp(x) and test distributions p(x) are different and have feature
correlation matrices **Σ[ˆ]** and Σ respectively, then the average training and test losses have the form

_Ltrain = Tr_ **ΣCˆ** _t_ _Ltest = Tr [ΣCt] ._ (67)
h i

As before, we will assume that there exist weights w[∗] which satisfy y = w[∗] _·ψ. We start by noticing_
that the update rule for gradient descent


**wt = wt** _ηgt, gt = [1]_
_−_ _m_


**_ψt,µψt,µ[⊤]_** [[][w][t] (68)
_µ=1_ _[−]_ **[w][∗][]]**

X


generates the following dynamics for the weight discrepancy correlation **Ct** =
(wt **w[∗])(wt** **w[∗])[⊤]** _t_ [.]
_−_ _−_ _D_

**Ct+1 = Ct** _ηΣC[ˆ]_ _t_ _ηCtΣ[ˆ]_ + η[2] 1 **ΣCˆ** _tΣ[ˆ]_ + _[η][2]_ **_ψψ[⊤]Ctψψ[⊤]_** (69)
_−_ _−_ _−_ _m[1]_ _m_
 

This formula can be obtained through the simple averaging procedure shown in B. Under the Gaussian approximation, we can obtain a simplification

**Ct+1 = (I** _ηΣ)Ct(I_ _ηΣ) +_ _[η][2]_ (70)
_−_ _−_ _m_ [[][ΣC][t][Σ][ +][ Σ][Tr][ΣC][t][]]


We can solve for the evolution of the diagonal and off-diagonal entries in this matrix giving


_t_

_λˆkλ[ˆ]ℓ_ _vkvℓ_ (71)




**u[⊤]k** **[C][t][u][k]** [=] **A[t]v[2][]k** _[,][ u]k[⊤][C][t][u][ℓ]_ [=] 1 − _ηλ[ˆ]k −_ _ηλ[ˆ]ℓ_ + η[2] 1 + m[1]

 




-----

To calculate the training and test error, we have

_⟨Ltest⟩_ = (ψ(x) · wt − **_ψ(x) · w[∗])[2]_** **x∼p(x),Dt** [=][ ⟨][(][w][t][ −] **[w][∗][)][Σ][ (][w][t][ −]** **[w][∗][)][⟩]** [=][ Tr][ΣC][t][.]

_⟨Ltrain⟩_ = (ψ(x) · wt − **_ψ(x) · w[∗])[2]_** **x∼pˆ(x),Dt** [=] (wt − **w[∗])Σ[ˆ]** (wt − **w[∗])** = TrΣC[ˆ] _t. (72)_

D E

Note that in the training error formula, since **Σ[ˆ]** has eigenvectors uk only the diagonal terms u[⊤]k **[C][t][u][k]**
enter into the formula for Ltrain, but off-diagonal components u[⊤]k **[C][t][u][ℓ]** [do enter into the formula]
for Ltest

J EXPERIMENTAL DETAILS

For Figures 3, we use the last two classes of MNIST and CIFAR-10. We encode the target values
as binary y ∈{+1, −1}. For Figure 5, we use 6000 random training points drawn from entire
MNIST and CIFAR-10 datasets to calculate the spectrum of the Fisher information matrix. We train
with SGD on these training data, using one-hot label vectors for each training example and plot the
error on the test set. We train our models on a Google Colab GPU and include code to reproduce
all experimental results in the supplementary materials. To match our theory, we use fixed learning
rate SGD. Both evaluation of the infinite width kernels and training were performed with the Neural
Tangents API (Novak et al., 2020).


-----

