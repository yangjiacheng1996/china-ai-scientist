# SIMPLER CALIBRATION FOR SURVIVAL ANALYSIS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Survival analysis, also known as time-to-event analysis, is the problem to predict
the distribution of the time of the occurrence of an event. This problem has applications in various fields such as healthcare, security, and finance. While there have
been many neural network models proposed for survival analysis, none of them
are calibrated. This means that the average of the predicted distribution is different
from the actual distribution in the dataset. Therefore, X-CAL has recently been
proposed for the calibration, which is supposed to be used as a regularization term
in the loss function of a neural network. X-CAL is formulated on the basis of the
widely used definition of calibration for distribution regression. In this work, we
propose new calibration definitions for distribution regression and survival analysis, and demonstrate a simpler alternative to X-CAL based on the new calibration
definition for survival analysis.

1 INTRODUCTION

_Survival analysis, also known as time-to-event analysis, is the problem to predict the time of the_
occurrence of an event. In healthcare applications, the event typically corresponds to a death or the
onset of disease in a patient. The time between a well-defined starting point and the occurrence of
the event is called the survival time or failure time. In survival analysis, we usually estimate the dis_tribution of the survival times of patients. Survival analysis has important applications in healthcare_
as well as various other fields (e.g., credit scoring (Dirick et al., 2017) and fraud detection (Zheng
et al., 2019)). The recent progress of prediction models for survival analysis has been summarized
in a survey paper (Wang et al., 2019).

In survival analysis, datasets are often censored, which means that events of interest might not be
observed for some instances. This may be due to either the limited observation time window or
missing traces caused by other irrelevant events. Typical censored data are right censored data.
These are the data points whose exact times of the events are unknown; we know only that the
events had not happened up to a certain time. In this paper, we focus on the uncensored data and the
_right censored data, as shown in Figure 1. Here, the event for data point x1 is observed during the_
period of study and hence this data is categorized as uncensored data. The data points x2 and x3 are
categorized as right censored data because we did not observe the events during the period of study.
The time between a well-defined starting point and the last observation time (e.g., the time of the
end of study) is called the censoring time.

One of the classical methods to solve the survival analysis problem is the Kaplan-Meier estimator (Kaplan & Meier, 1958). This is a non-parametric method to estimate the distribution of the
survival times as a survival function S(t), where the value S(t[∗]) for a specific time t[∗] represents the
_survival rate at time t[∗]_ (i.e., the ratio of the patients who survived at time t[∗]). It is easy to estimate
the survival function S(t) if the dataset contains only uncensored data points, but the Kaplan-Meier
estimator is designed to work for datasets that include censored data. Here, we briefly explain the
algorithm of the Kaplan-Meier estimator. Let {ti}i[k]=1 [be the set of distinct times when at least one]
uncensored event was observed in the dataset. Let di be the number of (uncensored) events that
happened exactly at time ti, and let ni be the number of data points that are known to have survived
at time ti. Then, the Kaplan-Meier estimator outputs the survival function


1
_−_ _n[d][i]i_


_S(t) =_


_i:ti≤t_


-----

Start of study End of study

|ncensored × ght censored 2 ght censored|×|
|---|---|
|||



Time

Uncensored
**_x1_**
_×_

Right censored
**_x2_**

Right censored
**_x3_**
_×_


Figure 1: Uncensored data and right censored data, where the symbols × show event
occurrences.


1.0

0.8

Survival rate 0.60.4

0.2

Kaplan-Meier

0.0

0 1000 2000 3000 4000 5000

Time


Figure 2: Survival function S(t) estimated
by the Kaplan-Meier estimator for the flchain
dataset.


Figure 2 shows an example of the survival function S(t) estimated by the Kaplan-Meier estimator
for the flchain dataset (Dispenzieri et al., 2012). Here, we can see that the survival rate at time
_t = 2500 is approximately 80%. Note that the true survival rate S(t) at t = ∞_ must be zero, but the
Kaplan-Meier estimator outputs the survival function S(t) only for time t [0, tmax], where tmax
_∈_
is the maximum survival time of the uncensored data points in a dataset. This is because we cannot
estimate the survival rate S(t) for the time t > tmax.

A drawback of the Kaplan-Meier estimator is that it outputs a survival function S(t) for the entire
population and not for a specific patient. Therefore, there have been many algorithms to estimate
the survival rate S(t|x) for each patient x so as to enable personalized medicine (Wang et al.,
2019). In particular, many neural network models that predict the survival function S(t|x) have
been proposed (Lee et al., 2018; Ren et al., 2019; Zheng et al., 2019; Tjandra et al., 2021).

1.1 CALIBRATION

When we use a prediction model, it should be calibrated. In binary classification, this means that
a prediction model that outputs a confidence of a true label for an input is expected to satisfy the
condition that the average of the confidence values over all inputs are equal to the ratio of the data
points with a true label in the dataset. For example, if a dataset contains 40% of the data points
with a true label, then we expect that a calibrated prediction model will output a confidence of 0.4
on average. Even though calibration is important for prediction models, and neural network models
have been widely used for prediction models, (Guo et al., 2017) showed that neural network models
are often miscalibrated.

In regression analysis, quantile-based calibration is widely used as the definition of calibration (Kuleshov et al., 2018; Song et al., 2019; Cui et al., 2020; Zhao et al., 2020). In survival
analysis, the definition of calibration is based on this definition for regression analysis. Goldstein
et al. (2020) showed a method to train a neural network model to achieve calibration by adding a
new regularizer, X-CAL, to the loss function of the neural network to achieve calibration during the
training. This method is in contrast to the widely-used calibration methods for regression analysis
such as Platt scaling and isotonic regression, which are post-training methods.

Note that a calibrated model is not necessarily useful. For example, in binary classification we can
construct a trivial calibrated model that always outputs the ratio of true label data points in a dataset
as a confidence value. Therefore, a useful prediction model also needs to be sharp, which means that
it outputs a confidence value close to one or zero for each input. To obtain a prediction model that is
both calibrated and sharp, one approach is to train a neural network with a loss function consisting
of one loss term aimed at sharp prediction and another aimed at calibration. X-CAL utilizes this
approach in that it is a regularizer for calibration and is used in combination with another loss term
that aims at sharp prediction.


-----

1.2 OUR CONTRIBUTIONS

In this paper, we propose a Kaplan-Meier regularizer as a simpler alternative to X-CAL. An advantage of our regularizer is that the obtained prediction model is calibrated for any time t [0, tmax].
_∈_
Another advantage is that our regularizer does not require any hyperparameter, whereas X-CAL requires hyperparameters. The idea behind our new regularizer is simple: just to reduce the difference
between the average predicted survival function and the Kaplan-Meier survival function (see Figure 3). Our new regularizer is based on a new definition of calibration for survival analysis, and we
discuss its advantages in Section 4.

2 SURVIVAL ANALYSIS

We formally describe the problem settings of regression analysis and survival analysis. In regression
_analysis, we assume that there is an unknown probability distribution P on the sample space X ×Y,_
where X is the domain of features and Y is the interval [−∞, ∞]. We refer to the associated random
variables with capital letters (i.e., X and Y ) and realizations with lower case letters (x, y) ∼P. We
assume that we can obtain independent samples {(xi, yi)}i[n]=1 [from][ X × Y][ according to distribution]
_P._

In survival analysis, we assume that there is an unknown probability distribution Q on the sample
space X × T × C, where X is the domain of features and T and C are time intervals [0, ∞]. We
refer to the associated random variables with capital letters (i.e., X, T, and C) and realizations with
lower case letters (x, t, c) ∼Q. The random variable T corresponds to the survival time (i.e., the
time of the occurrence of an event), which might not be observable due to the censoring, and the
random variable C corresponds to the censoring time. We assume that the censoring time is ∞ for
an uncensored event. The random variable defined by Z = min{T, C} corresponds to the time
of the last observation (i.e., the survival time or the censoring time). Different from the regression
analysis, we cannot obtain samples {(xi, ti, ci)}i[n]=1 [from][ X ×T ×C][ according to distribution][ Q][ due]
to the censoring in survival analysis. However, we assume that we can obtain independent samples
_D = {(xi, zi, δi)}i[n]=1_ [instead, where][ δ][i][ is a binary value indicating if the][ i][-th data point is censored]
or not. Here an uncensored data point (xi, zi, δi) ∈ _D satisfies δi = 1 and zi = ti and a censored_
data point (xi, zi, δi) ∈ _D satisfies δi = 0 and zi = ci. Hence δi = 0 means that we know only the_
fact that ti ≥ _ci and the exact survival time ti is unknown._

The task of survival analysis is to predict the probability of an event of interest occurring at time t
for x ∈X as a probability distribution function f (t|x). This function f (t|x) is often represented
in other equivalent forms. For example, it can be represented as its cumulative distribution function
(CDF)

_t_
_F_ (t|x) = 0 _f_ (τ _|x)dτ_
Z

or as the survival function S(t|x) defined by

_S(t|x) = 1 −_ _F_ (t|x).

Intuitively, F (t|x) represents the probability of observing the event by time t for x and the survival
function S(t|x) represents the probability of not-observing the event until time t for x.

Many neural network models have been proposed for survival analysis (e.g., (Lee et al., 2018; Ren
et al., 2019; Zheng et al., 2019; Tjandra et al., 2021)). They output the probability distribution
in the form of fθ(t **_x), Fθ(t_** **_x), Sθ(T_** **_x), or something equivalent to these forms, where θ is the_**
_|_ _|_ _|_
parameters of the neural network.

3 QUANTILE-BASED CALIBRATION

In this section, we review the definitions of calibration for distribution regression and survival analysis in the literature. First, we consider the distribution regression whose task is to predict the
distribution of the target variable as a CDF Fθ(y **_x) for x_**, where θ is the parameters of the
_|_ _∈X_
prediction model. In distribution regression, quantile-based calibration (Kuleshov et al., 2018) is
widely used as the definition of calibration (e.g., (Song et al., 2019; Cui et al., 2020; Zhao et al.,
2020)).


-----

**Definition 3.1 (Quantile-based calibration.) A prediction model Fθ(y|x) for distribution regression**
_is quantile-calibrated if this equation holds for any quantile level τ ∈_ [0, 1]:
Pr(X,Y )∼P (Fθ(Y |X) ≤ _τ_ ) = τ. (1)

If we can compute the inverse of Fθ(y **_x), we can rewrite Eq. (1) as_**
_|_
Pr(X,Y )∼P (Y ≤ _Fθ[−][1](τ_ _|X)) = τ._
This equation means that, for a random sample (x, y) ∼P, y must be at most the τ -th quantile
of the predicted CDF (i.e., Fθ[−][1](τ _|x)) exactly with probability τ_ . We can rewrite Definition 3.1 in
another equivalent formulation: the following equation holds for any subinterval [τ1, τ2] [0, 1]:
_⊆_
Pr(X,Y )∼P (Fθ(Y |X) ∈ [τ1, τ2]) = τ2 − _τ1._
This equation means that the quantile level Fθ(y **_x) predicted for a random sample (x, y)_** is
_|_ _∼P_
contained in a subinterval [τ1, τ2] [0, 1] exactly with probability τ2 _τ1._
_⊆_ _−_

On the basis of Definition 3.1, Goldstein et al. (2020) define calibration for survival analysis.

**Definition 3.2 (Quantile-based calibration for survival analysis.) A prediction model Fθ(t|x) for**
_survival analysis is quantile-calibrated if this equation holds for any quantile level τ ∈_ [0, 1]:
Pr(X,T,C)∼Q(Fθ(T |X) ≤ _τ_ ) = τ. (2)

We can rewrite Definition 3.2 into another equivalent formulation: the following equation holds for
any subinterval I = [τ1, τ2] [0, 1]:
_⊆_
Pr(X,T,C)∼Q(Fθ(T |X) ∈ _I) = |I| = τ2 −_ _τ1._ (3)
A problem when using Definition 3.2 is that we cannot get samples directly from T due to the
censoring. As such, we cannot verify Eq. (3) for datasets that include censored data. Goldstein et al.
(2020) resolved this problem by showing how to estimate the probability Pr(Fθ(t **_x)_** _I) for any_
_|_ _∈_
_t_ [c, ] from Pr(Fθ(c **_x)_** _I) for a randomly sampled censored data point (x, c, 0). Under the_
_∈_ _∞_ _|_ _∈_
assumption that T and C are independent (i.e., T ⊥ **_C | X), they show_**

Pr(Fθ(t **_x)_** _I) = [(][τ][2][ −]_ _[v][)][1][[][v][ ∈]_ _[I][]]_ + [(][τ][2][ −] _[τ][1][)][1][[][v < τ][1][]]_ _,_ (4)
_|_ _∈_ 1 _v_ 1 _v_

where v = Fθ(c **_x) and 1[_** ] denotes the step function. By using this estimation, we can compute − _−_
_|_ _·_
the left-hand side of Eq. (3) from dataset D that include censored data points.

On the basis of Eq. (3) and the approaches described in (Andres et al., 2018; Haider et al., 2020),
Goldstein et al. (2020) proposed a metric called distributional calibration (D-CAL), which is defined
as
_ℓD_ CAL(θ) = E(X,T,C) 1[Fθ(T **_X)_** _I]_ _I_ 2,
_−_ _∼Q_ _|_ _∈_ _−|_ _|_

_IX∈I_   

where the collection I is chosen to contain disjoint contiguous subintervals of C ⊆ [0, 1] that cover
the whole interval [0, 1].

A prediction model Fθ(t|x) with a lower ℓD−CAL(θ) is said to be more calibrated, but we cannot
construct a neural network model that directly minimizes D-CAL because ℓD−CAL(θ) is not a differentiable function due to its step function. Therefore, Goldstein et al. (2020) defined the explicit
_calibration (X-CAL), an approximation of D-CAL, by replacing the step function with a sigmoid_
function so that X-CAL becomes a differentiable function. Moreover, X-CAL is designed to handle
a set of data points B = {(xi, zi, δi)}i[b]=1 [as a mini-batch, which makes it possible to integrate X-]
CAL into the loss function of neural network models because most of those models use a mini-batch
training rather than the full batch training. Formally, X-CAL is defined as

2

X CAL(θ) = EB E(X,T,C) _Bζ(Fθ(T_ **_X); I, γ)_** _I_ _,_
_R_ _−_ _∼Q_ _∼_ _|_ _−|_ _|_

_IX∈I_   

where ζ(z; I, γ) is a sigmoid function to approximate the step function in D-CAL. (See (Goldstein
et al., 2020) for the precise definition of the function ζ.) Here, we abuse notation (X, T, C) ∼ _B to_
indicate that we obtain sample data points from mini-batch B (rather than the probability distribution
_Q). Note that Goldstein et al. (2020) proposed using X-CAL in the loss function of a neural network_
model as a regularizer, which means that it is intended to be combined with other loss functions. This
is because a prediction model that aims only at calibration is useless (as discussed in Section 1) and
the balance between sharpness and calibration must be considered for obtaining a useful prediction
model.


-----

4 VALUE-BASED CALIBRATION

In this section, we propose alternative definitions of calibration, value-based calibration, for distribution regression and survival analysis. Then, on the basis of the new calibration definition for
survival analysis, we propose our new Kaplan-Meier regularizer, and we discuss its advantages over
D-CAL and X-CAL.

We first propose an alternative definition of calibration for distribution regression.

**Definition 4.1 (Value-based calibration.) A prediction model Fθ(y|x) for distribution regression is**
value-calibrated if this equation holds for any value y ∈ [−∞, ∞]:

E(X,Y ) _Fθ(y_ **_X) = Pr(X,Y )_** (Y _y)._
_∼P_ _|_ _∼P_ _≤_

This equation means that the average probability of prediction having a value of at most y must
be equal to the actual ratio of data points having a value of at most y. The difference between
Definitions 3.1 and 4.1 is on the choice of the axis. Whereas Definition 3.1 gives a natural condition
for calibration with respect to the τ -axis of a prediction model τ = Fθ(y **_x), Definition 4.1 gives a_**
_|_
natural condition for calibration with respect to the y-axis. In appendix (Section A.1), we show that
a quantile-calibrated model is not necessarily a value-calibrated model and vice versa.

On the basis of Definition 4.1, we define the value-based calibration for survival analysis.

**Definition 4.2 (Value-based calibration for survival analysis.) A prediction model Fθ(t|x) for sur-**
_vival analysis is value-calibrated if this equation holds for any time t ∈_ [0, tmax], where tmax is the
_maximum time of the uncensored data points in the dataset D:_

E(X,T,C) _Fθ(t_ **_X) = Pr(X,T,C)_** (T _t)._ (5)
_∼Q_ _|_ _∼Q_ _≤_

Note that we ask Eq. (5) to hold for t [0, tmax] rather than t [0, ]. This is because the
_∈_ _∈_ _∞_
prediction model Fθ(t **_x) for survival analysis is usually trained for t_** [0, tmax] and the prediction
_|_ _∈_
_Fθ(t|x) for t > tmax is less accurate._

We can rewrite Definition 4.2 into another equivalent form: the following equation holds for any
subinterval [t1, t2] [0, tmax]:
_⊆_

E(X,T,C) (Fθ(t2 **_X)_** _Fθ(t1_ **_X)) = Pr(X,T,C)_** (t1 **_T_** _t2)._
_∼Q_ _|_ _−_ _|_ _∼Q_ _≤_ _≤_

This condition exactly matches the explanation of calibration in (Goldstein et al., 2020), i.e., a
model’s predicted number of events within any time interval is similar to the observed number. Note
also that we can rewrite Eq. (5) into another equivalent form:

E(X,T,C) _Sθ(t_ **_X) = Pr(X,T,C)_** (T > t). (6)
_∼Q_ _|_ _∼Q_

Here, the left-hand side of Eq. (6) is changed from the CDF Fθ(t **_X) in Eq. (5) to the survival_**
_|_
function Sθ(t **_X), and the right-hand side of the equation is also changed accordingly._**
_|_

On the basis of Eq. (6), we propose our new metric, Kaplan-Meier loss, which is defined as

_tmax_ 2
_ℓKM(θ) =_ E(X,T,C) _Sθ(t_ **_X)_** Pr(X,T,C) (T > t) _dt._ (7)

0 _∼Q_ _|_ _−_ _∼Q_

Z

  

In contrast to D-CAL, we can use this loss directly in the loss function of a neural network model because this loss is differentiable. However, we also propose a new regularizer KM(θ) by modifying
_R_
_ℓKM(θ) so that it can be used in a mini-batch training with a set of data points B = {(xi, zi, δi)}i[b]=1[:]_

_tmax_ 2

KM(θ) = EB E(X,T,C) _BSθ(t_ **_X)_** Pr(X,T,C) _B(T > t)_ _dt._ (8)
_R_ _∼Q_ 0 _∼_ _|_ _−_ _∼_

Z

  

Again, we abuse notation (X, T, C) ∼ _B to indicate that we obtain sample data points from mini-_
batch B (rather than the probability distribution Q). Although we used ℓ2 loss in Eq. (7)–(8), we can
use any other metric to measure the difference between two distributions. Note that we can compute
the first term in the parentheses by


E(X,T,C) _BSθ(t_ **_X) =_**
_∼_ _|_


(1 _Fθ(t_ _xi)),_
_−_ _|_
(xi,zXi,δi)∈B


_|B|_


-----

|←|→ ←|Col3|→ ←|Col5|→ →|Col7|← →|Col9|← target|
|---|---|---|---|---|---|---|---|---|---|
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||


Figure 4: Each box corresponds to a bin in C
and each circle corresponds to a data point.
The arrows show the directions of the moves
of the data points during training.


_S(t)_

Time t

Kaplan-Meier survival curve

_↕_

Predicted
survival curve

_↕_


Figure 3: Our Kaplan-Meier regularizer is
designed to reduce the ℓ2 loss between the
Kaplan-Meier survival curve and the average
predicted survival curve.


and we can estimate the second term in the parentheses Pr(X,T,C)∼Q(T > t) by the KaplanMeier estimator. Recall that the Kaplan-Meier estimator is designed to estimate Pr(T > t) even
if the dataset contains censored data. Figure 3 illustrates the proposed loss ℓKM(θ) and regularizer
KM(θ), in which the first term in the parentheses of Eq. (7)–(8) corresponds to the average pre_R_
dicted survival function (thick curve) and the second term corresponds to the Kaplan-Meier survival
curve (red curve) and each equation computes the ℓ2 loss between these two curves.

The advantages of our approach can be summarized as follows.

1. Our value-based calibration for survival analysis (Definition 4.2) is intuitive for practitioners in healthcare. Practitioners are often interested in the prediction performance at a
specific time t[∗] [0, tmax], and Eq. (5) shows a natural condition that should hold at time
_∈_
_t[∗]. In contrast, Eq. (2) states nothing about specific time t[∗]._

2. Our value-based calibration for survival analysis (Definition 4.2) is defined only for the
range t [0, tmax]. Regarding the quantile-based calibration, Definition 3.1 means that, if
_∈_
a prediction model Fθ(y **_x) for distribution regression is quantile-calibrated, then the distri-_**
_|_
bution of Fθ(Y **_X) for (X, Y )_** is equal to the uniform distribution over [0, 1] (Zhao
_|_ _∼P_
et al., 2020). However, even if a prediction model Fθ(t **_x) for survival analysis is quantile-_**
_|_
calibrated, the distribution of Fθ(T **_X) is usually not equal to the uniform distribution_**

**_[′]|_**
when T **_[′]_** refers to the random variable for the survival time of uncensored events. This
means that the condition in Definition 3.2 can be satisfied only if we can estimate the distribution of Fθ(T **_X) from the dataset D that include censored data by using some method_**
_|_
such as Eq. (4). In other words, we need to estimate Fθ(t|x) for t > tmax when we use
Definition 3.2.

3. Our Kaplan-Meier loss and regularizer do not require any hyperparameter (other than the
batch size |B|), whereas D-CAL and X-CAL require the hyperparameters γ and C.

4. Our Kaplan-Meier regularizer uses a simple ℓ2 loss and it avoids the binning approach used
in X-CAL. In the following, we explain the problem in the binning approach. In X-CAL,
we count the number of data points in each bin and we update the parameters θ of a neural
network during the training so that the numbers of data points in the bins are balanced.
Figure 4 illustrates that, for a bin in which the number of data points exceed the target,
the parameters θ is updated so that the data points in the first half of the bin are pushed to
the left and the data points in the second half of the bin are pushed to the right due to the
approximation of the step function as the sigmoid function. However, since the numbers of
data points in the three bins in the left exceed the target and the numbers of the data points
in the two bins in the right are below the target in the case of Figure 4, no data point in the
three bins in the left should be pushed left and the data points in these bins should be used
to fill the two bins in the right. Therefore, we sometimes find difficulties in minimizing XCAL due to the binning approach. Note that Goldstein et al. (2020) showed how to avoid
this phenomenon for the left-most and right-most bins, but we cannot avoid this problem
for the other bins.


-----

1.0 1.0 1.0

DeepHit
DRSA
Survival CRPS

0.8 0.8 Kaplan-Meier 0.8

Survival rate 0.60.4 Survival rate 0.60.4 Survival rate 0.60.4

0.2 DeepHitDRSA 0.2 0.2 DeepHitDRSA

Survival CRPS Survival CRPS
Kaplan-Meier Kaplan-Meier

0.0 0 1000 2000 3000 4000 5000 0.0 0 250 500 750 1000 1250 1500 1750 2000 0.0 0 200 400 600 800 1000 1200 1400 1600

Time Time Time


(a) flchain


(b) support


(c) retinopathy


Figure 5: Comparison of average predicted survival functions by using loss functions DeepHit,
DRSA, and Survival CRPS with Kaplan-Meier survival curve.

5 EXPERIMENTS

In this section, we compare the performance of our Kaplan-Meier regularizer with X-CAL on real
datasets. We show that, by using our Kaplan-Meier regularizer or X-CAL, we can reduce both
Kaplan-Meier loss and D-CAL. This means that we can use one of these two regularizers both for
quantile-based calibration and value-based calibration and do not need to combine the two regularizers for two different definitions of calibration.

5.1 NEURAL NETWORK AND DATASETS

We constructed a single neural network for our experiments and combined it with various loss functions. This neural network was a two-layer perceptron with a single hidden layer containing 128
neurons, and the number of outputs was 32. The activation function after the hidden layer was the
ReLU type, and the activation function at the output node was softmax. We used Python 3.7.4 and
PyTorch 1.4.0 for the implementation. The Adam optimizer (Kingma & Ba, 2015) was utilized
for the training algorithm, with the learning rate set to 0.001, the batch size to 1024, and the other
parameters to their default values. We run training for 100 epochs.

We used three datasets for survival analysis: two obtained from the packages in R (R Core Team,
2016) and one from a private data source. Of these former two, one is the flchain dataset (Dispenzieri
et al., 2012), which was obtained from the ‘survival’ package and contains 7874 data points (69.9%
of which are censored), and the other is the support dataset (Knaus et al., 1995), which was obtained
from the ‘casebase’ package and contains 9104 data points (31.9% of which are censored). The
dataset from the private data source is of patients with retinopathy disease. This dataset contains
6951 data points (33.3% of which are censored).

We split each of the three datasets into the training (80%) and test (20%) sets by using random
partitioning. The neural network model was trained using the training set and the experimental
results shown in this section (including the Kaplan-Meier survival curves) were obtained using the
test set. When we computed the metrics (D-CAL and Kaplan-Meier loss) on a test dataset, we used
full batch (rather than the mini-batch). Regarding the parameters for D-CAL and X-CAL, we used
_γ = 10000 and the collection C was the set of 20 equally sized bins disjointed over [0, 1]._

5.2 RESULTS

**Average predicted survival functions without calibration.** We compared the average survival
functions predicted by the state-of-the-art neural networks with the Kaplan-Meier survival curve on
each dataset. In our experiments, we used the fixed neural network and the loss functions presented
in DeepHit (Lee et al., 2018), DRSA (Ren et al., 2019), and Survival CRPS (Avati et al., 2019). We
used α = σ = 1.0 for DeepHit and α = 0.25 for DRSA as the hyperparameters. Figure 5 shows
the average survival functions predicted by using these three loss functions and the Kaplan-Meier
survival curve on each dataset. The average of the predicted survival functions should be close to
the Kaplan-Meier survival curve if a prediction model is calibrated. However, none of the predicted
results was close to the Kaplan-Meier survival curve, which means the loss functions do not achieve
value-based calibration.


-----

1.0 1.0 1.0

lambda=10
lambda=1000
lambda=100000

0.8 0.8 Kaplan-Meier 0.8

Survival rate 0.60.4 Survival rate 0.60.4 Survival rate 0.60.4

0.2 lambda=10lambda=1000 0.2 0.2 lambda=10lambda=1000

lambda=100000 lambda=100000
Kaplan-Meier Kaplan-Meier

0.0 0 1000 2000 3000 4000 5000 0.0 0 250 500 750 1000 1250 1500 1750 2000 0.0 0 200 400 600 800 1000 1200 1400 1600

Time Time Time


(a) flchain


(b) support


(c) retinopathy


Figure 6: Average predicted survival curves with Kaplan-Meier regularizer with varying λ.


1.0 1.0 1.0

lambda=10
lambda=1000
lambda=100000

0.8 0.8 Kaplan-Meier 0.8

Survival rate 0.60.4 Survival rate 0.60.4 Survival rate 0.60.4

0.2 lambda=10lambda=1000 0.2 0.2 lambda=10lambda=1000

lambda=100000 lambda=100000
Kaplan-Meier Kaplan-Meier

0.0 0 1000 2000 3000 4000 5000 0.0 0 250 500 750 1000 1250 1500 1750 2000 0.0 0 200 400 600 800 1000 1200 1400 1600

Time Time Time


(a) flchain


(b) support


(c) retinopathy


Figure 7: Average predicted survival curves with X-CAL regularizer with varying λ.

**Calibration with our Kaplan-Meier regularizer and X-CAL.** We compared the performances
of the proposed Kaplan-Meier regularizer and X-CAL by using each of them as a regularization term
in conjunction with Survival CRPS (Avati et al., 2019). More specifically, we used the following
loss function for the Kaplan-Meier regularizer:

_ℓ(θ) = ℓS−CRPS(θ) + λRKM(θ),_

where λ is a parameter, and we used the following loss function for X-CAL:


_ℓ(θ) = ℓS−CRPS(θ) + λRX−CAL(θ),_

where λ is the parameter. Figures 6 and 7 show results by using these loss functions with various
_λ from {10, 10[3], 10[5]}. We can see that the average predicted survival curves are not close to the_
Kaplan-Meier survival curve with small λ and therefore the predictions were not calibrated. However, regarding the average predicted survival curves with λ = 10[5], the predicted curves with our
Kaplan-Meier regularizer were close to the Kaplan-Meier survival curves and they are calibrated
enough. In contrast, the predicted curves with X-CAL were close to the Kaplan-Meier survival
curves, but the curves predicted by using our Kaplan-Meier regularizer were better than those with
X-CAL.

Table 1 shows the D-CAL and the Kaplan-Meier loss of these predicted results. We can see here that
using the Kaplan-Meier regularizer with large λ leads to not only reducing the Kaplan-Meier loss
but also reducing D-CAL. Similarly, using X-CAL with large λ also leads to not only reducing DCAL but also reducing Kaplan-Meier loss. These facts demonstrate that we can use one of these two
regularizers (the Kaplan-Meier regularizer or X-CAL) to reduce the two metrics (Kaplan-Meier loss
and D-CAL) for the real datasets, although we theoretically show that a quantile-calibrated model is
not necessarily a value-calibrated model and vice versa in the appendix (Section A.1).

6 RELATED WORK

We use the Kaplan-Meier estimator (Kaplan & Meier, 1958) in our Kaplan-Meier loss to estimate
the survival rate S(t) = Pr(X,T,C)∼Q(T > t). Our loss function is not restricted to the KaplanMeier estimator and we can use any other nonparametric method to estimate the survival rate S(t).
For example, we can use the Nelson-Aalen estimator (Aalen, 1978; Nelson, 1969; 1972) instead.


-----

Table 1: D-CAL and Kaplan-Meier loss for the various combinations of dataset, regularizer, and λ

**Kaplan-Meier regularizer (proposed)** _λ = 10_ _λ = 10[3]_ _λ = 10[5]_

D-CAL 0.1181 0.0587 0.0023
Kaplan-Meier loss 0.1110 0.0705 0.0004
flchain
**X-CAL (Goldstein et al., 2020)** _λ = 10_ _λ = 10[3]_ _λ = 10[5]_

D-CAL 0.1176 0.0588 0.0005
Kaplan-Meier loss 0.1110 0.0730 0.0049

**Kaplan-Meier regularizer (proposed)** _λ = 10_ _λ = 10[3]_ _λ = 10[5]_

D-CAL 0.2948 0.0938 0.0089
Kaplan-Meier loss 0.0914 0.0167 0.0009
support
**X-CAL (Goldstein et al., 2020)** _λ = 10_ _λ = 10[3]_ _λ = 10[5]_

D-CAL 0.0839 0.0186 0.0008
Kaplan-Meier loss 0.0889 0.0426 0.0030

**Kaplan-Meier regularizer (proposed)** _λ = 10_ _λ = 10[3]_ _λ = 10[5]_

D-CAL 0.0800 0.0505 0.0010
Kaplan-Meier loss 0.1731 0.0074 0.0004
retinopathy
**X-CAL (Goldstein et al., 2020)** _λ = 10_ _λ = 10[3]_ _λ = 10[5]_

D-CAL 0.0783 0.0218 0.0003
Kaplan-Meier loss 0.1733 0.0524 0.0005

Regarding another calibration for survival analysis, there was study on 1-Calibration (Haider et al.,
2020). In 1-Calibration, we consider the calibration of a single point of the predicted distribution, but
the entire distribution is not evaluated for calibration. We also note that the distributional divergence
for calibration (DDC) Kamran & Wiens (2021) is similar to D-CAL. The difference between DDC
and D-CAL are that D-CAL uses a binning approach and DDC uses the Kullback Leibler divergence
to measure the distance between the predicted survival function and the uniform distribution.

7 CONCLUSION

In this work, we presented new definitions of calibration, value-based calibration, for distribution
regression and survival analysis. On the basis of the new calibration definition for survival analysis,
we then proposed a new metric called Kaplan-Meier loss for value-based calibration. The results
of experiments showed that it can be used as the regularizer of a loss function in a neural network
model for calibration, and it can be seen as a simpler alternative to X-CAL.


-----

8 REPRODUCIBILITY STATEMENT

We summarize our efforts for reproducibility. The details of our Kaplan-Meier loss and regularizer
are described in Section 4. In addition, we describe the algorithm of the Kaplan-Meier estimator
in Section 1 to make our paper self-contained, although the Kaplan-Meier estimator is a famous
algorithm in survival analysis. Regarding our experiments, we described all of the details about
our Python environment and the parameters used in Section 5. We used three datasets, and two of
them are publicly available datasets. Although we do not attach our source code, we believe that
readers of our paper can easily reproduce our results, because our algorithm is easy to implement
and influence of the undescribed factors (e.g., choice of the random seed) can be ignored (e.g., the
conclusion of our paper does not change even if the numbers in Table 1 fluctuate by 20%).

REFERENCES

Odd Aalen. Nonparametric inference for a family of counting processes. The Annals of Statistics, 6
(4):701–726, 1978.

Axel Andres, Aldo Montano-Loza, Russell Greiner, Max Uhlich, Ping Jin, Bret Hoehn, David
Bigam, James Andrew Mark Shapiro, and Norman Mark Kneteman. A novel learning algorithm
to predict individual survival after liver transplantation for primary sclerosing cholangitis. PLoS
_One, 13(3):e0193523, 2018._

Anand Avati, Tony Duan, Sharon Zhou, Kenneth Jung, Nigam H. Shah, and Andrew Y. Ng. Countdown regression: Sharp and calibrated survival predictions. In Proceedings of UAI 2019, pp.
145–155, 2019.

Peng Cui, Wenbo Hu, and Jun Zhu. Calibrated reliable regression using maximum mean discrepancy. In Proceedings of NeurIPS 2020, 2020.

Lore Dirick, Gerda Claeskens, and Bart Baesens. Time to default in credit scoring using survival
analysis: a benchmark study. Journal of the Operational Research Society, 68(6):652–665, 2017.

Angela Dispenzieri, Jerry A. Katzmann, Robert A. Kyle, Dirk R. Larson, Terry M. Therneau,
Colin L. Colby, Raynell J. Clark, Graham P. Mead, Shaji Kumar, L. Joseph Melton III, and
S. Vincent Rajkumar. Use of nonclonal serum immunoglobulin free light chains to predict overall
survival in the general population. Mayo Clinic Proceedings, 87(6):517–523, 2012.

Mark Goldstein, Xintian Han, Aahlad Manas Puli, Adler Perotte, and Rajesh Ranganath. X-CAL:
Explicit calibration for survival analysis. In Proceedings of NeurIPS 2020, 2020.

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In Proceedings of ICML 2017, pp. 1321–1330, 2017.

Humza Haider, Bret Hoehn, Sarah Davis, and Russell Greiner. Effective ways to build and evaluate
individual survival distributions. Journal of Machine Learning Research, 21(85):1–63, 2020.

Fahad Kamran and Jenna Wiens. Estimating calibrated individualized survival curves with deep
learning. In AAAI 2021, 2021.

Edward L. Kaplan and Paul Meier. Nonparametric estimation from incomplete observations. Journal
_of the American Statistical Association, 53(282):457–481, 1958._

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings
_of ICLR 2015, 2015._

W. A. Knaus, F. E. Harrell Jr., J. Lynn, L. Goldman, R. S. Phillips, A. F. Connors Jr., N. V. Dawson,
W. J. Fulkerson Jr., R. M. Califf, N. Desbiens, P. Layde, R. K. Oye, P. E. Bellamy, R. B. Hakim,
and D. P. Wagner. The SUPPORT prognostic model. objective estimates of survival for seriously
ill hospitalized adults. study to understand prognoses and preferences for outcomes and risks of
treatments. Annals of Internal Medicine, 122(3):191–203, 1995.

Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning
using calibrated regression. In Proceedings of ICML 2018, pp. 2796–2804, 2018.


-----

Changhee Lee, William R. Zame, Jinsung Yoon, and Mihaela van der Schaar. Deephit: A deep
learning approach to survival analysis with competing risks. In Proceedings of AAAI-18, pp.
2314–2321, 2018.

Wayne Nelson. Hazard plotting for incomplete failure data. Journal of Quality Technology, 1:27–52,
1969.

Wayne Nelson. Theory and applications of hazard plotting for censored failure data. Technometrics,
14(4):945–966, 1972.

R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statis[tical Computing, Vienna, Austria, 2016. URL https://www.R-project.org/.](https://www.R-project.org/)

Kan Ren, Jiarui Qin, Lei Zheng, Zhengyu Yang, Weinan Zhang, Lin Qiu, and Yong Yu. Deep
recurrent survival analysis. In Proceedings of AAAI-19, pp. 4798–4805, 2019.

Hao Song, Tom Diethe, Meelis Kull, and Peter Flach. Distribution calibration for regression. In
_Proceedings of ICML 2019, pp. 5897–5906, 2019._

Donna E. Tjandra, Yifei He, and Jenna Wiens. A hierarchical approach to multi-event survival
analysis. In Proceedings of AAAI 2021, pp. 591–599, 2021.

Ping Wang, Yan Li, and Chandan K. Reddy. Machine learning for survival analysis: A survey. ACM
_Computing Surveys, 51(6):1–36, 2019._

Shengjia Zhao, Tengyu Ma, and Stefano Ermon. Individual calibration with randomized forecasting.
In Proceedings of ICML 2020, pp. 11387–11397, 2020.

Panpan Zheng, Shuhan Yuan, and Xintao Wu. Safe: A neural survival analysis model for fraud early
detection. In Proceedings of AAAI-19, pp. 1278–1285, 2019.

A APPENDIX

A.1 RELATIONSHIP BETWEEN TWO DEFINITIONS OF CALIBRATION

We show that Definition 3.1 and Definition 4.1 are not equivalent. To see this, suppose that we have
a probability distribution P on X × Y, where X = Y = [0, 1], such that the corresponding random
variables X and Y are the uniform random variables on [0, 1] that satisfy X = Y .

**A quantile-calibrated model is not a value-calibrated model.** We consider a prediction model
_F_ (y|x) defined by

0 (x = 0),

_F_ (y|x) = ( 1y ((0x < x < = 1). 1),

Since the distribution F (Y |X) is the uniform distribution on [0, 1], this prediction model F (y|x) is
quantile-calibrated by Definition 3.1. However, since we have
E(X,Y ) _F_ (y **_X) = 0.5_**
_∼P_ _|_
and
Pr(X,Y )∼P (Y ≤ _y) = y_
for any y ∈ (0, 1), the prediction model F (y|x) does not satisfy the condition of calibration in
Definition 4.1.

**A value-calibrated model is not a quantile-calibrated model.** We consider a prediction model
_F_ (y|x) defined by

0 (x > y),
_F_ (y _x) =_
_|_ 1 (x _y)._
 _≤_
Then this prediction model F (y|x) is value-calibrated by Definition 4.1, because we have
E(X,Y ) _F_ (y **_X) = y = Pr(X,Y )_** (Y _y)_
_∼P_ _|_ _∼P_ _≤_
for any y ∈ [0, 1]. However, since F (y|x) = 1 for any sample (x, y) ∼P, the prediction model
_F_ (y|x) does not satisfy the condition of calibration in Definition 3.1.


-----

