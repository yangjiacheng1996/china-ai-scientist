# PROVABLY IMPROVED CONTEXT-BASED OFFLINE META-RL WITH ATTENTION AND CONTRASTIVE LEARNING

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Meta-learning for offline reinforcement learning (OMRL) is an understudied problem with tremendous potential impact by enabling RL algorithms in many realworld applications. A popular solution to the problem is to infer task identity as
augmented state using a context-based encoder, for which efficient learning of
robust task representations remains an open challenge. In this work, we provably
improve upon one of the SOTA OMRL algorithms, FOCAL, by incorporating
intra-task attention mechanism and inter-task contrastive learning objectives, to
robustify task representation learning against sparse reward and distribution shift.
Theoretical analysis and experiments are presented to demonstrate the superior
performance and robustness of our end-to-end and model-free framework compared
to prior algorithms across multiple meta-RL benchmarks. [1]

1 INTRODUCTION

Deep reinforcement learning (RL) has achieved many successes with human- or superhuman-level
performance across a wide range of complex domains (Mnih et al., 2015; Silver et al., 2017; Vinyals
et al., 2019; Ye et al., 2020). However, all these major breakthroughs focus on finding the bestperforming strategy by trial-and-error interactions with a single environment, which poses severe
constraints for scenarios such as healthcare (Gottesman et al., 2019), autonomous driving (ShalevShwartz et al., 2016) and controlled-environment agriculture (An et al., 2021; Cao et al., 2021) where
safety is paramount. Moreover, these RL algorithms require tremendous explorations and training
samples, and are also prone to over-fitting to the target task (Song et al., 2019; Whiteson et al., 2011),
resulting in poor generalization and robustness. To make RL truly practical in many real-world
applications, a new paradigm with better safety, sample efficiency and generalization is in need.

Offline meta-RL, as a marriage between offline RL and meta-RL, has emerged as a promising
candidate to address the aforementioned challenges. Like supervised learning, offline RL restricts
the agent to solely learn from fixed and limited data, circumventing potentially risky explorations.
Additionally, offline algorithms are by nature off-policy, which by reusing prior experience, have
proven to achieve far better sample efficiency than on-policy counterparts (Haarnoja et al., 2018).

Meta-RL, on the other hand, exploits the shared structure of a distribution of tasks and enables the
agent to adapt to new tasks with minimal data. One popular approach is by learning a single universal
policy conditioned on a latent task representation, known as context-based method (Hallak et al.,
2015). Alternatively, the shared skills can be learned with a meta-controller (Oh et al., 2017).

In this work we restrict our attention on context-based offline meta-RL (COMRL), an understudied
framework with a few existing algorithms (Li et al., 2019; Dorfman & Tamar, 2020; Mitchell et al.,
2020; Li et al., 2021a), for a set of tasks that differ in reward or transition dynamics. One major
challenge associated with this scenario is termed Markov Decision Process (MDP) ambiguity (Li
et al., 2019; Dorfman & Tamar, 2020), namely the task-conditioned policies spuriously correlate task
identity with state-action pairs due to biased distribution of the fixed datasets. This phenomenon can
be interpreted as a special form of memorization problem in classical meta-learning (Yin et al., 2019),
where the value and policy functions overfit the training distributions without capturing causality

1Source code is provided in the supplementary material.


-----

from reward and transition functions, often leading to degenerate task representations (Li et al.,
2021a) and poor generalization. To alleviate such over-fitting, Li et al. (2021a) proposes a framework
named FOCAL which decouples the learning of task inference from control by using self-supervised
distance metric learning. However, they made a strong assumption on the existence of an injective
_map from each transistion tuple {s, a, s[′], r} to its task identity. Under extreme scenarios such as_
sparse reward, where a considerable portion of aggregated experience provides little information
regarding task identity, efficient and robust learning of task representations is still challenging.

To address the aforementioned problem, in this paper we propose intra-task attention mechanism
and inter-task contrastive learning objectives to achieve robust task inference. More specifically, for
each task, we apply a batch-wise gated attention to recalibrate the weights of transition samples, and
use sequence-wise self-attention (Vaswani et al., 2017b) to better capture the correlation within the
transition (state, action, reward) dimensions. In addition, we implemented a matrix-form objective of
the Momentum Contrast (MoCo) (He et al., 2020) for task-level representation learning, by replacing
its dictionary queue with a meta-batch sampled on-the-fly. We provide theoretical analyses showing
that our objective serves as a better surrogate than naive contrastive loss for task inference and the
proposed attention mechanism on top can also reduce the variance of task representation. Moreover,
empirical evaluations demonstrate that the proposed design choices of attention and contrastive
learning mechanisms not only boost the performance of task inference, but also significantly improve
its robustness against sparse reward and distribution shift. We name our new method FOCAL++.

2 RELATED WORK

**Attention in RL Although attention mechanism has proven a powerful tool across of a broad spectrum**
of domains (Mnih et al., 2014; Vaswani et al., 2017a; Wang & Shen, 2017; Velickoviˇ c et al., 2018;´
Devlin et al., 2018), to our best knowledge, its applications in RL remain relatively understudied.
Most of previous works in RL (Mishra et al., 2018; Sukhbaatar et al., 2019; Kumar et al., 2020;
Parisotto et al., 2020) focus on applying temporal attention in order to capture the time-dependent
correlation in MDPs or POMDPs. Raileanu et al. (Raileanu et al., 2020) uses transformer as the
default dynamics/policy encoder for meta-RL, similar to our proposed sequence-wise attention,
without giving any intuition or comparative study on such design choice. Wang et al. (2021) recently
implemented attention in meta-RL but didn’t consider the offline setting.

The closest work we found by far (Barati & Chen, 2019; Li et al., 2021b) employ attention in
multi-view/multi-agent RL, to learn different weights on various workers or agents, aggregated by a
global network to form a centralized policy. Analogous to our proposal, such architecture has the
advantage of adaptively accounting for inhomogeneous importance of each input in the decision
making process, and makes the global agent robust to noise and partial observability.

**Contrastive Learning** Contrastive learning (Chopra et al., 2005; Hadsell et al., 2006) has emerged
as a powerful framework for representation learning. In essence, it aims to capture data structures
by learning to distinguish between semantically similar and dissimilar pairs. Recent progress in
contrastive learning focuses mostly on learning visual representations as pretext tasks. MoCo (He
et al., 2020) formulates contrastive learning as dictionary look-up, and builds a dynamic dictionary
with a queue and a moving-averaged encoder. SimCLR (Chen et al., 2020) further pushes the
SOTA benchmark with careful composition of data augmentations. However, all these algorithms
concentrate primarily on generating pseudo-labels and contrastive pairs, whereas in COMRL scenario,
the task labels and transition samples are naturally given.

There are a few recent works which apply contrastive learning in RL (Laskin et al., 2020) or meta-RL
(Fu et al., 2020) settings. Fu et al. (2020) employs InfoNCE (Oord et al., 2018) loss to train a
contrastive context encoder. They investigated the technique in the online setting, where the encoder
requires an information-gain-based exploration strategy to be effective. In contrast, this paper focuses
on how contrastive learning performs in the fully-offline setting.

**Context-Based Offline Meta-RL (COMRL)** Context-based offline meta-RL employs models
with memory such as recurrent (Duan et al., 2016; Wang et al., 2016; Fakoor et al., 2020), recursive
(Mishra et al., 2018) or probabilistic (Rakelly et al., 2019) structures to achieve fast adaptation by
aggregating experience into a latent representation on which the policy is conditioned. To address
the bootstrapping error problem (Kumar et al., 2019) for offline learning, framework like FOCAL


-----

enforces behavior regularization (Wu et al., 2019), which constrains the distribution mismatch
between the behavior and learning policies in actor-critic objectives. We follow the same paradigm.

3 METHOD

To tackle the COMRL problem, we follow the procedure described in FOCAL (Li et al., 2021a), by
first learning an effective representation of tasks on latent space Z, on which a single universal policy
is conditioned and trained with behavior-regularized actor-critic method (Wu et al., 2019). As an
improved version of FOCAL, our main contribution is twofold:

1. To our best knowledge, we are the first to apply attention mechanism in offline multitask/meta-RL setting, for learning robust task representations. We combine batch-wise gated
attention with sequence-wise transformer encoder, and demonstrate its lower variance as
well as robustness against sparse reward and MDP ambiguity compared to prior COMRL
methods.

2. On top of attention, we incorporate a matrix reformulation of Momentum Contrast (He
et al., 2020) for task representation learning, with theoretical guarantees and provably better
performance than ordinary contrastive objective.

3.1 PROBLEM SETUP

Consider a family of stationary MDPs defined by M = (S, A, P, R, γ) where (S, A, P, R, γ) are
the corresponding state space, action space, transition function, reward function and discount factor.
A task T is defined as an instance of M, which is associated with a pair of time-invariant transition
and reward functions, P (s[′]|s, a) ∈P and R(s, a) ∈R, respectively. In this work, we focus on tasks
which share the same state and action space. Consequently, a task distribution can be modeled as a
joint distribution of P and R, which usually can be factorized:

_p(T ) := p(P, R) = p(P)p(R)._ (1)

In the offline setting, each task Ti (i being the task label) is associated with a static dataset of transition
tuples Di = {ci} = {(si, ai, s[′]i[, R][i][(][s][i][, a][i][))][}][, for which][ p][(][D][i][) =][ p][(][T][i][)][. Each tuple][ c][i][ ∼D][i][ is a]
sequence along the so-called transition/sequence dimension. A meta-batch B is a set of mini-batches
_i_ _i. Consider a meta-optimization objective in a multi-task form (Rakelly et al., 2019; Fakoor_
_Bet al., 2020), ∼D_

(θ, ψ) = E _i_ _p(_ )[ actor( _i; θ) +_ critic( _i; ψ)]_ (2)
_L_ _D_ _∼_ _D_ _L_ _D_ _L_ _D_

= E _i_ _p(_ )[ _i_ (θ, ψ)], (3)
_D_ _∼_ _D_ _LD_

where LDi (θ, ψ) is the objective evaluated on transition samples drawn from Di, parameterized by θ
and ψ. Assuming a common uniform distribution for a set of n tasks, the meta-training procedure
turns into minimizing the average losses across all training tasks


_θˆmeta,_ _ψ[ˆ]meta = arg min_
_θ,ψ_


E [ _k_ (θ, ψ)] . (4)
_LD_
_k=1_

X


For COMRL problem, a task distribution corresponds to a family of MDPs on which a single universal
policy is supposed to perform well. Since the MDP family is considered partially observed if no task
identity information is given, a task inference module Eφ(z **_c) is required to map context information_**
_|_
**_c ∼D to a latent task representation z ∈Z to form an augmented state, i.e.,_**

_Saug ←S × Z,_ **_saug ←_** concat(s, z). (5)

Such an MDP family is formalized as Task-Augmented MDP (TA-MDP) in FOCAL. Additionally,
Li et al. (2021a) proves that a good task representation z is crucial for optimization of the taskconditioned meta-objective in Eqn 4, which is the prime focus of this paper. We now show how to
address the issue with the proposed attention architectures and contrastive learning framework.

3.2 ATTENTION ARCHITECTURES


-----

Figure 1: Context encoder as a stack of attention blocks.

We employ two forms of intra-task attention in
the context encoder Eφ(z|c): batch-wise gated
**attention and sequence-wise self-attention,**
for learning better task representations. The architectures are shown in Figure 2.


**Batch-Wise Gated Attention** When performing task inference, transitions inside the same
batch may contribute differently to the representation learning, especially in sparse reward situations. For tasks that differ in rewards, intuitively,
transition samples with non-zero rewards contain more information regarding the task identity.
Therefore, we utilize a gating mechanism similar to (Hu et al., 2018) along the batch dimension to adaptively recalibrates this batch-wise
response by computing a scalar multiplier for
every sample as in Figure 1.


Figure 2: Attention modules for task inference.
**BA: batch-wise attention. SA: sequence-wise at-**
tention.


**Sequence-Wise Self-Attention** A naive MLP encoder maps a concatenated 1-D sequence
(s, a, s[′], r) from context buffer to a 1-D embedding z. This seq2seq model can be implemented
with sequence-wise attention to apply self-attention along the sequence dimension. The intuition
behind sequence-wise attention is that the attentive context encoder should in principle better capture
the correlation in (s, a, s[′], r) sequence related to task-specific reward function R(s, a) and transition function P (s[′]|s, a), compared to normal MLP layers employed by common context-based RL
algorithms.

Illustrated in Figure 1, since two attention modules operate on separate dimensions, we connect them
in parallel to generate task embedding z by addition.

Figure 3: Inter-task matrix-form momentum contrast.
Given two meta-batches of transitions {c[q]} and {c[k]}, a
quickly progressing query encoder and a slowly progressing key encoder compute the corresponding batch-wise
mean task representations in latent space Z. A matrix multiplication is performed between the set of query and key
vectors to produce the supervised contrastive loss in Eqn
8. T, C, Z are the meta-batch, transition and latent space
dimensions respectively.


-----

3.3 THE CONTRASTIVE LEARNING FRAMEWORK

Inspired by the successes of contrastive learning in computer vision (He et al., 2020), we process the
raw transition data with momentum encoders to generate a latent query vector z[q] as classifier and a
set of K latent key vectors {z0[k][,][ z]1[k][, ...,][ z]K[k] _[}][ as][ task representations][. Suppose one of the keys][ z]+[k]_ [is]
the only match to z[q], we employ the InfoNCE (Oord et al., 2018) objective as the building block:

_z =_ log _Kexp(z[q]_ _· z+[k]_ _[/τ]_ [)] _,_ (6)
_L_ _−_

_i=0_ [exp(][z][q][ ·][ z]i[k][/τ] [)]

where τ is a temperature hyper-parameter (Wu et al., 2018).P

To ensure maximum sample efficiency, for each pair of meta-batches = _i_ _i_ _i = 1, ..., T_
where T is the meta-batch size, one can construct T InfoNCE objectives by taking the average B _{B_ _∼D_ _|_ _}_
latent vector of each task as the key, which is also crucial for our theoretical analysis (Theorem
3.1). Namely, given a meta-batch of encoded queries {zi[q] _[∼]_ _[E]φ[q]_ [(][z][i][|B][i][)][|][i][ = 1][, ..., T] _[}][ and keys]_
_{zi[k]_ _[∼]_ _[E]φ[k][(][z][i][|B][i][)][|][i][ = 1][, ..., T]_ _[}][, our proposed contrastive loss is]T_

exp(zi[q] _i_ _[/τ]_ [)]

_z =_ log _T_ _[·][ z][k]_ _,_ (7)
_L_ _−_

_i=1_ _j=1_ [exp(][z]i[q] _j_ _[/τ]_ [)]

X _[·][ z][k]_

which can be written in a matrix-form

P

exp(zi[q] _j_ _[/τ]_ [)]
_z =_ Tr(M ), _Mij = log_ _T_ _[·][ z][k]_ _._ (8)
_L_ _−_

_j=1_ [exp(][z]i[q] _[·][ z]j[k][/τ]_ [)]

The training scheme of our proposed inter-task momentum contrast is illustrated in Figure 3.

P

Now we provide a theoretical analysis of the objective in Eqn 8. Intuitively, it is the log loss of a
_T_ -way softmax-based classifier trying to classify each zi[k] [as][ z]i[q][. With this interpretation, we compare]
it to a linear classifier with supervised loss and show that it can be recovered by the linear classifier if
the weight matrix is a specific mean task classifier (Theorem 3.1). Furthermore, we prove that our
proposed objective is a better surrogate than traditional contrastive loss for task inference.

**Definition 3.1 (Supervised Contrastive Loss)**

_Lsup(T, g) := E Tci∼Di,Tii′,c∼i′p∼D(T )i′_ [ℓ({g(ci) − _g(ci′_ )})]. (9)

_where ℓ_ _can be standard hinge or logistic losses as in (Saunshi et al., 2019)._

Consider a linear classifier g(c) = W E(c), where the encoded latent vector E(c) is used as a
deterministic representation (Li et al., 2021a) and W ∈ R[N] _[×][Z]_ is a weight matrix trained to minimize
_Lsup(_ _, W E), Z is the dimension of the task latent space_ . Such construction of contrastive
_T_ _Z_
objective enables self-supervised task representation learning for task inference, without requiring
access to full labels of all possible tasks, which is flexible and has better potential for generalization.
Hence the supervised loss of E on T is defined as

_Lsup(_ _, E) =_ inf (10)
_T_ **_W_** R[N] _[×][Z][L][sup][(][T][,][ W][ E][)][.]_
_∈_

Since the optimal W requires full knowledge and labels of the underlying task distribution, which is
infeasible given only training tasks. As with Saunshi et al. (2019), we consider a particular choice of
**_W_** _[µ]:_

**Definition 3.2 (Mean Task Classifier) For an encoder function E and a task set T of cardinality**
_N_ _, the mean task classifier W_ _[µ]_ _is an N × Z weight matrix whose i[th]_ _row is the mean latent vector_
**_µi of inputs with task label i. We use as a shorthand for its loss L[µ]sup[(][T][, E][) :=][ L][sup][(][T][,][ W][ µ][E][)][.]_**

In pratice, we estimate the mean task representation of z[q] and z[k] using its batch-wise mean

**_µ[q,k]i_** := Ez[q,k]i _∼cEi∼Dφ[q,k](izi|ci)[z[q,k]i_ ] ≈ Ez[q,k]i _∼cEiφ∼B[q,k](izi|ci)[z[q,k]i_ ], (11)

which induces the following definitions:


-----

**Definition 3.3 (Averaged Supervised Contrastive Loss) Average supervised loss for an encoder**
_function E on T_ _-way classification of task representation is defined as_


_Lsup({Ti}i[T]=1[, E][)]_ _._ (12)



_Lsup(E) :=_ E
_{Ti}i[T]=1[∼][p][(][T][ )]_


_The average supervised loss of its mean classifier (Definition 3.2) is_


_L[µ]sup[(][E][) :=]_ E
_{Ti}i[T]=1[∼][p][(][T][ )]_


_L[µ]sup[(][{T][i][}][T]i=1[, E][)]_ _._ (13)



When the loss function ℓ is the convex logistic loss, we prove in Appendix B that

**Theorem 3.1 The matrix-form momentum contrast objective Lz (Eqn 8) is equivalent to the average**
_supervised loss of its mean classifier L[µ]sup_ _[(Eqn 13) if][ E][ =][ E][k][(][z][|][c][)][ is the][ key][ encoder and the mean]_
_task classifier W_ _[µ]_ _whose i[th]_ _row is the mean of latent query vectors with task label i._

If we compare our proposed loss function with the classical unsupervised contrastive loss

**Definition 3.4 (Unsupervised Contrastive Loss)**

_Lun(E) := E_ _ℓ({E(c)[T]_ (E(c[+]) − _E(c[−]))})_ _._ (14)
 

Given T as the number of distinct tasks in meta-batches, c, c[+] are contexts from the same task, and
_c[−]_ is from the other T − 1 tasks. Such construction is employed by prior COMRL methods like
FOCAL, which allows for task interpolation during meta-testing.

By Lemma 4.3 in (Saunshi et al., 2019), using convexity of ℓ and Jensen’s inequality, assuming no
repeated task labels in each meta-batch, we have

**Theorem 3.2 For all context encoder E**

_Lsup(E) ≤_ _L[µ]sup[(][E][)][ ≤]_ _[L][un][(][E][)][.]_ (15)

Combined with Theorem 3.1, it shows that our proposed contrastive objective in Eqn 8: _z_
_L[µ]sup[(][E][φ][(][z][|][c][))][ serves as a better surrogate for][ L][sup]_ [than the ordinary unsupervised contrastive losses] L _≡_
employed by prior methods, to ensure similarity-preserving task representation for COMRL.

3.4 VARIANCE OF TASK EMBEDDINGS BY FOCAL++

In experiments, we found that our proposed algorithm, FOCAL++, which combines attention mechanism and matrix-form momentum contrast, exhibit significant smaller variance compared to the
baselines on tasks with sparse reward (Table 2). We provide a proof of this observation for a simplified
version of FOCAL++, by only considering the batch-wise attention along with contrastive learning
objective defined in Eqn 8, in presence of sparse reward. Assuming all tasks differ only in reward
_function, we begin with the following definition:_

**Definition 3.5 (Absolutely Sparse Transition) Given a set of tasks {T} which only differ by reward**
_function, a transition tuple (s,a,s’,r) is absolutely sparse if_ _i_ _, Ri(s, a) = constant._
_∀T_ _∈{T }_

According to policy invariance under reward transformations (Ng et al., 1999), without loss of
generality, we assume the constant above to be zero for the rest of the paper.

**Definition 3.6 (Task with Sparse Reward) For a dataset Di = {(si, ai, s[′]i[, R][i][(][s][i][, a][i][))][}][ sampled]**
_from any task Ti with sparse reward, it can be decomposed as a disjoint union of two sets of_
_transitions:_

_Di = {(si, ai, s[′]i[, R][i][(][s][i][, a][i][))][} ∪{][(][s][i][, a][i][, s][′]i[,][ 0)][}]_ (16)
= _cn_ _cs_ _,_ (17)
_{_ _} ∪{_ _}_

where _cs_ is the set of absolutely sparse transitions (Definition 3.5), which by definition are shared
_{_ _}_
across all tasks. _cn_ consists of the rest of the transitions, and is unique to task _i._
_{_ _}_ _T_


-----

**Definition 3.7 (Batch-Wise Gated Attention) The batch-wise gated attention assigns inhomoge-**
_neous weights W for batch-wise estimation of the mean task representation of µ[q,k]_ _in Eqn 11:_

**_µ[q,k]i_** (W ) := Ec∼Di [W (c)E[q,k](c)] (18)

= pnE[W (cn)E[q,k](cn)] + psE[W (cs)E[q,k](cs)], (19)

where pn, ps are the measures of {cn}, {cs} respectively and W is normalized such that
Ec _i_ [W (c)] = 1. pn + ps = 1 by Definition 3.6.
_∼D_

**Theorem 3.3 Given a learned batch-wise gated attention weight W and context encoder E that**
_minimize the contrastive learning objective L[µ]sup[(][W][, E][)][, we have]_

_Var(µ[q,k]i_ (W )) _Var(µ[q,k]i_ ), (20)
_≤_

_when the sparsity ratio exceeds a threshold._

i.e., the variance of learned task embeddings with batch attention is upper-bounded by its counterpart
without attention given the dataset is sparse enough. We prove Theorem 3.3 in Appendix B.

4 EXPERIMENTS

In the following experiments, we show FOCAL++ outperforms the existing COMRL algorithms by a
clear margin in three key aspects: a) asymptotic performance of learned policy; b) task representations
with lower variance; and c) robustness to sparse reward and MDP ambiguity.

All trials are averaged over 3 random seeds. The offline training data are generated in accordance with
the protocol of FOCAL by training stochastic SAC (Haarnoja et al., 2018) models for every distinct
task and roll out policies saved at each checkpoint to collect trajectories. The offline training datasets
can be collected as a selection of the saved trajectories, which facilitates tuning of the performance
level and state-action distributions (Table 3). Both training and testing sets are pre-collected, making
our method fully-offline. Rewards are sparsified by constructing a neighborhood of goal in state or
_velocity space, where transition samples which lie outside the area are assigned zero reward. Since_
_the focus of this paper is robust task representation learning which can be decoupled from control_
_according to FOCAL, we use sparse-reward data only when training the context encoders. Learning_
_of meta-policy in presence of sparse reward is another active but orthogonal area of research where_
_quite a few successful solutions have been found (Andrychowicz et al., 2017; Eysenbach et al., 2020)._
A concrete description of the hyper-parameters and experimental settings is covered in Appendix D.

Table 1: Average testing return (standard deviation in parenthesis) of FOCAL and variants of
FOCAL++.

Algorithm Sparse-Point-Robot Point-Robot-Wind Sparse-Cheetah-Dir Sparse-Ant-Dir Sparse-Cheetah-Vel Walker-2D-Params

FOCAL 11.84(1.05) -5.61(0.59) 1351.40(90.46) 429.92(41.52) -183.32(40.16) 302.70(12.94)

FOCAL++
(contrastive) 12.53(0.31) -5.78(0.44) 1309.76(115.33) 504.00(145.80) -158.95(21.36) 366.35(55.08)
FOCAL++
(batch-wise) 12.54(0.23) -5.57(0.34) 1330.56(162.03) 687.37(85.95) -150.58(11.75) 376.52(36.59)
FOCAL++
(seq-wise) 12.64(0.14) **-5.09(0.01)** 1293.40(129.99) 573.26(186.22) -140.63(11.52) 375.67(45.72)
FOCAL++ **12.96(0.09)** -5.39(0.57) **1470.52(68.29)** **719.77(57.58)** **-137.31(7.06)** **391.02(42.44)**


Table 2: Variance of context embeddings averaged over all training tasks and latent dimensions.

Algorithm Sparse-Point-Robot Point-Robot-Wind Sparse-Cheetah-Dir Sparse-Ant-Dir Sparse-Cheetah-Vel Walker-2D-Params

FOCAL 8.54E-5 3.05E-3 4.31E-3 2.24E-3 2.57E-3 1.06E-2

FOCAL++
(contrastive) 7.83E-5 1.68E-3 6.86E-4 1.77E-3 1.73E-3 5.79E-3
FOCAL++
(batch-wise) **7.73E-5** 1.70E-3 **4.66E-4** **7.51E-4** 1.04E-3 5.85E-3
FOCAL++
(seq-wise) 7.94E-5 1.84E-3 9.43E-4 8.00E-4 **9.76E-4** 5.46E-3
FOCAL++ 8.27E-5 **1.68E-3** 7.82E-4 1.35E-3 1.06E-3 **5.23E-3**


-----

(b) Point-Robot-Wind


(c) Sparse-Ant-Dir


(a) FOCAL++ vs. 4 baselines.


Figure 4: Left: Test-task performance vs. transition steps sampled for meta-training. Right: t-SNE
visualization of the learned task embeddings z[q] on Point-Robot-Wind and Sparse-Ant-Dir. Each
point represents a query vector which is color-coded according to its task label.

4.1 ASYMPTOTIC PERFORMANCE

We evaluate FOCAL++ on 6 continuous control meta-environments of robotic locomotion (Todorov
et al., 2012) adopted from FOCAL. 4 (Sparse-Point-Robot, Sparse-Cheetah-Vel, Sparse-Cheetah-FwdBack, Sparse-Ant-Fwd-Back) and 2 (Point-Robot-Wind, Walker-2D-Params) environments require
adaptation by reward and transition functions respectively. For inference, FOCAL++ aggregates
context from a fixed test set to infer task embedding, and is subsequently evaluated online. Besides
FOCAL, three other baselines are compared: an offline variant of the PEARL algorithm (Rakelly et al.,
2019) (Batch PEARL), a context-based offline BCQ algorithm (Fujimoto et al., 2019) (Contextual
BCQ) and a two-stage COMRL algorithm with reward/dynamics relabelling (Li et al., 2019) (MBML).

Shown in Figure 4a, FOCAL outperforms other methods across almost all domains with context
embeddings of higher quality in Figure 4b,4c. In Table 1, our ablation studies also show that each
design choice of FOCAL++ alone can improve the performance of the learned policy, and combining
the orthogonal intra-task attention mechanism with inter-task contrastive learning yields the best
outcome.

4.2 ROBUSTNESS TO MDP AMBIGUITY AND SPARSE REWARD

In our experimental setup, an ideal context encoder should capture the generalizable information for
task inference, namely the difference between reward/dynamics functions across a distribution of
tasks. However, as discussed in Section 1, there are two major challenges that impede conventional
COMRL algorithms from learning robust representations:

Table 3: Average testing return of FOCAL and FOCAL++ on Sparse-Point-Robot with different
distributions of training/testing sets. The numbers in parenthesis represent performance drop due to
distribution shift. Additional experiments are presented in Apppendix C.

|Environment|Training|Testing|FOCAL|FOCAL++|
|---|---|---|---|---|
|Sparse- Point- Robot|expert|expert medium random|8.16 7.12 (1.04) 4.43 (3.73)|12.60 12.47 (0.13) 10.17 (2.43)|
||medium|medium expert random|8.44 8.25 (0.19) 6.76 (1.68)|12.54 12.44 (0.10) 10.49 (2.05)|
|Walker-2D- Params|mixed|mixed expert|302.70 271.69 (31.01)|391.02 377.46 (13.56)|


-----

(a) State distribution of relabeled
dataset


(b) Test-task performance (c) Batch-wise attention weight


Figure 5: Result on the relabeled Sparse-Point-Robot dataset. (a) State distributions of the expert
datasets for 20 distinct tasks, with goals uniformly distributed on a semicircle. (b) On mixed dataset,
FOCAL completely fails in this scenario whereas FOCAL++ variants with batch-wise attention are
able to learn. (c) Probability distribution of the batch-wise attention weight of samples with absolutely
zero and non-zero reward. Binary classification AUC = 0.969.

**MDP ambiguity arises due to COMRL algorithms’ sensitivity to fixed dataset distributions (Li et al.,**
2019; Dorfman & Tamar, 2020). Take Sparse-Point-Robot for example, as in Figure 5a, for tasks
with a goal on the semicircle, the state-action distribution exhibits specific pattern which may reflect
task identity. Given D = {(s, a, s[′], r)} as input, the context encoder may learn a spurious correlation
between state-action distributions and task identity, which causes performance degradation under
distribution shifts (Table 3).

**Sparse reward in meta-environments could exacerbate MDP ambiguity by making a considerable**
portion of transitions uninformative for task inference, such as the samples outside any goals in Figure
5a. Attention mechanism, especially the batch-wise channel attention, helps the context encoder
attend to the informative portion of the input transitions, and therefore significantly improve the
robustness of the learned policies.

To demonstrate the robustness of FOCAL++ in presence of the two challenges above, we tested it
against distribution shift by using datasets of various qualities: expert, medium, random and mixed
which combines all three. Shown in Table 3, we observe that overall the performance drop due to
distribution shift is significantly lower when attention and contrastive learning are applied.

Moreover, we are aware that even mixing of datasets generated by different behavior policies cannot
fully eliminate the risk of MDP ambiguity since the state-action distributions for each task still
do not completely overlap. To show that the attention modules introduced by FOCAL++ indeed
works as intended by capturing the reward-task dependency, we create a new dataset on SparsePoint-Robot by merging the state-action support across all tasks and relabelling the sparse reward
according to the task-specific reward functions. In principle, this fully prevents information leakage
from the state-action distributions, forcing the context encoder to learn to distinguish the reward
functions between tasks while minimizing the contrastive loss. Shown in Figure 5b, we experimented
with 3 attention variants of FOCAL++ on the relabeled dataset, and found that batch-wise attention
significantly improves the performance as intended. Additionally, we visualize the density distribution
of batch-wise attention weights assigned to samples in Figure 5c. We see a clear tendency for the
batch-attention module to assign zero weight to samples with zero rewards (the absolutely sparse data
points which lie outside all goal circles in Figure 5a) and maximum weights to the non-zero-reward
transitions, with binary classification AUC = 0.969, which is clear evidence of FOCAL++ learning
the correct correlation for task inference by attending to the informative context.

5 CONCLUSION

In this work, we address the understudied COMRL problem and provably improve upon the existing
SOTA baselines such as FOCAL, by focusing on more effective and robust learning of task representations. Key to our framework is the combination of intra-task attention mechanism and inter-task
contrastive learning, for which we provide theoretical grounding and experimental evidence on the
superiority of our design.


-----

REFERENCES

Zhicheng An, Xiaoyan Cao, Yao Yao, Wanpeng Zhang, Lanqing Li, Yue Wang, Shihui Guo, and
Dijun Luo. A simulator-based planning framework for optimizing autonomous greenhouse control
strategy. In Proceedings of the International Conference on Automated Planning and Scheduling,
volume 31, pp. 436–444, 2021.

Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay.
In Advances in neural information processing systems, pp. 5048–5058, 2017.

Elaheh Barati and Xuewen Chen. An actor-critic-attention mechanism for deep reinforcement
learning in multi-view environments. arXiv preprint arXiv:1907.09466, 2019.

Xiaoyan Cao, Yao Yao, Lanqing Li, Wanpeng Zhang, Zhicheng An, Zhong Zhang, Shihui Guo,
Li Xiao, Xiaoyu Cao, and Dijun Luo. igrow: A smart agriculture solution to autonomous
greenhouse control. arXiv preprint arXiv:2107.05464, 2021.

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning, pp.
1597–1607. PMLR, 2020.

Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with
application to face verification. In 2005 IEEE Computer Society Conference on Computer Vision
_and Pattern Recognition (CVPR’05), volume 1, pp. 539–546. IEEE, 2005._

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Ron Dorfman and Aviv Tamar. Offline meta reinforcement learning. arXiv preprint arXiv:2008.02598,
2020.

Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl[2]: Fast
reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.

Benjamin Eysenbach, Xinyang Geng, Sergey Levine, and Ruslan Salakhutdinov. Rewriting history
with inverse rl: Hindsight inference for policy improvement. arXiv preprint arXiv:2002.11089,
2020.

Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander J. Smola. Meta-q-learning. In
_[International Conference on Learning Representations, 2020. URL https://openreview.](https://openreview.net/forum?id=SJeD3CEFPH)_
[net/forum?id=SJeD3CEFPH.](https://openreview.net/forum?id=SJeD3CEFPH)

Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. arXiv preprint arXiv:1703.03400, 2017.

Haotian Fu, Hongyao Tang, Jianye Hao, Chen Chen, Xidong Feng, Dong Li, and Wulong Liu.
Towards effective context for meta-reinforcement learning: an approach based on contrastive
learning. 2020.

Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052–2062, 2019.

Omer Gottesman, Fredrik Johansson, Matthieu Komorowski, Aldo Faisal, David Sontag, Finale
Doshi-Velez, and Leo Anthony Celi. Guidelines for reinforcement learning in healthcare. Nat Med,
25(1):16–18, 2019.

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Conference
_on Machine Learning, pp. 1861–1870. PMLR, 2018._

Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant
mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
_(CVPR’06), volume 2, pp. 1735–1742. IEEE, 2006._


-----

Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual markov decision processes. arXiv
_preprint arXiv:1502.02259, 2015._

Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
_Computer Vision and Pattern Recognition, pp. 9729–9738, 2020._

Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE
_conference on computer vision and pattern recognition, pp. 7132–7141, 2018._

Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. In Advances in Neural Information Processing
_Systems, pp. 11784–11794, 2019._

Shakti Kumar, Jerrod Parker, and Panteha Naderian. Adaptive transformers in rl. arXiv preprint
_arXiv:2004.03761, 2020._

Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations
for reinforcement learning. In International Conference on Machine Learning, pp. 5639–5650.
PMLR, 2020.

Jiachen Li, Quan Vuong, Shuang Liu, Minghua Liu, Kamil Ciosek, Keith Ross, Henrik Iskov
Christensen, and Hao Su. Multi-task Batch Reinforcement Learning with Metric Learning. arXiv
_e-prints, art. arXiv:1909.11373, September 2019._

Lanqing Li, Rui Yang, and Dijun Luo. FOCAL: Efficient fully-offline meta-reinforcement learning
via distance metric learning and behavior regularization. In International Conference on Learning
_[Representations, 2021a. URL https://openreview.net/forum?id=8cpHIfgY4Dj.](https://openreview.net/forum?id=8cpHIfgY4Dj)_

Wenhao Li, Xiangfeng Wang, Bo Jin, Dijun Luo, and Hongyuan Zha. Structured cooperative
reinforcement learning with time-varying composite action space. IEEE Transactions on Pattern
_Analysis and Machine Intelligence, 2021b._

Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive
[meta-learner. In International Conference on Learning Representations, 2018. URL https:](https://openreview.net/forum?id=B1DmUzWAW)
[//openreview.net/forum?id=B1DmUzWAW.](https://openreview.net/forum?id=B1DmUzWAW)

Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. Offline metareinforcement learning with advantage weighting. arXiv preprint arXiv:2008.06043, 2020.

Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. Recurrent models of visual
attention. In Proceedings of the 27th International Conference on Neural Information Processing
_Systems-Volume 2, pp. 2204–2212, 2014._

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. nature, 518(7540):529–533, 2015.

Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In Icml, volume 99, pp. 278–287, 1999.

Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization with
multi-task deep reinforcement learning. In International Conference on Machine Learning, pp.
2661–2670. PMLR, 2017.

Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.

Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar,
Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers
for reinforcement learning. In International Conference on Machine Learning, pp. 7487–7498.
PMLR, 2020.


-----

Roberta Raileanu, Max Goldstein, Arthur Szlam, and Rob Fergus. Fast adaptation to new environments via policy-dynamics value functions. In International Conference on Machine Learning, pp.
7920–7931. PMLR, 2020.

Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy
meta-reinforcement learning via probabilistic context variables. In International conference on
_machine learning, pp. 5331–5340, 2019._

Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal
meta-policy search. arXiv preprint arXiv:1810.06784, 2018.

Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A
theoretical analysis of contrastive unsupervised representation learning. In International Confer_ence on Machine Learning, pp. 5628–5637. PMLR, 2019._

Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354–359, 2017.

Xingyou Song, Yiding Jiang, Stephen Tu, Yilun Du, and Behnam Neyshabur. Observational overfitting
in reinforcement learning. arXiv preprint arXiv:1912.02975, 2019.

Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention
span in transformers. arXiv preprint arXiv:1905.07799, 2019.

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033.
IEEE, 2012.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 6000–6010, 2017a. URL
[http://papers.nips.cc/paper/7181-attention-is-all-you-need.](http://papers.nips.cc/paper/7181-attention-is-all-you-need)

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
_processing systems, pp. 5998–6008, 2017b._

Petar Velickoviˇ c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li´ o, and Yoshua`
Bengio. Graph attention networks. In International Conference on Learning Representations,
[2018. URL https://openreview.net/forum?id=rJXMpikCZ.](https://openreview.net/forum?id=rJXMpikCZ)

Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung¨
Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in
starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.

Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv
_preprint arXiv:1611.05763, 2016._

Jane X Wang, Michael King, Nicolas Pierre Mickael Porcel, Zeb Kurth-Nelson, Tina Zhu, Charlie
Deck, Peter Choy, Mary Cassin, Malcolm Reynolds, H Francis Song, et al. Alchemy: A benchmark
and analysis toolkit for meta-reinforcement learning agents. In Thirty-fifth Conference on Neural
_Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021._

Wenguan Wang and Jianbing Shen. Deep visual attention prediction. IEEE Transactions on Image
_Processing, 27(5):2368–2378, 2017._

Shimon Whiteson, Brian Tanner, Matthew E Taylor, and Peter Stone. Protecting against evaluation
overfitting in empirical reinforcement learning. In 2011 IEEE symposium on adaptive dynamic
_programming and reinforcement learning (ADPRL), pp. 120–127. IEEE, 2011._


-----

Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
_arXiv preprint arXiv:1911.11361, 2019._

Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via nonparametric instance discrimination. In Proceedings of the IEEE Conference on Computer Vision
_and Pattern Recognition, pp. 3733–3742, 2018._

Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang,
Xipeng Wu, Qingwei Guo, et al. Mastering complex control in moba games with deep reinforcement learning. In AAAI, pp. 6672–6679, 2020.

Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn. Meta-learning
without memorization. arXiv preprint arXiv:1912.03820, 2019.


-----

APPENDIX A PSEUDO-CODE[2]

**Algorithm 1: FOCAL++ Meta-training**



-  Pre-collected batch _i =_ (sj, aj, s[′]j[, r][j][)][}][j][:1][,...,N][ from a set of training tasks][ {T][i][}][i][=1][,...,n]
_D_ _{_
drawn from p(T )

-  Learning rates α1, α2, α3, temperature τ, momentum m

**1 Initialize context replay buffer Ci for each task Ti**

**2 Initialize context encoder network Eφ[q,k][(][z][|][c][)][, learning policy][ π][θ][(][a][|][s, z][)][ and Q-network]**
_Qψ(s, z, a) with parameters φq, φk, θ and ψ_

**3 while not done do**

**4** **for each Ti do**

**5** **for t = 0, T −** 1 do

**6** Sample mini-batches of B transitions {(si,t, ai,t, s[′]i,t[, r][i,t][)][}][t][:1][,...,B][ ∼D][i][ and update]
_i_
**7** **end** _C_

**8** **end**

**9** Sample a pair of query-key meta-batches of T tasks ∼ _p(T )_

**10** **for step in training steps do**

**11** **for each Ti do**

**12** Sample mini-batchesidentical by default, the rewards in ci and bi ∼Ci for context encoder and policy training ( bi are always non-sparse) _bi, ci are_

**13** Compute zi[q] [=][ E]φ[q] [(][c][i][)]

**14** **for each Tj do**

**15** Sample mini-batches cj from Cj and compute zj[k] [=][ E]φ[k][(][c][j][)]

**16** _Mij = Mz(zi[q][,][ z]j[k][)]_ _▷_ **matrix-form momentum contrast**

**17** **end**

**18** _Lactor[i]_ [=][ L][actor][(][b][i][, E]φ[q] [(][c][i][))]

**19** _Lcritic[i]_ [=][ L][critic][(][b][i][, E]φ[q] [(][c][i][))]

**20** **end**

**21** _Lz = Tr(M_ )

**22** _φq_ _φq_ _α1_ _φq_ _z_
_←_ _−_ _∇_ _L_

**23** _φk ←_ _mφk + (1 −_ _m)φq_ _▷_ **momentum update**

**24** _θ_ _θ_ _α2_ _θ_ _i_ _actor_

**25** _ψ ←_ _ψ −_ _α3∇_ _ψ_ _[L]i_ _[i]_ _critic_

**26** **end** _←_ _−_ _∇_ P _[L][i]_

P

**27 end**


**Algorithm 2: FOCAL++ Meta-testing**



-  Pre-collected batch Di′ = {(sj′ _, aj′_ _, s[′]j[′]_ _[, r][j][′]_ [)][}][j][′][:1][,...,M][ from a set of testing tasks]
_{Ti′_ _}i′=1...m drawn from p(T )_

**1 Initialize context replay buffer** _i′ for each task_ _i_
_C_ _T_

**2 for each** _i′ do_
_T_

**3** **for t = 0, T −** 1 do

**4** Sample mini-batches of B transitions ci′ = {(si′,t, ai′,t, s[′]i[′],t[, r][i][′][,t][)][}][t][:1][,...,B][ ∼D][i][′][ and]
update _i′_
_C_

**5** Compute zi[q][′][ =][ E]φ[q] [(][c][i][′] [)]

**6** Roll out policy πθ(a|s, zi[q][′] [)][ for evaluation]

**7** **end**

**8 end**

2To prevent conflict or misunderstanding, all non-hyperlink equation/theorem numbers in the appendix refer
to those in the main text.


-----

APPENDIX B DEFINITIONS AND PROOFS

B.1 PROOF OF THEOREM 3.1

Consider a task set = 1, ..., _T_ drawn uniformly from p( ). In Definition 3.1, the loss
incurred byof a T -dimensional vector of differences in the coordinates. Given the definition of the mean task g on point {T } (c, Ti {T) ∈C × {T } T _}[3]_ is defined as ℓ({g(c)i − _gT(c)i′_ _}i′≠_ _i), which is a function_
classifier W _[µ]_ that g(c) = W E(c) and ℓ(v) = log(1 + _i_ [exp(][−][v][i][))][ the standard logistic loss as]
in (Saunshi et al., 2019), the supervised contrastive loss defined in Eqn 10 can be rewritten as

[P]

_Lsup(T, g) := ETic∼i∼Dp(T )_ log 1 + _iX[′]≠_ _i_ exp  _j_ (Wi′jE(ci)j − **_WijE(ci)j)_** _._ (21)

  [X] 

Since the i[th] row of W is the mean of latent key vectors with task label i, and E = E[k](z|c) is the
key encoder, Eqn 21 turns into

_Lsup(T, g) := ETi∼p(T )_ log 1 + _iX[′]≠_ _i_ exp  zi[k][′][ ·][ z]i[q] _[−]_ **_[z]i[k]_** _[·][ z]i[q]_ _._ (22)

  

In practice, we estimate the latent vectors zi[q,k] using batch-wise mean to approximate the the mean
task representation µ[q,k]i . Therefore Lsup in 22 is equivalent to the mean task classifier L[µ]sup [defined]
in Definition 3.2. One step futher, assuming uniform distribution of the task set {T }[4], the averaged
supervised contrastive loss by Definition 3.3 is


_L[µ]sup[(][E][) :=]_ E
_{Ti}i[T]=1[∼][p][(][T][ )]_


_L[µ]sup[(][{T][i][}]i[T]=1[, E][)]_


_T_

= T[1] Xi=1 log 1 + _iX[′]≠_ _i_ exp  zi[k][′][ ·][ z]i[q] _[−]_ **_[z]i[k]_** _[·][ z]i[q]_ (23)

_T_  

exp (zi[q] _i_ [)]

= log _T_ _[·][ z][k]_ _,_ (24)
_−_ _T[1]_

_i=1_ _j=1_ [exp (][z]i[q] _j_ [)]

X _[·][ z][k]_

P

which is precisely the matrix-form momentum contrast objective (Eqn 8,9) if one rescales W by a
factor of τ .


B.2 PROOF OF THEOREM 3.3

With Definition 3.5, 3.6 and 3.7, we hereby provide a simplified proof by assuming a constant weight
**_W (c) on the non-sparse set_** _cn_ and the absolutely sparse set _cs_ (Definition 3.5) respectively,
_{_ _}_ _{_ _}_
then we have

_µ[q,k]i_ (W ) = pnW (cn)Ecn∼{cn}[E[q,k](cn)] + psW (cs)Ecs∼{cs}[E[q,k](cs)], (25)

where the normalization condition Ec _i_ [W (c)] = 1 implies pnW (cn)+ _psW (cs) = 1. Therefore,_
_∼D_
adding the batch-wise attention is effectively modulating pn and ps. Since pn + ps = 1, without loss
of generality, we apply the following notations:

34CNote that the task set = {(si, ai, s′i[, R][i][(][s][i][, a] {T }[i][))][}] discussed here is a[ is the context space] subset of the whole task set and does not necessarily
**cover the whole support of p(T ). It is sampled for the sole purpose of computing the contrastive loss.**


-----

_pn = p,_ _pnW (cn) = p[′]_ (26)

Ecn _cn_ [E[q,k](cn)] = x[q,k]n _[,]_ Ecs _cs_ [E[q,k](cs)] = x[q,k]s _[.]_ (27)
_∼{_ _}_ _∼{_ _}_

Assuming i.i.d xn and xs, which gives

Var(µ[q,k]i (W )) = Var(p[′]x[q,k]n + (1 − _p[′])x[q,k]s_ [) = (][p][′][)][2][Var][(][x]n[q,k][) + (1][ −] _[p][′][)][2][Var][(][x]s[q,k][)]_ (28)

Var(µ[q,k]i ) = Var(px[q,k]n + (1 − _p)x[q,k]s_ [) =][ p][2][Var][(][x]n[q,k][) + (1][ −] _[p][)][2][Var][(][x]s[q,k][)][.]_ (29)

By B.1, the averaged supervised loss L[µ]sup[(][W][, E][)][ is equivalent to the matrix-form contrastive]
objective, which can be written as


_L[µ]sup[(][W][, E][) = 1]_

_T_

= [1]


exp (µ[k]i[′][ −] **_[µ][k]i_** [)][ ·][ µ]i[q]
_iX[′]≠_ _i_   

exp _p[′](x[k]i[′][ −]_ **_[x]i[k][)][ ·][ µ]i[q]_**
_iX[′]≠_ _i_  


log





log






1 +





1 +






_i=1_

_T_

_i=1_

X


_,_ (30)






where we use the definition of µ in Eqn 25 and the fact that x[q,k]s is the same across all tasks. Since
the learned **_W,_** _E ∈_ arg minW ∈A,E∈E _L[µ]sup[(][W][, E][)][, and][ p][′][ ≈]_ _[p][ by the identity map initialization of]_
the residual attention module, we have, for learned _p[′],_ **_x and_** **_µ,_**

[c] [b]

_p[′]_ _≥_ _p,_ (x[k]i[′][ −] **_x[b][b][k]i_** b[)][ ·][ b]µ[q]i b[<][ 0][.] (31)

Now subtract Eqn 28 by 29, we have

b b

Var(µ[q,k]i (W[c] )) − Var(µ[q,k]i ) = [(p[b][′])[2] _−_ _p[2]]Var(x[q,k]n_ [) + [(1][ −] _p[b][′])[2]_ _−_ (1 − _p)[2]]Var(x[q,k]s_ [)]

= (p[b][′] _−_ _p)_ (p[b][′] + p)Var(x[q,k]n [)][ −] [(2][ −] _[p][ −]_ _p[b][′])Var(x[q,k]s_ [)]
h i

_s_ [)][ −] _[p][Var][(][x]n[q,k][)]_
_≤_ 0, _if_ _p ≤_ _p[b][′]_ _≤_ [(2][ −]Var[p][)][Var](x[q,k]n[(][x][) +][q,k] [ Var][(][x]s[q,k][)] _._ (32)

The left inequality automatically holds by Eqn 31, the RHS is satisfied when


Var(x[q,k]s [)]
_p ≤_ Var(x[q,k]n [) +][ Var][(][x]s[q,k][)] _,_ (33)


or equivalently,


Var(x[q,k]n [)]
_ps = (1 −_ _p) ≥_ Var(x[q,k]n [) +][ Var][(][x]s[q,k][)] _,_ (34)

which means when the sparsity of reward exceeds the threshold, a learned batch attention module can
reduce the variance of the mean task representation µ[q,k]i . Eqn 34 is corroborated by our experiments
on the relabeled Sparse-Point-Robot dataset (Figure 6).


-----

APPENDIX C ADDITIONAL EXPERIMENTS

In Table 4, we present more experimental evidence that FOCAL++ is more robust against distribution
shift compared to FOCAL on Walker-2D-Params, which is consistent with Table 3 in the main text.

Table 4: Extension of Table 3 in the main text. Average testing return of FOCAL and FOCAL++ for
more settings of distribution shift on Walker-2D-Params.

|Environment|Training|Testing|FOCAL|FOCAL++|
|---|---|---|---|---|
|Walker-2D- Params|expert|expert mixed random|373.92 322.24 (51.68) 284.94 (88.98)|364.75 340.60 (24.15) 297.43 (67.32)|
||mixed|mixed expert random|302.70 271.69 (31.01) 260.02 (42.68)|391.02 377.46 (13.56) 346.95 (44.07)|



Figure 6: The variance-sparsity relation for FOCAL++/FOCAL on the relabeled Sparse-Point-Robot
dataset. The y-axis measures the variance of the bounded task embeddings z ∈ (−1, 1)[l] averaged
over all l latent dimensions. See more details in D.2.

Moreover, to testify our conclusion in B.2, we present the variance of task embedding vectors of
FOCAL++ and FOCAL under various sparsity levels. Shown in Figure 6, the variance of the weighted
embeddings µ[q,k]i ( W[ˆ] ) becomes lower than its unweighted counterpart µ[q,k]i when sparse ratio exceeds
a threshold about 0.6. The observation matches well with Eqn 34 we derived in B.2.


-----

APPENDIX D EXPERIMENTAL DETAILS AND HYPERPARAMETER

D.1 OVERVIEW OF THE META ENVIRONMENTS

The meta-environments could be divided into two categories: meta-environments that only differ in
_reward function and that only differ in transition function. For the meta-environments that only differ_
_in reward functions, we additionally introduce sparsity to the reward function._

-  Sparse-Point-Robot is a 2D-navigation task with sparse reward, introduced in Rakelly et al.
(2019). Each task is associated with a goal sampled uniformly on a unit semicircle. The
agent is trained to navigate to set of goals, then tested on a distinct set of unseen test goals.
Tasks differ in reward function only.

-  Point-Robot-Wind is another variant of Sparse-Point-Robot. Each task is associated with
the same reward but a distinct ”wind” sampled uniformly from [−l, l][2]. Every time the
agent takes a step, it drifts by the wind vector. We set l = 0.05 in this paper. Tasks differ in
_transition function only._

-  Sparse-Cheetah-Vel, Sparse-Ant-Fwd-Back, Sparse-Cheetah-Fwd-Back are sparsereward variants of the popular meta-RL benchmarks Half-Cheetah-Vel, Sparse-Ant-Dir
and Sparse-Cheetah-Fwd-Back based on MuJoCo environments, introduced by Finn et al.
(2017) and Rothfuss et al. (2018). Tasks differ in reward function only.

-  Walker-2D-Params is a unique environment compared to other MuJoCo environments.
Agent is initialized with some system dynamics parameters randomized and must move
forward. Transitions function is dependent on randomized task-specific parameters such as
mass, inertia and friction coefficients. Tasks differ in transition function only.

The way we sparsify the reward functions is as follows.

sparsified reward = reward| goal radius−goal radius | _,_ if reward > goal radius (35)

0, otherwise .



Intuitively, we set rewards of states that lie outside a neighborhood of the goal to 0, and re-scaled
the rewards otherwise so that the sparse reward function is continuous. For each of the sparsified
environments other than the relabeled Sparse-Point-Robot, we set its goal radius to achieve a nonsparse rate of about 50%. Note that only the transitions used for training the context-encoder are
_sparsified, since the focus of this paper is learning effective and robust task representations._

D.2 RELABELED DATASET

As discussed in Section 4.3, to prevent information leakage of task identity from state-action distribution, we construct the relabeled Sparse-Point-Robot dataset from a pre-collected dataset of the
Sparse-Point-Robot environment.

Figure 7 illustrates the generating process for task 2 of the original dataset. The original state
distribution of five example tasks on Sparse-Point-Robot is shown in the upper-left. After merging
the transition state-action support across all tasks, the (state, action, next state) distribution are
identical for every specific task. Then we recompute the reward for each transition according to the
task-specific reward functions and sparsify the result. We perform the merge-relabel-sparsify process
for all tasks on Sparse-Point-Robot to enhance the importance of the non-sparse samples for task
inference. The sparse samples in Figure 7 of the main text are those that lie outside of all goals, i.e.
_transitions with zero reward across all tasks._

[The dataset can be accessed and downloaded from relabeled dataset.](https://drive.google.com/file/d/1YQfTPwKuZvL1ITi9Zw8mf5fXbZ6Ip2h6/view?usp=sharing)

D.3 HYPERPARAMETERS

Tables 5 and 6 describe the hyperparameters used in our empirical evaluations.


-----

Figure 7: Generating process of the relabeled Sparse-Point-Robot dataset.

Table 5: Specifications of the environments experimented in our paper.

|Training Set|Training Tasks|Testing Tasks|Goal Radius|
|---|---|---|---|
|Sparse-Point-Robot Sparse-Point-Robot (relabeled) Point-Robot-Wind Sparse-Cheetah-Vel Sparse-Ant-Fwd-Back Sparse-Cheetah-Fwd-Back Walker2d-Rand-Params|80 80 40 80 2 2 20|20 20 10 20 2 2 5|-0.2 -0.5 N/A -0.1 3 6 N/A|


Training Set Training Tasks Testing Tasks Goal Radius

Sparse-Point-Robot 80 20 -0.2

Sparse-Point-Robot (relabeled) 80 20 -0.5

Point-Robot-Wind 40 10 N/A

Sparse-Cheetah-Vel 80 20 -0.1

Sparse-Ant-Fwd-Back 2 2 3

Sparse-Cheetah-Fwd-Back 2 2 6

Walker2d-Rand-Params 20 5 N/A


D.4 IMPLEMENTATION

All experiments are carried out on 64-bit CentOS 7.2 with Tesla P40 GPUs. Code is implemented
and run with PyTorch 1.2.0. One can refer to the source code in the supplementary material for a
complete list of dependencies of the running environment.


-----

Table 6: Hyperparameters used for training to produce Figure 4(a). Meta-batch size refers to the
number of distinct tasks for computing the DML or contrastive loss at a time. Larger meta-batch size
leads to faster convergence but requires greater computational power. For Fwd-Back environments, a
meta-batch size of 4 suffices for stability and efficiency.

|Hyperparameters|Point-Robot|Mujoco|
|---|---|---|
|reward scale discount factor maximum episode length target divergence behavior regularization strength(α) latent space dimension meta-batch size dml lr(α ) 1 actor lr(α ) 2 critic lr(α ) 3 DML loss weight(β) contrastive T contrastive m buffer size (per task) batch size (sac) batch size (context encoder) g lr(f-divergence discriminator) transformer hidden size (context encoder) multihead (if enabled) reduction (batch attention) transformer blocks (context encoder) dropout (context encoder) network width (others) network depth (others)|100 0.9 20 N/A 0 5 16 1e-3 1e-3 1e-3 1 0.5 0.9 1e4 256 512 1e-4 128 8 16 3 0.1 256 3|5 0.99 200 0.05 500 20 16* 3e-3 3e-3 3e-3 1 0.5 0.9 1e4 256 512 1e-4 128 8 16 3 0.1 256 3|


Hyperparameters Point-Robot Mujoco

reward scale 100 5

discount factor 0.9 0.99

maximum episode length 20 200

target divergence N/A 0.05

behavior regularization strength(α) 0 500

latent space dimension 5 20

meta-batch size 16 16*

dml lr(α1) 1e-3 3e-3

actor lr(α2) 1e-3 3e-3

critic lr(α3) 1e-3 3e-3

DML loss weight(β) 1 1

contrastive T 0.5 0.5

contrastive m 0.9 0.9

buffer size (per task) 1e4 1e4

batch size (sac) 256 256

batch size (context encoder) 512 512

g lr(f-divergence discriminator) 1e-4 1e-4

transformer hidden size (context encoder) 128 128

multihead (if enabled) 8 8

reduction (batch attention) 16 16

transformer blocks (context encoder) 3 3

dropout (context encoder) 0.1 0.1

network width (others) 256 256

network depth (others) 3 3


-----

