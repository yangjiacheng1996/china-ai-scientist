# DARA: DYNAMICS-AWARE REWARD AUGMENTATION
## IN OFFLINE REINFORCEMENT LEARNING

**Jinxin Liu[123][∗]** **Hongyin Zhang[1][∗]** **Donglin Wang[13][†]**

1 2
Westlake University. Zhejiang University.
3
Institute of Advanced Technology, Westlake Institute for Advanced Study.
_{liujinxin, zhanghongyin, wangdonglin}@westlake.edu.cn_


ABSTRACT

Offline reinforcement learning algorithms promise to be applicable in settings
where a fixed dataset is available and no new experience can be acquired. However, such formulation is inevitably offline-data-hungry and, in practice, collecting
a large offline dataset for one specific task over one specific environment is also
costly and laborious. In this paper, we thus 1) formulate the offline dynamics
adaptation by using (source) offline data collected from another dynamics to relax
the requirement for the extensive (target) offline data, 2) characterize the dynamics
shift problem in which prior offline methods do not scale well, and 3) derive a simple dynamics-aware reward augmentation (DARA) framework from both modelfree and model-based offline settings. Specifically, DARA emphasizes learning
from those source transition pairs that are adaptive for the target environment and
mitigates the offline dynamics shift by characterizing state-action-next-state pairs
instead of the typical state-action distribution sketched by prior offline RL methods. The experimental evaluation demonstrates that DARA, by augmenting rewards in the source offline dataset, can acquire an adaptive policy for the target
environment and yet significantly reduce the requirement of target offline data.
With only modest amounts of target offline data, our performance consistently
outperforms the prior offline RL methods in both simulated and real-world tasks.


1 INTRODUCTION

Offline reinforcement learning (RL) (Levine et al., 2020; Lange
et al., 2012), the task of learning from the previously collected
dataset, holds the promise of acquiring policies without any costly
active interaction required in the standard online RL paradigm.
However, we note that although the active trail-and-error (online
exploration) is eliminated, the performance of offline RL method
heavily relies on the amount of offline data that is used for training.
As shown in Figure 1, the performance deteriorates dramatically as
the amount of offline data decreases. A natural question therefore
arises: can we reduce the amount of the (target) offline data without
significantly affecting the final performance for the target task?


100

50

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||BEAR CQL||
||||||MOPO||
||||||||


100% 50% 20% 10% 5%

BEAR
CQL
MOPO

Amount of data used for training

Figure 1: Solid and dashed
lines denote offline MediumReplay and Medium-Expert
data in D4RL (Walker2d) _resp._


Bringing the idea from the transfer learning (Pan & Yang, 2010), we assume that we have access
to another (source) offline dataset, hoping that we can leverage this dataset to compensate for the
performance degradation caused by the reduced (target) offline dataset. In the offline setting, previous work (Siegel et al., 2020; Chebotar et al., 2021) has characterized the reward (goal) difference between the source and target, relying on the ”conflicting” or multi-goal offline dataset (Fu
et al., 2020), while we focus on the relatively unexplored transition dynamics difference between
the source dataset and the target environment. Meanwhile, we believe that this dynamics shift is
not arbitrary in reality: in healthcare treatment, offline data for a particular patient is often limited,
whereas we can obtain diagnostic data from other patients with the same case (same reward/goal)

_∗Equal contribution._
_†Corresponding author._


-----

and there often exist individual differences between patients (source dataset with different transition
dynamics). Careful treatment with respect to the individual differences is thus a crucial requirement.

Given source offline data, the main challenge is to cope with the transition dynamics difference, i.e.,
strictly tracking the state-action supported by the source offline data can not guarantee that the same
transition (state-action-next-state) can be achieved in the target environment. However, in the offline
setting, such dynamics shift is not explicitly characterized by the previous offline RL methods, where
they typically attribute the difficulty of learning from offline data to the state-action distribution
shift (Chen & Jiang, 2019; Liu et al., 2018). The corresponding algorithms (Fujimoto et al., 2019;
Abdolmaleki et al., 2018; Yu et al., 2020) that model the support of state-action distribution induced
by the learned policy, will inevitably suffer from the transfer problem where dynamics shift happens.

Our approach is motivated by the well established connection between reward modification and
dynamics adaptation (Kumar et al., 2020b; Eysenbach & Levine, 2019; Eysenbach et al., 2021),
which indicates that, by modifying rewards, one can train a policy in one environment and make the
learned policy to be suitable for another environment (with different dynamics). Thus, we propose
to exploit the joint distribution of state-action-next-state: besides characterizing the state-action
distribution shift as in prior offline RL algorithms, we additionally identify the dynamics (i.e., the
conditional distribution of next-state given current state-action pair) shift and penalize the agent with
a dynamics-aware reward modification. Intuitively, this reward modification aims to discourage
the learning from these offline transitions that are likely in source but are unlikely in the target
environment. Unlike the concurrent work (Ball et al., 2021; Mitchell et al., 2021) paying attention to
the offline domain generalization, we explicitly focus on the offline domain (dynamics) adaptation.

Our principal contribution in this work is the characterization of the dynamics shift in offline RL and
the derivation of dynamics-aware reward augmentation (DARA) framework built on prior modelfree and model-based formulations. DARA is simple and general, can accommodate various offline
RL methods, and can be implemented in just a few lines of code on top of dataloader at training. In
our offline dynamics adaptation setting, we also release a dataset, including the Gym-MuJoCo tasks
(Walker2d, Hopper and HalfCheetah), with dynamics (mass, joint) shift compared to D4RL, and
a 12-DoF quadruped robot in both simulator and real-world. With only modest amounts of target
offline data, we show that DARA-based offline methods can acquire an adaptive policy for the target
tasks and achieve better performance compared to baselines in both simulated and real-world tasks.

2 RELATED WORK

Offline RL describes the setting in which a learner has access to only a fixed dataset of experience,
while no interactive data collection is allowed during policy learning (Levine et al., 2020). Prior
work commonly assumes that the offline experience is collected by some behavior policies on the
same environment that the learned policy be deployed on. Thus, the main difficulty of such offline
setting is the state-action distribution shift (Fujimoto et al., 2019; Liu et al., 2018). Algorithms
address this issue by following the two main directions: the model-free and model-based offline RL.

_Model-free methods for such setting typically fall under three categories: 1) Typical methods mit-_
igate this problem by explicitly (Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019) or
implicitly (Siegel et al., 2020; Peng et al., 2019; Abdolmaleki et al., 2018) constraining the learned
policy away from OOD state-action pairs. 2) Conservative estimation based methods learn pessimistic value functions to prevent the overestimation (Kumar et al., 2020a; Xu et al., 2021). 3)
Importance sampling based methods directly estimate the state-marginal importance ratio and obtain an unbiased value estimation (Zhang et al., 2020; Nachum & Dai, 2020; Nachum et al., 2019b).

_Model-based methods typically eliminate the state-action distribution shift by incorporating a reward_
penalty, which relies on the uncertainty quantification of the learned dynamics (Kidambi et al., 2020;
Yu et al., 2020). To remove this uncertainty estimation, Yu et al. (2021) learns conservative critic
function by penalizing the values of the generated state-action pairs that are not in the offlinedataset.

These methods, however, define their objective based on the state-action distribution shift, and ignore the potential dynamics shift between the fixed offline data and the target MDP. In contrast, we
account for dynamics (state-action-next-state) shift and explicitly propose the dynamics aware reward augmentation. A counterpart, close to our work, is off-dynamics RL (Eysenbach et al., 2021),
where they set up dynamics shift in the interactive environment while we focus on the offline setting.


-----

3 PRELIMINARIES

We study RL in the framework of Markov decision processes (MDPs) specified by the tuple M :=
( _,_ _, r, T, ρ0, γ), where_ and denote the state and action spaces, r(s, a) [ _Rmax, Rmax] is_
_S_ _A_ _S_ _A_ _∈_ _−_
the reward function, T (s[′] **s, a) is the transition dynamics, ρ0(s) is the initial state distribution, and**
_|_
_γ is the discount factor. The goal in RL is to optimize a policy π(a|s) that maximizes the expected_
discounted return ηM (π) := Eτ _∼p[π]M_ [(][τ] [)][ [][P]t[∞]=0 _[γ][t][r][(][s][t][,][ a][t][)]][, where][ τ][ := (][s][0][,][ a][0][,][ s][1][,][ a][1][, ...][)][. We]_
also define Q-values Q(s, a) := Eτ _∼p[π]M_ [(][τ] [)][ [][P]t[∞]=0 _[γ][t][r][(][s][t][,][ a][t][)][|][s][0][ =][ s][,][ a][0][ =][ a][]][, V-values][ V][ (][s][) :=]_
Ea∼π(a|s) [Q(s, a)], and the (unnormalized) state visitation distribution d[π]M [(][s][) :=][ P][∞]t=0 _[γ][t][P]_ [(][s][|][π,]
_M, t), where P_ (s|π, M, t) denotes the probability of reaching state s at time t by running π in M .

In the offline RL problem, we are provided with a static dataset D := {(s, a, r, s[′])}, which consists of transition tuples from trajectories collected by running one or more behavioral policies,
denoted by πb, on MDP M . With a slight abuse of notation, we write = (s, a, r, s[′])
_D_ _{_ _∼_
_dD(s)πb(a|s)r(s, a)T_ (s[′]|s, a)}, where the dD(s) denotes state-marginal distribution in D. In the
offline setting, the goal is typically to learn the best possible policy using the fixed offline dataset.

**Model-free RL algorithms based on dynamic programming typically perform policy iteration to**
find the optimal policy. Such methods iteratively conduct 1) policy improvement with _M_ _Q :=_
_G_
arg maxπ Es∼d[π]M [(][s][)][,][a][∼][π][(][a][|][s][)][ [][Q][(][s][,][ a][)]][ and 2) policy evaluation by iterating the Bellman equation]
_Q(s, a) = BM[π]_ _[Q][(][s][,][ a][) :=][ r][(][s][,][ a][) +][ γ][E][s][′][∼][T][ (][s][′][|][s][,][a][)][,][a][′][∼][π][(][a][′][|][s][′][)]_ [[][Q][(][s][′][,][ a][′][)]][ over][ d][π]M [(][s][)][π][(][a][|][s][)][. Given]
off-policy D, we resort to 1) improvement with GDQ := arg maxπ Es∼dD(s),a∼π(a|s) [Q(s, a)] and
2) evaluation by iterating Q(s, a) = [[][Q][(][s][′][,][ a][′][)]]
_BD[π]_ _[Q][(][s][,][ a][) :=][ r][(][s][,][ a][) +][ γ][E][s][′][∼][T]D[(][s][′][|][s][,][a][)][,][a][′][∼][π][(][a][′][|][s][′][)]_
over all (s, a) in D. Specifically, given any initial Q[0], it iterates[1]

Policy improvement: π[k][+1] = GDQ[k], Policy evaluation: Q[k][+1] = BD[π][k][+1] _Q[k]._ (1)

**Model-free offline RL based on the above iteration suffers from the state-action distribution shift,**
_i.e., policy evaluation_ _[Q][k][−][1][ may encounter unfamiliar state action regime that is not covered]_
_BD[π][k]_
by the fixed offline dataset, causing erroneous estimation of Q[k]. Policy improvement _Q[k]_
_D_ _GD_
further exaggerates such error, biasing policy π[k][+1] towards out-of-distribution (OOD) actions with
erroneously high Q-values. To address this distribution shift, prior works 1) explicitly constrain
policy to be close to the behavior policy (Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019;
Ghasemipour et al., 2021), introducing penalty αD(π(a|s), πb(a|s)) into GD or BD[π] [in Equation 1:]

_Q = arg max_ Es _d_ (s),a _π(a_ **s) [Q(s, a)** _αD(π(a_ **s), πb(a** **s))],**
_GD_ _π_ _∼_ _D_ _∼_ _|_ _−_ _|_ _|_ (2)

**s[′]** _T_ (s[′] **s,a),a[′]** _π(a[′]_ **s[′])** [[][Q][(][s][′][,][ a][′][)][ −] _[αD][(][π][(][a][′][|][s][′][)][, π][b][(][a][′][|][s][′][))]][,]_
_BD[π]_ _[Q][(][s][,][ a][) =][ r][(][s][,][ a][) +][ γ][E]_ _∼_ _D_ _|_ _∼_ _|_

where D is a divergence function between distributions over actions (e.g., MMD or KL divergence),
or 2) train pessimistic value functions (Kumar et al., 2020a; Yu et al., 2021; Xu et al., 2021), penalizing Q-values at states in the offline dataset D for actions generated by the current policy π:

_Q = arg min_ Es _d_ (s),a _π(a_ **s) [Q(s, a)],** s.t. Q = (3)
_Q_ _∼_ _D_ _∼_ _|_ _BD[π]_ _[Q.]_

**Model-based RL algorithms iteratively 1) model the transition dynamics T** (s[′]|s, a), using the data
collected in M : max ˆT [E][s][,][a][,][s][′][∼][d]M[π] [(][s][)][π][(][a][|][s][)][T][ (][s][′][|][s][,][a][)][[log ˆ]T (s[′]|s, a)], and 2) infer a policy π from the
modeled _M[ˆ] = (S, A, r,_ _T, ρ[ˆ]_ 0, γ), where we assume that r and ρ0 are known, maximizing η ˆM [(][π][)]
with a planner or the Dyna-style algorithms (Sutton, 1990). In this paper, we focus on the latter.

**Model-based offline RL algorithms similarly suffer from OOD state-action (Kidambi et al., 2020;**
Cang et al., 2021) if we directly apply policy iteration over _T[ˆ] := max ˆT_ [E][s][,][a][,][s][′][∼D][[log ˆ]T (s[′]|s, a)].
Like the conservative estimation approach described in Equation 3, recent conservative model-based
offline RL methods provide the policy with a penalty for visiting states under the estimated _T[ˆ] where_
_Tˆ is likely to be incorrect. Taking u(s, a) as the oracle uncertainty (Yu et al., 2020) that provides a_
consistent estimate of the accuracy of model _T[ˆ] at (s, a), we can modify the reward function to obtain_
a conservative MDP: _M[ˆ]_ _c = (S, A, r −_ _αu,_ _T, ρ[ˆ]_ 0, γ), then learn a policy π by maximizing η ˆMc [(][π][)][.]

1For parametric Q-function, we often perform Qk+1 ← arg minQ E(s,a)∼D[(BDπ[k][+1] _Q[k](s, a)−Q(s, a))[2]]._


-----

4 PROBLEM FORMULATION

In standard offline RL problem, the static offline dataset D consists of samples {(s, a, r, s[′]) ∼
_dD(s)πb(a|s)r(s, a)T_ (s[′]|s, a)}. Although offline RL methods learn policy for the target MDP
_M := (_ _,_ _, r, T_ _, ρ0, γ) without (costly) online data, as we shown in Figure 1, it requires a fair_
_S_ _A_
amount of (target) offline data D collected on M . Suppose we have another (source) offline dataset
_D[′], consisting of samples {(s, a, r, s[′]) ∼_ _dD′_ (s)πb′ (a|s)r(s, a)T _[′](s[′]|s, a)} collected by the behav-_
ior policy πb′ on MDP M _[′]_ := (S, A, r, T _[′], ρ0, γ), then we hope the transfer of knowledge between_
offline dataset {D[′] _∪_ _D} can reduce the data requirements on D for learning policy for the target M_ .

4.1 DYNAMICS SHIFT IN OFFLINE RL

Although offline RL methods in Section 3 have incorporated the state-action distribution constrained
backups (policy constraints or conservative estimation), they also fail to learn an adaptive policy for
the target MDP M with the mixed datasets {D[′] _∪_ _D}, as we show in Figure 4 (Appendix). We_
attribute this failure to the dynamics shift (Definition 2) between D[′] and M in this adaptation setting.

**Definition 1 (Empirical MDP) An empirical MDP estimated from D is** _M[ˆ] := (S, A, r,_ _T, ρ[ˆ]_ 0, γ)
_where_ _T[ˆ] = max ˆT_ [E][s][,][a][,][s][′][∼D][[log ˆ]T (s[′]|s, a)] and _T[ˆ](s[′]|s, a) = 0 for all (s, a, s[′]) not in dataset D._

**Definition 2 (Dynamics shift) Let** _M[ˆ] := (S, A, r,_ _T, ρ[ˆ]_ 0, γ) be the empirical MDP estimated from
_. To evaluate a policy π for M := (_ _,_ _, r, T, ρ0, γ) with offline dataset_ _, we say that the_
_D_ _S_ _A_ _D_
_dynamics shift (between D and M_ _) in offline RL happens if there exists at least one transition pair_
(s, a, s[′]) ∈{(s, a, s[′]) : d[π]Mˆ [(][s][)][π][(][a][|][s][) ˆ]T (s[′]|s, a) > 0} such that _T[ˆ](s[′]|s, a) ̸= T_ (s[′]|s, a).

In practice, for a stochastic M and any finite offline data D collected in M, there always exists the
dynamics shift. The main concern is that finite samples are always not sufficient to exactly model
stochastic dynamics. Following Fujimoto et al. (2019), we thus assume both MDPs M and M _[′]_ are
deterministic, which means the empirical _M[ˆ] and_ _M[ˆ]_ _[′]_ are both also deterministic. More importantly,
such assumption enables us to explicitly characterize the dynamics shift under finite offline samples.

**Lemma 1 Under deterministic transition dynamics, there is no dynamics shift between D and M** _._

For offline RL tasks, prior methods generally apply
_BD[π]_ _[Q][ along with the state-action distribution]_
correction (Equations 2 and 3), which overlooks the potential dynamics shift between the (source)
offline dataset and the target MDP (e.g., D[′] _→_ _M_ ). As a result, these methods do not scale well to
the setting in which dynamics shift happens, e.g., learning an adaptive policy for M with (source) D[′].

4.2 DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS

From the model-free (policy iteration) view, an exact policy evaluation on M is characterized by
iterating Q(s, a) = BM[π] _[Q][(][s][,][ a][)][ for all][ (][s][,][ a][)][ such that][ d][π]M_ [(][s][)][π][(][a][|][s][)][ >][ 0][. Thus, to formalize the pol-]

icy evaluation with offline D or D[′] (for an adaptive π on target M ), we require that Bellman operator

_M_ _[Q][(][s][,][ a][)][ for all][ (][s][,][ a][)][ in][ S][π][ or][ S]π[′]_ [, where][ S][π]

_BD[π]_ _[Q][(][s][,][ a][)][ or][ B]D[π]_ _[′]_ _[Q][(][s][,][ a][)][ approximates the oracle][ B][π]_

and Sπ[′] [denote the sets][ {][(][s][,][ a][) :][ d][D][(][s][)][π][(][a][|][s][)][ >][ 0][}][ and][ {][(][s][,][ a][) :][ d][D][′] [(][s][)][π][(][a][|][s][)][ >][ 0][}][ respectively.]

1) To evaluate a policy π for M with (i.e., calling the Bellman operator
_D_ _BD[π]_ [), notable model-]

free offline method BCQ (Fujimoto et al., 2019) translates the requirement of [=][ B]M[π] [into the]
_BD[π]_

requirement of _T[ˆ](s[′]|s, a) = T_ (s[′]|s, a). Note that under deterministic environments, we have the
property that for all (s, a, s[′]) in offline data D, _T[ˆ](s[′]|s, a) = T_ (s[′]|s, a) (Lemma 1). As a result, such
property permits BCQ to evaluate a policy π by calling _M_ [, meanwhile]
_BD[π]_ [, replacing the oracle][ B][π]

constraining Sπ to be a subset of the support of d (s)πb(a **s). This means a policy π which only**
_D_ _|_
traverses transitions contained in (target) offline data D, can be evaluated on M without error.

2) To evaluate a policy π for M with (i.e., calling the Bellman operator
_D[′]_ _BD[π]_ _[′]_ [), we have lemma 2:]

**Lemma 2 Dynamics shift produces that** _M_ _[Q][(][s][,][ a][)][ for some][ (][s][,][ a][)][ in][ S]π[′]_ _[.]_
_BD[π]_ _[′]_ _[Q][(][s][,][ a][)][ ̸][=][ B][π]_


With the offline data, lemma 2 suggests that the above requirement _M_ [becomes infea-]
_D[′]_ _BD[π]_ _[′][ =][ B][π]_

sible, which limits the practical applicability of prior offline RL methods under the dynamics shift.


-----

To be specific, characterizing an adaptive policy for target MDP M with D[′] moves beyond the reach
of the off-policy evaluation based on iterating Q =
_BD[π]_ _[′]_ _[Q][ (Equations 2 and 3). Such iteration may]_

cause the evaluated Q (or learned policy π) overfits to _T[ˆ][′]_ and struggle to adapt to the target T . To
overcome the dynamics shift, we would like to resort an additional compensation ∆Tˆ[′],T [such that]

_BD[π]_ _[′]_ _[Q][(][s][,][ a][) + ∆]T[ˆ][′],T_ [(][s][,][ a][) =][ B]M[π] _[Q][(][s][,][ a][)]_ (4)


for all (s, a) in Sπ[′] [. Thus, we can apply][ B]D[π] _[′]_ _[Q][ + ∆]T[ˆ][′],T_ [to act as a substitute for the oracle][ B]M[π] _[Q][.]_

From the model-based view, the oracle ηM (π) (calling the Bellman operator BM[π] [on the target][ M] [)]

and the viable η ˆM _[′]_ [(][π][)][ (calling][ B]M[π]ˆ _[′][ on the estimated][ ˆ]M_ _[′]_ from source D[′]) have the followinglemma.

**Lemma 3 Let BM[π]** _[V][ (][s][) =][ E][a][∼][π][(][a][|][s][)]_ _r(s, a) + γEs′∼T (s′|s,a) [V (s[′])]_ _. For any π, we have:_
 


_η ˆM_ _[′]_ [(][π][) =][ η][M] [(][π][) +][ E][s][∼][d]M[π]ˆ _[′]_ [(][s][)] _BM[π]ˆ_ _[′]_ _[V][M]_ [(][s][)][ −B]M[π] _[V][M]_ [(][s][)] _._

 

Lemma 3 states that if we maximize η ˆM _[′]_ [(][π][)][ subject to][ |][E][s][∼][d]M[π]ˆ _[′]_ [(][s][)][[][B]M[π]ˆ _[′]_ _[V][M]_ [(][s][)][ −B]M[π] _[V][M]_ [(][s][)]][| ≤] _[ϵ][,]_

_ηM_ (π) will be improved. If F is a set of functions f : S → R that contains VM, then we have
Es∼d[π]Mˆ _[′]_ [(][s][)] _BM[π]ˆ_ _[′]_ _[V][M]_ [(][s][)][ −B]M[π] _[V][M]_ [(][s][)] _≤_ _γEs,a∼d[π]Mˆ_ _[′]_ [(][s][)][π][(][a][|][s][)] _dF_ (T[ˆ][′](s[′]|s, a), T (s[′]|s, a)) _,_ (5)

  h i

where dF (T[ˆ][′](s[′]|s, a), T (s[′]|s, a)) = supf _∈F |Es′∼Tˆ[′](s[′]|s,a)_ [[][f] [(][s][′][)]] _[−]_ [E][s][′][∼][T][ (][s][′][|][s][,][a][)][ [][f] [(][s][′][)]][ |][, which is]
the integral probability metric (IPM). Note that if we directly follow the admissible error assumption
in MOPO (Yu et al., 2020) i.e., assuming dF (T[ˆ][′](s[′]|s, a), T (s[′]|s, a)) ≤ _u(s, a) for all (s, a), this_
would be too restrictive: given that _T[ˆ][′]_ is estimated from the source offline samples collected under

_T_ _[′], not the target T_, thus such error would not decrease as the source data increases. Further, we find


_dF_ (T[ˆ][′](s[′]|s, a), T (s[′]|s, a)) ≤ _dF_ (T[ˆ][′](s[′]|s, a), _T[ˆ](s[′]|s, a)) + dF_ (T[ˆ](s[′]|s, a), T (s[′]|s, a)). (6)

Thus, we can bound the dF (T[ˆ][′], T ) term with the admissible error assumption over dF (T[ˆ], T ), as in
MOPO, and the auxiliary constraints dF (T[ˆ][′], _T[ˆ]). See next section for the detailed implementation._

**In summary, we show that both prior offline model-free and model-based formulations suffer from**
the dynamics shift, which also suggests us to learn a modification (∆ or d ) to eliminate this shift.
_F_

5 DYNAMICS-AWARE REWARD AUGMENTATION

In this section, we propose the dynamics-aware reward augmentation (DARA), a simple data augmentation procedure based on prior (model-free and model-based) offline RL methods. We first
provide an overview of our offline reward augmentation motivated by the compensation ∆Tˆ[′],T [in]

Equation 4 and the auxiliary constraints dF (T[ˆ][′], _T[ˆ]) in Equation 6, and then describe its theoretical_
derivation in both model-free and model-based formulations. With the (reduced) target offline data
_D and the source offline data D[′], we summarize the overall DARA framework in Algorithm 1._

**Algorithm 1 Framework for Dynamics-Aware Reward Augmentation (DARA)**


**Require: Target offline data D (reduced) and source offline data D[′]**

2: Set dynamics-aware1: Learn classifiers (qsas ∆ andr(s qtsa, a) that distinguish source datat, st+1) = log _qqsassas((sourcetarget_ _|sstt,,aatt D,,sstt[′]+1+1from target data))_ _qsa(target Dst. (See Appendix A.1.3),at)_ [.]

_|_ _[−]_ [log][ q][sa][(][source]|[|][s][t][,][a][t][)]

4: Learn policy with3: Modify rewards for all {D ∪ (sDt,[′] a} using prior model-free or model-based offline RL algorithms.t, rt, st+1) in D[′]: rt ← _rt −_ _η∆r._

5.1 DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION

Motivated by the well established connection of RL and probabilistic inference (Levine, 2018),
we first cast the model-free RL problem as that of inference in a particular probabilistic model.
Specifically, we introduce the binary random variable O that denotes whether the trajectory τ :=


-----

(s0, a0, s1, ...) is optimal ( = 1) or not ( = 0). The likelihood of a trajectory can then be modeled
_O_ _O_
as p(O = 1|τ ) = exp ([P]t _[r][t][/η][)][, where][ r][t][ :=][ r][(][s][t][,][ a][t][)][ and][ η >][ 0][ is a temperature parameter.]_

**(Reward Augmentation with Explicit Policy/Value Constraints) We now introduce a variational**
distribution p[π]Mˆ _[′]_ [(][τ] [) =][ p][(][s][0][)][ Q]t=1 _T[ˆ][′](st+1|st, at)π(at|st) to approximate the posterior distribution_

_p[π]M_ [(][τ] _[|O][ = 1)][, which leads to the evidence lower bound of][ log][ p]M[π]_ [(][O][ = 1)][:]

_M_ [(][τ] [)]

log p[π]M [(][O][ = 1) = log][ E]τ _∼p[π]M_ [(][τ] [)][ [][p][(][O][ = 1][|][τ] [)]][ ≥] [E][τ] _[∼][p][π]Mˆ_ _[′]_ [(][τ] [)] "log p(O = 1|τ ) + log _p[p][π]M[π]ˆ_ _[′]_ [(][τ] [)] #

_Tˆ_ (st+1 **st, at)**

= Eτ _∼p[π]Mˆ_ _[′]_ [(][τ] [)] _t_ _rt/η −_ log _T[′](st+1_ _|st, at)_ !# _._ (7)

_|_

"X

Since we are interested in infinite horizon problems, we introduce the discount factor γ and take
the limit of steps in each rollout, i.e., H →∞. Thus, the RL problem on the MDP M, cast as
the inference problem arg maxπ log p[π]M [(][O][ = 1)][, can be stated as a maximum of the lower bound]

Eτ _∼p[π]Mˆ_ _[′]_ [(][τ] [)] _∞t=0_ _[γ][t][ ]rt −_ _η log_ _TTˆ[′] ((sstt+1+1||sstt,,aatt))_ . This is equivalent to an RL problem on _M[ˆ]_ _[′]_ with

the augmented rewardhP _r ←_ _r(s, a) −_ _η log_ _TTˆ[′] ((sis[′][′]||ss,,aa))_ [. Intuitively, the][ −][η][ log] _TTˆ[′] ((ss[′][′]||ss,,aa))_ [term discour-]

ages transitions (state-action-next-state) in D[′] that have low transition probability in the target M .
In the model-free offline setting, we can add the explicit policy or Q-value constraints (Equations2 and 3) to mitigate the OOD state-actions. Thus, such formulation allows the oracleexpressed by _TTˆ[′]_ [, which makes the motivation in Equation 4 practical.] BM[π] [to be re-]

_BD[π]_ _[′][ and the modification][ log]_

**(Reward Augmentation with Implicit Policy Constraints) If we introduce the variational distri-**
bution p[π]Mˆ[′] _[′]_ [(][τ] [) :=][ p][(][s][0][)][ Q]t=1 _T[ˆ][′](st+1|st, at)π[′](at|st), we can recover the weighted-regression-_

style (Wang et al., 2020; Peng et al., 2019; Abdolmaleki et al., 2018; Peters et al., 2010) objective by
maximizing J (π[′], π) := Eτ _∼pπMˆ[′][′]_ [(][τ] [)] _∞t=0_ _[γ][t][ ]rt −_ _η log_ _TTˆ[′] ((sstt+1+1||sstt,,aatt))_ _[−]_ _[η][ log][ π]π[′]([(]a[a]t[t]|[|]s[s]t[t])[)]_ (lower

bound of log p[π]M [(][O][ = 1)][). Following the Expectation Maximization (EM) algorithm, we can maxi-]hP i

mize J (π[′], π) by iteratively (E-step) improving J (π[′], ·) w.r.t. π[′] and (M-step) updating π w.r.t. π[′].

_(E-step) We define_ _Q[˜](s, a, s[′]) = Eτ_ _∼pπMˆ[′][′]_ [(][τ] [)] _t_ _[γ][t][ log]_ _TTˆ[′] ((ss[′][′]||ss,,aa))_ _[|][s][0][ =][ s][,][ a][0][ =][ a][,][ s][1][ =][ s][′][i]. Then,_

given offline data, we can rewrite (π[′], ) as a constrained objective (Abdolmaleki et al., 2018):
_D[′]_ _J_ _·_ hP

maxπ[′][ E][d][D′] [(][s][)][π][′][(][a][|][s][)][ ˆ]T _[′](s[′]|s,a)_ _Q(s, a) −_ _ηQ[˜](s, a, s[′])_ _,_ s.t. Es∼dD′ (s) [DKL (π[′](a|s)∥π(a|s))] ≤ _ϵ._
h i

When considering a fixed π, the above optimization over π[′] can be solved analytically (Vieillard
et al., 2020; Geist et al., 2019; Peng et al., 2019). The optimal π[′]
_∗_ [is then given by][ π]∗[′] [(][a][|][s][)][ ∝]
_π(a|s) exp (Q(s, a)) exp(−ηQ[˜](s, a,_ _T[ˆ][′](s[′]|s, a))). As the policy evaluation in Equation 1 (Footnote-_
2), we estimate Q(s, a) and _Q[˜](s, a, s[′]) by minimizing the Bellman error with offline samples in D[′]._

_(M-step) Then, we can project π[′]_
_∗_ [onto the manifold of the parameterized][ π][:]

arg min Es _d_ (s) [DKL (π[′]
_π_ _∼_ _D′_ _∗[(][a][|][s][)][∥][π][(][a][|][s][))]]_

= arg maxπ Es,a,s′∼D′ log π(a|s) exp (Q(s, a)) exp _−ηQ[˜](s, a, s[′])_ _._ (8)

h  i

From the regression view, prior work MPO (Abdolmaleki et al., 2018) infers actions with Q-value
weighted regression, progressive approach compared to behavior cloning; however, such paradigm
lacks the ability to capture transition dynamics. We explicitly introduce the exp(−ηQ[˜](s, a, s[′]))
term, which as we show in experiments, is a crucial component for eliminating the dynamics shift.

in Eysenbach et al. (2021):Implementation:M, and introduce a pair of binary classifiers, In practice, we adopt offline samples in log _TTˆ[′] ((ss[′][′]|ss,,aa))_ [= log] qsas[ q](q·|[sas]sass[(](,[source]target a, s[′][|]s D[s]),[,] anda[a], to approximate the true dynamics[,]s[s][′][′])[)] qsa(·|s, aqsa)(, to replacetarget **s,a)** [. (See Appendix-] log _TTˆ[′] ((ss[′][′]||ss,, Taa)) of[as]_

A.1.3 for details). Although the amount of data| _D sampled from the target|_ _[−]_ [log][ q][sa][(][source] M is reduced in our prob-|[|][s][,][a][)]
lem setup, we experimentally find that such classifiers are sufficient to achieve good performance.


-----

5.2 DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-BASED FORMULATION


Following Equation 6, we then characterize the dynamics shift compensation term as in the above
model-free analysis in the model-based offline formulation. We will find that across different derivations, our reward augmentation ∆r has always maintained the functional consistency and simplicity.

Following MOPO, we assume F = {f : ∥f _∥∞_ _≤_ 1}, then we have1 _dF_ (T[ˆ][′](s[′]|s, a), _T[ˆ](s[′]|s, a)) =_
_DTV(T[ˆ][′](s[′]|s, a),_ _T[ˆ](s[′]|s, a)) ≤_ (DKL(T[ˆ][′](s[′]|s, a), _T[ˆ](s[′]|s, a))/2)_ 2, where DTV is the total variance
distance. Then we introduce the admissible error u(s, a) such that d (T[ˆ](s[′] **s, a), T** (s[′] **s, a))**

1 _F_ _|_ _|_ _≤_
_u(s, a) for all (s, a), and η and δ such that (DKL(T[ˆ]_ _,_ _T[ˆ])/2)_ 2 _ηDKL(T[ˆ]_ _,_ _T[ˆ]) + δ. Following_

_[′]_ _≤_ _[′]_
Lemma 3, we thus can maximize the following lower bound with the samples in _M[ˆ]_ _[′]_ (λ := _[γR]1_ _[max]γ_ [):]

_−_

_ηM_ (π) ≥ Es,a,s′∼dπMˆ _[′]_ [(][s][)][π][(][a][|][s][)][ ˆ]T _[′](s[′]|s,a)_ "r(s, a) − _ηλ log_ _TTˆˆ[′]((ss[′][′]|ss,, a a))_ _−_ _λu(s, a) −_ _λδ#_ _._ (9)

_|_

_Implementation: We model the dynamics_ _T[ˆ][′]_ and _T[ˆ] with an ensemble of 2*N parameterized Gaus-_
sian distributions: NT[i]ˆ[′] [(][µ][θ][′] [(][s][,][ a][)][,][ Σ][φ][′] [(][s][,][ a][))][ and][ N][ i]Tˆ[(][µ][θ][(][s][,][ a][)][,][ Σ][φ][(][s][,][ a][))][, where][ i][ ∈] [[1][, N] []][. We]

approximate u with the maximum standard deviation of the learned models in the ensemble:
_u(s, a) = max[N]i=1_
as in MOPO. For the[∥][Σ] log[φ][(][s]T[,]Tˆˆ[ a][′] [)][term, we resort to the above classifiers (][∥][F][, omit the training-independent][ δ][, and treat][q][sas][ and][ λ][ q][ as a hyperparameter][sa][) in model-free set-]

ting. (See Appendix-A.3.2 for comparison between using classifiers and estimated-dynamics ratio.)


_ηM_ (π) ≥ Es,a,s′∼dπMˆ _[′]_ [(][s][)][π][(][a][|][s][)][ ˆ]T _[′](s[′]|s,a)_


_r(s, a) −_ _ηλ log_


6 EXPERIMENTS

We present empirical demonstrations of our dynamics-aware reward augmentation (DARA) in a
variety of settings. We start with two simple control experiments that illustrate the significance of
DARA under the domain (dynamics) adaptation setting. Then we incorporate DARA into state-ofthe-art (model-free and model-based) offline RL methods and evaluate the performance on the D4RL
tasks. Finally, we compare our framework to several cross-domain-based baselines on simulated
and real-world tasks. Note that for the dynamics adaptation, we also release a (source) dataset as a
complement to D4RL, along with the quadruped robot dataset in simulator (source) and real (target).


6.1 HOW DOES DARA HANDLE THE DYNAMICS SHIFT IN OFFLINE SETTING?





|6|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|6 0.2|||||||
|0.2|||||||


Time step

source/target source target

goal

× × w/o Aug.

DARA

Q (w/o Aug.) Q (DARA)

0

20

(s1, a1)

40 (s2, a2)

Restricted
in target

Figure 3: Internal dynamics shift: (left) source

Figure 2: External dynamics shift: (left) source and target MDPs (range of the right-back-leg of
and target MDPs (target contains an obstacle rep- the ant (state[11]) is limited: [−0.52, 0.52] in
resented with the dashed line); (middle) top plots source MDP → [−0.26, 0.26] in target MDP);
(w/o Aug.) depict the trajectories that are gen- _(right) the solid (orange) line denotes the state_
erated by the learned policy with vanilla MPO; of the right-back-leg over one trajectory col_(middle) bottom plots (DARA) depict the tra-_ lected in source, dashed (blue) line denotes the
jectories that are generated by the learned pol- learned reward modification −∆r over the traicy with DARA-based MPO; (right) learned Q- jectory, and green and red slices denote transivalues on the state-action pairs in left subfigure. tion pairs where −∆r ≥ and −∆r < 0, resp.

Here we characterize both external and internal dynamics shifts: In Map tasks (Figure 2 left), the
source dataset D[′] is collected in a 2D map and the target D is collected in the same environment but
with an obstacle (the dashed line); In Ant tasks (Figure 3 left), the source dataset D[′] is collected using
the Mujoco Ant and the target D is collected with the same Ant but one joint of which is restricted.

Using MPO, as an example of offline RL method, we train a policy on dataset {D[′] _∪_ _D} and deploy_
the acquired policy in both source and target MDPs. As shown in Figure 2 (middle-top, w/o Aug.),
such training paradigm does not produce an adaptive policy for the target. By modifying rewards in


-----

Table 1: Normalized scores for the (target) D4RL tasks, where our results are averaged over5seeds.
The arrows in each four-tuple indicate whether the current performance has improved (↑) or not (↓)
compared to the previous value. If 1T+10S DARA achieves comparable (less than 10% degradation)
or better performance compared to baseline 10T, we highlight our scores in bold (in each four-tuple).

1T+10S 1T+10S 1T+10S 1T+10S 1T+10S 1T+10S
Body Mass Shift 10T 1T w/o Aug. DARA 10T 1T w/o Aug. DARA 10T 1T w/o Aug. DARA


BEAR BRAC-p AWR

Hopper Medium-EMedium-RRandomMedium 96.333.711.452.1 0.81.31.00.8 ↓ ↓ ↓ ↓ 18.20.60.94.6 ↓ ↑ ↑ ↑ **34.11.21.68.4 ↑ ↑ ↑ ↑** 11.032.70.61.9 34.510.929.05.4 ↑ ↑ ↓ ↓ 20.132.329.29.6 ↓ ↑ ↓ ↑ **30.834.732.911.0 ↑ ↑ ↑ ↑** 28.427.135.910.2 27.030.910.38.8 ↓ ↓ ↓ ↑ 26.820.84.13.4 ↓ ↓ ↓ ↓ **26.628.94.24.5 ↑ ↑ ↓ ↑**


BCQ CQL MOPO

Hopper Medium-EMedium-RRandomMedium 110.933.110.654.5 10.637.19.358 ↓ ↓ ↓ ↓ 75.428.725.78.3 ↓ ↑ ↑ ↓ 84.232.838.49.7 ↑ ↑ ↑ ↑ 98.748.658.010.8 59.743.010.69.6 ↓ ↓ ↓ ↓ 53.644.910.21.4 ↓ ↓ ↑ ↓ **99.759.310.43.7 ↑ ↑ ↑ ↑** 23.767.528.011.7 1.61.04.14.8 ↓ ↓ ↓ ↓ 4.85.55.02.0 ↑ ↑ ↑ ↓ 10.75.88.42.1 ↑ ↑ ↑ ↑

BEAR BRAC-p AWR

Walker2d Medium-RRandomMediumMedium-E 19.259.140.17.3 -0.5-0.10.71.5 ↓ ↓ ↓ ↓ 6.50.61.53.1 ↑ ↑ ↑ ↑ 7.30.32.33.2 ↑ ↓ ↑ ↑ 77.576.9-0.3-0.2 20.66.48.50.0 ↓ ↓ ↑ ↑ 70.064.19.91.3 ↑ ↑ ↑ ↑ **78.018.677.53.2 ↑ ↑ ↑ ↑** 17.415.553.81.5 14.835.51.37.4 ↓ ↓ ↓ ↓ 17.152.51.62.0 ↓ ↑ ↑ ↑ **17.253.32.41.5 ↑ ↓ ↑ ↑**

BCQ CQL MOPO

Walker2d Medium-EMedium-RRandomMedium 57.515.053.14.9 32.532.86.91.8 ↓ ↓ ↓ ↓ 55.214.950.94.5 ↑ ↑ ↑ ↑ **57.215.152.34.8 ↑ ↑ ↑ ↑** 111.026.779.27.0 42.949.54.61.7 ↓ ↓ ↓ ↓ 63.580.00.83.2 ↓ ↑ ↑ ↑ 93.381.72.03.4 ↑ ↑ ↑ ↑ 44.639.017.813.6 -0.25.35.17.0 ↓ ↓ ↓ ↓ -0.15.53.15.7 ↑ ↓ ↓ ↑ 17.214.211.0-0.1 ↑ ↑ ↑ ↓

source D[′], we show that applying the same training paradigm on the reward augmented data exhibits
a positive transfer ability in Figure 2 (middle-bottom, DARA). In Figure 2 (right), we show that our
DARA produces low Q-values on the obstructive state-action pairs (in left) compared to the vanilla
MPO, which thus prevents the Q-value weighted-regression on these unproductive state-action pairs.

More generally, we illustrate how DARA can handle the dynamics adaptation from the reward modification view. In Figure 3 (right), the learned reward modification −∆r (dashed blue line) clearly
produces a penalty (red slices) on these state-action pairs (in source) that produce infeasible nextstate transitions in the target MDP. If we directly apply prior offline RL methods, these transitions
that are beyond reach in target and yet are high valued, would yield a negative transfer. Thus, we can
think of DARA as finding out these transitions that exhibit dynamics shifts and enabling dynamics
adaptation with reward modifications, e.g., penalizing transitions covered by red slices (−∆r < 0).

6.2 CAN DARA ENABLE AN ADAPTIVE POLICY WITH REDUCED OFFLINE DATA IN TARGET?

To characterize the offline dynamics shift, we consider the Hopper, Walker2d and Halfcheetah from
the Gym-MuJoCo environment, using offline samples from D4RL as our target offline dataset. For
the source dataset, we change the body mass of agents or add joint noise to the motion, and, similar
to D4RL, collect the Random, Medium, Medium-R and Medium-E offline datasets for the three environments. Based on various offline RL algorithms (BEAR, BRAC-p, BCQ, CQL, AWR, MOPO),
we perform the following comparisons: 1) employing the 100% of D4RL data (10T), 2) employing
only 10% of the D4RL data (1T), 3) employing 10% of the D4RL data and 100% of our collected
source offline data (1T+10S w/o Aug.), and 4) employing 10% of the D4RL data and 100% of our
collected source offline data along with our reward augmentation (1T+10S DARA). Due to page
limit, here we focus on the dynamics shift concerning the body mass on Walker2d and Hopper. We
refer the reader to appendix for more experimental details, tasks, and more baselines (BC, COMBO).

As shown in Table 1, in most of the tasks, the performance degrades substantially when we decrease
the amount of target offline data, i.e., 10T → _1T. Training with additional ten times source offline_
data (1T+10S w/o Aug.) also does not bring substantial improvement (compensating for the reduced
data in target), which even degrades the performance in some tasks. We believe that such degradation
(compared to 10T) is caused by the lack of target offline data as well as the dynamics shift (induced
by the source data). Incorporating our reward augmentation, we observe that compared to 1T and
_1T+10S w/o Aug. that both use 10% of the target offline data, our 1T+10S DARA significantly_
improves the performance across a majority of tasks. Moreover, DARA can achieve comparable or
better performance compared to baseline 10T that training with ten times as much target offline data.


-----

Table 2: Normalized scores in (target) D4RL tasks, where ”Tune” denotes baseline ”fine-tune”. We
observe that with same amount (10%) of target offline data, DARA greatly outperforms baselines.

Body Mass Shift Tune DARA Tune DARA Tune DARA Tune DARA Tune DARA _πpT[ˆ]_ _Tπˆ_ _p_

BEAR BRAC-p BCQ CQL MOPO MABE

Random 0.8 8.4 ↑ 6.0 11.0 ↑ 8.8 9.7 ↑ **31.6** 10.4 ↓ 0.7 2.1 ↑ 10.6 9.0

Hopper MediumMedium-R 0.80.7 **34.11.6 ↑ ↑** 22.714.7 32.930.8 ↑ ↑ 31.727.5 38.432.8 ↑ ↑ 44.51.3 **59.33.7 ↑ ↑** 0.70.6 10.78.4 ↑ ↑ 48.817.1 23.120.4

Medium-E 0.9 1.2 ↑ 19.2 34.7 ↑ 85.9 84.2 ↓ 47.6 **99.7 ↑** 2.2 5.8 ↑ 28.1 38.9

BEAR BRAC-p BCQ CQL MOPO MABE

Random **6.6** 3.2 ↓ 3.9 3.2 ↓ 4.7 4.8 ↑ 1.1 3.4 ↑ 0.1 -0.1 ↓ 6.0 -0.2
Medium 0.3 0.3 ↓ 76.0 78.0 ↑ 28.4 52.3 ↑ 72.3 **81.7 ↑** -0.2 11.0 ↑ 30.1 56.7
Medium-R 1.2 7.3 10.0 **18.6** 10.4 15.1 1.8 2.0 0.0 14.2 13.3 12.5

Walker2d _↑_ _↑_ _↑_ _↑_ _↑_

Medium-E 2.4 2.3 ↓ 74.5 77.5 ↑ 22.7 57.2 ↑ 68.6 **93.3 ↑** 7.3 17.2 ↑ 43.7 82.7


6.3 CAN DARA PERFORM BETTER THAN CROSS-DOMAIN BASELINES?

In Section 6.2, 1T+10S w/o Aug. does not explicitly learn policy for the target dynamics, thus one
proposal (1T+10S fine-tune) for adapting the target dynamics is fine-tuning the model that learned
with source offline data, using the (reduced) target offline data. Moreover, we also compare DARA
with the recently proposed MABE (Cang et al., 2021), which is suitable well for our cross-dynamics
setting by introducing behavioral priors πp in the model-based offline setting. Thus, we implement
two baselines, 1) 1T+10S MABE πpT[ˆ] and 2) 1T+10S MABE _Tπ[ˆ]_ _p, which denote 1) learning πp with_
target domain data and _T[ˆ] with source domain data, and 2) learning πp with source domain data_
and _T[ˆ] with target domain data, respectively. We show the results for the Walker (with body mass_
shift) in Table 2, and more experiments in Appendix A.3.5. Our results show that DARA achieves
significantly better performance than the na¨ıve fine-tune-based approaches in a majority of tasks
(67 ”↑” vs. 13 ”↓”, including results in appendix). On twelve out of the sixteen tasks (including
results in appendix), DARA-based methods outperform the MABE-based methods. We attribute
MABE’s failure to the difficulty of the reduced target offline data, which limits the generalization
of the learned πp or _T[ˆ] under such data. However, such reduced data (10% of target) is sufficient to_
modify rewards in the source offline data, which thus encourages better performance for our DARA.

**_For real-world tasks, we also test DARA in a new offline dataset_** Table 3: Average distance covon the quadruped robot (see appendix for details). Note that we ered in an episode in real robot.
can not access the privileged information (e.g., coordinate) in

(BCQ) w/o Aug. DARA

real robot, thus the target offline data (collected in real-world)
does not contain rewards. This means that prior fine-tune-based Medium 0.85 1.35 ↑

Medium-E 1.15 1.41

and MABE-based methods become unavailable. However, our _↑_
reward augmentation frees us from the requisite of rewards in Medium–R-E 1.27 1.55 ↑
target domain. We can freely perform offline training only using the augmented source offline data as
long as the learned ∆r is sufficient. For comparison, we also employ a baseline (w/o Aug.): directly
deploying the learned policy with source data into the (target) real-world. We present the results
(deployed in real with obstructive stairs) in Table 3 and videos in supplementary material. We can
observe that training with our reward augmentation, the performance can be substantially improved.
Due to page limit, we refer readers to Appendix A.3.6 for more experimental results and discussion.

7 CONCLUSION

In this paper, we formulate the dynamics shift in offline RL. Based on prior model-based and modelfree offline algorithms, we propose the dynamics-award reward augmentation (DARA) framework
that characterizes constraints over state-action-next-state distributions. Empirically we demonstrate
DARA can eliminate the dynamics shift and outperform baselines in simulated and real-world tasks.

In Appendix A.2, we characterize our dynamics-aware reward augmentation from the density regularization view, which shows that it is straightforward to derive the reward modification built on
prior regularized max-return objective e.g., AlgaeDICE (Nachum et al., 2019b). We list some related works in Table 4, where the majority of the existing work focuses on regularizing state-action
distribution, while dynamics shift receives relatively little attention. Thus, we hope to shift the focus
of the community towards analyzing how dynamics shift affects RL and how to eliminate the effect.


-----

REPRODUCIBILITY STATEMENT

Our experimental evaluation is conducted with publicly available D4RL (Fu et al., 2020) and NeoRL (Qin et al., 2021). In Appendix A.4 and A.5, we provide the environmental details and training
setup for our real-world sim2real tasks. In supplementary material, we upload our source code and
the collected offline dataset for the the quadruped robot.

ACKNOWLEDGMENTS

We thank Zifeng Zhuang, Yachen Kang and Qiangxing Tian for helpful feedback and discussions.
This work is supported by NSFC General Program (62176215).

REFERENCES

Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, R´emi Munos, Nicolas Heess, and Martin A. Riedmiller. Maximum a posteriori policy optimisation. In 6th International Conference on
_Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Confer-_
_ence Track Proceedings. OpenReview.net, 2018._

Philip J. Ball, Cong Lu, Jack Parker-Holder, and Stephen J. Roberts. Augmented world models
facilitate zero-shot dynamics generalization from a single offline environment. In Marina Meila
and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,
_ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning_
_Research, pp. 619–629. PMLR, 2021._

Catherine Cang, Aravind Rajeswaran, Pieter Abbeel, and Michael Laskin. Behavioral priors
and dynamics models: Improving performance and domain transfer in offline RL. _CoRR,_
[abs/2106.09119, 2021. URL https://arxiv.org/abs/2106.09119.](https://arxiv.org/abs/2106.09119)

Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jacob Varley, Alex
Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, and Sergey Levine. Actionable models:
Unsupervised offline reinforcement learning of robotic skills. In Marina Meila and Tong Zhang
(eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18_24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 1518–_
[1528. PMLR, 2021. URL http://proceedings.mlr.press/v139/chebotar21a.](http://proceedings.mlr.press/v139/chebotar21a.html)
[html.](http://proceedings.mlr.press/v139/chebotar21a.html)

Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International
_Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,_
volume 97 of Proceedings of Machine Learning Research, pp. 1042–1051. PMLR, 2019.

Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. Bail: Best-action
imitation learning for batch deep reinforcement learning. arXiv preprint arXiv:1910.12179, 2019.

Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games,
[robotics and machine learning. http://pybullet.org, 2016–2021.](http://pybullet.org)

Benjamin Eysenbach and Sergey Levine. If maxent RL is the answer, what is the question? CoRR,
[abs/1910.01913, 2019. URL http://arxiv.org/abs/1910.01913.](http://arxiv.org/abs/1910.01913)

Benjamin Eysenbach, Shreyas Chaudhari, Swapnil Asawa, Sergey Levine, and Ruslan Salakhutdinov. Off-dynamics reinforcement learning: Training for transfer with domain classifiers. In 9th
_International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May_
_3-7, 2021. OpenReview.net, 2021._

Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: datasets for deep
[data-driven reinforcement learning. CoRR, abs/2004.07219, 2020. URL https://arxiv.](https://arxiv.org/abs/2004.07219)
[org/abs/2004.07219.](https://arxiv.org/abs/2004.07219)


-----

Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
_International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, Cali-_
_fornia, USA, volume 97 of Proceedings of Machine Learning Research, pp. 2052–2062. PMLR,_
2019.

Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision
processes. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
_International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, Cali-_
_fornia, USA, volume 97 of Proceedings of Machine Learning Research, pp. 2160–2169. PMLR,_
2019.

Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expectedmax q-learning operator for simple yet effective offline and online RL. In Marina Meila and Tong
Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML
_2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research,_
pp. 3682–3691. PMLR, 2021.

Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic al[gorithms and applications. CoRR, abs/1812.05905, 2018. URL http://arxiv.org/abs/](http://arxiv.org/abs/1812.05905)
[1812.05905.](http://arxiv.org/abs/1812.05905)

Behzad Haghgoo, Allan Zhou, Archit Sharma, and Chelsea Finn. Discriminator augmented model[based reinforcement learning. CoRR, abs/2103.12999, 2021. URL https://arxiv.org/](https://arxiv.org/abs/2103.12999)
[abs/2103.12999.](https://arxiv.org/abs/2103.12999)

Atil Iscen, Ken Caluwaerts, Jie Tan, Tingnan Zhang, Erwin Coumans, Vikas Sindhwani, and
Vincent Vanhoucke. Policies modulating trajectory generators. In 2nd Annual Conference
_on Robot Learning, CoRL 2018, Z¨urich, Switzerland, 29-31 October 2018, Proceedings, vol-_
ume 87 of Proceedings of Machine Learning Research, pp. 916–926. PMLR, 2018. URL
[http://proceedings.mlr.press/v87/iscen18a.html.](http://proceedings.mlr.press/v87/iscen18a.html)

Nan Jiang and Jiawei Huang. Minimax value interval for off-policy evaluation and policy optimization. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual
_Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,_
_[2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/](https://proceedings.neurips.cc/paper/2020/hash/1cd138d0499a68f4bb72bee04bbec2d7-Abstract.html)_
[1cd138d0499a68f4bb72bee04bbec2d7-Abstract.html.](https://proceedings.neurips.cc/paper/2020/hash/1cd138d0499a68f4bb72bee04bbec2d7-Abstract.html)

Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Modelbased offline reinforcement learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing
_Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,_
_December 6-12, 2020, virtual, 2020._

Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning
with fisher divergence critic regularization. In Marina Meila and Tong Zhang (eds.), Proceedings
_of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual_
_Event, volume 139 of Proceedings of Machine Learning Research, pp. 5774–5783. PMLR, 2021._

Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. In Hanna M. Wallach, Hugo Larochelle, Alina
Beygelzimer, Florence d’Alch´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neu_ral Information Processing Systems 32: Annual Conference on Neural Information Processing_
_Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 11761–11771,_
2019.

Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: An_nual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,_
_2020, virtual, 2020a._


-----

Saurabh Kumar, Aviral Kumar, Sergey Levine, and Chelsea Finn. One solution is not
all you need: Few-shot extrapolation via structured maxent RL. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
_Advances in Neural Information Processing Systems 33:_ _Annual Conference on Neu-_
_ral Information Processing Systems 2020,_ _NeurIPS 2020,_ _December 6-12,_ _2020,_ _vir-_
_tual,_ 2020b. URL [https://proceedings.neurips.cc/paper/2020/hash/](https://proceedings.neurips.cc/paper/2020/hash/5d151d1059a6281335a10732fc49620e-Abstract.html)
[5d151d1059a6281335a10732fc49620e-Abstract.html.](https://proceedings.neurips.cc/paper/2020/hash/5d151d1059a6281335a10732fc49620e-Abstract.html)

Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforce_ment learning, pp. 45–73. Springer, 2012._

Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning quadrupedal locomotion over challenging terrain. _Science Robotics, 5(47), 2020._ doi:
[10.1126/scirobotics.abc5986. URL https://robotics.sciencemag.org/content/](https://robotics.sciencemag.org/content/5/47/eabc5986)
[5/47/eabc5986.](https://robotics.sciencemag.org/content/5/47/eabc5986)

Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.
_[CoRR, abs/1805.00909, 2018. URL http://arxiv.org/abs/1805.00909.](http://arxiv.org/abs/1805.00909)_

Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning:
Tutorial, review, and perspectives on open problems. _CoRR, abs/2005.01643, 2020._ URL
[https://arxiv.org/abs/2005.01643.](https://arxiv.org/abs/2005.01643)

Jinxin Liu, Hao Shen, Donglin Wang, Yachen Kang, and Qiangxing Tian. Unsupervised domain
adaptation with dynamics-aware rewards in reinforcement learning. Advances in Neural Informa_tion Processing Systems, 34, 2021._

Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the
curse of horizon: Infinite-horizon off-policy estimation. pp. 5361–5371,
2018. URL [https://proceedings.neurips.cc/paper/2018/hash/](https://proceedings.neurips.cc/paper/2018/hash/dda04f9d634145a9c68d5dfe53b21272-Abstract.html)
[dda04f9d634145a9c68d5dfe53b21272-Abstract.html.](https://proceedings.neurips.cc/paper/2018/hash/dda04f9d634145a9c68d5dfe53b21272-Abstract.html)

Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. Offline metareinforcement learning with advantage weighting. In Marina Meila and Tong Zhang (eds.), Pro_ceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July_
_2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 7780–7791._
PMLR, 2021.

Ofir Nachum and Bo Dai. Reinforcement learning via fenchel-rockafellar duality. _CoRR,_
[abs/2001.01866, 2020. URL http://arxiv.org/abs/2001.01866.](http://arxiv.org/abs/2001.01866)

Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of
discounted stationary distribution corrections. arXiv preprint arXiv:1906.04733, 2019a.

Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice: Policy gradient from arbitrary experience. _CoRR, abs/1912.02074, 2019b._ URL
[http://arxiv.org/abs/1912.02074.](http://arxiv.org/abs/1912.02074)

Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
[learning with offline datasets. CoRR, abs/2006.09359, 2020. URL https://arxiv.org/](https://arxiv.org/abs/2006.09359)
[abs/2006.09359.](https://arxiv.org/abs/2006.09359)

Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. Knowl. Data Eng., 22
[(10):1345–1359, 2010. doi: 10.1109/TKDE.2009.191. URL https://doi.org/10.1109/](https://doi.org/10.1109/TKDE.2009.191)
[TKDE.2009.191.](https://doi.org/10.1109/TKDE.2009.191)

Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In 2018 IEEE International Conference on
_Robotics and Automation, ICRA 2018, Brisbane, Australia, May 21-25, 2018, pp. 1–8. IEEE,_
2018. doi: 10.1109/ICRA.2018.8460528. [URL https://doi.org/10.1109/ICRA.](https://doi.org/10.1109/ICRA.2018.8460528)
[2018.8460528.](https://doi.org/10.1109/ICRA.2018.8460528)


-----

Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. _CoRR, abs/1910.00177, 2019._ URL
[http://arxiv.org/abs/1910.00177.](http://arxiv.org/abs/1910.00177)

Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In Twenty-Fourth
_AAAI Conference on Artificial Intelligence, 2010._

Rongjun Qin, Songyi Gao, Xingyuan Zhang, Zhen Xu, Shengkai Huang, Zewen Li, Weinan Zhang,
and Yang Yu. Neorl: A near real-world benchmark for offline reinforcement learning. arXiv
_preprint arXiv:2102.00714, 2021._

Y. Sakakibara, K. Kan, Y. Hosoda, M. Hattori, and M. Fujie. Foot trajectory for a quadruped
walking machine. In EEE International Workshop on Intelligent Robots and Systems, Towards a
_New Frontier of Applications, pp. 315–322 vol.1, 1990. doi: 10.1109/IROS.1990.262407._

Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing
what worked: Behavioral modelling priors for offline reinforcement learning. arXiv preprint
_arXiv:2002.08396, 2020._

Richard S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Bruce W. Porter and Raymond J. Mooney (eds.),
_Machine Learning, Proceedings of the Seventh International Conference on Machine Learn-_
_ing, Austin, Texas, USA, June 21-23, 1990, pp. 216–224. Morgan Kaufmann, 1990._ doi:
10.1016/b978-1-55860-141-3.50030-4.

Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. 2017.

Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and q-function learning for offpolicy evaluation. In Proceedings of the 37th International Conference on Machine Learning,
_ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning_
_[Research, pp. 9659–9668. PMLR, 2020. URL http://proceedings.mlr.press/v119/](http://proceedings.mlr.press/v119/uehara20a.html)_
[uehara20a.html.](http://proceedings.mlr.press/v119/uehara20a.html)

Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, R´emi Munos, and Matthieu
Geist. Leverage the average: an analysis of KL regularization in reinforcement learning. In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neu_ral Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020._

[Xingxing Wang. Unitree robotics. https://www.unitree.com/products/a1, 2020.](https://www.unitree.com/products/a1)

Ziyu Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, Bobak
Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. arXiv preprint arXiv:2006.15134, 2020.

Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
_[CoRR, abs/1911.11361, 2019. URL http://arxiv.org/abs/1911.11361.](http://arxiv.org/abs/1911.11361)_

Haoran Xu, Xianyuan Zhan, and Xiangyu Zhu. Constraints penalized q-learning for safe offline
[reinforcement learning. CoRR, abs/2107.09003, 2021. URL https://arxiv.org/abs/](https://arxiv.org/abs/2107.09003)
[2107.09003.](https://arxiv.org/abs/2107.09003)

Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-policy evaluation
via the regularized lagrangian. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, MariaFlorina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems
_33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, De-_
_[cember 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/](https://proceedings.neurips.cc/paper/2020/hash/488e4104520c6aab692863cc1dba45af-Abstract.html)_
[2020/hash/488e4104520c6aab692863cc1dba45af-Abstract.html.](https://proceedings.neurips.cc/paper/2020/hash/488e4104520c6aab692863cc1dba45af-Abstract.html)


-----

Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y. Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. MOPO: model-based offline policy optimization. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances
_in Neural Information Processing Systems 33: Annual Conference on Neural Information Pro-_
_cessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020._

Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.
COMBO: conservative offline model-based policy optimization. CoRR, abs/2102.08363, 2021.
[URL https://arxiv.org/abs/2102.08363.](https://arxiv.org/abs/2102.08363)

Hongyin Zhang, Jilong Wang, Zhengqing Wu, Yinuo Wang, and Donglin Wang. Terrain-aware
risk-assessment-network-aided deep reinforcement learning for quadrupedal locomotion in tough
terrain. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
pp. 4538–4545. IEEE.

Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized offline estimation of stationary values. In 8th International Conference on Learning Representations,
_[ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https:](https://openreview.net/forum?id=HkxlcnVFwB)_
[//openreview.net/forum?id=HkxlcnVFwB.](https://openreview.net/forum?id=HkxlcnVFwB)


-----

A APPENDIX

A.1 DERIVATION

A.1.1 PROOF OF LEMMA 3

Let BM[π] _[V][ (][s][) =][ E][a][∼][π][(][a][|][s][)]_ _r(s, a) + γEs′∼T (s′|s,a) [V (s[′])]_ and r(s) = Ea∼π(a|s) [r(s, a)]. Then,
we have
 
_η ˆM_ _[′]_ [(][π][)][ −] _[η][M]_ [(][π][) =][ E][s]0[∼][ρ]0[(][s][)] _V ˆM_ _[′]_ [(][s][0][)][ −] _[V][M]_ [(][s][0][)]

_∞_

 

= _γ[t]Est∼P (st|π, ˆM_ _[′],t)[E][a]t[∼][π][(][a]t[|][s]t[)][ [][r][(][s][t][,][ a][t][)]][ −]_ [E][s]0[∼][ρ]0[(][s][)][ [][V][M] [(][s][0][)]]

_t=0_

X

_∞_

= _γ[t]Est∼P (st|π, ˆM_ _[′],t)_ [[][r][(][s][t][) +][ V][M] [(][s][t][)][ −] _[V][M]_ [(][s][t][)]][ −] [E][s]0[∼][ρ]0[(][s][)][ [][V][M] [(][s][0][)]]

_t=0_

X

_∞_

= _γ[t]Est∼P (st|π, ˆM_ _[′],t)_ [r(st) + γVM (st+1) − _VM_ (st)]

Xt=0 **st+1∼P (st+1|π,M[ˆ]** _[′],t+1)_

_∞_

= _γ[t]Est∼P (st|π, ˆM_ _[′],t)_ _r(st) + γVM_ (st+1) − _r(st) + γEa∼π(a|st),s′∼T (st,a) [VM_ (s[′])]

Xt=0 **st+1∼P (st+1|π,M[ˆ]** _[′],t+1)_    

_∞_

= _γ[t]Est∼P (st|π, ˆM_ _[′],t)_ _BM[π]ˆ_ _[′]_ _[V][M]_ [(][s][t][)][ −B]M[π] _[V][M]_ [(][s][t][)]

_t=0_

X  


= Es∼d[π]Mˆ _[′]_ [(][s][)] _BM[π]ˆ_ _[′]_ _[V][M]_ [(][s][)][ −B]M[π] _[V][M]_ [(][s][)] _._

 

A.1.2 MODEL-BASED FORMULATION

Here we provide detailed derivation of the lower bound in Equation 9 in the main text.

**Assumption 1 Assume a scale c and a function class F such that VM ∈** _cF._

Following MOPO (Yu et al., 2020), we set = _f :_ _f_ 1 . In Section Preliminaries, we
_F_ _{_ _∥_ _∥∞_ _≤_ _}_
have that the reward function is bounded: r(s, a) ∈ [−Rmax, Rmax]. Thus, we have ∥VM _∥∞_ _≤_
_∞_
_t=0_ _[γ][t][R][max][ =][ R]1[max]γ_ [and hence the scale][ c][ =][ R]1[max]γ [.]

_−_ _−_

As a direct corollary of Assumption 1 and Equation 5, we haveP
Es∼d[π]Mˆ _[′]_ [(][s][)] _BM[π]ˆ_ _[′]_ _[V][M]_ [(][s][)][ −B]M[π] _[V][M]_ [(][s][)] _≤_ _γc · Es,a∼d[π]Mˆ_ _[′]_ [(][s][)][π][(][a][|][s][)] _dF_ (T[ˆ][′](s[′]|s, a), T (s[′]|s, a)) _._

  h i(10)

Further, we find
_dF_ (T[ˆ][′](s[′]|s, a), T (s[′]|s, a)) ≤ _dF_ (T[ˆ][′](s[′]|s, a), _T[ˆ](s[′]|s, a)) + dF_ (T[ˆ](s[′]|s, a), T (s[′]|s, a)) (11)


For the first term dF (T[ˆ][′](s[′]|s, a), _T[ˆ](s[′]|s, a)) in Equation 12, through Pinsker’s inequality, we have_

1

_d_ (T[ˆ] (s[′] **s, a),** _T[ˆ](s[′]_ **s, a)) = DTV(T[ˆ]** (s[′] **s, a),** _T[ˆ](s[′]_ **s, a))** _T_ (s[′] **s, a),** _T[ˆ](s[′]_ **s, a))**
_F_ _[′]_ _|_ _|_ _[′]_ _|_ _|_ _≤_ 2 _[D][KL][(][ ˆ][′]_ _|_ _|_

r

(12)
To keep consistent with the DARA-based method-free offline methods, we introduce scale η and
bias δ to eliminate the square root in Equation 12. To be specific, we assume[2] scale η and bias δ


12 _[D][KL][(][ ˆ]T_ _[′],_ _T[ˆ]) ≤_ _ηDKL(T[ˆ][′],_ _T[ˆ]) + δ. Thus, we obtain_


such that


_d_ (T[ˆ] (s[′] **s, a),** _T[ˆ](s[′]_ **s, a)) = DTV(T[ˆ]** (s[′] **s, a),** _T[ˆ](s[′]_ **s, a))** _ηDKL(T[ˆ]_ (s[′] **s, a),** _T[ˆ](s[′]_ **s, a)) + δ**
_F_ _[′]_ _|_ _|_ _[′]_ _|_ _|_ _≤_ _[′]_ _|_ _|_
(13)


_TTˆˆ[′] ((ss[′][′]|ss,,aa))_ [for each][ (][s][,][ a][,][ s][′][)][, which thus makes]

_|_


2In implementation, we clip the maximum deviation of log

_DKL(T[ˆ][′](s[′]|s, a),_ _T[ˆ](s[′]|s, a)) bounded._


-----

For the second term d (T[ˆ](s[′] **s, a), T** (s[′] **s, a)) in Equation 11, we assume that we have access to an**
_F_ _|_ _|_
oracle uncertainty qualification module that provides an upper bound on the error of the estimated
empirical MDP _M[ˆ] :=_ _,_ _, r,_ _T[ˆ], ρ0, γ_ .
_{S_ _A_ _}_

**Assumption 2 Let F be the function class in Assumption 1. We say u : S ×A →** R is an admissible
_error estimator for_ _T[ˆ] if d_ (T[ˆ](s[′] **s, a), T** (s[′] **s, a))** _u(s, a) for all (s, a)._
_F_ _|_ _|_ _≤_

Thus, we have

Es,a∼d[π]Mˆ _[′]_ [(][s][)][π][(][a][|][s][)] _dF_ (T[ˆ](s[′]|s, a), T (s[′]|s, a)) _≤_ Es,a∼d[π]Mˆ _[′]_ [(][s][)][π][(][a][|][s][)][ [][u][(][s][,][ a][)]] (14)

h i


Bring Inequations 10, 11, 13, and 14 into Lemma 3, we thus have


_Tˆ[′](s[′]|s, a)_ _γcu(s, a)_ _γcδ_

_Tˆ(s[′]|s, a)_ _−_ _−_


_ηM_ (π) ≥ Es,a,s′∼dπMˆ _[′]_ [(][s][)][π][(][a][|][s][)][ ˆ]T _[′](s[′]|s,a)_


(15)


_r(s, a) −_ _ηγc log_


A.1.3 LEARNING CLASSIFIERS

Applying Bayes’ rule, we have

_Tˆ[′](s[′]|a, s) := p(s[′]|s, a, source) =_ _[p][(][source]p(source[|][s][,][ a]|s[,],[ s] a[′])[)]p[p]([(]s[s],[,] a[ a])[,][ s][′][)]_

_Tˆ(s[′]_ **a, s) := p(s[′]** **s, a, target) =** _[p][(][target][|][s][,][ a][,][ s][′][)][p][(][s][,][ a][,][ s][′][)]_ _._
_|_ _|_ _p(target|s, a)p(s, a)_


Then we parameterize p(·|s, a, s[′]) and p(·|s, a) with the two classifiers qsas and qsa respectively. Using the standard cross-entropy loss, we learn qsas and qsa with the following optimization objective:

max E(s,a,s′)∼D′ [log qsas(source|s, a, s[′])] + E(s,a,s′)∼D [log qsas(target|s, a, s[′])],

max E(s,a)∼D′ [log qsa(source|s, a)] + E(s,a)∼D [log qsa(target|s, a)] .

With the trained qsas and qsa, we have


_Tˆ[′](s[′]|s, a)_ = log _[q][sas][(][source][|][s][,][ a][,][ s][′][)]_ (16)

_Tˆ(s[′]|s, a)_ _qsas(target|s, a, s[′])_ _[−]_ [log][ q]q[sa]sa[(]([source]target|[|]s[s],[,] a[ a])[)] _[.]_


log


In our implementation, we also clip the above reward modification between −10 and 10.

A.2 REGULARIZATION VIEW OF DYNAMICS-AWARE REWARD AUGMENTATION

Here we shortly characterize our dynamics-aware reward augmentation from the density regularization. Note the standard max-return objective ηM (π) in RL can be written exclusively in terms of the
on-policy distribution d[π]M [(][s][)][π][(][a][|][s][)][. To introduce an off-policy distribution][ d][D][(][s][)][π][b][(][a][|][s][)][ in the ob-]

jective, prior works often incorporate a regularization (penalty): D(d[π]M [(][s][)][π][(][a][|][s][)][∥][d][D][(][s][)][π][b][(][a][|][s][))][, as]

in Equations 2 and 3. However, facing dynamics shift, such regularization should take into account
the transition dynamics, which is penalizing D(d[π]M [(][s][)][π][(][a][|][s][)][T] [(][s][′][|][s][,][ a][)][∥][d][D][′] [(][s][)][π][b][′] [(][a][|][s][)][ ˆ]T _[′](s[′]|s, a))._

From this view, it is also straightforward to derive the reward modification built on prior regularized
off-policy max-return objective e.g., the off-policy approach AlgaeDICE (Nachum et al., 2019b).

In Table 4, we provide some related works with respect to the (state-action pair) dD(s)πb(a|s)
regularization and the (state-action-next-state pair) dD′ (s)πb′ (a|s)T[ˆ][′](s[′]|s, a) regularization. We
can find that the majority of the existing work focuses on regularizing state-action distribution, while
dynamics shift receives relatively little attention. Thus, we hope to shift the focus of the community
towards analyzing how the dynamics shift affects RL and how to eliminate the effect.


-----

Table 4: Some related works with explicit (state-action p(s, a) or state-action-next-state p(s, a, s[′]))
**regularization. More papers with respect to unsupervised RL, inverse RL (imitation learning), meta**
RL, multi-agent RL, and hierarchical RL are not included.

**reg. with dD(s)πb(a|s)** **reg. with dD[′]** (s)πb[′] (a|s)T[ˆ][′](s[′]|s, a)

_Online:_
see summarization in Geist et al. Eysenbach et al. (2021) (DARC)
(2019) and Vieillard et al. (2020). Liu et al. (2021) (DARS);

Haghgoo et al. (2021)


_Offline (off-policy evaluation):_
Fujimoto et al. (2019) (BCQ); Kumar et al. (2019) (BEAR);

Wu et al. (2019) (BRAC-p); Abdolmaleki et al. (2018) (MPO);

Peng et al. (2019) (AWR); Nair et al. (2020) (AWAC);

Wang et al. (2020) (CRR); Siegel et al. (2020);

Chen et al. (2019); (BAIL) Kumar et al. (2020a) (CQL);

Xu et al. (2021) (CPQ); Kostrikov et al. (2021) (Fisher-BRC);

Liu et al. (2018); Nachum et al. (2019a) (DualDICE);

Nachum et al. (2019b) (AlgaeDICE); Zhang et al. (2020) (GenDICE);

Yang et al. (2020); Nachum & Dai (2020);

Jiang & Huang (2020); Uehara et al. (2020);

Yu et al. (2020) (MOPO); Kidambi et al. (2020) (MOReL);

Yu et al. (2021) (COMBO); Cang et al. (2021) (MABE);

A.3 MORE EXPERIMENTS


A.3.1 TRAINING WITH {D[′] _∪_ _D}_

As we show in Figure 1 in Section Introduction, the performance of prior offline RL methods deteriorates dramatically as the amount of (target) offline data D decreases. In Figure 4, we show that
directly training with the mixed dataset {D[′] _∪_ _D} will not compensate for the deteriorated perfor-_
mance caused by the reduced target offline data, and training with such additional source offline data
can even lead the performance degradation in some tasks.






Medium-Replay Medium-Replay Medium-Expert Medium-Expert

40

100 40

20

20

10 50 20

Normalized score Normalized score Normalized score Normalized score

0 0 0 0

50% 20% 10% 5% 50% 20% 10% 5% 50% 20% 10% 5% 50% 20% 10% 5%

Amount of target data used for training Amount of target data used for training Amount of target data used for training Amount of target data used for training

CQL with Source CQL MOPO with Source MOPO CQL with Source CQL MOPO with Source MOPO


Figure 4: Final performance on the D4RL (Walker2d) task: The orange bars denote the final performance with different amount (50%D, 20%D, 10%D, 5%D) of target offline data; The blue bars
denote the final performance of mixing 100% of source offline data D[′] and different amount of target
data x%D (x ∈ [50, 20, 10, 5]), i.e., training with {100%D[′] _∪_ _x%D}; The red lines denote the final_
performance of training with 100% of target offline data D. We can observe that 1) the performance
deteriorates dramatically as the amount of (target) offline data decreases (100%D (red line) →
50%D (orange bar) → 20%D (orange bar) → 10%D (orange bar) → 5%D (orange bar)), 2) after
training with the additional 100% of source offline data, {100%D[′] _∪_ _x%D}, the final performance_
is improved in some tasks, but most of the improvement is a pittance compared to the original performance degradation (compared to that training with the 100% of target offline data, i.e., the red
lines), and 3) what is worse is that adding source offline data D[′] even leads performance degradation
in some tasks, e.g., CQL with 50%D and 20%D in Medium-Random.


-----

A.3.2 COMPARISON BETWEEN LEARNING CLASSIFIERS AND LEARNING DYNAMICS (FOR
THE REWARD MODIFICATION)

Table 5: Normalized scores for the Hopper tasks with the body mass (dynamics) shift. Rat. and Cla.
denote estimating the reward modification with the estimated-dynamics ratio and learned classifiers
(Appendix A.1.3), respectively.


Body Mass Shift BEAR BRAC-p AWR BCQ CQL MOPO

Rat. Cla. Cla. Cla. Rat. Cla. Rat. Cla. Rat. Cla. Rat. Cla.

Random 9.9 _>_ 8.4 11.2 _>_ 11.0 3.7 _<_ 4.5 8.5 _<_ 9.7 11.8 _>_ 10.4 1.8 _<_ 2.1
Medium 0.8 _<_ 1.6 31.7 _<_ 32.9 18.0 _<_ 28.9 33.2 _<_ 38.4 45.9 _<_ 59.3 3.1 _<_ 10.7

Hopper Medium-R 28.4 _<_ 34.1 36.5 _>_ 30.8 2.5 _<_ 4.2 33.9 _>_ 32.8 2.0 _<_ 3.7 3.8 _<_ 8.4

Medium-E 0.8 _<_ 1.2 50.9 _>_ 34.7 45.8 _<_ 26.6 68.4 _<_ 84.2 107.3 _>_ 99.7 5.7 _<_ 5.8


In Table 5, we show the comparison between learning classifiers and learning dynamics (for our
reward modification) in the Hopper tasks. We can observe that the two schemes for estimating the
reward modification have similar performance. Thus, for simplicity and following Eysenbach et al.
(2021), we adopt the classifiers to modify rewards in the source offline data in our experiments.


A.3.3 MORE EXAMPLES WITH RESPECT TO THE REWARD AUGMENTATION

|6|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
|6 0.2|||||
|0.2|||||

|6|Col2|Col3|Col4|
|---|---|---|---|
|6 0.2||||
|0.2||||

|6|Col2|Col3|Col4|
|---|---|---|---|
|6 0.2||||
|0.2||||


Time step


Time step


Time step


Figure 5: We can observe that our reward augmentation 1) encourages (−∆r > 0, i.e., the green
slice parts) these transitions (−0.26 ≤ next-state[11] ≤ 0.26) that have the same dynamics with the
target environment, and 2) discourages (−∆r < 0, i.e., the red slice parts) these transitions that have
different (unreachable) dynamics (next-state[11] ≤−0.26 or next-state[11] ≥ 0.26) in the target.

In Figure 5, we provide more examples with respect to the reward augmentation in the Ant task in
Figure 3 (left).


A.3.4 COMPARISON BETWEEN 10T, 1T, 1T+10S w/o Aug., AND 1T+10S DARA

Based on various offline RL algorithms (BEAR (Kumar et al., 2019), BRAC-p (Wu et al., 2019),
BCQ (Fujimoto et al., 2019), CQL (Kumar et al., 2020a), AWR (Peng et al., 2019), MOPO (Yu
et al., 2020), BC (behavior cloning), COMBO (Yu et al., 2021)), we provide the additional results in
Tables 6, 7, 8, 9, and 10.


Table 6: Normalized scores for the Hopper tasks with the body mass (dynamics) shift. (The comparison results for BEAR, BRAC-p, AWR, CQL, and MOPO are provided in the main text.)

1T+10S 1T+10S 1T+10S 1T+10S 1T+10S 1T+10S
Body Mass Shift 10T 1T w/o Aug. DARA 10T 1T w/o Aug. DARA 10T 1T w/o Aug. DARA



BC COMBO

Hopper Medium-EMedium-RMediumRandom 111.911.829.09.8 21.527.97.89.8 ↓ ↓ ↓ ↑ 20.817.67.76.9 ↓ ↓ ↓ ↓ 35.711.625.010.1 ↑ ↑ ↑ ↑ 111.173.194.917.9 13.11.80.70.8 ↓ ↓ ↓ ↓ 14.911.033.75.4 ↑ ↑ ↓ ↑ 108.127.945.74.6 ↑ ↑ ↑ ↓


-----

Table 7: Normalized scores for the Hopper tasks with the joint noise (dynamics) shift.

1T+10S 1T+10S 1T+10S 1T+10S 1T+10S 1T+10S
Joint Noise Shift 10T 1T w/o Aug. DARA 10T 1T w/o Aug. DARA 10T 1T w/o Aug. DARA


BEAR BRAC-p AWR

Hopper Medium-RRandomMediumMedium-E 33.711.452.196.3 2.70.60.80.8 ↓ ↓ ↓ ↓ 3.62.00.87.4 ↑ ↑ ↑ ↓ 9.92.01.44.2 ↑ ↓ ↑ ↓ 11.032.70.61.9 13.419.810.826.6 ↑ ↑ ↓ ↓ 89.910.027.657.6 ↑ ↓ ↑ ↑ 101.410.837.687.8 ↑ ↑ ↑ ↑ 10.235.928.427.1 10.130.312.425.5 ↓ ↓ ↓ ↓ 38.827.03.66.7 ↓ ↓ ↑ ↑ 41.327.04.07.2 ↑ ↑ ↑ ↓

BCQ CQL MOPO

Hopper Medium-EMedium-RRandomMedium 110.933.110.654.5 44.613.010.545.8 ↓ ↓ ↓ ↓ 23.849.07.096 ↓ ↑ ↑ ↑ 32.054.41099.6 ↑ ↑ ↑ ↑ 98.748.658.010.8 13.650.746.210.4 ↓ ↓ ↓ ↓ 73.458.010.42.6 ↓ ↑ ↑ ↓ 108.958.010.83.6 ↑ ↓ ↑ ↑ 23.767.528.011.7 0.82.71.51 ↓ ↓ ↓ ↓ 6.12.39.21.3 ↑ ↑ ↑ ↓ 17.36.47.52.9 ↑ ↑ ↑ ↑

BC COMBO

Hopper Medium-EMedium-RRandomMedium 111.911.829.09.8 21.527.97.89.8 ↓ ↓ ↑ ↓ 53.529.08.57.5 ↑ ↓ ↑ ↑ 77.911.329.09.1 ↑ ↑ ↑ ↑ 111.173.194.917.9 13.11.80.70.8 ↓ ↓ ↓ ↓ 34.04.00.71.8 ↓ ↓ ↑ ↑ 45.99.69.64.9 ↑ ↑ ↑ ↑

Table 8: Normalized scores for the Walker2d tasks with the body mass (dynamics) shift. (The
comparison results for BEAR, BRAC-p, AWR, CQL, and MOPO are provided in the main text.)

1T+10S 1T+10S 1T+10S 1T+10S 1T+10S 1T+10S
Body Mass Shift 10T 1T w/o Aug. DARA 10T 1T w/o Aug. DARA 10T 1T w/o Aug. DARA

BC COMBO

Random 1.6 0.1 ↓ 1.7 ↑ 2.7 ↑ 7.0 1.8 ↓ 2.0 ↑ 3.5 ↑

Medium 6.6 5.5 ↓ 3.8 ↓ 6.6 ↑ 75.5 -1.0 ↓ 23.9 ↑ 36.6 ↑

Walker2d Medium-EMedium-R 11.36.4 3.16.6 ↓ ↓ 6.08.1 ↑ ↑ 11.06.2 ↑ ↑ 96.156.0 -0.90.1 ↓ ↓ 11.4-0.1 ↑ ↑ 22.6-0.1 ↑ ↑


Table 9: Normalized scores for the Walker2d tasks with the joint noise (dynamics) shift.

1T+10S 1T+10S 1T+10S 1T+10S 1T+10S 1T+10S
Joint Noise Shift 10T 1T w/o Aug. DARA 10T 1T w/o Aug. DARA 10T 1T w/o Aug. DARA


BEAR BRAC-p AWR

Walker2d MediumMedium-RMedium-ERandom 59.119.240.17.3 -0.4-0.22.20.4 ↓ ↓ ↓ ↓ 0.60.60.84 ↓ ↑ ↑ ↑ 10.42.60.10.6 ↑ ↓ ↓ ↑ 77.576.9-0.3-0.2 28.821.86.32.8 ↓ ↓ ↑ ↑ 32.155.262.33.3 ↑ ↑ ↑ ↑ 34.872.974.38.8 ↑ ↑ ↑ ↑ 15.517.453.81.5 12.240.40.96 ↓ ↓ ↓ ↓ 17.21.41.553 ↓ ↑ ↑ ↑ 17.253.61.52.1 ↓ ↑ ↓ ↑



BCQ CQL MOPO

Walker2d Medium-RMedium-EMediumRandom 57.553.14.915 44.55.73.743 ↓ ↓ ↓ ↓ 40.644.99.83.4 ↓ ↑ ↑ ↓ 14.657.252.75.2 ↑ ↑ ↑ ↑ 26.779.21117 43.946.81.80.5 ↓ ↓ ↓ ↓ 109.973.21.42.7 ↓ ↑ ↑ ↑ 116.581.21.86.4 ↑ ↑ ↑ ↑ 17.844.613.639 -0.35.80.82.9 ↓ ↓ ↓ ↓ 15.2-0.27.89.3 ↑ ↑ ↑ ↑ 12.216.426.3-0.2 ↑ ↑ ↑ ↓


BC COMBO

Walker2d RandomMediumMedium-RMedium-E 11.31.66.66.4 0.15.56.63.1 ↓ ↓ ↓ ↓ 0.96.44.66.2 ↑ ↑ ↓ ↑ 10.41.66.56.4 ↑ ↑ ↑ ↑ 75.556.096.17.0 -1.0-0.90.11.8 ↓ ↓ ↓ ↓ 0.10.45.60.8 ↓ ↑ ↑ ↑ -0.11.50.77.4 ↓ ↑ ↑ ↑


-----

Table 10: Normalized scores for the Halfcheetah tasks with the joint noise (dynamics) shift.

1T+10S 1T+10S 1T+10S 1T+10S 1T+10S 1T+10S
Joint Noise Shift 10T 1T w/o Aug. DARA 10T 1T w/o Aug. DARA 10T 1T w/o Aug. DARA

BEAR BRAC-p AWR

Halfcheetah Medium-RMedium-EMediumRandom 38.653.441.725.1 17.8-1.2-0.29.3 ↓ ↓ ↓ ↓ 25.0-0.61.00.8 ↑ ↑ ↑ ↓ 25.1-0.5-1.41.5 ↑ ↑ ↓ ↑ 45.444.243.824.1 43.010.02.56.9 ↓ ↓ ↓ ↓ 52.425.0-2.30.9 ↓ ↑ ↑ ↓ 45.345.353.026.7 ↑ ↑ ↑ ↑ 40.337.452.72.5 38.232.22.72.6 ↑ ↓ ↑ ↓ 48.780.62.33.1 ↓ ↑ ↑ ↑ 37.479.248.92.3 ↓ ↓ ↓ ↑

BCQ CQL MOPO

Halfcheetah MediumMedium-RMedium-ERandom 40.738.264.72.2 37.637.32.31.1 ↑ ↓ ↓ ↓ 55.340.039.42.2 ↑ ↑ ↑ ↓ 48.641.376.92.3 ↑ ↑ ↑ ↑ 44.446.262.435.4 35.4-3.3-2.30.6 ↓ ↓ ↓ ↓ 40.7-2.47.72.0 ↑ ↑ ↑ ↓ 52.610.41.71.9 ↓ ↑ ↑ ↓ 42.335.453.163.3 -0.13.22.34.2 ↓ ↓ ↓ ↓ 3.51.22.61.5 ↓ ↓ ↑ ↓ 1.15.34.27.2 ↓ ↑ ↑ ↑

BC COMBO

Halfcheetah Medium-EMedium-RMediumRandom 35.838.436.12.1 36.336.536.52.0 ↑ ↓ ↓ ↑ 49.024.649.42.2 ↑ ↓ ↑ ↑ 49.315.749.82.2 ↑ ↑ ↓ ↑ 90.055.154.238.8 15.724.0-2.64.4 ↓ ↓ ↓ ↓ 18.714.9-2.46.5 ↑ ↓ ↓ ↑ 11.115.920.34.8 ↑ ↑ ↑ ↑


A.3.5 COMPARISON WITH THE CROSS-DOMAIN BASED BASELINES

In Tables 11 and 12, we provide the comparison between our DARA-based methods, fine-tune
based methods, and MABE-based methods in Hopper and Walker2d tasks, over the dynamics shift
concerning the joint noise of motion. We can observe that in a majority of tasks, our DARAbased methods outperforms the fine-tune-based method (67 ”↑” vs. 13 ”↓”, including the results in
the main text). Moreover, our DARA can achieve comparable or better performance compared to
MABE-based baselines on eleven out of sixteen tasks (including the results in the main text).

Table 11: Normalized scores in the (target) D4RL Hopper tasks with the joint noise shift., where
”Tune” denotes baseline ”fine-tune”.

Joint Noise Shift Tune DARA Tune DARA Tune DARA Tune DARA Tune DARA _πpT[ˆ]_ _Tπˆ_ _p_

BEAR BRAC-p BCQ CQL MOPO MABE

Random 0.8 4.2 ↑ 6.4 10.8 ↑ 8.1 9.6 ↑ **32.2** 10.8 ↓ 0.6 2.9 ↑ 10.8 8.1

Hopper Medium-RMedium 1.90.7 2.09.9 ↑ ↑ 32.444.9 **101.437.6 ↓ ↑** 29.647.7 32.054.4 ↑ ↑ 52.51.3 58.03.6 ↑ ↑ 0.81.8 17.36.4 ↑ ↑ 21.563.5 57.735.4

Medium-E 0.8 1.4 ↑ 98.2 87.8 ↓ 90.5 **109.0 ↑** 107.3 108.9 ↑ 4.9 7.5 ↑ 15.5 104.8


Table 12: Normalized scores in the (target) D4RL Walker2d tasks with the joint noise shift., where
”Tune” denotes baseline ”fine-tune”.

Joint Noise Shift Tune DARA Tune DARA Tune DARA Tune DARA Tune DARA _πpT[ˆ]_ _Tπˆ_ _p_

BEAR BRAC-p BCQ CQL MOPO MABE

Random 2.7 2.6 ↓ 1.4 **8.8 ↑** 3.0 5.2 ↑ 6.7 6.4 ↓ -0.4 -0.2 ↑ 5.0 -0.2
Medium 0.5 0.1 ↓ 55.8 72.9 ↑ 45.1 52.7 ↑ 76.6 **81.2 ↑** 7.0 12.2 ↑ 49.4 48.7
Medium-R 3.2 10.4 12.2 **34.8** 13.5 14.6 -0.4 1.8 1.9 16.4 4.5 1.6

Walker2d _↑_ _↑_ _↑_ _↑_ _↑_

Medium-E -0.4 0.6 ↑ 71.7 74.3 ↑ 44.8 57.2 ↑ 104 **116.5 ↑** 11.3 26.3 ↑ 84.7 82.6


A.3.6 ADDITIONAL RESULTS ON THE QUADRUPED ROBOT

In this offline sim2real setting, we collect the source offline data in the simulator (10[6] or 2 ∗ 10[6]
steps) and target offline data in the real world (3 ∗ 10[4] steps). See Appendix A.4 for details. For
testing, we directly deploy the learned policy in the real (flat or obstructive) environment and adopt
the average distance covered in an episode (300 steps) as our evaluation metrics.


-----

Figure 6: Illustration of the real environment (for testing): (left) the flat and static environment,
_(right) the obstructive and dynamic environment._

Table 13: Average distance (m) covered in an episode (300 steps) in flat and static (real) environment.

Sim2real (Flat and Static) w/o Aug. DARA w/o Aug. DARA w/o Aug. DARA

BCQ CQL MOPO

Quadruped Robot Medium-RMedium 1.560.00 1.640.00 ↑ 1.80— **1.82—** _↑_ 0.00— 0.00—

Medium-E 2.16 **2.47 ↑** 2.03 2.02 ↓ 0.00 0.00
Medium-R-E 1.69 **2.28 ↑** 0.00 0.00 — —

Average performance improvement 13.6% 0.2% 0.0%


**(Flat and static environment) We first deploy our learned policy in the flat and static environment.**
The results (distance covered in an episode) are provided in Table 13.

1) BCQ (Figure 7): We find that with Medium-R offline data, w/o Aug. BCQ and DARA BCQ both
could not acquire the locomotion skills, which we think is caused by the lack of high-quality offline
data. With more ”expert” data (Medium-R → Medium → Medium-E, or Medium-R → MediumR-E), w/o-Aug. BCQ allows for progressive performance (0.00 → 1.56 → 2.16, or 0.00 → 1.69 in
BCQ), but with our reward augmentation, such performance can be further improved (with average
improvement 13.6%).

2) CQL (Figure 8): We find that with Medium-R or Medium-R-E offline data, w/o Aug. CQL and
_DARA CQL both could not learn the locomotion skills, which we think is caused by the low-quality_
”Replay” offline data. With Medium or Medium-E offline data, w/o Aug. CQL and DARA CQL
acquire similar performance on this flat and static environment.

3) MOPO: We find that the model-based MOPO (both w/o Aug. and DARA) could hardly learn the
locomotion skill under the provided offline data.

Table 14: Average distance (m) covered in an episode (300 steps) in the obstructive and dynamic
(real) environment.

Sim2real (Obstructive and Dynamic) w/o Aug. DARA w/o Aug. DARA w/o Aug. DARA

BCQ CQL MOPO

Quadruped Robot Medium-RMedium 0.85— 1.35— _↑_ 0.92— **1.40— ↑** —— ——

Medium-E 1.15 **1.41 ↑** 0.77 1.32 ↑ — —
Medium-R-E 1.27 **1.55 ↑** — — — —

Average performance improvement 25.9% 30.9% —


**(Obstructive and dynamic environment) We then deploy our learned policy in the obstructive and**
dynamic environment. The results (distance covered in an episode) are provided in Table 14.

1) BCQ (Figure 9): In this obstructive environment, we can obtain similar results as in the flat
environment. With more ”expert” data (Medium → Medium-E → Medium-R-E), w/o Aug. BCQ
allows for progressive performance (0.85 → 1.15 → 1.27), and with our reward augmentation,
such performance can be further improved (with average improvement 25.9%). At the same time,
we can also find that due to the presence of environmental obstacles, the performance of both w/o


-----

_Aug. BCQ and w/o Aug. DARA is decreased compared to the deployment on the flat environment._
However, we find that our DARA exhibits greater average performance improvement under this
obstructive environment (13.6% → **25.9%) compared to that in the flat environment. These results**
_demonstrate that our DARA can learn an adaptive policy for the target environment and thus show_
_a greater advantage over w/o-Aug. in more complex environments._

2) CQL (Figure 10): Similar to BCQ, our DARA CQL exhibits a greater performance improvement
over baseline in the obstructive and dynamic environment (0.2% → **30.9%) compared to that in the**
flat and static environment.

**In summary, The results in the quadruped robot tasks support our conclusion in the main text**
regarding the dynamics shift problem in offline RL — with only modest amounts of target offline
data (3 ∗ 10[4] steps), DARA-based methods can acquire an adaptive policy for the (both flat and
obstructive) target environment and exhibit better performance compared to baselines under the
dynamics adaptation setting.

Figure 7: Deployment on the flat and static environment of BCQ.

Figure 8: Deployment on the flat and static environment of CQL.


-----

Figure 9: Deployment on the obstructive and dynamic environment of BCQ.

Figure 10: Deployment on the obstructive and dynamic environment of CQL.

A.3.7 ABLATION STUDY WITH RESPECT TO THE AMOUNT OF TARGET OFFLINE DATA

To see whether the amount of target offline data can be further reduced, we show the results of the
ablation study with respect to the amount of target offline data in Tables 15 and 16.

Table 15: Ablation study with respect to the amount of target Hopper data (body mass shift tasks).
10%, 5% and 1% denote training with 10%, 5% and 1% of target offline data, respectively, and
additional 100% source offline data.

Body Mass Shift 10% 5% 1% 10% 5% 1% 10% 5% 1% 10% 5% 1%

BEAR BRAC-p BCQ CQL

Hopper Medium-R 34.1 10.7 6.4 30.8 27.7 20.0 32.8 20.5 16.3 3.7 3.2 2.3

Medium-E 1.2 0.6 0.6 34.7 25.1 20.6 84.2 65.1 55.6 99.7 52.3 38.5

Table 16: Ablation study with respect to the amount of target Walker2d data (body mass shift tasks).
10%, 5% and 1% denote training with 10%, 5% and 1% of target offline data, respectively, and
additional 100% source offline data.

Body Mass Shift 10% 5% 1% 10% 5% 1% 10% 5% 1% 10% 5% 1%

BEAR BRAC-p BCQ CQL


Walker2d


Medium-R 7.3 5.9 1.3 18.6 21.6 15.8 15.1 12.7 9.7 2.0 1.3 0.5
Medium-E 2.3 -0.2 -0.3 77.5 2.0 -0.2 57.2 29.7 20.6 93.3 0.1 -0.3


A.3.8 ILLUSTRATION OF WHETHER THE LEARNED POLICY IS LIMITED TO THE SOURCE
OFFLINE DATA

If we directly perform DARA with only the source offline data D[′], the learned behaviors will be
restricted to the source offline data. For example, in the Map task, collecting source dataset with the
obstacle and collecting target dataset without the obstacle. In this case, it can be harder for DARA
(with only the source D[′]) to capture the change in the transition dynamics, thus harder for the agent


-----

to figure out the new optimal policy (the shorter path without the obstacle). However, as stated in
Algorithm 1, we perform offline RL algorithms with both target offline data and source offline data
_{D[′]_ _∪_ _D}. Thus, to some extent, such limitation can be overcome as long as offline RL algorithm_
captures the information (eg. the short path without the obstacle) contained in the (limited) target
_D, see Figure 11 for the illustration._


target

0 k


target

1 k


target

2 k


target

5 k


target

10 k

|RA|Col2|
|---|---|
|DA||

|RA|Col2|
|---|---|
|DA||

|RA|Col2|
|---|---|
|DA||

|RA|Col2|
|---|---|
|DA||

|RA|Col2|
|---|---|
|DA||


Figure 11: We exchange the source environment and the target environment in Figure 2 (in the main
text) so that the source environment has an obstacle and the target environment has no obstacles. In
the source domain, we collect 100k of random transitions. In the target domain, we collect 0k, 1k,
2k, 5k, and 10k random transitions respectively. We set η = 0.1. We can find that if we perform
DARA with only source offline data D[′] (i.e., 0k target data), we indeed can not acquire the optimal
trajectory (eg. the short path without the obstacle). However, even there is no transition of passing
through obstacles in the source data, performing DARA with {D[′] _∪_ _D} enables us to acquire the_
behavior of moving through obstacles. As we increase the number of target offline data D, training
with {D[′] _∪_ _D} can gradually acquire optimal trajectories._

A.3.9 COMPARISON BETWEEN DARA AND IMPORTANCE SAMPLING (IS) BASED DYNAMICS
CORRECTION


In Table 17, we report the experimental comparison between DARA and importance sampling based
dynamics adaption. We can find that in most of the tasks, our DARA performs better than the ISbased approaches.

Table 17: Comparison between DARA and importance sampling (IS) based dynamics correction.


Body Mass Shift IS DARA IS DARA IS DARA

BEAR BRAC-p AWR


Random 4.6 ± 2.8 **8.4 ± 1.2** 10.8 ± 0.5 **11 ± 0.6** **10.2 ± 0.3** 4.5 ± 0.9

Hopper MediumMedium-R 117.3 ± 0.4 ± 4.7 **1.634.1 ± ± 1 5.8** 21.617.4 ± ± 10.6 8.3 **32.930.8 ± ± 7.5 4.9** **1424.8 ± ± 2.2 7.7** 4.228.9 ± ± 3.5 5.5

Medium-E 0.8 ± 0.2 **1.2 ± 0.5** **36 ± 13.5** 34.7 ± 8.5 **29.3 ± 2.6** 26.6 ± 2

BCQ CQL MOPO

Random 9.2 ± 1.1 **9.7 ± 0.2** 10.3 ± 0.4 **10.4 ± 0.4** **2.8 ± 3** 2.1 ± 1.7

Hopper MediumMedium-R 28.214.2 ± ± 8.8 1.3 **38.432.8 ± ± 1.8 0.9** 43.32.2 ± ± 0.3 10 **59.33.7 ± ± 1.4 12.2** 4.97.6 ± ± 7.2 3.8 **10.78.4 ± ± 3.5 5.1**

Medium-E 83.4 ± 23.7 **84.2 ± 9.8** 87.8 ± 16.9 **99.7 ± 16.4** 4.6 ± 2.9 **5.8 ± 2.3**

A.3.10 THE SENSITIVITY OF THE COEFFICIENT OF THE REWARD MODIFICATION


In Table 18, We check the sensitivity of hyper-parameter η, i.e., the coefficient of the reward modification in r(s, a) − _η∆r(s, a, s[′])._


-----

Table 18: We show the normalized scores for the Hopper tasks with body mass shift, by varying
_η ∈{0, 0.05, 0.1, 0.2, 0.5} over BEAR, BRAC-p, AWR, BCQ, CQL, and MOPO._

Hyper-parameter η
Body Mass Shift

0 0.05 0.1 0.2 0.5

BEAR


Random 4.6 ± 3.4 7.7 ± 0.9 **8.4 ± 1.2** 7 ± 1.2 4.2 ± 1.1
Medium 0.9 ± 0.3 1.1 ± 0.6 **1.6 ± 1** 0.9 ± 0.2 0.7 ± 0.1
Medium-R 18.2 ± 5 28.5 ± 5.9 **34.1 ± 5.8** 29.1 ± 4.4 18.1 ± 4.3
Medium-E 0.6 ± 0 0.8 ± 0.1 **1.2 ± 0.5** **1.2 ± 0.6** 0.7 ± 0.1

BRAC-p

Random 9.6 ± 3.3 **11.2 ± 0.8** 11 ± 0.6 10.6 ± 2.4 5.3 ± 1.2
Medium 29.2 ± 2.1 26.5 ± 1.8 **32.9 ± 7.5** 16.1 ± 0.9 16.7 ± 1.7
Medium-R 20.1 ± 4.8 17.8 ± 3.2 **30.8 ± 4.9** 13.9 ± 1.7 10.4 ± 2.4
Medium-E 32.3 ± 7.8 **40.4 ± 4.4** 34.7 ± 8.5 29.4 ± 6.5 25.2 ± 4.1

AWR

Random 3.4 ± 0.7 4.1 ± 1 **4.5 ± 0.9** 3.4 ± 0.7 2.5 ± 0.1
Medium 20.8 ± 6.3 31.8 ± 2.9 **28.9 ± 5.5** 26.6 ± 3.2 17.4 ± 1.5
Medium-R 4.1 ± 1.7 3 ± 0.5 4.2 ± 3.5 2.6 ± 0.6 **4.3 ± 1.3**
Medium-E 26.8 ± 0.4 **27 ± 0** 26.6 ± 2 17.8 ± 5.6 24.2 ± 3.9

BCQ

Random 8.3 ± 0.3 9.6 ± 0.3 **9.7 ± 0.2** 7.4 ± 0.1 7.6 ± 0.3
Medium 25.7 ± 5.5 24.1 ± 0.8 **38.4 ± 1.8** 27.1 ± 1.7 26.7 ± 0.8
Medium-R 28.7 ± 1.9 29.5 ± 3 **32.8 ± 0.9** 25.9 ± 6 21 ± 2.2
Medium-E 75.4 ± 7.8 70.4 ± 5.4 **84.2 ± 9.8** 67.9 ± 8 61.9 ± 4.3

CQL

Random 10.2 ± 0.3 10 ± 0 **10.4 ± 0.4** 10 ± 0 10 ± 0
Medium 44.9 ± 2.7 **59.8 ± 6** 59.3 ± 12.2 44.2 ± 1 37.1 ± 2.6
Medium-R 1.4 ± 0.3 2.1 ± 0.2 3.7 ± 1.4 **3.9 ± 1.7** 3.4 ± 1
Medium-E 53.6 ± 21.2 65.3 ± 15.4 **99.7 ± 16.4** 60.5 ± 16 75.9 ± 30

MOPO

Random 2 ± 2.1 1.8 ± 0 **2.1 ± 1.7** 1.2 ± 0.4 0.8 ± 0
Medium 5 ± 5.3 6.5 ± 1 **10.7 ± 5.1** 5.3 ± 1.6 2.8 ± 0.7
Medium-R 5.5 ± 4.6 7.5 ± 0.8 **8.4 ± 3.5** 5.7 ± 3.5 1.9 ± 0.6
Medium-E 4.8 ± 2.9 **8.1 ± 1** 5.8 ± 2.3 4.7 ± 0.7 2.1 ± 0.2


Hopper

Hopper

Hopper

Hopper

Hopper

Hopper


-----

A.4 ENVIRONMENTS AND DATASET

Figure 12: Illustration of the suite of tasks considered in this work: (from left to right) Hopper,
Walker2d, Halfcheetah, simulated and real-world quadruped robots. These tasks require the RL
agent to learn locomotion gaits for the illustrated characters.

In this work, the tasks include Hopper, Walker2d, HalfCheetah, simulated (see the dynamics parameters in Zhang et al.) and real-world quadruped robot, which are illustrated in Figure 12.

Table 19: Dynamics shift for Hopper, Walker2d, and Halfcheetah tasks. For the body mass shift,
we change the mass of the body in the source MDP M _[′]. For the joint noise shift, we add a noise_
(randomly sampling in [−0.05, +0.05]) to the actions when we collect the source offline data, i.e.,
_D[′]_ := {(s, a, r, s[′])} ∼ _dD′_ (s)πb′ (a|s)r(s, a)T _[′](s[′]|s, a + noise)._

Hopper Walker2d HalfCheetah

Body Mass Shfit Joint Noise Shift Body Mass Shfit Joint Noise Shift Body Mass Shfit Joint Noise Shift
Source mass[-1]=2.5 action[-1]+noise mass[-1]=1.47 action[-1]+noise mass[4]=0.5 action[-1]+noise
Target mass[-1]=5.0 action[-1]+0 mass[-1]=2.94 action[-1]+0 mass[4]=1.0 action[-1]+0

In the Hopper, Walker2d and HalfCheetah dynamics adaptation setting, we set the D4RL (Fu et al.,
2020) dataset as our target domain. For the source dynamics, we change the body mass (body mass
shift) or add noises to joints (joint noise shift) of the agents (see Table 19 for the details) and then
collect the source offline dataset in the changed environment. Following Fu et al. (2020), on the
changed source environment, we collect the 1) ”Random” offline data, generated by unrolling a randomly initialized policy, 2) ”Medium” offline data, generated by a trained policy with the “medium”
level of performance in the source environment, 3) ”Medium-Replay” (Medium-R) offline data, consisting of recording all samples in the replay buffer observed during training until the policy reaches
the “medium” level of performance, 4) ”Medium-Expert” (Medium-E) offline data, mixing equal
amounts of expert demonstrations and ”medium” data in the source environment.


In the sim2real setting (for the quadruped robot), we use the
A1 dog from Unitree (Wang, 2020). We collect the target offline data using five target behavior policies in the real-world
with changing terrains, as shown in Figure 13, and collect
the ”Medium”, ”Medium-Replay” (Medium-R), ”MediumExpert” (Medium-E), ”Medium-Replay-Expert” (Medium-RE) source offline data in the simulator, where ”MediumReplay-Expert” denotes mixing equal amounts of ”MediumReplay” data and expert demonstrations in the simulator. In
Section A.5, we provide the details of how to obtain the target and source behavior policy, so as to collect our target and
source offline data.


Figure 13: Real-world terrains
(for collecting the target offline
data).


We list our tasks properties in Table 20 and provide our collected dataset in supplementary material.
In implementation, we set η = 0.1 for all simulated tasks and set η = 0.01 for the sim2real task. In
Table 18, we also report the sensitivity of DARA on the hyper-parameters η.

A.5 TRAINING THE (TARGET AND SOURCE) BEHAVIOR POLICY FOR THE QUADRUPED ROBOT

To obtain a behavior policy that can be deployed in simulator (for collecting the source offline data)
or real-world (for collecting the target offline data), we introduce the prior knowledge (Iscen et al.,
2018) and domain randomization (Tobin et al., 2017; Peng et al., 2018).


-----

Table 20: Statistics for each task in our adaptation setting.

**Environment** **Dynamics Shift** **Task Name** **Target (1T)** **Source (10S)**

Random 10[5] (D4RL) 10[6]

Medium 10[5] (D4RL) 10[6]

Body Mass Shfit Medium-Replay 20092 (D4RL) 10[6]

Hopper Medium-Expert 2 ∗ 10[5] (D4RL) 2 ∗ 10[6]

Random 10[5] (D4RL) 10[6]

Medium 10[5] (D4RL) 10[6]

Joint Noise Shift Medium-Replay 20092 (D4RL) 10[6]

Medium-Expert 2 ∗ 10[5] (D4RL) 2 ∗ 10[6]

Random 10[5] (D4RL) 10[6]

Medium 10[5] (D4RL) 10[6]

Body Mass Shfit Medium-Replay 10093 (D4RL) 10[6]

Walker2d Medium-Expert 2 ∗ 10[5] (D4RL) 2 ∗ 10[6]

Random 10[5] (D4RL) 10[6]

Medium 10[5] (D4RL) 10[6]

Joint Noise Shift Medium-Replay 10093 (D4RL) 10[6]

Medium-Expert 2 ∗ 10[5] (D4RL) 2 ∗ 10[6]

Random 10[5] (D4RL) 10[6]

Medium 10[5] (D4RL) 10[6]

Body Mass Shfit Medium-Replay 10100 (D4RL) 10[6]

HalfCheetah Medium-Expert 2 ∗ 10[5] (D4RL) 2 ∗ 10[6]

Random 10[5] (D4RL) 10[6]

Medium 10[5] (D4RL) 10[6]

Joint Noise Shift Medium-Replay 10100 (D4RL) 10[6]

Medium-Expert 2 ∗ 10[5] (D4RL) 2 ∗ 10[6]

Medium 3 ∗ 10[4] (real-world) 10[6] (simulator)
Medium-Replay 3 10[4] (real-world) 10[6] (simulator)

A1 robot (Unitree) Sim2Real _∗_

Medium-Expert 3 ∗ 10[4] (real-world) 2 ∗ 10[6] (simulator)
Medium-Replay-Expert 3 ∗ 10[4] (real-world) 2 ∗ 10[6] (simulator)

**Prior Knowledge: To reduce the impact of the foot at the moment of touching the ground during**
the robot locomotion, we designed a compound cycloid trajectory (Sakakibara et al., 1990) as prior
knowledge. In our implementation for the foot trajectory, four aspects are mainly considered: 1) The
robot walks stably without obvious shaking; 2) The joint impact of the robot during the locomotion
is small; 3) The joint speed and acceleration of the robot during the locomotion are continuous and
smooth; 4) The feet of the robot will not slide when they are in contact with the ground. Similar
to Lee et al. (2020), we define a periodic phase variable φi [0.0, 0.6), i = 1, 2, 3, 4 for each leg,
which represents swing phase if φi [0.0, 0.3) and contact phase if ∈ _φi_ [0.3, 0.6). At every time
step t, φi = (t ∗ _f0 + φ0[i] + φoffset ∈_ [i])(mod 2Tm) where Tm = 0.3 ∈, and f0 = 1.1 is the base
frequency, and φ0 = [0, 0.3, 0.3, 0] is the initial phase. φoffset is part of the output of the controller.
The trajectory of the swing leg is:


_xxy =ii = = Y S S0_ hh _TTttmm_ _[−][−]_ 2211ππ [sin][sin]  22TTπtπtmm ii +− SS +0, i S = 10, i, = 3 2 _, 4_

_z = H_ sgn _T2m_ (2fE(t) 1) + 1 + Z0

_[−]_ _[t]_ _−_

    

_t_ 4πt
_fE(t) =_ _,_

_Tm_ _−_ 4[1]π [sin] _Tm_

 

sgn _T2m_ = 11 0T2 ≤m _t <_ _[T]2[m]_
 _[−]_ _[t]_  _−_ _[≤]_ _[t < T][m][ .]_


where

and


-----

The trajectory of the standing leg is:

_xi = S_ 2TTmm−t + 21π [sin] 2Tπtm + S0, i = 1, 2

 _xy =i = Y S0_  2TTmm−t + 21π [sin]  2Tπtm  _−_ _S + S0, i = 3, 4_

_z = Z0_




where S = 0.14m, H = 0.18m are the maximum foot length and height. _S0_ =

[0.17, 0.17, −0.2, −0.2], Y0 = [−0.13, 0.13, −0.13, 0.13], Z0 = [−0.32, −0.32, −0.32, −0.32] are
the default target foot position in body frame.

**Domain Randomization: To encourage the policy to be robust to variations in the dynamics, we**
incorporate the domain randomization. In Table 21, we provide the dynamics parameters and their
respective range of values.

Table 21: Dynamic parameters and their respective range of values utilized during training.

**Parameter** **Range**

Mass [0.95, 1.1] ×default value
Inertia [0.80, 1.2] ×default value
Motor Strength [0.80, 1.2] ×default value
Latency [0, 0.04] s
Lateral Friction [0.5, 1.25] Ns/m
Joint Friction [0, 0.05] Nm

**State Space, Action Space and Reward Function: The action is a 16-dimensional vector consist-**
ing of leg phase and target foot position residuals in the body frame. The design of state space and
reward function mainly follows the prior work Lee et al. (2020). In Table 22, we provide the state
representation.

Table 22: State representation for the behavior policy.

**Data** **Dimension**

Desired direction _BIBv[ˆ]d_ _xy_ 2

Euler angleBase angular velocity(rpy)  _BIB[ω]_  33
Base linear velocity _BIB[v]_ 3
  

Joint position/velocity _θi,_ _θ[˙]i_ 24

  

FTG phases(sin (φi), cos ( _φi))_ 8
FTG frequencies(fi) 4
Base frequency(fo) 1
Joint position error history 24
Joint velocity history 24
Foot target history (rf,d)t−1,t−2 24
 


The reward function is defined as

0.1rlv + 0.05ry + 0.05rrp + 0.005rb + 0.02rbc + 0.025rs + 2 · 10[−][5]rτ _._

The individual terms are defined as follows.

1) Linear velocity reward rlv :

_exp(_ 30 _vpr_ 0.2 ) _vpr < 0.2_
_rlv :=_ 1 _−_ _|_ _−_ _|_ _vpr_ 0.2 _[,]_
 _≥_


-----

where vpr = vxy · ˆvxy is the base linear velocity projected onto the command direction.

2) Yaw angle reward ry :
_ry := exp(−(y −_ _yˆ)[2]),_ (17)

where y and ˆy is the yaw and desired yaw angle.

3) Roll and pitch reward rrp :


_rrp := exp(_ 1.5 (φ [0, arccos( _[< P][xz][,][ (0][,][ 0][,][ 1)][T][ >]_ ) _π/2])[2]),_ (18)
_−_ _−_ _Pxz_ _−_
X _∥_ _∥_

position in world frame. The advantage of designing the target pitch angle in this way is to ensurewhere φ are the roll and pitch angle. Pxz = P1 − _P4 or Pxz = P2 −_ _P3,Pi, i ∈_ [1, 4] are the foot
that the body of the robot is parallel to the supporting surface of the stand legs, thereby ensuring that
the robot can smoothly over challenge terrain, such as upward stairs.

4) Base motion reward rb :

_rb := exp(−1.5_ (vxy − _vpr ∗_ _vˆxy)[2]) + exp(−1.5_ (ωxy)[2]), (19)

where ωxy are the roll and pitch rates.X X

5) Body collision reward rbc :
_rbc := −|Ibody/Ifoot|,_ (20)

where Ibody and Ifoot are the contact numbers of robot’s body parts and foot with the terrain, respectively.

6) Target smooth reward rs :

_rs :=_ _fd,t_ 2fd,t 1 + fd,t 2 _,_ (21)
_−||_ _−_ _−_ _−_ _||_

where fd,i(i = t, t 1, t 2) are the target foot positions in the time-step t, t 1 and t 2.
_−_ _−_ _−_ _−_

7) Torqure reward rτ :
_rτ := −_ _|τi|,_ (22)

_i_

X

where τi is the joint torques.

**Training Details: Both the behavior policy and value networks are Multilayer Perceptron (MLP)**
with 3 hidden layers, which have 256, 128 and 64 nodes. The activation function is the Tanh function, and the optimizer is Adam. With the above prior knowledge, domain randomization and reward
function, we train our behavior policy with SAC (Haarnoja et al., 2018) in PyBullet (Coumans &
Bai, 2016–2021).

A.6 ADDITIONAL RESULTS

Here we provide additional results regarding the error bars (Tables 23 and 24).


-----

5.5 2
0.9 3.5
_±_ _±_
_±_ _±_
4.528.94.226.6

6.3 0.4
0.7 1.7
_±_ _±_
_±_ _±_

AWR 3.420.84.126.8

0.3 0.4
_± ± 4.9 1.3_
_± ±_
10.330.98.827

10.235.928.427.1

7.5 4.9 8.5
0.6 ± ± ±
_±_
1132.930.834.7

2.1 4.8 7.8
3.3
_± ± ±_
_±_
9.629.220.132.3

BRAC-p

0.1 14.7
_± 6.2 3.3 ±_
10.929 ±5.4 ±34.5

1132.70.61.9

5.8
1.2 1 ± 0.5
_± ±_ _±_
8.41.634.11.2

3.4 0.3 5 0
_±_
_± ±_ _±_

BEAR 4.60.918.20.6

0 1.5 0.1
0.5 ± ± ±
_±_
10.81.30.8

11.452.133.796.3
RandomMediumMedium-RMedium-E

Hopper


5.1
1.7 3.5 2.3
_±_
_±_ _± ±_
2.110.78.45.8

4.6 2.9
2.1 5.3 ± ±
_± ±_
255.54.8

MOPO

2.4 ± 2 ± 0.6 0.6 ±
4.84.11 ±1.6

11.72867.523.7

0.4 12.2 16.4
1.4
_± ±_ _±_
_±_
10.459.33.799.7

0.3 2.7 21.2
0.3
_± ±_ _±_
_±_

CQL 10.244.91.453.6

0.1 34.5
_± 9.2 5.2 ±_
10.643 ±9.6 ±59.7

10.85848.698.7

1.8 0.9 9.8
0.2
_± ± ±_
_±_
9.738.432.884.2

5.5 1.9 7.8
0.3
_± ± ±_
_±_
8.325.728.775.4

BCQ

0.1 6.3
_± ± 4.4 16.2_
10.637.19.3 ±58 ±

10.654.533.1110.9
RandomMediumMedium-RMedium-E

Hopper


0.1 0.3
0.8 0.3
_±_ _±_
_±_ _±_
2.417.21.553.3

0.2 1.2
0.4
1 ± ± ±

AWR 2 ±17.11.652.5

2.8 10.4
0.4 2.1
_±_ _±_
_±_ _±_
1.314.87.435.5

1.517.415.553.8

6.5 3.1
2.5 3.1 ± ±
_± ±_
3.27818.677.5

10.8
0.7 10.1 2 ±
_± ± ±_
1.3709.964.1

BRAC-p

16.8
9.9 2.2
0.2 ± ± ±
_±_
06.48.520.6

-0.277.5-0.376.9

0.4 0.7 1.3 2.2
_± ± ± ±_
3.20.37.32.3

0.9 0.5 5.1 2.5
_± ± ± ±_
3.10.66.51.5

BEAR

0.3 0.1
0.9 0.6
_±_ _±_
_±_ _±_
1.5-0.50.7-0.1

7.359.119.240.1
RandomMediumMedium-RMedium-E

Walker2d


-----

-----

