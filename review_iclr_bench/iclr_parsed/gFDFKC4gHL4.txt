# HOW DID THE MODEL CHANGE? EFFICIENTLY AS## SESSING MACHINE LEARNING API SHIFTS

**Lingjiao Chen, Matei Zaharia, James Y. Zou**
Stanford University
{lingjiao,jamesz}@stanford.edu matei@cs.stanford.edu

ABSTRACT

ML prediction APIs from providers like Amazon and Google have made it simple
to use ML in applications. A challenge for users is that such APIs continuously
change over time as the providers update models, and changes can happen silently
without users knowing. It is thus important to monitor when and how much the ML
APIs’ performance shifts. To provide detailed change assessment, we model ML
API shifts as confusion matrix differences, and propose a principled algorithmic
framework, MASA, to provably assess these shifts efficiently given a sample
budget constraint. MASA employs an upper-confidence bound based approach to
adaptively determine on which data point to query the ML API to estimate shifts.
Empirically, we observe significant ML API shifts from 2020 to 2021 among 12 out
of 36 applications using commercial APIs from Google, Microsoft, Amazon, and
other providers. These real-world shifts include both improvements and reductions
in accuracy. Extensive experiments show that MASA can estimate such API shifts
more accurately than standard approaches given the same budget.

1 INTRODUCTION

Machine learning (ML) prediction APIs have made it dramatically easier to build ML applications.
For example, one can use Microsoft text API (Mic, a) to determine the polarity of a text review
written by a customer, or Google speech API (Goo, b) to recognize users’ spoken commands received
by a smart home device. These APIs have been gaining in popularity (MLa; Chen et al., 2020), as
they eliminate the need for ML users to train their own models.

Monitoring and assessing the performance of these third-party ML APIs over time, however, is
under-explored. ML API providers continuously collect new data or change their model architectures
(Qi et al., 2020) to update their services, which could silently help or harm downstream applications’
performance. For example, as shown in Figure 1 (a) and (b), we observe a 7% overall accuracy drop
of IBM speech API on the AUDIOMNST dataset in March 2021 compared March 2020. In our
systematic study of 36 API and dataset combinations, there are 12 cases where the API’s performance
changed by more than 1% on the same dataset from 2020 to 2021 (sometimes for the worse). Such
performance shifts are of serious concern not only because of potential disruptions to downstream
tasks, but also because consistency is often required for audits and oversight. Therefore, it is important
to precisely assess shifts in an API’s predictions over time. Moreover, in this assessment, it is often
more informative to quantify how the entire confusion matrix of the API has changed rather than
just the overall accuracy. In the IBM case in Figure 1, it is interesting that a major culprit of the
drop in performance is the 2021 model mistaking “four” for “five”. In other settings, changes in the
confusion matrix could still cause issues even if the overall accuracy stays the same.

In this paper, we formalize the problem of assessing API shifts as estimating changes in the confusion
matrix on the same dataset in a sample-efficient manner (i.e., with few calls to the API itself to
minimize dollar cost). The straightforward approach is to compare the API’s prediction on randomly
sampled data. However, this can require a large number of API calls to estimate the confusion matrix,
which is expensive given that each call costs money. To help address this challenge, we propose
MASA, a principled algorithm for ML API shift assessments. MASA efficiently estimates shifts
in the API’s confusion matrix by clustering the dataset and adaptively sampling data from different
clusters to query the API. MASA automates its sampling rate from different data clusters based on


-----

**(a) Confusion matrix 2020 (%)** **(b) Confusion matrix 2021 (%)** **(c) Shift estimator performance**

Figure 1: ML API shift for IBM speech recognition API on AMNIST, a spoken digit dataset. (a) and
(b) give its (normalized) confusion matrix in April 2020 and 2021, respectively. There is an overall
7% accuracy drop. One factor is the 2021 model incorrectly predicting more “four” as “five”. (c)
Given a sample budget, the proposed MASA can assess the API shift with much smaller error in
Frobenius norm compared to standard uniform sampling.

the uncertainty in the confusion matrix estimation. For example, MASA may query the ML API on
more samples with the true label “four” than “one”, if it is less sure about the estimated performance
change on the former. Employing an upper-confidence-bound approach to estimate the uncertainties,
MASA enjoys a low computation and space cost as well as a fast estimation error rate guarantee.

MASA’s adaptive sampling substantially improves the quality of estimation for API shifts. In
extensive experiments on real world ML APIs, MASA’s assessment error is often an order of
magnitude smaller than that of standard uniform sampling with same sample size (e.g., Figure 1 (c)).
To reach the same tolerable target estimation error, MASA can reduce the required sample size by
more than 50%, sometimes up to 90%.

**Contributions. In short, our main contributions include:**

1. We demonstrate that commercial Ml APIs can experience significant performance shifts over
time, and formulate ML API shift assessments via confusion matrix difference estimation as
an important practical problem. We will release the dataset consisting of 1,224,278 samples
annotated by ML APIs in different date to stimulate more research on ML API shifts.

2. We propose MASA, an algorithm to assess the ML API performance shifts efficiently.
MASA adaptively determines querying the ML API on which data points to minimize the
shift estimation error under a sample size constraint. We show that MASA enjoys a low
computation cost and performance guarantees.

3. We evaluate MASA on real world APIs from Google, Microsoft, Amazon and other providers
for tasks including speech recognition, sentiment analysis, and facial emotion recognition.
MASA leads to estimation errors an order of magnitude smaller than standard uniform
sampling using the same sample size, or over 90% fewer samples to reach the same tolerable
estimation error. Our code and datasets are also released [1].

**Related Work.** **Distribution shifts in ML deployments: Performance shifts in ML systems have**
been observed in applications like disease diagnosis (Lipton et al., 2018), facial recognition (Wang
et al., 2020), and Inference of molecular structure (Koh et al., 2020). Most of them are attributed
to distribution shifts, i.e., the distribution of the test and training datasets are different. Distribution
shifts are usually modeled as covariate shifts (Shimodaira, 2000; Sugiyama et al., 2007; QuiñoneroCandela et al., 2009) or label shifts (Lipton et al., 2018; Saerens et al., 2002; Azizzadenesheli et al.,
2019; Zhao et al., 2021). API shifts are orthogonal to distribution shifts: instead of attributing the
performance shifts to data distribution changes, API shifts concern with ML APIs changes which
changes its predictions on the same dataset. The methods for detecting distribution drifts typically
rely on changes in data feature statistics and can not detect changes in the API on the same data. To
the best of our knowledge, MASA is the first work to systematically investigate ML API shifts.

[1https://github.com/lchen001/MASA](https://github.com/lchen001/MASA)


-----

**(a) Sentiment Analysis** **(b) Facial Emotion Recognition** **(c) Speech Recognition**

Figure 2: Observed overall accuracy changes. Each row corresponds to an ML API, and each column
represents a dataset. The entry is the overall accuracy difference between evaluation in spring 2020
and spring 2021. In 12 out of 36 cases, the API’s overall accuracy changed by more than 1%; this
includes several cases of substantial drops in performance.

**Deploying and monitoring ML APIs: Several issues in deployed ML APIs have been studied.**
For example, Buolamwini & Gebru (2018) showed that strong biases toward minority can exist in
commercial APIs and Ribeiro et al. (2020) revealed that several bugs in commercial APIs can be
detected using checklists. Kang et al. (2020) extend program assertions to monitor and improve
deployed ML models. Chen et al. (2020) consider the trade-offs between accuracy performance and
cost via exploiting multiple APIs. On the other hand, the proposed MASA focuses on estimating
(silent) API performance changes cheaply and accurately, which has not been studied before.

**Stratified sampling and multi-arm bandits: Stratified sampling has proved to be useful in various**
domains, such as approximate query processing (Chaudhuri et al., 2007), population mean estimation
(Carpentier et al., 2011; 2015), and complex integration problems (Leprêtre et al., 2017). A common
approach is to model stratified sampling as a multi-arm bandit (MAB) problem: view each data
partition as an arm, and set the regret as the variance of the obtained estimator. Plenty of algorithms
exist for solving standard MAB problems, such as the epsilon greedy algorithm (Slivkins, 2019),
upper-confidence-bound approach (Auer et al., 2002), and Thompson sampling method (Russo et al.,
2018). While those algorithms aim at playing the best arm as often as possible, stratified sampling’s
goal is often to estimate the average of all arms, and thus needs to allocate the number of calls for
all arms aptly based on their variance. In contrast to estimating the population average in standard
stratified sampling, our goal is to simultaneously estimate a matrix whose entries can be correlated.
Thus, we have an objective (uncertainty score) that is different from theirs (variance), which also
leads to an optimal allocation different from that of standard stratified sampling. This difference
requires additional statistical bounds to prove convergence, which we provide in our paper. It also
results in a different upper-confidence-bound term and convergence rate compared to standard bandit
algorithms.

2 THE API SHIFT PROBLEM

**Empirical assessment of ML API shifts.** We start by making an interesting observation: Commer_cial ML APIs’ performance can change substantial over time on the same datasets. We investigated_
twelve standard datasets across three different tasks, namely, YELP (Dat, c), IMDB (Maas et al.),
WAIMAI (Dat, b), SHOP (Dat, a) for sentiment analysis, FER+ (Goodfellow et al., 2015), RAFDB
(Li et al.), EXPW (Zhang et al.), AFNET (Mollahosseini et al., 2019) for facial emotion recognition,
and DIGIT (Dat, d), AMNIST (Becker et al., 2018), CMD (Warden, 2018), FLUENT (Lugosch et al.),
for speech recognition. For each dataset, we evaluated three commercial ML APIs’ accuracy in April
2020 and April 2021. Figure 2 summarizes the overall accuracy changes.

There are several interesting empirical findings. First, API performance changes are quite common. In
fact, as shown in Figure 2, API performance changes exceeding 1% occurred in about 33% of all (36)
considered ML API-dataset combinations. Since the data distribution remains fixed, such a change is
due to ML APIs’ updates. Second, the API updates can either help or hurt the accuracy performance
depending on the datasets. For example, as shown in Figure 2 (a), the Amazon sentiment analysis


-----

**Pick partition by** **Get one** **Query** **Update uncertainty score**

**Partition the dataset**

**uncertainty score** **sample** **ML API** **and other parameters**


Figure 3: How MASA works. MASA first partitions the dataset. Then it picks which partition to
sample based on some uncertainty measurement, queries the ML API on the drawn sample, and uses
the API’ prediction to update uncertainty and estimated shifts on this partition. This is repeated until
the ML API has been queried N times. Finally, the estimated shifts on different partitions are aptly
fused to obtain the desired API shifts.

API’s accuracy increases on YELP, WAIMAI, and SHOP, but decreases on IMDB. In addition, the
update of Microsoft facial emotion recognition API only affects performance on the FER+ dataset, as
shown in Figure 2 (b). Another interesting finding is that the magnitude of the performance change
can be quite different. In fact, most of the accuracy differences are between 1–3%, but on DIGIT
dataset, Google’s accuracy change is more than 20%.

**Fine-grained assessment of API shift as changes in the confusion matrix.** Based on feedback
from practitioners, accuracy change alone is insufficient, and attribution to per class change is often
much more informative (Tsipras et al., 2020; Hou et al., 2019; Luque et al., 2019). Thus, a natural
idea is to quantify an ML API’s performance by its confusion matrix. We assess the change of the
confusion matrix over time as a measure of API shift.

Formally, consider an ML service for a classification task with L labels. For a data point x from
some domain X, let ˆy(x) ∈ [L] denote its predicted label on x, and y(x) be the true label. For
example, for sentiment analysis, x is a text paragraph, and the task is to predict if the polarity
of x is positive or negative. Here L = 2, and ˆy(x) = 1 implies positive predicted label while
_y(x) = 2 indicates negative true label. The confusion matrix is denoted by CCC ∈_ R[L][×][L] where
_CCC_ _i,j ≜_ Pr[y(x) = i, ˆy(x) = j]. Given a confusion matrix of the ML API measured previously (say,
a few months ago), CCC _[o], the ML API shift is defined as ∆CCC ≜_ _CCC −_ _CCC_ _[O]._

Using confusion matrix difference to quantify the ML API shift is informative. E.g, the overall
accuracy change is simply the trace of ∆CCC. It also explains which label gets harder or easier for
the updated API. Still consider, e.g., sentiment analysis. Given a 2% overall accuracy change,
∆CCC 1,2 = 1% and ∆CCC 2,1 = −3% implies that the change is due to predicting less (-3%) negative
texts as positive, by sacrificing the accuracy on positive texts slightly (1%). This suggests that the
API could have been updated with more negative training texts.

3 MASA: ML API SHIFT ASSESSMENT

Now we present MASA, an algorithmic framework efficiently to assess ML API shifts. Suppose the
old confusion matrix CCC _[o]_ and a large labeled dataset D are available. Given a query budget N, our
goal is to generate ∆CCC[ˆ], an estimation of the API shifts as accurately as possible by querying the ML
API ˆy(·) on N samples drawn from D.

MASA achieves its goal via an adaptive sampling approach (Figure 3). It first divides the given
dataset D into several partitions (clusters). Then it adaptively decides which sample to query the ML
API in an iterative manner: at each iteration, it selects one data partition based on some uncertainty
measure (defined below), and queries the ML API on one sample randomly drawn from this partition.
The API’s prediction is obtained to update the uncertainty measure as well as the estimated shift ∆CCC[ˆ] .
This process is repeated until the ML API has been queried N times or if a stopping rule is reached.
We explain each step in detail next.

3.1 DATA PARTITIONING

A key intuition in MASA is that not all samples are equally informative for estimating API shifts.
Consider, for example, a vision API makes perfect predictions on “dog” images, and guesses randomly
on ”cat“ pictures. The “dog” images are less informative, as even a small sample of “dog” queries


-----

would tell that there is essentially no confusion for this class. Intuitively, within a sample budget,
an estimator with more samples from “cat” pictures should be more accurate overall than that from
“dog”. Generally, harder images tend to be more informative.

Thus, it is a natural idea to partition all data points based on factors that may correlate with their
informativeness, and sample from those partitions separately. In MASA, we use partitions Di,k
that each contain the points with true label i and difficulty level k. The difficulty level is an integer
indicating how hard it is to predict the data point’s label. It needs not be perfect, and can be simply
the discretized prediction confidence generated by some simple ML models. A total of L labels and
_K distinct difficulty labels lead to a total of LK partitions. If the uncertainty or variability of the ML_
API’s prediction on each partition is different, then drawing a different number of samples from each
partition may improve the shift assessment performance compared to standard uniform sampling. We
verify this empirically in our evaluation (Section 4).

3.2 BUDGET ALLOCATION PROBLEM

Given the data partition, two questions arise: (i) how many samples should be drawn from each
partition, and (ii) how to estimate the ML API shifts given available samples. The second question is
relatively straightforward. Note that the API shifts satisfy


∆CCC _i,j = Pr[y(x) = i, ˆy(x) = j] −_ _CCC_ _[o]i,j_ [=]


Pr[y(x) = i, ˆy(x) = j, x _Di,k]_ _CCC_ _[o]i,j_
_∈_ _−_
_k=1_

X

_K_

Pr[x _Di,k] Pr[ˆy(x) = j_ _x_ _Di,k]_ _CCC_ _[o]i,j_
_∈_ _|_ _∈_ _−_
_k=1_

X


_i=1_


Pr[ˆy(x) = j, x ∈ _Di,k] −_ _CCC_ _[o]i,j_ [=]
_k=1_

X


where the first equation is by definition, the second is due to total probability rule, the third uses
the fact that x ∈ _Di,k implies y(x) = i, and the last equation applies conditional probability. Here,_
Pr[x ∈ _Di,k] is simply ratio of size of partition Di,k and entire dataset D, known a prior. To assess_
∆CCC _i,j, we only need to estimate Pr[ˆy(x) = j_ _x_ _Di,k], the predicted label distribution on partition_
_|_ _∈_
_Di,k. It can be estimated simply via the frequency of predicting label j among all available samples_
drawn from Di,k.

Now we consider the sample allocation problem. For ease of notation, we denote Pr[xi _Di,j]_
by pppi,k, Pr[ˆy(x) = j|x ∈ _Di,k] and its estimation by µµµi,k,j and ˆµµµi,k,j, respectively. Then for ∈_
deterministic sample allocations, the squared Frobenius norm error can be written as

2

2

E ∆CCC ∆CCC[ˆ] _F_ = E ∆CCC _i,j_ ∆CCC[ˆ] _i,j_ = E _pppi,k[µµµi,k,j_ _µµµˆi,k,j]_
_∥_ _−_ _∥[2]_ _i,j_ _−_ _i,j_ _k_ _−_ !
h i X   X X

2

= _i,j,k_ _ppp[2]i,k[E]_ [µµµi,k,j − _µµµˆi,k,j]_
X   

2

Thus we use the losssample budget allocation algorithm L(A, N ) ≜ [P]i,j,k using[p]pp[2]i,k N[E] samples. For any fixed[µµµi,k,j − _µµµˆi,k,j]_ to measure the performance of any N, our goal is to find a sample

_A_   
budget allocation algorithm A to minimize the loss L(A, N ). Notably, we can generalize it for other
scenarios by replacing (∆CCC − ∆CCC[ˆ] ) with WWW ⊙ (∆CCC − ∆CCC[ˆ] ), where ⊙ is element-wise multiplication
and WWW is an L × L weight matrix. Different choices of WWW can penalize the error of each entry in ∆CCC[ˆ]
differently and serve for different purposes. For example, if misclassifying label 1 as label 2 is the
only focus, then we can simply set WWW 1,2 = 1 and WWW i,j = 0, ∀(i, j) ̸= (1, 2). If we use the identity
matrix as the weight WWW, then it becomes equivalent to estimating the overall accuracy by minimizing
the variance. To minimize the loss for the general scenarios, Algorithm 1 (which is explained in
the rest of this section) is still applicable by simply multiplying the API selection formula (line 3 in
Algorithm 1) with its corresponding weights.

3.3 UNCERTAINTY SCORE AND OPTIMAL ALLOCATION

The optimal sample allocation is directly connected to how informative each data partition is. To see
this, let us first introduce the notation of uncertainty score for each data partition.


-----

**Algorithm 1 MASA’s ML API shift assessment algorithm.**
**Output :Input** **:Estimated ML API ShiftML API ˆy(·), query budget ∆CCC N[ˆ] ∈, partitionsR[L][×]L[ˆ]** _Di,k, ppp ∈_ R[L][×][K], CCC _[o]_ _∈_ R[L][×][L], and a > 0

**1 Set NNN = 000L×K, ˆµµµ = 000L×K×L, ˆσσσ = 000L×K,HHH = 000L×K×L** _▷_ Initialization

**2 for n ←** 1 to N do

(i, k), _if NNN i,k < 2_

**3** (i[∗], k[∗]) ← arg maxi,k _NNNpppi,k i,k_ _σσσˆi,k +_ 4 _NNNa i,k_ _,_ _o/w_ _▷_ Determine data partition



 

**4** Sample xn from Di∗,k∗ and query the ML API to obtainq ˆy(xn)

**5** _NNN i[∗],k[∗]_ _NNN i∗,k∗_ + 1 _▷_ Update sample size

**6** _µµµˆi∗,k∗,j ←←_ _µµµˆ21i[H]∗HH,ki∗∗,j,k +∗,yˆ(1xyˆn(xn)[,]_ )=NNN ij∗[−],kµµµ[ˆ]∗i∗ _,k∗_ _,j_ _, ∀j ∈_ [ifL N]NN i[∗],k[∗] _< 2▷_ Update predicted label distribution

**7** _σσσˆ[2]i[∗],k[∗]_ _[←]_ σσσˆ[2]i[∗],k[∗] [+] 1− _HHHiNNNi∗_ _,k∗N,kNN∗_ _,∗− iyˆ∗(xn,k1_ _∗)_ _−σσσˆ[2]i[∗]_ _,k[∗]_ _,_ _o/w_ _▷_ Update uncertainty score

**8** _HHH_ _i∗,k∗,yˆ(xn)_ _HH_ _i∗,k∗,yˆ(xn)_ [+ 1] _▷_ Update label frequency

**9 end**  _[←]_ _[H]_

**10 Return ∆CCC[ˆ] ∈** R[L][×][L] where ∆CCC[ˆ] _i,j =_ _k=1_ _[p]ppi,kµµµˆi,k,j −_ _CCC_ _[o]i,j[,][ ∀][i, j]_ _▷_ Confusion estimation

[P][K]

**Definition 1. σσσ[2]i,k** [≜] [(1][ −] [P]j[L]=1 [Pr][2][[ˆ]y(x) = j|x ∈ _Di,k]) denotes the uncertainty score of Di,k._

The uncertainty score quantifies how informative each Di,k is by subtracting from 1 the sum of
the square of each label’s probability mass. The uncertainty score is related to collision entropy
(discussed in Appendix A), and determines the optimal allocation as follows.
**Lemma 1. Let A[∗]** _be the sample allocation algorithm that achieves the smallest expected squared_
_Frobenius norm error. Then the number of samples drawn from Di,k by A[∗]_ _is_

_NNN_ _i,k[∗]_ [=] _i,kpppi,k[p]ppσσσi,ki,kσσσi,k_ _N_
P


Lemma 1 shows that the optimal budget allocation depends on the uncertainty score, but in practice,
we do not know the uncertainty score before drawing samples and querying the ML API. Thus, a
natural question is how to estimate the uncertainty score σσσ[2]i,k[. Suppose][ n][ samples,][ x][1][, x][2][,][ · · ·][, x][n][,]
are drawn from partition Di,k. Then we can estimate σσσ[2]i,k [by]


_σσσˆ[2]i,k_ [≜] [1][ −]


1yˆ(xs)=ˆy(xt) (3.1)
_t:t=1X,t≠_ _s_


_n(n −_ 1)


_s=1_


3.4 AN UNCERTAINTY-AWARE ADAPTIVE SAMPLING ALGORITHM

Now we have a chicken-and-egg problem: estimating the uncertainty scores is needed to find the
optimal sample allocation, but sampling from all partitions is needed to estimate their uncertainty
scores. To overcome this issue, we adopt an iterative sampling approach, as shown in Algorithm
1. At each iteration, it alternates between (i) uncertainty score-based new sample selection (line 3

-  4) and (ii) uncertainty score and predicted label distribution update using the new sample (line 5

-  8). After querying the ML API N times, the API shifts are obtained by (iii) fusing the estimated
predicted label distribution on each partition (line 10). We give the details as follows.

**Uncertainty score and predicted label distribution update.** After obtaining the predicted label for
a sample from partition Di∗,k∗, we need to update (i) the number of samples already drawn from this
partition, denoted by NNN i∗,k∗, (ii) the estimated predicted label distribution, denoted by ˆµµµi∗,k∗,j, _j,_
_∀_
and (iii) the estimated uncertainty score, ˆσσσ[2]i[∗],k[∗][. For][ N]NN i∗,k∗ and ˆµµµi∗,k∗,j (line 5-6), we use standard
incremental update approach (Cotton, 1975), which requires constant space and computational cost
per iteration. For ˆσσσ[2]i[∗],k[∗][, we additionally maintain the number of label][ j][ being predicted among all]
samples drawn from Di∗,k∗, denoted by HHH _i∗,k∗,j (line 8). This enables a fast incremental update of_
_σσσˆ[2]i[∗],k[∗]_ [(line 7).]


-----

**Uncertainty score-based new sample selection.** To determine on which partition to select a new
sample, we use an upper-confidence-bound approach on the weighted uncertainty score (second case
in line 3), after ensuring two samples have been drawn from each partition (first case in line 3). Two
samples are needed for an initial estimation of each partition’s uncertainty score. Here, we use a
parameter a > 0 to balance between exploiting knowledge of uncertainty score (σσσˆ[2]i,k[) and exploring]


more partitions ( [4]


1

_NNN i,k_ [).]


We quantify the performance of MASA v.s. the optimal allocation algorithm A[∗] as follows.
**Theorem 2. If a > 2 log L + log K +** 4[9] [log][ N][ and][ N >][ 4][LK][, then we have]

1

_L(MASA, N_ ) −L(A[∗], N ) ≤ _O(N_ _[−]_ 4[5] log 4 N )


Roughly speaking, Theorem 2 shows that the loss gap between the API shift estimated by MASA
and the (unreachable) optimal allocation algorithm ceases in the rate of N _[−][5][/][4]. Note that the loss of_
the optimal allocation decays in the rate of N _[−][1]. Thus, as N gets larger and larger, the relative gap_
becomes more and more negligible.

4 EXPERIMENTS

We apply MASA to estimate the shifts of several real world ML services for various tasks. Our
goal is three-fold: (i) understand if and why MASA assess the API shifts efficiently, (ii) examine
how much sample cost MASA can reduce compared to standard sampling, and (iii) exploit the
trade-offs between estimation accuracy and query cost achieved by MASA. We also study how the
hyperparameters affect MASA’s performance, left to Appendix C.

**Tasks, ML APIs, and datasets.** As shown in Section 2, we have observed 12 of 36 cases where
there is a >1% overall accuracy performance change of an ML API. Thus, we focus on evaluating
MASA’s performance on those 12 cases. Except for case study, all experiments were averaged over
1500 runs. In all tasks, we created partitions using difficulty levels induced by a cheap open source
model from GitHub. More details are in Appendix C.

**Sentiment analysis: a case study on Amazon API.** We start by a case study on Amazon API on
a sentiment analysis dataset, YELP to understand MASA’s performance. We adopt MASA with
sample budget 2000. The dataset is divided into 4 partitions D+,l, D+,h, D−,l, D−,h, depending on
whether the true label is positive (+) or negative (-), and quality score produced by the 2020 version
is lower (l) or higher (h) than the median.

We first note that the API shift gives an interesting explanation to Amazon API’s accuracy change.
In fact, as shown in Figure 4 (a-c), the accuracy increase is mostly because more texts (2.7%) with
negative altitudes are correctly classified. One possible explanation is that the API has been retrained
on a dataset with more negative texts. Next, we observe that MASA produces accurate estimation of
the API shift by comparing Figure 4 (c) and (d). This is primarily due to (i) that the data partitioning
separates more uncertain data from less uncertain ones, and (ii) that adaptive sampling learns the
uncertainty level effectively. Figure 4 (e-f) shows that while the partitions’ size is similar, their
uncertainty scores are different. For example, higher quality score implies a much smaller uncertainty
for positive texts (D+,h and D+,l in Figure 4(f)). As shown in Figure 4 (g), MASA indeed learns to
utilize such imbalanced uncertainty: its sampling allocation for each partition is quite close to the
optimal allocation (dark star point). Finally, it is worth noting that MASA outperforms standard
uniform sampling notably, as shown in Figure 4(h). This is because uniform sampling does not
exploit the uncertainty of each partition.

**Budget savings achieved by MASA.** In many applications, it suffices to obtain an estimated API
shift close to the true shift, e.g., within a 1% Frobenius norm error. Thus, a natural question arises: to
_reach the same tolerable estimation error, how much sampling cost can MASA reduce compared to_
_standard sampling approaches?_

To answer this question, we compare MASA with two natural approaches: (i) uniform sampling and
(ii) stratified sampling by drawing same number of samples for each true label. For each approach,


-----

40

48.2 1.8

positive

20

11.2 38.8

negative

positive negative

(a) CM 2020


40

47.5 2.5

positive

20

8.6 41.4

negative

positive negative

(b) CM 2021


2

-0.6 0.6

positive

0

-2.7 2.7

negative 2

positive negative

(c) True API shift


2

-0.5 0.5

positive

0

-2.6 2.6

negative 2

positive negative

(d) Estimated Shift



0.3

0.2

0.1

Partition fraction

0.0

D+, l D+, h D, l [D], h

(e) Partition Size


0.3

0.2

0.1

Uncertainty score

0.0

D+, l D+, h D, l [D], h

(f) Uncertainty score


100

MASA

10 1 Uniform

10 2

3

Frobenius norm error10

0 1000 2000

Iteration

(h) Performance


1000 D+, l

D+, h

800 D, l

600 D, h

400

Number of queries 200

0

0 1000 2000

Iteration

(g) Sample allocation


Figure 4: Case study for Amazon API’s performance shift on dataset YELP. (a) and (b) give its
confusion matrix in spring 2020 and spring 2021, respectively. (c) is their differences, i.e., the API
shift. MASA’s estimated shift using 2000 samples is in (d). The dataset is divided into 4 partitions
based on (i) positive (+) or negative (−) true labels, and (ii) low (l) or high (h) quality score. (e) and
(f) give the size and uncertainty score of each partitions. (g) shows MASA’s sampling decision per
iteration, where the dark dot points represent the (unreachable) optimal sample allocation. (h) reveals
its performance.

Table 1: Required sample size to reach 1% Frobnius norm error. Here we compare MASA with
uniform (U) sampling and stratified (S) sampling. U and S required very similar sample sizes and
are reported in the same column. The sample size is obtained when a 1% Frobenius norm error is
achieved with probability 95%.

|API;Dataset|Sample size|Col3|Save|API;Datase|Sample s t MASA|ize Save U/S|
|---|---|---|---|---|---|---|
||MASA|U/S|||||
|Amazon;YELP|4.5K|19.7K|77%|IBM;DIGI|T 3.6K|17.0K 79%|
|Amazon;IMDB|10.3K|20.8K|51%|IBM;AMNI|ST 2.4K|18.5K 87%|
|Amazon;WAIMAI|7.8K|18.0K|57%|Google;DIG|IT 4.2K|17.0K 75%|
|Amazon;SHOP|4.8K|20.8K|77%|Google;AMN|IST 1.1K|18.5K 94%|
|MS; FER+|2.6K|19.9K|87%|Google;CM|D 1.6K|15.2K 89%|
|Google; EXPW|4.2K|17.9K|77%|MS;DIGIT|3.3K|17.0K 81%|


we measure the number of samples needed to reach 1% Frobenius norm error with probability 95%,
via an upper bound on the estimated Frobenius error. The details are left to Appendix C. As shown in
Table 1, MASA usually requires more than 70% fewer samples to reach such tolerable Frobenius
norm error than the uniform and stratified sampling, primarily due to its shift estimation is more
accurate. Uniform and stratified sampling required the similar number of samples because the upper
bounds on their estimated Frobenius error are similar.

**Trade-offs between estimation error and query budget .** Next we examine the trade-offs between
API shift estimation error and sample size achieved by MASA, shown in Figure 5. We first note
that, across all 12 observed API shifts, MASA consistently outperforms standard uniform sampling
for any fixed sample size. In fact, the achieved estimation error of MASA is usually an order of
magnitude smaller than that of uniform sampling. This verifies that MASA can provide more accurate


-----

10 2

10 3

10 4

Squared F norm error

0 1000 2000 3000

|Col1|MASA Uniform|
|---|---|
|||
|||


MASA
Uniform

Sample size

(a) Amazon YELP

10 2

10 3

10 4

0 1000 2000 3000

Sample size


(e) Microsoft FER+


10 2

10 3

10 4

0 1000 2000 3000

Sample size


10 3

10 4

0 1000 2000 3000

Sample size


10 3

10 4

0 1000 2000 3000

Sample size


(b) Amazon IMDB

10 2

10 3

10 4

0 1000 2000 3000

Sample size


(f) Google EXPW


(c) Amazon WAIMAI


(d) Amazon SHOP


10 2

10 3

10 4

0 1000 2000 3000

Sample size

(h) Google AMNIST


10 2

10 3

10 4

0 1000 2000 3000

Sample size


(g) Google DIGIT



10 2

10 3

10 4

0 1000 2000 3000

Sample size

(k) IBM AMNIST




10 2

10 3

10 4

0 1000 2000 3000

Sample size

(l) Microsoft DIGIT


10 2

10 3

10 4

0 1000 2000 3000

Sample size


10 2

10 3

10 4

0 1000 2000 3000

Sample size


(i) Google CMD


(j) IBM DIGIT


Figure 5: API shift estimation performance and sample size trade-offs. We compare the expected
squared Frobenius norm error of MASA with K = 3 partitions versus standard uniform sampling.
For any sample size, MASA consistently leads to an estimation error much smaller than uniform
sampling across different API and dataset combinations.

assessments of API shifts in diverse applications. Second, some API shifts are easier to estimate
than others. For example, for Google API shift on AMNIST, using 1000 samples already gives an
expected squared Frobenius norm error lower than 10[−][4], while it usually requires 2000 samples for
other shifts. This is probably because the skew in its uncertainties among different partitions is more
severe than other shifts.


5 CONCLUSION

In this paper, we identify and formulate the problem of characterizing ML API shifts. Our systematic
empirical study has shown that API model updates are frequent, and that some updates can reduce
performance substantially. Quantifying such shifts is an important but understudied problem that
can greatly affect the reliability of applications using ML-as-a-service. To capture fine-grained
change assessment, we model the ML API shifts as confusion matrix differences. Adaptive sampling
methods are typically designed for estimating a single scalar (e.g., adaptive population mean) or
training a model (e.g., active learning), and thus not directly applicable for API shift estimation.
Uniform sampling is natural but requires a large number of samples. Thus, we propose an algorithmic
framework, MASA, to adaptively assess the API shifts using as few samples as possible. Our work
focuses on estimating changes in the confusion matrix because the confusion matrix is often what is
used by practitioners to assess API performance. We acknowledge that confusion matrices are most
applicable for classification tasks, and other measures need to be used for more complex APIs (e.g.
OCR, NLP). While this is a limitation, classification with a small to moderate number of classes of
interest is a common use case for ML APIs, and this is an important starting point since it has not
been studied before. We also release our dataset of 1,224,278 samples annotated by commercial APIs
in different dates as the first dataset and resource for studying ML API performance shifts.


-----

REFERENCES

[Amazon Comprehend API. https://aws.amazon.com/comprehend. [Accessed March-](https://aws.amazon.com/comprehend)
2020 and March-2021].

[Baidu API. https://ai.baidu.com/. [Accessed March-2020 and March-2021].](https://ai.baidu.com/)

SHOP dataset. [https://github.com/SophonPlus/ChineseNlpCorpus/tree/](https://github.com/SophonPlus/ChineseNlpCorpus/tree/master/datasets/online_shopping_10_cats)
[master/datasets/online_shopping_10_cats, a.](https://github.com/SophonPlus/ChineseNlpCorpus/tree/master/datasets/online_shopping_10_cats)

WAIMAI dataset. [https://github.com/SophonPlus/ChineseNlpCorpus/tree/](https://github.com/SophonPlus/ChineseNlpCorpus/tree/master/datasets/waimai_10k)
[master/datasets/waimai_10k, b.](https://github.com/SophonPlus/ChineseNlpCorpus/tree/master/datasets/waimai_10k)

[YELP dataset. https://www.kaggle.com/yelp-dataset/yelp-dataset, c.](https://www.kaggle.com/yelp-dataset/yelp-dataset)

[DIGIT dataset. https://github.com/Jakobovski/free-spoken-digit-dataset,](https://github.com/Jakobovski/free-spoken-digit-dataset)
d.

Face++ Emotion API. [https://www.faceplusplus.com/emotion-recognition/.](https://www.faceplusplus.com/emotion-recognition/)

[Accessed March-2020 and March-2021].

[Google NLP API. https://cloud.google.com/natural-language. [Accessed March-](https://cloud.google.com/natural-language)
2020 and March-2021].

[Google Vision API. https://cloud.google.com/vision, a. [Accessed March-2020 and](https://cloud.google.com/vision)
March-2021].

Google Speech API. [https://cloud.google.com/speech-to-text, b.](https://cloud.google.com/speech-to-text) [Accessed
March-2020 and March-2021].

[IBM Speech API. https://cloud.ibm.com/apidocs/speech-to-text. [Accessed](https://cloud.ibm.com/apidocs/speech-to-text)
March-2020 and March-2021].

Machine Learning as a Service Market Report . [https:](https://www.mordorintelligence.com/industry-reports/global-machine-learning-as-a-service-mlaas-market)
[//www.mordorintelligence.com/industry-reports/](https://www.mordorintelligence.com/industry-reports/global-machine-learning-as-a-service-mlaas-market)
[global-machine-learning-as-a-service-mlaas-market.](https://www.mordorintelligence.com/industry-reports/global-machine-learning-as-a-service-mlaas-market)

[Microsoft computer vision API. https://azure.microsoft.com/en-us/services/](https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision)
[cognitive-services/computer-vision, a. [Accessed March-2020 and March-2021].](https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision)

Microsoft speech API. [https://azure.microsoft.com/en-us/services/](https://azure.microsoft.com/en-us/services/cognitive-services/speech-to-text)
[cognitive-services/speech-to-text, b.](https://azure.microsoft.com/en-us/services/cognitive-services/speech-to-text) [Accessed March-2020 and March2021].

K.B. Athreya and S.N. Lahiri. Measure Theory and Probability Theory. Springer, 2006.

Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
[problem, 2002. URL https://doi.org/10.1023/A:1013689704352.](https://doi.org/10.1023/A:1013689704352)

Kamyar Azizzadenesheli, et al. Regularized learning for domain adaptation under label shifts. In
_ICLR 2019._

Sören Becker, et al. Interpreting and explaining deep neural networks for classification of audio
signals. CoRR, abs/1807.03418, 2018.

Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial
gender classification. In FAT 2018.

A. Carpentier et al. Finite time analysis of stratified sampling for monte carlo. In NIPS 2011.

Alexandra Carpentier, Rémi Munos, and András Antos. Adaptive strategy for stratified monte carlo
sampling. J. Mach. Learn. Res., 16:2231–2271, 2015.

Surajit Chaudhuri, Gautam Das, and Vivek R. Narasayya. Optimized stratified sampling for approximate query processing. ACM Trans. Database Syst., 32(2):9, 2007.


-----

Lingjiao Chen, Matei Zaharia, and James Y. Zou. FrugalML: How to use ML prediction apis more
accurately and cheaply. In NeurIPS 2020.

Ira W. Cotton. Remark on stably updating mean and standard deviation of data. Commun. ACM, 18
(8):458, 1975.

Ian J. Goodfellow, et al. Challenges in representation learning: A report on three machine learning
contests. Neural Networks, 64:59–63, 2015.

Saihui Hou, et al. Learning a unified classifier incrementally via rebalancing. In CVPR 2019.

Daniel Kang, et al. Model assertions for monitoring and improving ML models. In MLSys 2020.

Pang Wei Koh, et al. WILDS: A benchmark of in-the-wild distribution shifts. CoRR, abs/2012.07421,
2020.

Florian Leprêtre, Fabien Teytaud, and Julien Dehos. Multi-armed bandit for stratified sampling:
Application to numerical integration. In TAAI 2017.

Shan Li, Weihong Deng, and JunPing Du. Reliable crowdsourcing and deep locality-preserving
learning for expression recognition in the wild. In CVPR 2017.

Zachary C. Lipton, Yu-Xiang Wang, and Alexander J. Smola. Detecting and correcting for label shift
with black box predictors. In ICML 2018,.

Loren Lugosch, et al. Speech model pre-training for end-to-end spoken language understanding. In
_Interspeech 2019._

Amalia Luque, et al. The impact of class imbalance in classification performance metrics based on
the binary confusion matrix. Pattern Recognit., 91:216–231, 2019.

Andrew L. Maas, et al. Learning word vectors for sentiment analysis. In HLT 2011.

Ali Mollahosseini, Behzad Hasani, and Mohammad H. Mahoor. Affectnet: A database for facial
expression, valence, and arousal computing in the wild. IEEE Trans. Affect. Comput., 10(1):18–31,
2019.

Haode Qi, et al. Benchmarking intent detection for task-oriented dialog systems. _CoRR,_
abs/2012.03929, 2020.

Joaquin Quiñonero-Candela, et al. Covariate Shift by Kernel Mean Matching, pp. 131–160. 2009.

M. Ribeiro, et al. Beyond accuracy: Behavioral testing of NLP models with checklist. In ACL 2020.

Daniel Russo, et al. A tutorial on thompson sampling. Found. Trends Mach. Learn., 11(1):1–96,
2018.

Marco Saerens, Patrice Latinne, and Christine Decaestecker. Adjusting the outputs of a classifier to
new a priori probabilities: A simple procedure. Neural Comput., 14(1):21–41, 2002.

Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the loglikelihood function. In JSPI 2000.

Aleksandrs Slivkins. Introduction to multi-armed bandits. In Found. Trends Mach. Learn. 2019.

Masashi Sugiyama, et al. Direct importance estimation with model selection and its application to
covariate shift adaptation. In NIPS 2007.

Dimitris Tsipras, et al. From imagenet to image classification: Contextualizing progress on benchmarks. In ICML 2020.

Zhongyuan Wang, et al. Masked face recognition dataset and application. In Arxiv 2020.

Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. In Arxiv
_2018._

Zhanpeng Zhang, et al. Learning social relation traits from face images. In ICCV 2015.

Eric Zhao, et al. Active learning under label shift. In AISTATS 2021.


-----

ACKNOWLEDGEMENT

This work was supported in part by a Google PhD Fellowship, NSF CCF 1763191, NSF CAREER
1651570 and 1942926, NIH P30AG059307, NIH U01MH098953, grants from the Chan-Zuckerberg
Initiative, Sutherland, and affiliate members and other supporters of the Stanford DAWN project—Ant
Financial, Meta, Google, Infosys, NEC, and VMware—as well as Cisco and SAP. We also thank
anonymous reviewers for helpful discussion and feedback.

**Appendix Outline** The appendix is organized as follows. Section A provides additional technical
details. All proofs are presented in Section B. We give experimental setups, details of datasets and
ML APIs, and further empirical results in Section C.

A TECHNICAL DETAILS

**Computation and space cost of MASA.** One attractive property of MASA is its low computation
and space cost. In fact, it can be easily verified from Algorithm 1 that, the computation cost is only
linear in the number of samples N . The occupied space is only constant. Therefore, MASA can be
easily applied for large number of samples.

**Choice of parameter a.** The parameter a is used to balance between exploiting and exploration in
Algorithm 1. Throughout this paper, we set a = 1 as the default value. While theoretically a should
depend on the partition size and sample number, in practice we found that a = 1 works well. An
in-depth analysis for this remains an interesting open problem.

**Stopping rule under loss requirements.** For MASA, we establish the upper bound on the loss by
(i) computing the upper bound on the estimated uncertainty score for each partition, and (ii) summing
up all those upper bounds weighted by the partition size to form the upper bound on the loss. For
Uniform sampling or stratified sampling, we directly use the upper bound on the Frobenius loss.
Here, we adopt the standard upper bound for Bernoulli variables. That is to say, for any estimator
using n samples, we use _n_ [as its upper bound, where][ c][ is a parameter to control the confidence.]

For both methods, we choose c to ensure a 1% error under 95% confidence level.

[p][ c]

B PROOFS

We present all missing proofs here. For ease of expositions, let us first introduce a few notations. We
let xn denote the nth sample drawn in Algorithm 1, and use In to indicate from which partition the
sample xn is drawn. For example, In = (i, k) indicates that xn is drawn from the partition Di,k.

Let zzzℓ,k,t [L] denote the ML API’s predicted label for the tth sample drawn from the partition
_Dℓ,k. Abusing the notation a little bit, let ∈_ _NNN ℓ,k,n denote the value of NNN ℓ,k after the n −_ 1th iteration
and before the nth iteration in Algorithm 1. Similarly, let ˆσσσℓ,k,n be the value of ˆσσσℓ,k, ˆµµµℓ,k,j,n be the
value ofiteration in Algorithm 1. In addition, let ˆµµµℓ,k,j, and HHH _ℓ,k,,j,n be the value of ∆ℓ,k HHH ≜ℓ,k,j, all after theℓ[′]_ _,kppp[′]ℓ,k[ p]ppℓσσσ′_ _,kℓ,k′σσσℓ′_ _,k n′_ _−[and]1[ ∆]th iteration and before the[min][ = min ∆][ℓ,k][. Similarly,] nth_

let us denote σσσmin ≜ min _σσσℓ,k. By assumption thatP pppℓ,k > 0 and σσσℓ,k > 0, we must have ∆min > 0_
and σσσmin > 0.

B.1 USEFUL LEMMAS

Let us first give a few useful lemmas. The first gives a high probability bound on our estimated
uncertainty score.
**Lemma 3. Let the event A be**


log 2/δ

2t


1

_A ≜_ 1 1zzzℓ,k,i=zzzℓ,k,j _σσℓ,k_

1≤k1≤K,\t 1N≤ℓ≤L  vuu _−_ _t(t −_ 1) Xi=1 _j=1X,j≠_ _i_ _[−]_ _[σ]_
_≤_ _≤_ t

_Then for any δ > 0, we have_ Pr[A] ≥ 1 − _LKNδ._


-----

_Proof. For any fixed t, let us first denote_

_f_ (zzzℓ,k,1,zzzℓ,k,2, · · ·,zzzℓ,k,t) ≜ 1 −

Its expectation is simply


1
_zzzℓ,k,i=zzzℓ,k,j_
_j=1X,j≠_ _i_


_t(t −_ 1)


_i=1_


1
_zzzℓ,k,i=zzzℓ,k,j_ []]
_j=1X,j≠_ _i_


E[f (zzzℓ,k,1,zzzℓ,k,2, · · ·,zzzℓ,k,t)] =E[1 −


_t(t −_ 1)


_−_ _i=1_ _j=1,j≠_ _i_

_t_ _t_

1
=1 1zzzℓ,k,i=zzzℓ,k,j []]
_−_ _t(t_ 1) [E][[]

_−_ Xi=1 _j=1X,j≠_ _i_

=1 − E[1zzzℓ,k,i=zzzℓ,k,j []]

where the second equation applies the linearity of expectation, and the third equation uses the fact
that all zzzℓ,k,i are identically independent. Note that

E[1zzzℓ,k,i=zzzℓ,k,j []]

= Pr[zzzℓ,k,i = zzzℓ,k,j]

_L_

= Pr[zzzℓ,k,i = r] Pr[zzzℓ,k,j = r]

_r=1_

X

_L_

2

= Pr[zzzℓ,k,i = r]

_r=1_

X

where the first equation uses the definition of indicator function, the second uses the fact that two
sample are independent and there are only L many possible labels, and the last equation uses the fact
that those samples’ distribution is identical. Applying this in the above equation, we get

E[f (zzzℓ,k,1,zzzℓ,k,2, · · ·,zzzℓ,k,t)] =1 − E[1zzzℓ,k,i=zzzℓ,k,j []]


=1 −


Pr[zzzℓ,k,i = r] = σσσ[2]ℓ,k


=1 −


_r=1_


That is to say, its expectation is simply the uncertainty score σσσ[2]ℓ,k[. On the other hand, we note that,]
for any i, we have

_f_ (zzzℓ,k,1, _,zzzℓ,k,i_ 1,zzzℓ,k,i,zzzℓ,k,i+1, _,zzzℓ,k,t)_ _f_ (zzzℓ,k,1, _,zzzℓ,k,i_ 1,zzz[′]ℓ,k,i[,z]zzℓ,k,i+1, _,zzzℓ,k,t)_
_· · ·_ _−_ _· · ·_ _−_ _· · ·_ _−_ _· · ·_

_t_

1 1
= _t(t −_ 1) _j=1X,j≠_ _i_ 1zzz[′]ℓ,k,i[=][z]zzℓ,k,j _[−]_ [1]zzzℓ,k,i=zzzℓ,k,j _[≤]_ _t(t −_ 1) _[·][ (][t][ −]_ [1) = 1]t

where the inequality is due to the fact that the indicator function can only take values in {0, 1}.
Similarly, we have

_f_ (zzzℓ,k,1, _,zzzℓ,k,i_ 1,zzzℓ,k,i,zzzℓ,k,i+1, _,zzzℓ,k,t)_ _f_ (zzzℓ,k,1, _,zzzℓ,k,i_ 1,zzz[′]ℓ,k,i[,z]zzℓ,k,i+1, _,zzzℓ,k,t)_
_· · ·_ _−_ _· · ·_ _−_ _· · ·_ _−_ _· · ·_


1

_t(t −_ 1) _[· −][(][t][ −]_ [1) =][ −] [1]t


1
= _t(t −_ 1) _j=1X,j≠_ _i_ 1zzz[′]ℓ,k,i[=][z]zzℓ,k,j _[−]_ [1]zzzℓ,k,i=zzzℓ,k,j _[≥]_

By Mcdiarmid inequality, we have


2ϵ[2]

_t_

Pr[|f (zzzℓ,k,1,zzzℓ,k,2, · · ·,zzzℓ,k,t) − E[f (zzzℓ,k,1,zzzℓ,k,2, · · ·,zzzℓ,k,t)]| ≥ _ϵ] ≤_ 2e− Pi=1 _[t][−][2]_ = 2e[−][2][tϵ][2]

Set δ = 2e[−][2][tϵ][2] . This simply becomes, with probability at most δ,


_|f_ (zzzℓ,k,1,zzzℓ,k,2, · · ·,zzzℓ,k,t) − _σσσ[2]ℓ,k[|]_

=|f (zzzℓ,k,1,zzzℓ,k,2, · · ·,zzzℓ,k,t) − E[f (zzzℓ,k,1,zzzℓ,k,2, · · ·,zzzℓ,k,t)]| ≥


log 2/δ

2t


-----

Note that f is positive, we can take square root of both side, and obtain with probability at most δ,


4 log 2/δ
_f_ (zzzℓ,k,1,zzzℓ,k,2, _,zzzℓ,k,t)_ _σσσℓ,k_

_|_ _· · ·_ _−_ _| ≥_ 2t

r

q

Or alternatively, with probability at least 1 − _δ,_

4 log 2/δ
_f_ (zzzℓ,k,1,zzzℓ,k,2, _,zzzℓ,k,t)_ _σσσℓ,k_

_|_ _· · ·_ _−_ _| ≤_ 2t

r

q

which holds for fixed t, ℓ, k. Taking union bound, we know that with probability 1 − _KLNδ,_


4 log 2/δ
_f_ (zzzℓ,k,1,zzzℓ,k,2, _,zzzℓ,k,t)_ _σσσℓ,k_

_|_ _· · ·_ _−_ _| ≤_ 2t

r

q

which holds for all t, ℓ, k. Plugging in the form of f completes the proof.


The next one is more technical: it gives a connection between stopping time and adaptive sampling.
We omit the proof and refer the interested readers to (Athreya & Lahiri, 2006).
**Lemma 4 (Wald’s second inequality). Let {Ft}t=1,...,n be a filtration and {Xt}t=1,...,n be an Ft**
_adapted sequence of i.i.d. random variables with finite expectation µ and variance V ar. Assume that_
_Ft and σ({Xs : s ≥_ _t + 1}) are independent for any t ≤_ _n, and let T_ (≤ _n) be a stopping time with_
_respect to_ _t. Then_
_F_ _T_

2

E _i=1_ _Xi −_ _T µ_ # = E[T ] Var .

" X 


B.2 PROOF OF LEMMA 1

_Proof. Recall that the loss, defined as the expected squared Frobenius norm error, is_


2

2

E ∆CCC ∆CCC[ˆ] _F_ = E ∆CCC _i,j_ ∆CCC[ˆ] _i,j_ = E _pppi,k[µµµi,k,j_ _µµµˆi,k,j]_
_∥_ _−_ _∥[2]_ _i,j_ _−_ _i,j_ _k_ _−_ !
h i X   X X

2

= _i,j,k_ _ppp[2]i,k[E]_ [µµµi,k,j − _µµµˆi,k,j]_
X   

Here we basically apply the definition of each entry. Suppose NNN i,k samples are allocated to estimate
_µµµi,k,j. Then we have_


∆CCC ∆CCC[ˆ] _F_
_∥_ _−_ _∥[2]_


2 1
E [µµµi,k,j _µµµˆi,k,j]_ = Pr[ˆy(x) = j _x_ _Di,k](1_ Pr[ˆy(x) = j _x_ _Di,k])_
_−_ _NNN i,k_ _|_ _∈_ _−_ _|_ _∈_

since µµµi,k,j  is effectively a Bernoulli variable. Then the loss becomes


2
_i,j,k_ _ppp[2]i,k[E]_ [µµµi,k,j − _µµµˆi,k,j]_

X   

1
_ppp[2]i,k_ _NNN i,k_ Pr[ˆy(x) = j|x ∈ _Di,k](1 −_ Pr[ˆy(x) = j|x ∈ _Di,k])_
_i,j,k_

X

1
_ppp[2]i,k_ _NNN i,k_ Pr[ˆy(x) = j|x ∈ _Di,k](1 −_ Pr[ˆy(x) = j|x ∈ _Di,k])_
_i,k_ _j_

X X


∆CCC ∆CCC[ˆ] _F_
_∥_ _−_ _∥[2]_


where the last equation is simply by rearranging the summation. Note that

Pr[ˆy(x) = j _x_ _Di,k] = 1_
_|_ _∈_
_j_

X

The last summation is simply


Pr[ˆy(x) = j _x_ _Di,k](1_ Pr[ˆy(x) =j _x_ _Di,k]) = 1_
_|_ _∈_ _−_ _|_ _∈_ _−_

=σσσ[2]i,k


Pr[ˆy(x) = j _x_ _Di,k])_
_|_ _∈_


-----

Thus, the loss becomes

E ∆CCC ∆CCC[ˆ] _F_
_∥_ _−_ _∥[2]_
h

By Cauchy Schwarz inequality, we have


_ppp[2]i,k[σ]σσ[2]i,k_
_i,k_

X


_NNN i,k_


_ppp[2]i,k[σ]σσ[2]i,k_

 _NNN i,k_

_i,k_

[X]

where the equality holds if and only if


_NNN i,k_

  _≥_

_i,k_

[X] 


_pppi,kσσσi,k_



_i,k_

[X]


_ppp[2]i,k[σ]σσ[2]i,k_ _ppp[2]i[′],k[′][σ]σσ[2]i[′],k[′]_

=
_NNN_ [2]i,k _NNN_ [2]i[′],k[′]

for any i, i[′], k, k[′]. That is to say, there exists some constant c, such that


_pppi,kσσσi,k_ _ppi′,k′σσσi′,k′_

= _[p]_ = [1]
_NNN i,k_ _NNN i′,k′_ _c_

And thus, NNN i,k = pppi,kσσσi,kc. Summing over i, k gives


_N =_


_NNN i,k =_
_i,k_

X


_pppi,kσσσi,kc_

_i,k_

X


Thus,

_N_
_c =_

_i,k_ _[p]ppi,kσσσi,k_
and P
_NNN i,k = pppi,kσσσi,kc = pppi,kσσσi,k_ 1 = _pppi,kσσσi,k_
_·_ _i,k_ _[p]ppi,kσσσi,k_ _i,k_ _[p]ppi,kσσσi,k_

which completes the proof. P P


B.3 PROOF OF THEOREM 2

_Proof. To prove this theorem, we need a few more lemmas._

**Lemma 5. Algorithm 1’s computational cost is O(LKN** ) and space cost is O(L[2]K). Furthermore,
_for any n > 2LK, after the n −_ 1th iteration and before the nth iteration, we have

_n−1_

_NNN ℓ,k = NNN ℓ,k,n =_ 1Ii=(ℓ,k)

_i=1_

X


1 _n−1_

1 1

_NNN ℓ,k,n_ _Ii=(ℓ,k)_ _yˆ(xi)=j_

_i=1_

X


_µµµˆℓ,k,j = ˆµµµℓ,k,j,n =_


1 _n−1_

_NNN ℓ,k,n(NNN ℓ,k,n_ 1)

_i=1_

_−_ X


_n−1_

1 1
_Ii=Ij_ =(ℓ,k) _yˆ(xi)=ˆy(xj_ )
_j=1X,j≠_ _i_


_σσσˆℓ,k = ˆσσσℓ,k,n = 1 −_


_n−1_

1 1
_Ii=(ℓ,k)_ _yˆ(xi)=j_
_i=1_

X


_HHH_ _ℓ,k,j = HHH_ _ℓ,k,j,n =_


_Proof. The computational and space cost can be easily verified: as shown in Algorithm 1, the_
variables ˆσσσ, ˆµµµ,HHH,NNN take space LK, L2K, L[2]K, LK. Therefore, the space is bounded by O(L[2]K).
For the first 2LK iterations (line 3-8) in Algorithm 1, the computation cost is clearly O(LK).


-----

For the rest iterations (line 10- 16), the most expensive cost is computing In, which requires LK
computations per iteration. Therefore, the total computational cost is O(LKN ).

Next we show that the above four equations hold for every n > 2LK. We prove this by induction.

1) n = 2LK + 1: One can easily verify this by plugging the initial values established in line 3-8 in
Algorithm 1.

2) Suppose the four equations hold for the case when n = m. Now consider n = m + 1. Now let us
consider two cases.

-  Any ℓ, k such that Im+1 = (ℓ, k): There is nothing update,
_̸_

_m−1_ _m−1_ _m−1_

_NNN ℓ,k,m+1 = NNN ℓ,k,m =_ 1Ii=(ℓ,k) = 1Ii=(ℓ,k) + 0 = 1Ii=(ℓ,k) + 1Im=(ℓ,k)

_i=1_ _i=1_ _i=1_

X X X


= 1Ii=(ℓ,k)

_i=1_

X

Similarly, one can show that

_µµµˆℓ,k,j,m+1 = ˆµµµℓ,k,j,m =_


1 1
_Ii=(ℓ,k)_ _yˆ(xi)=j_
_i=1_

X


_NNN ℓ,k,m_


1 1
_Ii=Ij_ =(ℓ,k) _yˆ(xi)=ˆy(xj_ )
_j=1X,j≠_ _i_


_σσσˆℓ,k,m+1 = ˆσσσℓ,k,m = 1 −_


_NNN ℓ,k,m(NNN ℓ,k,m −_ 1)


_i=1_


1 1
_Ii=(ℓ,k)_ _yˆ(xi)=j_
_i=1_

X


_HHH_ _ℓ,k,j,m+1 = HHH_ _ℓ,k,j,m =_

-  For some ℓ[∗], k[∗] such that Im+1 = (ℓ[∗], k[∗]).


Let us first consider NNN ℓ∗,k∗ . We increment NNN ℓ∗,k∗ by one, and thus


_m−1_ _m−1_

1 1 1
_Ii=(ℓ∗,∗k) + 1 =_ _Ii=(ℓ∗,k∗) +_ _Im=(ℓ∗,k∗)_
_i=1_ _i=1_

X X


_NNN ℓ∗,k∗,m+1 = NNN ℓ∗,k∗,m + 1 =_

_m_

= 1Ii=(ℓ∗,k∗)

_i=1_

X


Next we consider ˆµµµℓ∗,k∗,j. Using a similar argument as above, we have

_µµµˆℓ∗,k∗,j,m+1 =ˆµµµℓ∗,k∗,j,m +_ [1]y[ˆ](xmNNN)= ℓ∗j _,k[−]∗,mµµµ[ˆ]ℓ+1∗,k∗,j,m_ _,_

_NN ℓ∗,k∗,m+1_ 1 1yˆ(xm)=j
=[N]NNN ℓ∗,k∗,m+1 − _µµµˆℓ∗,k∗,j,m +_ _NNN ℓ∗,k∗,m+1_ _,_

_NN ℓ∗,k∗,m_ 1yˆ(xm)=j
=NNN[N] ℓ∗,k∗,m+1 _µµµˆℓ∗,k∗,j,m +_ _NNN ℓ∗,k∗,m+1_ _,_


_m−1_ 1 1 1yˆ(xm)=j

_Ii=(ℓ∗,k∗)_ _yˆ(xi)=j_ [+]
_i=1_ _NNN ℓ∗,k∗,m+1_

X


_NN ℓ∗,k∗,m_
= _[N]_

_NNN ℓ∗,k∗,m+1_

1
=

_NNN ℓ∗,k∗,m+1_


_NNN ℓ∗,k∗,m_


1 1
_Ii=(ℓ∗,k∗)_ _yˆ(xi)=j_
_i=1_

X


-----

where the first equation is due to the update rule of ˆµµµℓ∗,k∗,j, the second equation is simply
grouping by ˆµµµℓ∗,k∗,j,m, the third equation is due to the fact that NNN ℓ∗,k∗,m+1 = NNN ℓ∗,k∗,m +1,
the forth equation is due to the induction assumption, and the forth equation is simply
algebraic rewriting.

Now let us consider ˆσσσ[2]ℓ[∗],k[∗],m+1[. We can write]

_σσσˆ[2]ℓ[∗],k[∗],m+1_


2 _HH_ _ℓ∗,k∗,yˆ(xm),m_
=ˆσσσ[2]ℓ[∗],k[∗],m [+] (1 _σσσ[2]ℓ[∗],k[∗],m[)]_

_NNN ℓ∗,k∗,m+1_ _−_ _NNN[H] ℓ∗,k∗,m+1_ 1

_−_ _[−]_ [ˆ]

_NN ℓ∗,k∗,m+1_ 2 2 _HH_ _ℓ∗,k∗,yˆ(xm),m_
=[N] _−_ _σσσˆ[2]ℓ[∗],k[∗],m_ [+] (1

_NNN ℓ[∗],k[∗],m+1_ _NNN ℓ[∗],k[∗],m+1_ _−_ _NNN[H] ℓ[∗],k[∗],m+1_ 1 [)]

_−_

=[N]NN ℓ∗,k∗,m+1 − 2 (1 1 _m−1_ _m−1_ 1Ii=Ij =(ℓ∗,k∗)1yˆ(xi)=ˆy(xj )[)]

_NNN ℓ∗,k∗,m+1_ _−_ _NNN ℓ∗,k∗,m(NNN ℓ∗,k∗,m_ 1) _i=1_ _j=1,j=i_

_−_ X X̸

2 _HH_ _ℓ∗,k∗,yˆ(xm),m_
+ (1

_NNN ℓ∗,k∗,m+1_ _−_ _NNN[H] ℓ∗,k∗,m+1_ 1 [)]

_−_

=[N]NN ℓ[∗],k[∗],m+1 − 2 (1 1 _m−1_ _m−1_ 1Ii=Ij =(ℓ∗,k∗)1yˆ(xi)=ˆy(xj )[)]

_NNN ℓ∗,k∗,m+1_ _−_ (NNN ℓ∗,k∗,m+1 2)(NNN ℓ∗,k∗,m+1 1) _i=1_ _j=1,j=i_

_−_ _−_ X X̸

2 _HH_ _ℓ∗,k∗,yˆ(xm),m_
+ (1

_NNN ℓ∗,k∗,m+1_ _−_ _NNN[H] ℓ∗,k∗,m+1_ 1 [)]

_−_

1 _m−1_ _m−1_
=1 1Ii=Ij =(ℓ∗,k∗)1yˆ(xi)=ˆy(xj )
_−_ _NNN ℓ∗,k∗,m+1(NNN ℓ∗,k∗,m+1_ 1) _i=1_ _j=1,j=i_

_−_ X X̸

2HHH _ℓ∗,k∗,yˆ(xm),m_
_−NNN ℓ∗,k∗,m+1(NNN ℓ∗,k∗,m+1_ 1)

_−_

where the first equation is by the update rule in Algorithm 1, the second equation is simply
rearranging the terms, the third equation uses the induction assumption, the forth one uses
the update rule on NNN ℓ∗,k∗ and thus NNN ℓ∗,k∗,m = NNN ℓ∗,k∗,m+1 1, and the fifth equation is
also rearranging the terms. _−_

On the other hand, by induction assumption, we have


_m−1_

1 1
_Ii=(ℓ∗,k∗)_ _yˆ(xi)=ˆy(xm)_
_i=1_

X


_HHH_ _ℓ∗,k∗,yˆ(xm),m_ [=]


And thus


_m−1_ _m−1_

1Ii=Ij =(ℓ∗,k∗)1yˆ(xi)=ˆy(xj ) [+ 2][H]HH _ℓ∗,k∗,yˆ(xm),m_

Xi=1 _j=1X,j≠_ _i_


1 1
_Ii=(ℓ∗,k∗)_ _yˆ(xi)=ˆy(xm)_
_j=1X,j≠_ _i_


_i=1_


Hence, the above equation becomes


1 1
_Ii=Ij_ =(ℓ∗,k∗) _yˆ(xi)=ˆy(xj_ )
_j=1X,j≠_ _i_


_σσσˆ[2]ℓ[∗],k[∗],m+1_ [= 1][ −]


_NNN ℓ∗,k∗,m+1(NNN ℓ∗,k∗,m+1_ 1)
_−_


_i=1_


-----

Finally, let us consider HHH _ℓ∗,k∗,j. If j_ = ˆy(xm), it is clear that
_̸_

_m−1_

_HHH_ _ℓ∗,k∗,j,m+1 =HHH_ _ℓ∗,k∗,j,m =_ 1Ii=(ℓ∗,k∗)1yˆ(xi)=j [+ 0]

_i=1_

X

_m−1_

= 1Ii=(ℓ∗,k∗)1yˆ(xi)=j [+][ 1]Ii=(ℓ[∗],k[∗])[1]yˆ(xm)=j

_i=1_

Xm

= 1Ii=(ℓ∗,k∗)1yˆ(xi)=j

_i=1_

X


where the first is due to that there is no update for this j, the third equation is due to the fact
that ˆy(xm) = j, and all the other equations are algebraic rewriting.
_̸_

If j = ˆy(xm), it is clear that


_m−1_

1 1
_Ii=(ℓ∗,k∗)_ _yˆ(xi)=j_ [+ 1]
_i=1_

X


_HHH_ _ℓ∗,k∗,j,m+1 =HHH_ _ℓ∗,k∗,j,m + 1 =_


_m−1_

= 1Ii=(ℓ∗,k∗)1yˆ(xi)=j [+][ 1]Ii=(ℓ[∗],k[∗])[1]yˆ(xm)=j

_i=1_

Xm

= 1Ii=(ℓ∗,k∗)1yˆ(xi)=j

_i=1_

X

where the first is due to that there is no update for this j, the third equation is due to the fact
that ˆy(xm) = j, and all the other equations are algebraic rewriting.

That is to say, we have shown that,


1
_Ii=(ℓ,k)_
_i=1_

X


_NNN ℓ,k,m+1 =_


1 1
_Ii=(ℓ,k)_ _yˆ(xi)=j_
_i=1_

X


_µµµˆℓ,k,j,m+1 =_


_NNN ℓ,k,m+1_


1 1
_Ii=Ij_ =(ℓ,k) _yˆ(xi)=ˆy(xj_ )
_j=1X,j≠_ _i_


_σσσˆℓ,k,m+1 = 1 −_


_NNN ℓ,k,m+1(NNN ℓ,k,m+1_ 1)
_−_


_i=1_


_HHH_ _ℓ,k,j,m+1 =_ 1Ii=(ℓ,k)1yˆ(xi)=j

_i=1_

X

always hold. By induction, we can say that for any n > 2LK, the original equations hold, which
completes the proof.

**Lemma 6. Suppose that the event A holds. Set δ = 2e[−][a]. Then for each ℓ, k, we have**


log 2/δ

∆min _N_ _[−]_ [1]4


1 + 4LKN _[−][1]_ +




_NNN_ _ℓ,k[∗]_


_NNN ℓ,k_


_σσσmin_


_for any 1 ≤_ _ℓ_ _≤_ _L, 1 ≤_ _k ≤_ _K._


_Proof. To show this, let us first establish the following useful lemma._

**Lemma 7. Suppose that the event A holds. If Algorithm 1 draws at least one sample from Dℓ0,k0**
_after the first 2LK iterations, we must have, for every ℓ, k,_

_−1_

_NNN ℓ,k_ (NNN ℓ0,k0 1) _σσσℓ,k_ _pppℓ,k_ _σσσℓ0,k0 + 2_ 4 log 2/δ
_≥_ _−_ _pppℓ0,k0_ s 2(NNN ℓ0,k0 ‘1) !

_−_


-----

_Proof. Since the event A holds, we have_


_t_ _t_

1 4 log 2/δ
1 1zzzℓ,k,i=zzzℓ,k,j _σσℓ,k_

 vuu _−_ _t(t −_ 1) Xi=1 _j=1X,j≠_ _i_ _[−]_ _[σ]_ _≤_ r 2t 

t

for every ℓ, k, t. Since this holds for every fixed t, it should also holds for any random variable t.

 

Specifically, we must have


_NNN ℓ,k,n_

1
1
v _−_ _NNN ℓ,k,n(NNN ℓ,k,n_ 1)
u _i=1_
u _−_ X
t

Note that, by definition,


_NNN ℓ,k,n_

1zzzℓ,k,i=zzzℓ,k,j _σσℓ,k_
_j=1X,j≠_ _i_ _[−]_ _[σ]_


log 2/δ

2NNN ℓ,k,n


_NNN ℓ,k,n_ _NNN ℓ,k,n_

1
_zzzℓ,k,i=zzzℓ,k,j_

Xi=1 _j=1X,j≠_ _i_


1

_σσσˆℓ,k,n =_ 1

v _−_ _NNN ℓ,k,n(NNN ℓ,k,n_ 1)
u
u _−_
t

We can then rewrite the above inequality as

_σσσˆℓ,k,n_ _σσσℓ,k_ 4
_|_ _−_ _| ≤_


log 2/δ

2NNN ℓ,k,n


That is to say,


log 2/δ

_σσσˆℓ,k,n_ _σσσℓ,k +_
2NNN ℓ,k,n _≤_ _≤_


log 2/δ

2NNN ℓ,k,n

log 2/δ

2NNN ℓ,k,n

_σσσℓ,k + 2_


_σσσℓ,k_
_−_


log 2/δ

2NNN ℓ,k,n [to both sides, this becomes]


Adding


log 2/δ

_σσσℓ,k + 2_
2NNN ℓ,k,n _≤_


_σσσℓ,k ≤_ _σσσˆℓ,k,n +_ s4

Multiplying both sides by _NNNppp ℓ,k,nℓ,k_ [, we have]


_NNNppp ℓ,k,nℓ,k_ _σσσℓ,k ≤_ _NNNppp ℓ,k,nℓ,k_


log 2/δ

2NNN ℓ,k,n


_pppℓ,k_

_NNN ℓ,k,n_


log 2/δ

2NNN ℓ,k,n


(B.1)


_σσσˆℓ,k,n +_


which holds for any ℓ, k, n. Note that N > 2LK, there must exist some ℓ0, k0, such that Algorithm
1 draws a sample from the data partition Dℓ0,k0 after the first 2LK iterations. Suppose the last
time a sample is drawn fromn0 + 1, · · ·, N . Since Algorithm 1 chooses Dℓ0,k0 is n0 > ℓ 20, kLK0 at iteration. That is to say, n0, by line 11 in Algorithm 1, we have NNN ℓ0,k0,n0 = NNN ℓ0,k0,n − 1, ∀n =


_ℓ0, k0 = arg max_ _pppℓ,k_ (ˆσσσℓ,k,n0 +

_NNN ℓ,k,n0_

By definition of arg max, we have


log 2/δ

2NNN ℓ,k,n0


_pppℓ0,k0_ (ˆσσσℓ0,k0,n0 +

_NNN ℓ0,k0,n0_


log 2/δ ) _pppℓ,k_ (ˆσσσℓ,k,n0 +

2NNN ℓ0,k0,n0 _≥_ _NNN ℓ,k,n0_


log 2/δ

2NNN ℓ,k,n0


Setting n = n0 in the first half of inequality B.1, we have


_pppℓ,k_

_NNN ℓ,k,n0_


log 2/δ

2NNN ℓ,k,n0


_NNNppp ℓ,k,nℓ,k_ 0 _σσσℓ,k,n0_


_σσσˆℓ,k,n0 +_


Combining the above two inequalities gives

_pppℓ0,k0_ (ˆσσσℓ0,k0,n0 +

_NNN ℓ0,k0,n0_


2NNNlog 2 ℓ0,k/δ0,n0 ) ≥ _NNNppp ℓ,k,nℓ,k_ 0 _σσσℓ,k_


-----

Noting that by definition,the above inequality becomes NNN ℓ,k,n0 ≤ _NNN ℓ,k,N = NNN ℓ,k, we can lower bound 1/NNN ℓ,k,n0 by 1/NNN ℓ,k, and_


_pppℓ0,k0_ (ˆσσσℓ0,k0,n0 +

_NNN ℓ0,k0,n0_


2NNNlog 2 ℓ0,k/δ0,n0 ) ≥ _NNN[p]ppℓ,k ℓ,k_ _σσσℓ,k_


Now setting n = n0, ℓ = ℓ0, k = k0 in the second half of inequality B.1, we have


_pppℓ0,k0_

_NNN ℓ0,k0,n_


log 2/δ

2NNN ℓ0,k0,n0


_pppℓ0,k0_

_NNN ℓ0,k0,n0_


log 2/δ

2NNN ℓ0,k0,n0


_σσσˆℓ0,k0,n +_


_σσσℓ0,k0 + 2_


Combining the above two inequalities, we have

_pppℓ0,k0_ (σσσℓ0,k0 + 2

_NNN ℓ0,k0,n0_


2NNNlog 2 ℓ0,k/δ0,n0 ) ≥ _NNN[p]ppℓ,k ℓ,k_ _σσσℓ,k_


Observe that n0 is the last time a sample is drawn from partition Dℓ0,k0, we have NNN ℓ0,k0,n0 =
_NNNNNN ℓ ℓ00,k,k00,n,n −0 by1, N ∀NNn ℓ =0,k n0 −0 +11 in the above inequality, we get, · · ·, N_ . Specifically, NNN ℓ0,k0,n0 = NNN ℓ0,k0,N − 1 = NNN ℓ0,k0 − 1. Replacing


_pppℓ0,k0_ _σσℓ0,k0 + 2_

_NNN ℓ0,k0_ 1 [(][σ]
_−_


2(NNNlog 2 ℓ0,k0/δ ‘1) [)][ ≥] _NNN[p]ppℓ,k ℓ,k_ _σσσℓ,k_
_−_


which holds for every ℓ, k. Rearranging the terms completes the proof.

Now we are ready to prove the bound on NNN ℓ,k − _NNN_ _ℓ,k[∗]_ [.]

Let us first consider the lower bound. By definition, we have


_NNN ℓ,k = N_
_k=1_

X


_ℓ=1_

Subtracting 2 from each element, we have


_K_

(NNN ℓ,k 2) = N 2LK = _[N][ −]_ [2][LK]
_k=1_ _−_ _−_ _N_

X


_ℓ=1_


Note that by definition, N = _ℓ=1_ _Kk=1_ _[N]NN_ _ℓ,k[∗]_ [. We can now replace the second][ N][ in the above]
equality, and obtain

[P][L] P

_L_ _K_ _L_ _K_ _L_ _K_ _NNN_ _ℓ,k[∗]_ [(][N][ −] [2][LK][)]

_ℓ=1_ _k=1(NNN ℓ,k −_ 2) = _[N][ −]N[2][LK]_ _N =_ _[N][ −]N[2][LK]_ _ℓ=1_ _k=1_ _NNN_ _ℓ,k[∗]_ [=] _ℓ=1_ _k=1_ _N_

X X X X X X


Now let us consider two cases.

_NNN_ _ℓ,k[∗]_ [(][N] _[−][2][LK][)]_
(i) Assume NNN ℓ,k 2 _N_
_−_ _≥_


_NNN_ _ℓ,k[∗]_ [(][N] _[−][2][LK][)]_
. That is to say, NNN ℓ,k _N_
_≥_


+ 2. Then we have


_NNN_ _ℓ,k[∗]_ [(][N] _[−][2][LK][)]_


_NNN ℓ,k_


+ 2


-----

subtracting


1

_NNN_ _ℓ,k[∗]_ [from both sides, we get]


_NNN_ _ℓ,k[∗]_


_NNN_ _ℓ,k[∗]_ [(][N]N[−][2][LK][)] + 2 _−_ _NNN_ _ℓ,k[∗]_

_NNN_ _ℓ,k[∗]_ [(][N] _[−][2][LK][)]_
_NNN_ _ℓ,k[∗]_ _[−]NNN_ _ℓ,k[∗]_ [(][N]N[−][2][LK][)] _−_ 2

_NNN_ _ℓ,k[∗]_ _[·][ (]_ _NNN_ _ℓ,k[∗]_ [(][N]N[−][2][LK][)]+ 2)

_NNN_ _ℓ,k[∗]_ _[−]NNN_ _ℓ,k[∗]_ [(][N]N[−][2][LK][)]

_NNN_ _ℓ,k[∗]_ _[·][ (]2LKNNN_ _ℓ,k[∗]N_ )


_NNN ℓ,k_


_NNN_ _ℓ,k[∗]_ [(][N] _[−][2][LK][)]_
_NNN_ _ℓ,k[∗]_ _[·][ (]_ _N_

2LK


_NNN_ _ℓ,k[∗]_ [(][N][ −] [2][LK][)]

where the last inequality is simply by removing the constant 2. Now by assumption, N > 4LK, we
have N − 2LK < 2[1] _[N]_ [. The above inequality can be further simplified as]


1 2LK

_NNN_ _ℓ,k[∗]_ _≤_ _NNN_ _ℓ,k[∗]_ [(][N][ −] [2][LK][)][ ≤] _NNN[4][LK]ℓ,k[∗]_ _[N]_


_NNN ℓ,k_


By definition, we have NNN _ℓ,k[∗]_ [=][ N] [∆][ℓ,k][ ≤] _[N]_ [∆][min][. Therefore, we have]


1 2LK

_NNN_ _ℓ,k[∗]_ _≤_ _NNN_ _ℓ,k[∗]_ [(][N][ −] [2][LK][)][ ≤] _NNN[4][LK]ℓ,k[∗]_ _[N]_


_NNN ℓ,k_ _−_

1
_≤_ _NNN_ _ℓ,k[∗]_


That is to say,

And thus, apparently,

1

_NNN ℓ,k_


1 + [4][LK]


_NNN_ _ℓ,k[∗]_


_NNN ℓ,k_


log 2/δ

∆min _N_ _[−]_ [1]4


1 + 4LKN _[−][1]_ +




_σσσmin_


_NNN_ _ℓ,k[∗]_ [(][N] _[−][2][LK][)]_
(ii) Assume NNN ℓ,k 2 < _N_ . Then there must exists some ℓ0, k0 such that NNN ℓ0,k0 2 >

_NNN_ _ℓ[∗]0_ _,k0_ [(][N] _[−][2][LK][)]_ _−_ _−_

_N_ _> 0. That is to say, Algorithm 1 draws at least one sample from Dℓ0,k0 after the first_
2LK iterations. By Lemma 7, we must have


_−1_
!


_NNN ℓ,k ≥_ (NNN ℓ0,k0 − 1) _σσσℓ,k_ _ppppppℓ0ℓ,k,k0_


log 2/δ

2(NNN ℓ0,k0 ‘1)
_−_


_σσσℓ0,k0 + 2_


_NNN_ _ℓ[∗]0_ _,k0_ [(][N] _[−][2][LK][)]_
_NNN ℓ0,k0_ 2 > _N_
_−_


implies


_NNN_ _ℓ[∗]0,k0_ [(][N][ −] [2][LK][)]
_NNN ℓ0,k0_ 1 > NNN ℓ0,k0 2 >
_−_ _−_ _N_

Therefore, we can use this lower bound on NNN ℓ0,k0 1 in the above inequality and obtain
_−_


-----

_−1_

_−1_


_NNN_ _ℓ[∗]0,k0_ [(][N][ −] [2][LK][)]
_NNN ℓ,k_
_≥_ _N_


_σσσℓ,k_ _ppppppℓ0ℓ,k,k0_


log 2/δ

_NNN_ _ℓ[∗]0_ _,k0_ [(][N] _[−][2][LK][)]_
2 _N_

log 2/δ

_NNN_ _ℓ[∗]0_ _,k0_ [(][N] _[−][2][LK][)]_

_N_

_−1_


_σσσℓ0,k0 + 2_ [4]

 v

u
u

 t


_NNN_ _ℓ[∗]0,k0_ [(][N][ −] [2][LK][)]


_σσσℓ,kpppℓ,k_

_σσσℓ0,k0pppℓ0,k0_


1 +






_σσσℓ0,k0_

log 2/δ


_NNN_ _ℓ,k[∗]_ [(][N][ −] [2][LK][)]


_ℓ,k_ _[ −]_ 2 log 2/δ
= 1 + 4

_N_  _σσσℓ0,k0_ vu 2 _NNN_ _ℓ[∗]0_ _,k0_ [(]N[N] _[−][2][LK][)]_ 

u

 t 

where the first equality is by dividing σσσℓ0,k0 at both denominator and numerator, and the second
equality uses the fact that NNN _ℓ,k[∗]_ [is proportional to][ p][ℓ,k][σ]σσℓ,k. Taking inverse of the above inequality
gives


1 _N_ 2 log 2/δ

1 + 4

_NNN ℓ,k_ _≤_ _NNN_ _ℓ,k[∗]_ [(][N][ −] [2][LK][)]  _σσσℓ0,k0_ vu 2 _NNN_ _ℓ[∗]0_ _,k0_ [(]N[N] _[−][2][LK][)]_

u

 t

Now let us simplify this inequality. Let us first expand all terms and obtain


_N_ 2 log 2/δ

1 + 4

_NNN_ _ℓ,k[∗]_ [(][N][ −] [2][LK][)]  _σσσℓ0,k0_ vu 2 _NNN_ _ℓ[∗]0_ _,k0_ [(]N[N] _[−][2][LK][)]_ 

u

 t 

1 2LK _N_ 2 log 2/δ

+ 4
_NNN_ _ℓ,k[∗]_ _NNN_ _ℓ,k[∗]_ [(][N][ −] [2][LK][) +] _NNN_ _ℓ,k[∗]_ [(][N][ −] [2][LK][)][ ·] _σσσℓ0,k0_ vu 2 _NNN_ _ℓ[∗]0_ _,k0_ [(]N[N] _[−][2][LK][)]_

u

5 t

1 + 2LK 2 4 _N_ log 2/δ

_NNN_ _ℓ,k[∗]_ _NNN_ _ℓ,k[∗]_ [(][N][ −] [2][LK][) +] _σσσℓ0,k0_ s _N −_ 2LK  2NNN _ℓ,k[∗][4]_ _[N]NN_ _ℓ[∗]0,k0_


_NNN ℓ,k_


For the second term, by assumption, N > 4LK and thus N − 2LK > 1/2N, we have


2LK

_N −_ 2LK _[≤]_ [4][LK]N


Thus the above equation becomes


2 4

_σσσℓ0,k0_

s


5
log 2/δ
 2NNN _ℓ,k[∗][4]_ _[N]NN_ _ℓ[∗]0,k0_


1 + [4][LK]

_NNN_ _ℓ,k[∗]_ _NNN_ _ℓ,k[∗]_ _[N][ +]_


_NNN ℓ,k_


_N −_ 2LK


For the third term, N > 4LK also implies

_N_ 2LK 2LK

_N_ 2LK [= 1 +] _N_ 2LK [<][ 1 +] 4LK 2LK [= 2]
_−_ _−_ _−_

Thus the above inequality can be further simplified as


1 + [4][LK]

_NNN_ _ℓ,k[∗]_ _NNN_ _ℓ,k[∗]_ _[N][ +]_


log 2/δ

_NNN_ _ℓ,k[∗][4]_ _[N]NN_ _ℓ[∗]0,k0_

log 2/δ

_NNN_ _ℓ[∗]0,k0_ #


_NNN ℓ,k_


_σσσℓ0,k0_

4

_σσσℓ0,k0_


1 + [4][LK]


_NNN_ _ℓ,k[∗]_


-----

Now by definition,above inequality _σσσℓ0,k0 ≥_ _σσσmin, and NNN_ _ℓ[∗]0,k0_ [=][ N] [∆][ℓ]0[,k]0 _[≥]_ _[N]_ [∆][min][, we can further simplify the]


1 + [4][LK]

_N_

"

1 + [4][LK]

 _N_



1 + [4][LK]

 _N_




log 2/δ

_NNN_ _ℓ[∗]0,k0_

log 2/δ

_N_ ∆min

log 2/δ

_N_ ∆min


_NNN_ _ℓ,k[∗]_

1

_NNN_ _ℓ,k[∗]_

1

_NNN_ _ℓ,k[∗]_


_NNN ℓ,k_ _≤_

_≤_

_≤_

1 1

_NNN ℓ,k_ _≤_ _NNN_ _ℓ,k[∗]_


_σσσℓ0,k0_

4

_σσσℓ0,k0_

4

_σσσmin_

4

_σσσmin_


That is to say,


log 2/δ

∆min _N_ _[−]_ [1]4


1 1 4 4 log 2/δ

_NNN ℓ,k_ _≤_ _NNN_ _ℓ,k[∗]_ 1 + 4LKN _[−][1]_ + _σσσmin_ s ∆min _N_ _[−]_ [1]4 

 _NNN_ _ℓ,k[∗]_ [(][N] _[−][2][LK][)]_ 

That is to say, no matter NNN ℓ,k 2 < _N_ or not, this inequality always holds, which

completes the proof. _−_


Now we are ready to prove Theorem 2. Let us first note that the loss can be written as


_j=1_ _ppp[2]ℓ,k[E][[][µ]µµℓ,k,j −_ _µµµˆℓ,k,j][2]_

X

_L_ _NNN ℓ,k_


_LN =_


_ℓ=1_ _k=1_

_L_ _K_

_ℓ=1_ _k=1_

X X

_L_ _K_

_ℓ=1_ _k=1_

X X


1
_zzzℓ,k,t=j[)][2][1]A[]]_
_t=1_

X

_NNN ℓ,k_

1
_zzzℓ,k,t=j[)][2][1]A[C]_ []]
_t=1_

X


_j=1_ _ppp[2]ℓ,k[E][[(][µ]µµℓ,k,j −_

X

_L_

_j=1_ _ppp[2]ℓ,k[E][[(][µ]µµℓ,k,j −_

X


(B.2)

(B.3)

(B.4)


_NNN ℓ,k_

1

_NNN ℓ,k_


Let us first consider the first term.


_j=1_ _ppp[2]ℓ,k[E][[(][µ]µµℓ,k,j −_ _µµµˆℓ,k,j)[2]1A]_

X

_L_ _NNN ℓ,k_


_ℓ=1_

_L_

_ℓ=1_

X

_L_

_ℓ=1_

X


_k=1_

_K_

_k=1_

X

_K_

_k=1_

X


1
_zzzℓ,k,t=j[)][2][1]A[]]_
_t=1_

X

_NNN ℓ,k_


_j=1_ _ppp[2]ℓ,k[E][[(][µ]µµℓ,k,j −_

X


_NNN ℓ,k_


1

= _ppp[2]ℓ,k[E]_ _NNN ℓ,kµµµℓ,k,j_ 1zzzℓ,k,t=j 1A

_ℓ=1_ _k=1_ _j=1_ _NNN_ [2]ℓ,k  _−_ _t=1_ 

X X X X

 

where we plug in the definition of ˆµµµ. By Lemma 6, we have the upper bound on   1/NNN ℓ,k


_ppp[2]ℓ,k[E]_
_j=1_

X


log 2/δ

∆min _N_ _[−]_ [1]4


1 1

_NNN ℓ,k_ _≤_ _NNN_ _ℓ,k[∗]_ 1 + 4LKN _[−][1]_ +

Therefore, we can use this inequality to obtain


_σσσmin_


_NNN ℓ,k_

_NNN ℓ,kµµµℓ,k,j_ 1zzzℓ,k,t=j

 _−_ _t=1_

X




1
_A_


_NNN_ [2]ℓ,k


_NNN ℓ,k_

1
_zzzℓ,k,t=j_
_t=1_

X


_NNN1ℓ,k[∗]_ + ∆[4][LK]min _N_ _[−][2]_ +


log 2/δ

∆[5]min _N_ _[−]_ 4[5] ][2]E


_NNN ℓ,kµµµℓ,k,j_

 _−_





1
_A_


_≤[_


_σσσmin_


-----

It is not hard to see that NNN ℓ,k is a stopping time. In fact, for any ℓ, k, and any time n, a new sample is
drawn purely based on estimated uncertainty score ˆσσσ and observed sample number NNN ℓ,k,n 1 up to
_−_
the current iteration, which is part of the history. As NNN ℓ,k < N is bounded, NNN ℓ,k is a stopping time.
Hence, we can apply Lemma 4, and obtain


2[]




_NNN ℓ,k_

E NNN ℓ,kµµµℓ,k,j − _t=1_ 1zzzℓ,k,t=j 1A _≤_ E

X

 

_≤E[NNN ℓ,k] Pr[zzzℓ,k,1 = j](1 −_ Pr[zzzℓ,k, 1 = j])


_NNN ℓ,k_

1
_zzzℓ,k,t=j_
_t=1_

X


_NNN ℓ,kµµµℓ,k,j_

 _−_


where the first inequality uses the fact that square term must be non-negative, and the second inequality
uses the fact that, for Bernoulli distribution with mean a, its variance is a(1 − _a). Applying this in_
inequality B.4, we have

2

_NNN ℓ,k_

 


_NNN ℓ,kµµµℓ,k,j_

 _−_


1
_zzzℓ,k,t=j_
_t=1_

X


1
_A_


_NNN_ [2]ℓ,k


_NNN ℓ,k_

1
_zzzℓ,k,t=j_
_t=1_

X


1

+ [4][LK] _N_ +
_NNN_ _ℓ,k[∗]_ ∆min _[−][2]_

1

+ [4][LK] _N_ +
_NNN_ _ℓ,k[∗]_ ∆min _[−][2]_


log 2/δ

∆[5]min _N_ _[−]_ 4[5] ][2]E


1
_A_


_NNN ℓ,kµµµℓ,k,j_

 _−_


_≤[_

_≤[_


_σσσmin_

4

_σσσmin_


log 2/δ

∆[5]min _N_ _[−]_ 4[5] ][2]E[NNN ℓ,k] Pr[zzzℓ,k,1 = j](1 − Pr[zzzℓ,k,1 = j])


Now applying this in equality B.3, we get


_j=1_ _ppp[2]ℓ,k[E][[(][µ]µµℓ,k,j −_ _µµµˆℓ,k,j)[2]1A]_

X


_ℓ=1_

_L_

_ℓ=1_

X

_L_

_ℓ=1_

X

_L_

_ℓ=1_

X


_k=1_

_K_

_k=1_

X

_K_

_k=1_

X


_NNN ℓ,k_

1
_zzzℓ,k,t=j_
_t=1_

X


_ppp[2]ℓ,k[E]_
_j=1_

X


1

_NNN_ [2]ℓ,k NNN ℓ,kµµµℓ,k,j − _t=1_

X


 

_NNN1ℓ,k[∗]_ + ∆[4][LK]min _N_ _[−][2]_ + _σσσmin4_

_NNN1ℓ,k[∗]_ + ∆[4][LK]min _N_ _[−][2]_ + _σσσmin4_


1
_A_


log 2/δ

∆[5]min _N_ _[−]_ 4[5] ][2]E[NNN ℓ,k] Pr[zzzℓ,k,1 = j](1 − Pr[zzzℓ,k,1 = j])

log 2/δ

∆[5]min _N_ _[−]_ 4[5] ][2]E[NNN ℓ,k]


_ppp[2]ℓ,k[[]_
_j=1_

X


_ppp[2]ℓ,k[σ]σσ[2]ℓ,k[[]_
_k=1_

X


(B.5)


where the last equation uses the fact that σσσℓ,k = 1 − [P]j[L]=1 [Pr][2][[][z]zzℓ,k,1 = j] = _j=1_ [Pr[][z]zzℓ,k,1 =
_j](1 −_ Pr[zzzℓ,k,1 = j]). Applying the inequality 1/(1 + x) ≤ 1 − _x_

[P][L]


log 2/δ

∆min


1 + 4LKN _[−][1]_ +




_NNN_ _ℓ,k[∗]_


_NNN ℓ,k_


_σσσmin_


-----

Note that


4 log 2/δ
s ∆min _N_ _[−]_ 4[1]

log 2/δ

∆min _N_ _[−]_ 4[1] 




4

1 + 4LKN _[−][1]_ + _σσσmin_




][2]E[NNN ℓ,k]





E[NNN ℓ,k]


_ppp[2]ℓ,k[σ]σσ[2]ℓ,k[[]_


_NNN_ _ℓ,k[∗]_


_ppℓ,kσσσℓ,k_
=([p] )[2]

_NNN_ _ℓ,k[∗]_


1 + 4LKN _[−][1]_ +




_σσσmin_


4 4 log 2/δ

=N _[−][2](ℓ[′],k[′]_ _pppℓ′,k′σσσℓ′,k′_ )[2] 1 + 4LKN _[−][1]_ + _σσσmin_ s ∆min _N_ _[−]_ 4[1]  E[NNN ℓ,k]
X

where the last equation is by definition of _NNN ℓ,k. Now applying this in inequality B.5, we have_


_j=1_ _ppp[2]ℓ,k[E][[(][µ]µµℓ,k,j −_ _µµµˆℓ,k,j)[2]1A]_

X


_ℓ=1_

_L_

_ℓ=1_

X

_L_

_ℓ=1_

X


_k=1_


1 4

+ [4][LK] _N_ +
_NNN_ _ℓ,k[∗]_ ∆min _[−][2]_ _σσσmin_


log 2/δ

∆[5]min _N_ _[−]_ 4[5] ][2]E[NNN ℓ,k]


_ppp[2]ℓ,k[σ]σσ[2]ℓ,k[[]_
_k=1_

X


log 2/δ

∆min _N_ _[−]_ [1]4


1 + 4LKN _[−][1]_ +




_N_ _[−][2](_ _pppℓ′,k′σσσℓ′,k′_ )[2]
_k=1_ _ℓ[′],k[′]_

X X


E[NNN ℓ,k]


_σσσmin_


(B.6)


log 2/δ

∆min _N_ _[−]_ [1]4

log 2/δ

∆min _N_ _[−]_ [1]4


1 + 4LKN _[−][1]_ +



1 + 4LKN _[−][1]_ +




=N _[−][2](_ _pppℓ′,k′σσσℓ′,k′_ )[2]

_ℓ[′],k[′]_

X

=N _[−][2](_ _pppℓ′,k′σσσℓ′,k′_ )[2]

_ℓ[′],k[′]_

X


E[NNN ℓ,k]
_k=1_

X


_σσσmin_

4

_σσσmin_


_ℓ=1_


4 4 log 2/δ

=N ( _pppℓ,kσσσℓ,k)[2]_ 1 + 4LKN + _N_ 4

_[−][1]_ _ℓ,k_  _[−][1]_ _σσσmin_ s ∆min _[−]_ [1] 
X

 

where the second equation uses the fact that only E[NNN ℓ,k] depends on ℓ, k, the third equation uses the
_K_ _K_
fact that _ℓ=1_ _k=1_ _[N]NN ℓ,k = N and thus_ _ℓ=1_ _k=1_ [E][[][N]NN ℓ,k] = N . Note that δ = L[−][1]K _[−][1]N_ _[−]_ 4[5],

we have

[P][L] P [P][L] P


log 2/δ

∆min _N_ _[−]_ 4[1]


4

_pppℓ,kσσσℓ,k)[2]_ 1 + 4LKN +

 _[−][1]_ _σσσmin_

_ℓ,k_

X

_pppℓ,kσσσℓ,k)[2][ h]1 + O(N_ 4 log 14 N )

_[−]_ [1]
_ℓ,k_

X 1 i

_pppℓ,kσσσℓ,k)[2]_ + O(N 4 log 4 N )

_[−]_ [5]
_ℓ,k_

X


_N_ _[−][1](_

=N _[−][1](_

=N _[−][1](_


Applying this back to inequality B.6, we have


1

_pppℓ,kσσσℓ,k)[2]_ + O(N _[−]_ [5]4 log 4 N ) (B.7)
_ℓ,k_

X


_j=1_ _ppp[2]ℓ,k[E][[(][µ]µµℓ,k,j −_ _µµµˆℓ,k,j)[2]1A] ≤_ _N_ _[−][1](_

X


_ℓ=1_


_k=1_


Now consider the second term in equation B.2. As µµµ and ˆµµµ are within {0, 1}, we have

_NNN ℓ,k_

1
(µµµℓ,k,j − _NNN ℓ,k_ _t=1_ 1zzzℓ,k,t=j[)][2][ ∈] [[0][,][ 1]]

X


-----

Therefore,

_L_ _K_ _L_ _NNN ℓ,k_ _L_ _K_ _L_

1
_ppp[2]ℓ,k[E][[(][µ]µµℓ,k,j_ 1zzzℓ,k,t=j[)][2][1]A[C] []][ ≤] _ppp[2]ℓ,k_ [Pr[][A][C][]]

_ℓ=1_ _k=1_ _j=1_ _−_ _NNN ℓ,k_ _t=1_ _ℓ=1_ _k=1_ _j=1_

X X X X X X X

By Lemma 3, the probability of A is at least 1 − _KLNδ. Hence, the probability of A[C]_ is at most
_KLNδ. Hence,_

_L_ _K_ _L_ _NNN ℓ,k_


1
_zzzℓ,k,t=j[)][2][1]A[C]_ []]
_t=1_

X


_j=1_ _ppp[2]ℓ,k[E][[(][µ]µµℓ,k,j −_

X

_L_

_ppp[2]ℓ,k_ [Pr[][A][C][]]
_j=1_

X

_L_

_ppp[2]ℓ,k[LKNδ]_
_j=1_

X


_NNN ℓ,k_


_ℓ=1_

_L_

_ℓ=1_

X

_L_

_ℓ=1_

X


_k=1_

_K_

_k=1_

X

_K_

_k=1_

X


_≤_ _LLLKNδ = L[2]KNδ_

_j=1_

X

_pwhere the last inequality uses the fact thatppℓ,k_ 0. Since δ = L[−][2]K _[−][1]N_ 4, we have _ℓ=1_ _Kk=1_ _[p]pp[2]ℓ,k_ _[≤]_ [1][ since][ P]ℓ[L]=1 _Kk=1_ _[p]ppℓ,k = 1 and_

_[−]_ [9] P P
_≥_

_L_ _K_ _L_ [P][L] _NNN ℓ,k_


1
_zzzℓ,k,t=j[)][2][1]A[C]_ []]
_t=1_

X


_j=1_ _ppp[2]ℓ,k[E][[(][µ]µµℓ,k,j −_

X


_NNN ℓ,k_


_ℓ=1_


_k=1_


_≤L[2]KNδ ≤_ _N_ _[−]_ 4[5]

Applying this as well as inequality B.7 to the equation B.2, we have


_j=1_ _ppp[2]ℓ,k[E][[][µ]µµℓ,k,j −_ _µµµˆℓ,k,j][2]_

X

_L_ _NNN ℓ,k_


_LN =_


_ℓ=1_

_L_

_ℓ=1_

X

_L_

_ℓ=1_

X


_k=1_

_K_

_k=1_

X

_K_

_k=1_

X


1
_zzzℓ,k,t=j[)][2][1]A[]]_
_t=1_

X

_NNN ℓ,k_

1
_zzzℓ,k,t=j[)][2][1]A[C]_ []]
_t=1_

X


_j=1_ _ppp[2]ℓ,k[E][[(][µ]µµℓ,k,j −_

X

_L_

_j=1_ _ppp[2]ℓ,k[E][[(][µ]µµℓ,k,j −_

X


_NNN ℓ,k_

1

_NNN ℓ,k_


1

_N_ ( _pppℓ,kσσσℓ,k)[2]_ + O(N 4 log 4 N ) + N 4
_≤_ _[−][1]_ _[−]_ [5] _[−]_ [5]

_ℓ,k_

X

Note that the loss of the optimal allocation is simply LN[∗] [=][ N][ −][1][(][P]ℓ,k _[p]ppℓ,kσσσℓ,k)[2]. The above_

inequality is simply

1

_N_ _N_ 4 log 4 N )
_L_ _−L[∗]_ _[≤]_ _[O][(][N][ −]_ [5]

which completes the proof.

C EXPERIMENTAL DETAILS

**Experimental Setups.** All experiments were run on a machine with 2 E5-2690 v4 CPUs, 160 GB
RAM and 500 GB disk with Ubuntu 18.04 LTS as the OS. Our code is implemented and tested in
python 3.7. All experimental results were averaged over 1500 runs, except the case study. Overall
the experiments took about two month, including debugging and evaluation on all datasets. Running
MASA once to draw a few thousand samples typically only takes a few seconds. Our implementation
is purely in Python for demonstration purposes, and more code optimization (e.g., using cython or
multi-thread) can generate a much faster implementation.


-----

|Col1|Col2|Tab|ble 2: Dataset statistics.|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|Dataset|Size|# Classes|Dataset|Size|# Classes|Tasks|
|FER+|6358|7|RAFDB (Li et al.)|15339|7|FER|
|EXPW|31510|7|AFFECTNET|87401|7||
|YELP|20000|2|SHOP|62774|2|SA|
|IMDB|25000|2|WAIMAI|11987|2||
|DIGIT|2000|10|AUDIOMNIST|30000|10|STT|
|FLUENT|30043|31|COMMAND|64727|31||


Table 3: ML services used for each task. Price unit: USD/10,000 queries. We consider three tasks,
sentiment analysis (SA), facial emotion recognition (FER), and spoken command recognition (SCR)

|Col1|Col2|Col3|.|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|Tasks|ML service|Price|ML service|Price|ML service|Price|
|SA|Google NLP (GoN)|2.5|AMZN Comp (Ama)|0.75|Baidu NLP (Bai)|3.5|
|FER|Google Vision (Goo, a)|15|MS Face (Mic, a)|10|Face++ (Fac)|5|
|SCR|Google Speech (Goo, b)|60|MS Speech (Mic, b)|41|IBM Speech (IBM)|25|



**ML APIs and Dataset Statistics.** We focus on three common classification tasks, namely, sentiment
analysis, facial emotion recognition, and spoken command recognition. For each of the tasks, we
evaluated three APIs’ performance in spring 2020 and spring 2021, respectively, for four datasets.
The details of datasets and ML APIs are summarized in Table 2 and Table 3 respectively. Now we
give more context of the datasets.

For sentiment analysis, we use four datasets, YELP, IMDB, SHOP, and WAIMAI. YELP and IMDB
are both English text datasets. YELP (Dat, c) is generated by drawn twenty thousand samples from
the large YELP review challenge dataset. Each original review is labeled by rating in {1,2,3,4,5}.
We generate the binary label by transforming rating 1 and 2 into negative, and rating 4 and 5 into
positive. Ten thousand positive reviews and ten thousand negative reviews are then randomly drawn,
respectively. IMDB (Maas et al.) is a polarized sentiment analysis dataset with provided training
and testing partitions. We use its testing partition which has twenty-five thousand text paragraphs.
SHOP (Dat, a) and WAIMAI (Dat, b) are two Chinese text datasets. SHOP contains polarized
labels for reviews for various purchases including fruits, hotels, computers. WAIMAI is a dataset
for polarized delivery reviews. Both SHOP and WAIMAI are publicly available without licence
requirements. There is a dataset user agreement for YELP dataset, which disallows commercial usage
of the datasets but encourages academic study. Same thing applies to the IMDB dataset.

For facial emotion recognition, we use four datasets: FER+, RAFDB, EXPW, and AFNET. All
the datasets are annotated by the standard seven basic emotions, i.e., {anger, disgust, fear, happy,
sad, surprise, neutral}. The images in FER+ (Goodfellow et al., 2015) are from the ICML 2013
Workshop on Challenges in Representation. We use the provided testing portion in FER+. RAFDB
(Li et al.) and AFFECTNET (Mollahosseini et al., 2019) were annotated with both basic emotions
and fine-grained labels. In this paper, we only use basic emotions since commercial APIs cannot
work for compound emotions. EXPW (Zhang et al.) contains raw images and bound boxes pointing
out the face locations. Here we use the true bounding box associated with the dataset to create aligned
faces first, and only pick the images that are faces with confidence larger than 0.6. We cotnacted the
creators of RAFDB and AFNET to obtain the data access for academic purposes. FER+ and EXPW
are both publicly available online without consent or licence requirements.

For spoken command recognition, we use DIGIT, AMNIST, CMD, and FLUENT. DIGIT (Dat, d)
and AMNIST (Becker et al., 2018) are spoken digit datasets, where the label is is a spoken digit (i.e.,
0-9). The sampling rate is 8 kHz for DIGIT and 48 kHz for AMNIST. Each sample in CMD (Warden,
2018) is a spoken command such as “go”, “left”, “right”, “up”, and “down”, with a sampling rate of
16 kHz. In total, there are 30 commands and a few white noise utterances. FLUENT (Lugosch et al.)
is another recently developed dataset for speech command. The commands in FLUENT are typically


-----

Overall accuracy:79.1

positive negative

|44.1|5.9|
|---|---|
|15.0|35.0|


Predicted label


Overall accuracy:78.0

positive negative

|39.0|11.0|
|---|---|
|11.0|39.0|


Predicted label


Overall change:-1.1

positive negative

|-5.1|+5.1|
|---|---|
|-4.0|+4.0|


Predicted label


(a) Amazon IMDB 2020

Overall accuracy:81.4


(b) Amazon IMDB 2021

Overall accuracy:84.4


(c) Amazon IMDB 2021

Overall change:+3.0


anger


anger


anger


disgust

fear


disgust

fear


disgust

fear


anger disgust fear happy sad surprise neutral

|5.9|0.0|0.1|0.3|0.2|0.1|2.1|
|---|---|---|---|---|---|---|
|0.1|0.3|0.0|0.0|0.0|0.0|0.1|
|0.0|0.0|1.2|0.1|0.1|0.3|0.4|
|0.1|0.0|0.0|23.9|0.2|0.3|3.3|
|0.2|0.0|0.1|0.3|6.0|0.1|4.8|
|0.1|0.0|0.2|0.4|0.0|10.3|1.8|
|0.2|0.0|0.0|1.2|0.6|0.6|33.7|


Predicted label


happy


happy

sad


sad

surprise

neutral


anger disgust fear happy sad surprise neutral

|+0.3|+0.0|-0.0|-0.1|-0.0|-0.0|-0.1|
|---|---|---|---|---|---|---|
|+0.0|+0.0|+0.0|+0.0|+0.0|+0.0|+0.0|
|+0.0|+0.0|+0.0|-0.0|+0.0|+0.0|-0.0|
|-0.0|+0.0|-0.0|+1.0|-0.2|-0.1|-0.6|
|-0.1|+0.0|+0.0|-0.2|+0.5|-0.0|-0.2|
|-0.0|+0.0|+0.0|-0.2|-0.0|+0.4|-0.2|
|-0.1|+0.0|-0.0|-0.4|-0.0|-0.3|+0.8|


Predicted label


anger disgust fear happy sad surprise neutral

|6.2|0.0|0.0|0.2|0.1|0.1|2.1|
|---|---|---|---|---|---|---|
|0.1|0.3|0.0|0.0|0.0|0.0|0.1|
|0.0|0.0|1.3|0.0|0.1|0.3|0.4|
|0.0|0.0|0.0|24.9|0.0|0.2|2.7|
|0.1|0.0|0.1|0.1|6.5|0.0|4.6|
|0.1|0.0|0.2|0.3|0.0|10.7|1.7|
|0.1|0.0|0.0|0.8|0.5|0.3|34.6|


Predicted label

(e) Microsoft FER+ 2021


(d) Microsoft FER+ 2020


(f) Microsoft FER+ API shift


Overall accuracy:30.1

0 1 2 3 4 5 6 7 8 9 ""

|5.9|0.0|0.0|0.0|0.3|0.0|0.0|0.0|0.0|0.0|3.8|
|---|---|---|---|---|---|---|---|---|---|---|
|0.0|3.3|0.1|0.0|0.0|0.0|0.0|0.0|0.3|0.0|6.3|
|0.0|0.0|3.8|0.0|0.0|0.0|0.0|0.0|0.0|0.0|6.2|
|0.0|0.0|0.0|3.1|0.0|0.0|0.0|0.1|0.1|0.0|6.8|
|0.0|0.1|0.0|0.1|1.5|0.0|0.0|0.0|0.0|0.0|8.5|
|0.0|0.1|0.0|0.0|0.0|1.2|0.4|0.0|0.0|0.1|8.2|
|0.0|0.0|0.1|0.1|0.0|0.0|3.4|0.4|0.0|0.0|6.1|
|0.0|0.0|0.1|0.0|0.0|0.0|0.2|1.9|0.0|0.0|7.8|
|0.1|0.0|0.0|0.1|0.0|0.0|0.0|0.0|0.9|0.1|8.9|
|0.0|0.0|0.3|0.0|0.0|0.0|0.0|0.0|0.1|5.1|4.5|
|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|


Predicted label

(g) Google DIGIT 2020


Overall accuracy:54.2

0 1 2 3 4 5 6 7 8 9 ""

|8.3|0.0|0.0|0.0|0.7|0.0|0.0|0.1|0.0|0.1|0.9|
|---|---|---|---|---|---|---|---|---|---|---|
|0.0|5.1|0.0|0.0|0.0|0.0|0.0|0.0|0.1|0.0|4.8|
|0.0|0.0|5.7|0.0|0.0|0.0|0.0|0.0|0.0|0.0|4.3|
|0.0|0.0|0.4|4.7|0.0|0.0|0.0|0.0|0.1|0.0|5.0|
|0.0|0.1|0.0|0.0|3.6|0.0|0.0|0.0|0.0|0.0|6.3|
|0.0|0.1|0.0|0.0|0.0|3.1|0.3|0.0|0.0|0.0|6.5|
|0.1|0.0|0.1|0.0|0.0|0.0|5.2|0.2|0.0|0.0|4.3|
|0.1|0.0|0.1|0.0|0.0|0.1|0.1|4.5|0.0|0.0|5.1|
|0.1|0.0|0.0|0.1|0.0|0.0|0.0|0.0|4.8|0.0|5.1|
|0.0|0.0|0.2|0.0|0.0|0.0|0.0|0.0|0.0|9.2|0.5|
|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|


Predicted label

(h) Google DIGIT 2021


Overall change:+24.2

0 1 2 3 4 5 6 7 8 9 ""

|+2.4|+0.0|+0.0|+0.0|+0.4|+0.0|+0.0|+0.1|+0.0|+0.1|-2.8|
|---|---|---|---|---|---|---|---|---|---|---|
|+0.0|+1.8|-0.1|+0.0|+0.0|+0.0|+0.0|+0.0|-0.2|+0.0|-1.6|
|+0.0|+0.0|+1.9|+0.0|+0.0|+0.0|+0.0|+0.0|+0.0|+0.0|-1.9|
|+0.0|+0.0|+0.4|+1.5|+0.0|+0.0|+0.0|-0.1|+0.0|+0.0|-1.8|
|+0.0|+0.0|+0.0|-0.1|+2.1|+0.0|+0.0|+0.0|+0.0|+0.0|-2.1|
|+0.0|-0.1|+0.0|+0.0|+0.0|+1.9|-0.1|+0.0|+0.0|-0.1|-1.7|
|+0.1|+0.0|+0.0|-0.1|+0.0|+0.0|+1.9|-0.1|+0.0|+0.0|-1.8|
|+0.1|+0.0|+0.0|+0.0|+0.0|+0.1|-0.1|+2.6|+0.0|+0.0|-2.7|
|+0.0|+0.0|+0.0|-0.1|+0.0|+0.0|+0.0|+0.0|+3.9|-0.1|-3.8|
|+0.0|+0.0|-0.1|+0.0|+0.0|+0.0|+0.0|+0.0|-0.1|+4.1|-4.0|
|+0.0|+0.0|+0.0|+0.0|+0.0|+0.0|+0.0|+0.0|+0.0|+0.0|+0.0|


Predicted label

(i) Google DIGIT 2021


Figure 6: Confusion matrices of a few APIs in spring 2020/2021, along with their API shifts.

a phrase (e.g., “turn on the light” or “turn down the music”). There are in total 248 possible phrases,
which are mapped to 31 unique labels. The sampling rate is also 16 kHz. All those datasets are freely
available online for academic purposes.

Some of the datasets may contain personal information. For example, the human faces contained in
the facial emotion recognition dataset may be deemed as personal information. On the other hand, our
study focuses on whether there is a performance change on the dataset, and does not use or disclose
any personal information.

For sentiment analysis, we use the Google NLP API (GoN), Amazon Comprehend API (Ama), and
the Baidu NLP API (Bai). For facial emotion recognition, we use Google Vision API (Goo, a),
Microsoft Face API (Mic, a), and the Face++ API (Fac). For spoken command recognition, we adopt
Google speech API (Goo, b), Microsoft Speech API (Mic, b), and IBM speech API (IBM).


**Details of observed ML API Shifts.** Now we present a few more observed ML API shifts, as
shown in Figure 6. One observation is that individual entry’s change in the API shift can be larger
than the overall accuracy’s. For example, as shown in Figure 6 (c), the overall accuracy change is
about -1.1% for Amazon on IDMB, but the performance drop for positive texts is as large as 5%. This
indicates the importance of using fine-grained confusion matrix difference to measure API shifts. In
addition, when the overall accuracy increases, it is possible that the accuracy for each label has been
improved. This can be easily verified by Figure 6 (d-f). On the other hand, as shown in Figure 6 (g-i),
Google API’s large accuracy improvement (24%) is mostly because it is able to correctly predict
many samples that were previously deemed as empty. One possible explanation is that Google API
internally uses a higher threshold to generate a recognition. When the number of label increases, it


-----

10

10

|Col1|K=1|Col3|
|---|---|---|
||K=2 K=3|K=2 K=3|


1000 2000

K=1
K=2
K=3

Sample size

(a) Amazon YELP


10 3

10 4

1000 2000

Sample size


10 3

1000 2000

Sample size


10 3

10 4

1000 2000

Sample size


(b) Amazon SHOP

10 3

10 4

1000 2000

Sample size


(f) Google EXPW+


(c) Amazon IMDB

10 3

10 4

1000 2000

Sample size


(g) IBM DIGIT


(d) Amazon WAIMAI

10 3

10 4

1000 2000

Sample size


(h) IBM AMNIST


10 3

10 4

1000 2000

Sample size


(e) Microsoft FER+

10 3

1000 2000

Sample size


(i) Google DIGIT


10 3

10 4

1000 2000

Sample size

(j) Google AMINST


10 3

10 4

1000 2000

Sample size


10 3

10 4

1000 2000

Sample size


(k) Google CMD


(l) Microsoft DIGIT


Figure 7: Effects of partition parameter K. The total number of partitions is LK, and thus Larger K
implies more partitions. Generally, across 12 cases where API shifts are identified, larger number of
partitions usually leads to smaller estimation error for large samples. In practice, we observe that
_K = 3 is enough to reach good error rate._

might become hard to manually check the API shifts. For those cases, an anomaly detector can be
applied to quickly identify the most surprising components in the API shifts.


**Partition size’s effects on MASA.** Now we study how the partition number affects the performance
of MASA, as shown in Figure 7. Across all API shifts we estimated, we note that larger number
of partitions leads to a smaller overall Frobenious norm in general. This is expected, as larger K
effectively introduces more parameters to estimate and thus is more powerful. The trade-off is that
the computational cost increases, and more samples are needed for initial estimation. Interestingly, as
_K becomes large, the relative error reduction improvement becomes small. This is probably because_
there is no strong uncertainty difference within small partitions. In practice, we found that K = 3
already gives a small enough error reduction.

**Comparison with baselines for case study on YELP.** To further understand MASA’s performance,
We compared the performance of MASA with two baselines: random sampling and standard stratified
sampling (proportionate allocation). We drawed 2000 samples for all methods, and repeated the
experiments 1000 times to obtain an average of the Frobenius norm error. MASA outperforms both
baselines significantly: the observed error is 0.015 for random sampling, 0.009 for stratified sampling,
and 0.006 for MASA.


**Understanding uncertainty score.** MASA is developed based on the notion of uncertainty scores,
and thus it is worthy understanding how uncertainty scores of different partitions for an ML API
are computed. Here, we provide an illustrative example, as shown in Figure 8. The dataset contains
three partitions and each partition includes six data points. We use a small ball to represent each data
point, its interior color to denote its true label, and its edge color to denote the predicted label of an
evaluated ML API. For example, on partition 1 and partition 3, all edge colors match interior colors,


-----

## Partition 3

 Accuracy: 1.0 Uncertainty: 0.67


## Partition 1 Partition 2


## Accuracy: 1.0 Uncertainty: 0.50


## Accuracy: 0.5 Uncertainty: 0.00


Figure 8: Illustrative examples of uncertainty scores. The dataset contains three partitions, each of
which includes six data points. Here we use a ball to represent a data point, its interior color to denote
its true label, and its edge color to indicate an ML API’s predicted label. For example, as shown in
the left panel, three points are labeled as red and the other three are labeled as blue. All points are
predicted correctly, and thus the accuracy is 1.0. As the ML API predicts half of the points as red and
half as blue, the uncertainty score is 1 − 0.5 × 0.5 − 0.5 × 0.5 = 0.5. Note that high accuracy does
not necessarily imply low uncertainty. For example, accuracy on partition 1 (1.0) is higher than that
on partition 2(0.5), but its uncertainty score is actually larger than the latter. Yet, high diversity in
the predicted labels does imply higher uncertainty. For example, while accuracy on partition 1 and
partition 3 are both perfect, partition 3 incurs a higher uncertainty. This is because while only two
unique predicted labels exist in partition 1, three occur in partition 3.

1

and thus the accuracy is 1.0. On partition 2, interior and edge colors match only on half of the points,
and thus the accuracy is only 0.5.

To understand the calculation of the uncertainty score, let us take partition 1 as an example. The
ML API predicts the label red for half of the partition and blue for the other half. Thus, the
uncertainty score is 1 subtracting the sum of the square of likelihood of each predicted label, i.e.,
1 − 0.5 × 0.5 − 0.5 × 0.5 = 0.5. Similarly, on partition 2, the ML API always predicts the label
blue, and thus the uncertainty is simply 1 − 1 = 0. On partition 3, the ML API evenly predicts three
unique labels, and thus the uncertainty score becomes 1 − 3[1] _[×][ 1]3_ _[−]_ 3[1] _[×][ 1]3_ _[−]_ 3[1] _[×][ 1]3_ [=][ 1]3 _[≈]_ [0][.][67][.]

Two observations are worthy mentioning about uncertainty scores, in addition to their non-negativity
and upper bound of 1. First, higher accuracy on a partition does not imply lower uncertainty. To
see this, note that the accuracy on partition 1 (1.0) is higher than that on partition 2 (0.5), but its
uncertainty score is actually larger than that of partition 2. In fact, an API’s accuracy on a partition
is orthogonal to its uncertainty, as uncertainty score only depends on the predicted labels and is
independent of the true labels. Second, diversity of the predicted labels is correlated to the uncertainty
score. For example, the accuracy is same on partition 1 and 3, but the uncertainty score is higher on
partition 3, mainly because there are three unique labels (red, blue, and green) in partition 3. This is
expected, as uncertainty scores are designed to capture how diverse an ML API’s prediction can be.


-----

