# LEARNING HOMOPHILIC INCENTIVES IN SEQUENTIAL SOCIAL DILEMMAS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Promoting cooperation among self-interested agents is a long-standing and interdisciplinary problem, but receives less attention in multi-agent reinforcement learning
(MARL). Game-theoretical studies reveal that altruistic incentives are critical to
the emergence of cooperation but their analyses are limited to non-sequential social
dilemmas. Recent works using deep MARL also show that learning to incentivize
other agents has the potential to promote cooperation in more realistic sequential
social dilemmas (SSDs). However, we find that, with incentivizing mechanisms,
the team cooperation level does not converge and regularly oscillates between
cooperation and defection during learning. We show that a second-order social
dilemma resulting from the incentive mechanisms is the main reason for such fragile cooperation. We analyze the dynamics of second-order social dilemmas and find
that a typical tendency of humans, called homophily, provides a promising solution.
We propose a novel learning framework to encourage homophilic incentives and
show that it achieves stable cooperation in both SSDs of public goods and tragedy
of the commons.

1 INTRODUCTION

Deep multi-agent reinforcement learning (MARL) has achieved prominent progress (Sunehag et al.,
2018; Rashid et al., 2018; Baker et al., 2020; Wang et al., 2021). While much effort is devoted to fully
cooperative settings, decision-making individuals in many real-world situations may be self-interested,
such as autonomous vehicles (Bonnefon et al., 2016), taxpayers (Rothstein, 2001), and governments
dealing with climate change (Capstick, 2013). A unique phenomenon among self-interested agents is
social dilemma, in which individually rational behavior results in a situation where everyone suffers
in the long run. Historically, the problem of how cooperation emerges in social dilemmas has long
perplexed scientists from various fields, such as evolutionary biology, animal behavioristics (Packer
et al., 1991), and neuroscience (Rilling et al., 2002), and is listed as one of the 125 most compelling
science questions for the 21st century (Kennedy, 2005; Pennisi, 2005).

Cooperation emergence in non-sequential social dilemmas (SDs) have been extensively studied (Boyd,
1989; Rand & Nowak, 2011). However, these methods only consider the problems that can be cast
as a matrix game (Hughes et al., 2018), and typically assume the existence of atomic actions for
cooperative and defective strategies. Thus their applicability to real-world, temporally extended SDs
is limited, where cooperation and defection are policies that need to be learned. With the ability to
learn policies in complex tasks, deep MARL shows promise to fill the gap and study how cooperation
emerges in these more realistic sequential social dilemmas (SSDs) (Leibo et al., 2017).

Some previous works have studied SSDs from the perspective of MARL (Jaques et al., 2019; Hughes
et al., 2018). Among them, Yang et al. (2020) and Lupu & Precup (2020) extend (altruistic) incentives
to temporally extended cases. Referring to individuals paying costs to punish or reward others even
though there is no immediate gain by these actions for themselves (Ostrom et al., 1992; Güth, 1995;
Fehr & Gächter, 2002; Boyd et al., 2003; Mussweiler & Ockenfels, 2013), altruistic incentives have
been proven to be highly related to cooperation emergence in non-sequential SDs (Fowler, 2005;
Akçay & Roughgarden, 2011; Fong & Surti, 2009). However, altruistic incentives have not been fully
studied in SSDs. The aforementioned deep methods do not effectively scale up and are limited in
very small settings. With further investigation, we find that these methods do not converge to stable
cooperation, where the cooperation level regularly oscillates between cooperation and defection.


-----

In this paper, we first analyze the underlying reasons behind the phenomenon of such unstable
cooperation. We find that, although incentives make cooperation more likely to emerge, they also
introduce a second-order social dilemma (2nd-SDs) problem into the system – if someone else
would pay costs to punish or reward others, why should I bother to do so (Dreber et al., 2008)? The
consequence is that more and more second-order free-riders would exploit those agents who make
an effort to incentivize others, resulting in a degraded incentivizing mechanism and thus collapsed
cooperation. 2nd-SDs have been well studied in non-sequential social dilemmas (Fowler, 2005;
Greenwood, 2016). However, solving 2nd-SDs in sequential settings remains largely untouched in the
literature and posts new challenges – the identification of cooperation and defection for incentivizing
is non-trivial in SSDs and cooperation is more vulnerable to 2nd-SDs in SSDs than in SDs. Thus SSDs
require a different computational approach to promote cooperation. Specifically, in SSDs, agents
need to learn temporally extended strategies which are much more complex than simple cooperation
and defection action in SDs and can be dynamically mixed with cooperation and defection behaviors,
especially during the learning. The agents that have successfully learned cooperative incentivizing
policies can be exploited by other agents with temporarily learned mixing strategies, and the exploited
cooperators may abandon cooperative strategies, making it less likely for other agents to learn to
cooperate. This issue does not exist in SDs but will severely hinder cooperation emergence in SSDs.

To solve this problem, we first illustrate the dynamics of and formally analyze the impetus for the
second-order social dilemma on a fully-featured motivating example. Based on these analyses, we
propose a novel learning mechanism by encouraging agents with similar environmental behaviors
to have similar incentivizing behaviors. Our method is inspired by a concept called homophily, a
particularly common tendency for human individuals to associate or bound with similar others, as
in the proverb birds of a feather flock together. In the literature, homophily has been studied in
non-sequential social dilemmas, where agents interact with others of similar predefined types (Fu
et al., 2012; Fletcher & Doebeli, 2009; Ramazi et al., 2016). In non-sequential cases, agents can
choose their partners to interact with (Rand et al., 2011; Aksoy, 2015) or there is a matching system
that auto-matches similar agents (Bergstrom, 2003; Bilancini et al., 2016), but it is not clear how to
extend these insights to SSDs. Our work solves this problem and enables a group of self-interested
agents to cooperate stably when they are continually learning and acting in a shared environment.

It is worth noting that multi-person social dilemmas are classified into two broad categories: public
goods and the tragedy of the commons (HARD[˙]IN, 1968; Kollock, 1998; Ledyard, 1994; Hardin,
2009). We evaluate our method on both of these sequential social dilemmas and show that the agent
population learns stable cooperative behaviors with great efficiency and stability. Visualization of
the evolution process of cooperation shows that homophily effectively enables stable cooperation by
preventing agents that conduct altruistic incentives from being exploited by second-order free-riders.

2 PRELIMINARIES AND RELATED WORKS

Social dilemmas (SDs) are among the most important settings used to study the emergence of
cooperation. In a social dilemma, there exists a tension between the individual and collective
rationality, in which individually rational behavior results in a situation where everyone suffers in
the long run. Although studies on SDs have contributed significantly to the research of cooperation
emergence for decades (Axelrod & Hamilton, 1987; Peysakhovich & Lerer, 2017; Anastassacos et al.,
2020), they focus on fixed policies, where the cooperative and defective strategies are actions to be
chosen rather than policies to be learned. To be more realistic as in real-world situations, in this paper,
we consider sequential social dilemmas (SSDs, (Leibo et al., 2017; Blanco et al., 2014)).

An SSD can be modelled as a partially-observable general-sum Markov game (Littman, 1994)
= _I, S,_ _Ai_ _, P, O,_ Ωi _,_ _Ri_ _, n, γ_, where I is a finite set of n agents and γ is a discount
_M_ _⟨_ _{_ _}_ _{_ _}_ _{_ _}_ _⟩_
factor. At each time step, agentthe observation function O(s, i) i. Based on the observation, agent draws a partial observation oi ∈ iΩ selects an actioni of the state s ∈ aSi according toAi, which
together form a joint action a, leading to a next state s[′] according to the stochastic transition function ∈
_P_ (s[′]|s, a) and individual rewards ri = Ri(s, a) for each agent. In SSDs, instead of taking atomic
cooperation or defection actions, agents must learn cooperation or defection strategies consisting of
potentially long sequences of environmental actions. The goal of each agent is to maximize the local
expected return: Qi(s, a) = Es0:∞,a0:∞ [[P][∞]t=0 _[γ][t][R][i][(][s][t][,][ a][t][)][|][s][0][ =][ s,][ a][0][ =][ a][]][.]_


-----

**Altruistic incentive, including altruistically rewarding and punishing others, is known as one of the**
solutions for social dilemmas (Kollock, 1998). However, it introduces the problem of second-order
**social dilemma (2nd-SD), which arises from each individual’s inclination to free ride on a mechanism**
that is designed to solve the first-order social dilemma. To solve 2nd-SDs, previous works either
introduce additional mechanisms, such as extra punishing mechanisms (Fowler, 2005; Greenwood,
2016), reputation mechanisms (Panchanathan & Boyd, 2004), or change the game settings, such as
enabling the amount of public goods to grow exponentially with the number of contributors (Ye et al.,
2016), considering corruption and power asymmetries (Úbeda & Duéñez-Guzmán, 2011). These
works analyze the dynamics of predefined and fixed mechanisms and focus on non-sequential games.

We propose to solve 2nd-SDs in general cases using the tendency of homophily, which states that
agents tend to have similar incentivizing behaviors if their environmental behaviors are similar. In the
literature, homophily, frequently termed as assortative matching (Bergstrom, 2003), is often referred
to the tendency for agents to interact with others of similar types (Fu et al., 2012; Fletcher & Doebeli,
2009; Ramazi et al., 2016). These works assume that a game (such as Prisoner’s Dilemma and Public
Goods Dilemma) involves only a group of several agents from a large population. Based on the way
they form the group, these works can be divided into two broad categories. (1) Agents can choose
their partners to interact with (Rand et al., 2011; Aksoy, 2015). However, in SSDs, agents do not
have such an option and their partners are determined at initialization. They have to continually
learn and act in a shared environment instead of a separate environment instance for each group.
(2) There is a matching system that can auto-match agents with similar predefined types to join the
game (Bergstrom, 2003; Bilancini et al., 2016). However, in SSDs, agents are assumed to have
determined partners and thus the matching process changes the game settings. Moreover, the existence
of such a matching system is an unrealistic assumption because the policy types (such as cooperation
and defection) in SSDs cannot be predefined since temporally extended strategies are much more
complex and can be dynamically mixed with cooperation and defection, especially during learning.
Therefore, previous work studying homophily can not be applied to SSDs. Moreover, Wang et al.
(2018) also find that assortative matching of environmental behaviors cannot promote cooperation in
SSDs. In contrast, our work encourages homophily on the level of incentivizing behaviors, which we
find outperforming other algorithms on learning cooperation in SSDs. Furthermore, we discuss how
homophily relates to other possible solutions in Appendix B.9.

Recent works study some other mechanisms which may encourage cooperation in SSDs. Hughes
et al. (2018) study the effects of inequality aversion, which bypasses the problem of second-order
social dilemmas because the punishment does not incur costs to any agents other than the punished
ones. Jaques et al. (2019) find that encouraging mutual influence among agents can promote collective
behaviors. We empirically compare with these methods in Sec. 5.2.

In the following sections, we will first provide a motivating example to show the influence of SDs,
2nd-SDs, and how homophily can solve 2nd-SDs. Based on these analyses, we introduce how to
encourage homophily in temporally extended cases in Sec. 4.

3 MOTIVATING EXAMPLE: FRAGILE COOPERATION

In this section, we provide a detailed motivating example to explain first-order social dilemmas
(1st-SDs) and second-order social dilemmas (2nd-SDs) in the context of RL, and demonstrate that
homophily can promote cooperation by alleviating the 2nd-SDs. For clarity, in this section, we use
one-step games to illustrate our idea, which inspires our extension to sequential settings in Sec. 4.

We adopt a classic problem setting of public goods dilemma (Hauert et al., 2007; 2002a; Semmann
et al., 2003; Hauert et al., 2002b). A population of n agents has an opportunity to create a public good,
from which all can benefit, regardless of whether they have contributed to the good. Specifically,
there are three strategies (atomic actions). Contributors (C) pay a cost c to increase the size of
public good by b. Defectors (D) do not contribute. The public good is uniformly distributed among
cooperators and defectors. Agents can also choose to neither contribute to nor benefit from the public
good, but receive a fixed reward σ, as Nonparticipants (N ). For clarity, we conduct analyses using
an illustrative example with n=10, b=3, c=1, σ=1. We also provide a sensitivity analysis regarding
parameters in Appendix B.3 to consolidate that our conclusion generally holds.


-----

**A: Cooperation is fragile in 1st-SDs. We first showcase the influence of 1st-SDs. The Schelling**
diagram (Schelling, 1973; Perolat et al., 2017) (Fig. 1(a)) proves the existence of SDs in this case. To
visualize the learning dynamics of MARL algorithms under 1st-SDs, we learn independent policies
for agents using REINFORCE (Williams, 1992). In Fig. 1(b), we plot the change of the proportion of
cooperative agents in the population under 3 random seeds. We observe that the cooperation level
oscillates during learning. For explanation, we visualize the dynamics of population constituent
in a ternary plot (Fig. 1(c)). Each point X inside the equilateral triangle represents a distribution
of population members (pC, pD, pN ), where pC + pD + pN = 1, pC, pD, pN are represented by
the distances from point X to the edges ND, CN, and CD, respectively. Trajectories in Fig. 1(c)
correspond to the curves with the same color in Fig. 1(b). We can observe that all the trajectories
rotate counterclockwise in the vicinity of vertex N regardless of the starting position.

Formally, we calculate the closed-form gradients of agents’ true value functions and visualize the
_gradient field in Fig. 1(d). It can be observed that cooperation is not a stable solution, which can be_
easily taken over by defectors, and then by nonparticipants. This phenomenon is the result of 1st-SDs,
where cooperation is exploited by defection, eventually leading the system to a very ineffective state.
We refer readers to Appendix E for the detailed proof.

Figure 1: First-order social dilemmas.

**B: Unexploitable altruistic incentives make cooperation possible. To introduce altruistic incen-**
tives into the system, we add Punishers (P ) as the fourth type of strategy. The same as contributors, a
punisher contributes to and benefits from the public good, and importantly, it also pays a cost k to
incur a punishment p on defectors. This punishment is altruistic because it reduces its own immediate
reward but benefits the team in the long run. To show the effect of altruistic incentives, we need to
guarantee that no agent can exploit altruistic incentives by being a pure contributor that does not pay
cost to punish defectors. Therefore, we first consider the setting where punishers also pay a cost αk
to incur a punishment αp on the pure contributors for not punishing defectors (Fowler, 2005). We
denote this punishing type by PA, which represents a kind of unexploitable altruistic incentives.

For simplicity, we illustrate our analyses with p=2, k=0.35, α=1 (sensitivity analysis is deferred
to Appendix B.3). Each horizontal plane in Fig. 2(a) shows the Schelling diagram under the
corresponding number of first-order defectors. Similar to Fig. 1(c), we visualize the dynamics of
independent learners under this situation in Fig. 2(b) using a quaternary plot (refer to Appendix A.2
for more details). We see that although two trajectories are trapped in the C-D-N plane and similar
to those in Fig. 1(c), one of the three trajectories finds the stable cooperation solution PA.

We plot the closed-form gradients in Fig. 3(a), where the green (blue) region indicates that a population
initialized there would converge to cooperative (non-cooperative) solution. This figure proves that
introducing unexploitable altruistic incentives creates a “safe region” near PA, and the populations
initialized there converge to cooperative solutions.

**C: Exploitable altruistic incentives suffer from 2nd-SDs, which again lead to fragile coopera-**
**tion. Now we restrict the punishments incurred by punishers, and they can only pay a cost k to incur**
a punishment p on defectors. Now the punishers can be exploited by pure cooperators who do not pay
for but benefit from punishments. We call this type of punishers the exploitable punishers. On each
horizontal plane in Fig. 2(c), we show the Schelling diagram with different numbers of first-order
defectors, which reveals the existence of second-order social dilemmas.

Formally, assume that the probability of agent i taking three actions are θ[i,C], θ[i,D], θ[i,P], respectively.
It can be derived (Appendix E) that the gradient of agent i’s value function w.r.t. the probability of
second-order cooperative action θ[i,P] minus the the gradient w.r.t the probability of second-order
defective action θ[i,C] is −k _j≠_ _i_ _[θ][j,D][, where][ k >][ 0][ is the punishment cost. The gradient is negative]_

[P]


-----

for any θ[j,D], which indicates that second-order cooperators would be taken over by second-order
defectors. This is the second-order social dilemma. The result is that only strategies C, D, N
exist, after which the system degrades due to the 1st-SD as discussed before. We plot the closedform gradients in Fig. 2(d), from which we observe that the “safe region” disappears, and, for any
initialization, the population ends with non-cooperation.

Figure 2: Unexploitable and exploitable altruistic incentives.

**Takeaways We conclude that only unexploitable incentives can make cooperation a stable solution.**
It becomes particularly problematic in temporally extended cases, where the incentivizing policies
need to be learned. It is typical that some agents would learn altruistic incentives earlier than
others. However, as analysed before, these altruistic agents will be exploited by other agents, leading
to degraded altruistic behaviors. Further, with collapsed altruistic incentivizing mechanisms, the
population falls back to a 1st-SD, making cooperation much less likely to emerge.


**D: Homophily solves second-order social dilem-**
**mas. To show the effect of homophily, based on the**
settings in part C, we further encourage agents with
similar acting behaviors to have similar incentivizing behaviors. Since only contributors and punishers
have the same acting behavior of contributing to the
public good, we encourage their incentives to be the
same by converting the minority of P and C to the
majority with a probability of 0.2.

Figure 3: Homophily solves second-order so
We plot the closed-form gradient in Fig. 3(b). We find

cial dilemmas.

that the “safe region” reappears. The gradient w.r.t.
_θ[i,P]_ minus the gradient w.r.t θ[i,C] is _k_ _j=i_ _[θ][j,D][ + 2][λ][sign][(][P]j=i[(][θ][j,P][ −]_ _[θ][j,C][)) min(][θ][i,C][, θ][j,P][ )][,]_
_−_ _̸_ _̸_

which is positive when close to point P. Interestingly, the “safe region” is larger than that in Fig. 3(a).
In this way, we conclude that homophily helps solve 2nd-SDs.

[P]

By this motivating example, we show that homophilic incentives encourage cooperation. However,
this study focuses on one-step games and predefines the mechanism of punishers. The question is
how to encourage homophily in realistic settings without depending on any predefined mechanisms.

4 METHODS

As discussed in the previous section, 2nd-SDs disturb the learning process of incentivizing actions
– agents who learn altruistic incentives earlier tend to be exploited by other agents. Consequently,
the altruistic incentivizing behaviors are typically taken over and the population would fall back to
1st-SDs, resulting in a hopeless loop.

In this section, we discuss how to introduce homophily into sequential social dilemmas so that the
loop can be broken and cooperation is possible to emerge and stabilize. We propose to encourage
homophily by encouraging agents with similar acting (environmental) behaviors to have similar
incentivizing behaviors. We now describe our learning framework shown in Fig. 4.

4.1 INCENTIVIZING AND ENVIRONMENTAL BEHAVIOR LEARNING

To enable incentivizing behaviors, we add incentivizing actions to the action space. We use ai _j to_
_→_
denote the incentivizing action from i to j. The action ai _j induces an inter-agent reward η[e]ri_ _j_
_→_ _→_


-----

to agent j. Here, η[e] _> 0 is a scaling factor. In this paper, we consider three types of incentivizing_
actions with a positive, negative, and zero ri→j, respectively. Since we consider altruistic incentives,
the action ai _j itself costs η[c]_ _ri_ _j_, where η[c] _> 0 is also a scaling factor._
_→_ _|_ _→_ _|_

Each agent learns two Q functions, for selecting environmental actions and incentivizing actions, respectively. At each step, agents first simultaneously select
which is based on local action-observation historyenvironmental actions ai according to Q[i,]θi[env](τi, ai),
and is parameterized by θi. Then, conditioned on environmental actions of other agents, a-i, each agent
_i decides its incentivizing actions ai_ -i according to
_→_
_Q[i,]φi[inc]_ ((τi, a-i), ai→-i) parameterized by φi.

One question is what rewards should be considered
when training Q[i,]φi[inc][. Intuitively, incentivizing ac-]
tions are expected to positively influence the return
given by the environment. Therefore, we include environment rewards ri. We also consider the costs of Figure 4: Homophily learning framework.
incentivizing actions to prevent agents from excessively giving incentives. Moreover, we ignore the
rewards received from other agents, which can effectively prevent agents from learning trivial and
detrimental policies, such as keeping exchanging positive incentives regardless of the observations.

Another question of learningnetwork and most output heads would remain unchanged for long stretches of time. To solve this Q[i,]φi[inc] is that it requires 3[n][−][1] outputs using a conventional deep Qproblem, agent i can learn n − 1 incentivizing Q functions _Q[¯][i,]φi[inc][, each of which corresponds to one]_
agent j ̸= i. However, this alternative arises a new question because the environment rewards are
considered when training Q[i,]φi[inc] but they do not present an explicit decomposition over agents. To
solve this problem, we propose to estimate Q[i,]φi[inc] as a summation:


_Q[i,]φi[inc][((][τ][i][,][ a][-][i][)][,][ a][i][→][-][i][) =]_


_Q¯[i,]φi[inc][((][τ][i][, a][j][)][, a][i][→][j][)][.]_ (1)

Xj≠ _i_


Here, parameters ofhag et al., 2018), but we sum incentivizing Q’s of a single agent, rather than Q’s of different agents.Q[¯][i,]φi[inc] are shared to accelerate training. This formulation is similar to VDN (Sune
With these formulations, we train each agent’s incentivizing Q by minimizing the following TD loss:

2

_Li[inc][(][φ][i][) =][ E][D] ri −_ _η[c][ X]j≠_ _i_ _|ri→j| + γ[inc]_ **_amax[′]i→-i_** _Q[i,]φ[−]i[inc][((][τ][ ′]i_ _[,][ a]-[′]_ _i[)][,][ a]i[′]→-i[)][ −]_ _[Q][i,]φi[inc][((][τ][i][,][ a][-][i][)][,][ a][i][→][-][i][)]_ 

(2)
where γ[inc] is the discount factor, φ[−]i [is parameters of a][ target network][ that are periodically copied]
from φi, and the expectation is estimated by uniform samples from a replay buffer .
_D_

Environmental Q function is trained with rewards from the environment and the incentives received
from other agents, and we minimize the following TD loss for learning Q[i,][env]:


_i_ [(][θ][i][) =][ E][D] _yi[env]_ _Q[i,]θi[env](τi, ai)_ 2 _._ (3)
_L[env]_ _−_

Here, the expectation is estimated with uniform samples from the replay buffer   , yi[env] = ri +
_D_
_η[e][ P]j≠_ _i_ _[r][j][→][i]_ [+][γ][env][ max][a]i[′] _[Q]θ[i,]i[−][env](τi[′][, a][′]i[)][ is the target for environmental Q-learning.][ θ]i[−]_ [is parameters]

of a target network that are periodically copied from θi.

4.2 HOMOPHILY

Directly learning incentivizing policies can be difficult due to second-order social dilemmas as we
discussed in Sec. 3. To solve this problem and inspired by the stateless case in Sec. 3 Part D, we
encourage agents to be homophilic, i.e., agents with similar environmental behaviors should have


-----

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|.0||||

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||
||||||
|1 2 3 4 T (mil)|||||

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||
||||||
||||||
|1 2 3 4 T (mil)|||||


1998): Public goods dilemmas and Tragedy of commons dilemmas. In this paper, we consider

_Homophily (Ours)_ LIO Social Influence Inequity Aversion Selfish Actor-Critic

_w/o Homophily_ _w/ inc._ _Cont. inc. actions_

40 `Cleanup (n=3)` 50 `Cleanup (n=5)` `Cleanup (n=10)` `Harvest (n=10)`

125

30 40 100 100

20 30 75 75

20 50 50

10 10 25 25

Collective Return Mean Collective Return Mean Collective Return Mean Collective Return Mean

0 0 0 0

0.0 0.5 1.0 1.5 2.0 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5

T (mil) T (mil) T (mil) T (mil)

Figure 5: Comparison of our method against baselines and ablations.

similar incentive behaviors, which can be expressed as a loss to be minimized:

_Li[homo](φi) = ED_ _−_ _S_ [env](i, j)S [inc](i, j; φi) _,_ (4)

_j=i_

h X̸ i

where S [env] is 1 or 0, indicting the environmental behaviors are similar or not. S [inc] measures the
similarity between incentivizing behaviors of two agents.

The first question is how to define (i, j; φi). The idea is to measure the similarity between two
_S_ [inc]
agents’ incentive behaviors by comparing their incentive actions to each of other agents. For each
agent i, we measure its similarity to agent j as:

_S_ [inc](i, j; φi) = − _CE_ _Paj→k_ _σ( ¯Q[i,]φi[inc][((][τ][i][, a][k][)][,][ ·][))]_ _,_ (5)

_k/_ _i,j_
_∈{X_ _}_ h i

where σ(·) is the softmax function, Paj→k is a categorical distribution over agent j’s incentive actions,
with a probability of 1 for the action that agent j takes for agent k (aj→k). The cross entropy CE[·∥·]
measures the distance between these two distributions.

As for, if we parameterize and learn it with using _i_, we may get trivial solutions. For
_S_ [env] _S_ [inc] _L[homo]_
example, may be a constant 1, in which case _i_ is minimized given the same . To avoid
_S_ [env] _L[homo]_ _S_ [inc]
such solutions, we propose to use non-parametric S [env]. Details are discussed in Appendix C.

With these components, the loss for agent i to learn environmental and incentivizing behaviors is:

_Li(θi, φi) = Li[env][(][θ][i][) +][ λ][inc][L]i[inc][(][φ][i][) +][ λ][homo][L]i[homo](φi),_ (6)

where λ[inc] and λ[homo] are scaling factors. Agents learn their policies independently.

5 EXPERIMENTS

Our experiments aim to answer the following questions: (1) Can homophily promote the emergence
of cooperation? (Sec. 5.2) (2) What is the contribution of each component in the proposed learning
framework? (Sec. 5.3) (3) How does cooperation emerge and evolve under homophilic incentives?
(Sec. 5.4) (4) How does homophily affect incentive behaviors? (Sec. 5.5)

5.1 EXPERIMENTAL SETUP

We test our method in SSDs. There are two broad categories of multi-person social dilemmas (Kollock,

sequential versions of these two dilemmas, Cleanup and Harvest (Leibo et al., 2017). In our
learning framework, each agent has an environmental and an incentivizing Q function. The Q
network architecture is the same for all agents but they do not share parameters. For other details of
environments and our method, we refer readers to Appendix C.

5.2 PERFORMANCE

To test whether homophilic learning can promote the emergence of cooperation, we test our method
on Cleanup and Harvest with different numbers of agents and compare against various baselines:
LIO (Yang et al., 2020), Inequity Aversion (Hughes et al., 2018), Social Influence (Jaques et al.,
2019), and Selfish Actor-Critic. The details of baselines can be found in Appendix D.

We test all methods with 5 random seeds and show the mean value as well as 95% confidence intervals
in Fig. 5. It can be observed that our method helps agents learn cooperation with efficiency and


-----

Figure 6: Evolution of cooperation in Cleanup. Left: Sum of environmental rewards received by all
agents. Middle: Environmental and incentivizing behaviors at four different stages of learning. Right:
Corresponding descriptions.
stability. In comparison, baseline algorithms either cannot learn any cooperation strategies or the
cooperation level oscillates. Inequity aversion oscillates on Cleanup (n=5), which can be proved by
its large confidence intervals, and cannot learn to cooperate on Harvest (n=10). Social influence
proves that cooperation can be achieved by encouraging agents to influence each other, but it requires
many samples (typically 100M as reported in Jaques et al. (2019)) to learn cooperative strategies. In
contrast, our method needs around 5M samples to learn stable cooperative strategies. LIO can learn
cooperation on Cleanup (n=3), but is less effective in dilemmas with more agents.

5.3 ABLATION STUDY

There are three contributions that characterize our method. (1) First and the most important, the
homophily learning objective. (2) Discrete incentive actions and factored incentive Q-functions for
each agent. (3) Excluding received incentives when training incentive Q-functions. In this section,
we design the following ablations to test the contribution of each of these components.

(1) Without homophily (w/o homophily). We exclude the homophily loss _i_ from our learning
_L[homo]_
objective. We can see that the team performance drops significantly, especially in tasks with
more agents. Moreover, without homophily loss, our method performs worse than all baselines
on Cleanup (n=5) and (n=10). These observations suggest that our method works mainly because
of the homophily loss. (2) Continuous incentivizing actions. Ablation Cont. inc. actions shows
the influence of discrete actions. We learn a continuous incentivizing rewarding function for w/o
_homophily. We can observe further performance decrease on Cleanup (n=3) and Harvest (n=10)._
We hypothesize that this is because the search space for incentivizing policies grows, making the
strategy more difficult to learn. (3) Train incentivizing Q’s with received incentives. We ignore
incentives received by agents when training incentivizing strategies. For comparison, ablation w/
_inc. shows what would happen if they are included. We can observe that w/ inc. significantly_
underperforms the original method on all environments. The reason is that agents learn to give each
other positive incentives excessively regardless of observations. Since received incentives are also
considered in Q-learning for acting behaviors, excessive incentives would overwhelm environmental
rewards and significantly hurt learning performance.

5.4 EVOLUTION OF COOPERATION

To clearly show the evolution of emergent cooperation, the problem of 2nd-SDs, and how homophily
alleviates this problem, according to different team returns, we select four stages during learning and
analyze their corresponding behaviors. The detailed stage partition can be found in Fig. 6.


-----

_Phase 1: Exploring incentives. During this stage, agents are learning basic dynamics of the environ-_
ment. For example, as shown in the first row of Fig. 6, agents have not learned to eat apples to get
rewarded. Meanwhile, during exploration, agents occasionally give incentives to agents who cleaned
wastes (t1 + 3), which enables learning incentivizing behaviors in the following stages.

_Phase 2: Second-order social dilemmas. Some agents (the pink agent in the second row of Fig. 6)_
learn to give positive rewards to cleaning agents altruistically. However, other agents (e.g., orange and
_blue) typically do not give positive incentives but can enjoy the benefits of others’ altruistic behaviors._
We can observe an oscillation of team return during this stage. This is the effect of 2nd-SDs. If there
are no additional restrictions (such as homophily) to deal with this problem, the team will fall back to
the state where no one wants to perform incentives, resulting in the collapse of cooperation.

_Phase 3: Homophily solves 2nd-SDs. After some oscillations of cooperation, the homophily loss_
gradually encourages agents with similar environmental behaviors to have similar incentivizing
behaviors. As shown in the third row of Fig. 6, although there are still some noisy incentives at this
stage, agents who are close to the apple-spawning region simultaneously reward cleaning agents and
punish those who are next to the wastes but do not clean them. These incentivizing behaviors indicate
that the population has gotten over the 2nd-SD with the help of homophily. Correspondingly, the
team return increases in this stage.

_Phase 4: Stabilized cooperation. Taking advantage of the effect of homophily, during this stage, there_
are no second-order free-riders and all incentive rewards are given by agents harvesting apples to
agents in the region of wastes. Moreover, screenshots show that agents learn an efficient division of
labor – three agents eat apples and get environmental rewards while two agents clean the wastes and
get rewards from harvesting agents. These incentivizing and environmental behaviors are a stable
solution only when homophily learning objective is included.

Based on the illustrations of the evolution of cooperation, we conclude that homophily prevents
altruistic incentivizing behaviors from being exploited by second-order free riders, and thus solves
the problem of 2nd-SDs and leads to stable cooperation. For detailed agent behaviors at different
stages, we refer readers to our online videos[1].

5.5 HOMOPHILIC INCENTIVES

In previous sections, we show that homophilic incentives can
promote cooperation, but how homophily affects incentivizing behaviors remains largely unclear. To make up for this,
we compare our method with the ablation w/o homophily on
```
Cleanup (n=3) by plotting their collective return and altruistic

```
incentive (the average incentive that each Cleaner receives at
each time step) in Fig. 7.

We can observe a close connection between incentives and cooperation performance. Intuitively, the more positive incentives
cleaners receive, the more apples are expected to spawn and
be collected. However, without homophily, the received incen
Figure 7: Homophily eliminates in
tives oscillate dramatically, which is caused by second-order

centive oscillations.

social dilemmas and is in line with the discussions before. In
comparison, our method keeps incentivizing cleaners and thus learn cooperation with stability.

6 CLOSING REMARKS

In this paper, we study the problem of cooperation emergence. We show that altruistic incentives make
cooperation possible but cannot stabilize due to second-order social dilemmas. We then formally and
empirically show that homophily, a common tendency typical of humans, may solve this problem.
Combined with deep MARL, we propose an implementation of homophilic learning for sequential
social dilemmas. We expect that our work can encourage future works on studying the exciting topics
of cooperation emergence, evolution, and stability.

[1https://sites.google.com/view/homophily/](https://sites.google.com/view/homophily/)


-----

**Reproducibility** The source code for all the experiments along with a README file with instructions on how to run these experiments is attached in supplementary material. In addition, the settings
and parameters for all models and algorithms mentioned in the experiment section are detailed in
Appendix C.

REFERENCES

Erol Akçay and Joan Roughgarden. The evolution of payoff matrices: providing incentives to
cooperate. Proceedings of the Royal Society B: Biological Sciences, 278(1715):2198–2206, 2011.

Ozan Aksoy. Effects of heterogeneity and homophily on cooperation. Social Psychology Quarterly,
78(4):324–344, 2015.

Nicolas Anastassacos, Stephen Hailes, and Mirco Musolesi. Partner selection for the emergence
of cooperation in multi-agent systems using reinforcement learning. In Proceedings of the AAAI
_Conference on Artificial Intelligence, volume 34, pp. 7047–7054, 2020._

R Axelrod and W. D. Hamilton. The evolution of cooperation. Science, 1(1):1390–1396, 1987.

Robert Axelrod and William D. Hamilton. The Evolution of Cooperation. 1984.

Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor
Mordatch. Emergent tool use from multi-agent autocurricula. In Proceedings of the International
_Conference on Learning Representations (ICLR), 2020._

Theodore C Bergstrom. The algebra of assortative encounters and the evolution of cooperation.
_International Game Theory Review, 5(03):211–228, 2003._

Ennio Bilancini, Leonardo Boncinelli, and Jiabin Wu. The interplay of cultural aversion and
assortativity for the emergence of cooperation. Available at SSRN 2773097, 2016.

Mariana Blanco, Dirk Engelmann, Alexander K Koch, and Hans-Theo Normann. Preferences and
beliefs in a sequential social dilemma: a within-subjects analysis. Games and Economic Behavior,
87:122–135, 2014.

Jean-François Bonnefon, Azim Shariff, and Iyad Rahwan. The social dilemma of autonomous
vehicles. Science, 352(6293):1573–1576, 2016.

Robert Boyd. Mistakes allow evolutionary stability in the repeated prisoner’s dilemma game. Journal
_of theoretical Biology, 136(1):47–56, 1989._

Robert Boyd, Herbert Gintis, Samuel Bowles, and Peter J Richerson. The evolution of altruistic
punishment. Proceedings of the National Academy of Sciences, 100(6):3531–3535, 2003.

Stuart Bryce Capstick. Public understanding of climate change as a social dilemma. Sustainability, 5
(8):3484–3501, 2013.

Yunmei Chen and Xiaojing Ye. Projection onto a simplex, 2011.

Anna Dreber, David G Rand, Drew Fudenberg, and Martin A Nowak. Winners don’t punish. Nature,
452(7185):348–351, 2008.

Ernst Fehr and Simon Gächter. Altruistic punishment in humans. Nature, 415(6868):137–140, 2002.

Jeffrey A Fletcher and Michael Doebeli. A simple and general explanation for the evolution of
altruism. Proceedings of the Royal Society B: Biological Sciences, 276(1654):13–19, 2009.

Yuk-fai Fong and Jay Surti. The optimal degree of cooperation in the repeated prisoners’ dilemma
with side payments. Games and Economic Behavior, 67(1):277–291, 2009.

James H Fowler. Altruistic punishment and the origin of cooperation. Proceedings of the National
_Academy of Sciences, 102(19):7047–7049, 2005._

Feng Fu, Martin A Nowak, Nicholas A Christakis, and James H Fowler. The evolution of homophily.
_Scientific reports, 2(1):1–6, 2012._


-----

Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison
Guo. On differentiating parameterized argmin and argmax problems with application to bi-level
optimization. arXiv preprint arXiv:1607.05447, 2016.

Garrison W Greenwood. Altruistic punishment can help resolve tragedy of the commons social
dilemmas. In 2016 IEEE Conference on Computational Intelligence and Games (CIG), pp. 1–7.
IEEE, 2016.

Werner Güth. An evolutionary approach to explaining cooperative behavior by reciprocal incentives.
_International Journal of Game Theory, 24(4):323–344, 1995._

G HARD[˙]IN. Tragedy of the commons. new series, vol. 162, no. 3859, 1968.

Garrett Hardin. The tragedy of the commons. Journal of Natural Resources Policy Research, 1(3):
243–253, 2009.

Hauert, Christoph, Monte, Silvia, De, Hofbauer, Josef, Sigmund, and Karl. Volunteering as red queen
mechanism for cooperation in public goods games. Science, 2002a.

Christoph Hauert, Silvia De Monte, Josef Hofbauer, and Karl Sigmund. Replicator dynamics for
optional public good games. Journal of Theoretical Biology, 218(2):187–194, 2002b.

Christoph Hauert, Arne Traulsen, Hannelore Brandt, Martin A Nowak, and Karl Sigmund. Via
freedom to coercion: The emergence of costly punishment. Science, 316(5833):1905–1907, 2007.

Morris W Hirsch, Stephen Smale, and Robert L Devaney. Differential equations, dynamical systems,
_and an introduction to chaos. Academic press, 2012._

Edward Hughes, Joel Z Leibo, Matthew Phillips, Karl Tuyls, Edgar Dueñez-Guzman, Antonio García
Castañeda, Iain Dunning, Tina Zhu, Kevin McKee, Raphael Koster, et al. Inequity aversion
improves cooperation in intertemporal social dilemmas. In Advances in Neural Information
_Processing Systems, pp. 3330–3340, 2018._

Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, Dj Strouse,
Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep
reinforcement learning. In International Conference on Machine Learning, pp. 3040–3049, 2019.

Donald Kennedy. 125. Science, 309(5731):19–20, 2005.

Peter Kollock. Social dilemmas: The anatomy of cooperation. Annual Review of Sociology, 24:
183–214, 1998.

D Michael Kuhlman and Alfred F Marshello. Individual differences in game motivation as moderators
of preprogrammed strategy effects in prisoner’s dilemma. Journal of personality and social
_psychology, 32(5):922, 1975._

John O Ledyard. Public goods: A survey of experimental research. 1994.

Joel Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel. Multi-agent
reinforcement learning in sequential social dilemmas. In Proceedings of the 16th Conference on Au_tonomous Agents and MultiAgent Systems, pp. 464–473. International Foundation for Autonomous_
Agents and Multiagent Systems, 2017.

Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. Morgan
Kauffman Publishers, Inc., 1994.

Andrei Lupu and Doina Precup. Gifting in multi-agent reinforcement learning. In Proceedings of the
_19th International Conference on Autonomous Agents and MultiAgent Systems, pp. 789–797, 2020._

Thomas Mussweiler and Axel Ockenfels. Similarity increases altruistic punishment in humans.
_Proceedings of the National Academy of Sciences, 110(48):19318–19323, 2013._

Andrei Novikov. PyClustering: Data mining library. Journal of Open Source Software, 4(36):1230,
[apr 2019. doi: 10.21105/joss.01230. URL https://doi.org/10.21105/joss.01230.](https://doi.org/10.21105/joss.01230)


-----

Elinor Ostrom, James Walker, and Roy Gardner. Covenants with and without a sword: Selfgovernance is possible. American political science Review, 86(2):404–417, 1992.

Craig Packer, Dennis A Gilbert, AE Pusey, and SJ O’Brieni. A molecular genetic analysis of kinship
and cooperation in african lions. Nature, 351(6327):562–565, 1991.

Karthik Panchanathan and Robert Boyd. Indirect reciprocity can stabilize cooperation without the
second-order free rider problem. Nature, 432(7016):499–502, 2004.

Dan Pelleg, Andrew W Moore, et al. X-means: Extending k-means with efficient estimation of the
number of clusters. In International Conference of Machine Learning, volume 1, pp. 727–734,
2000.

Elizabeth Pennisi. How did cooperative behavior evolve? Science, 309(5731):93–93, 2005.

Lawrence Perko. Differential equations and dynamical systems, volume 7. Springer Science &
Business Media, 2013.

Julien Perolat, Joel Z Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, and Thore Graepel. A
multi-agent reinforcement learning model of common-pool resource appropriation. arXiv preprint
_arXiv:1707.06600, 2017._

Alexander Peysakhovich and Adam Lerer. Consequentialist conditional cooperation in social dilemmas with imperfect information. arXiv preprint arXiv:1710.06975, 2017.

Pouria Ramazi, Ming Cao, and Franz J Weissing. Evolutionary dynamics of homophily and heterophily. Scientific reports, 6(1):1–9, 2016.

David G Rand and Martin A Nowak. The evolution of antisocial punishment in optional public goods
games. Nature communications, 2(1):1–7, 2011.

David G Rand, Samuel Arbesman, and Nicholas A Christakis. Dynamic social networks promote
cooperation in experiments with humans. Proceedings of the National Academy of Sciences, 108
(48):19193–19198, 2011.

Amnon Rapoport, Gary Bornstein, and Ido Erev. Intergroup competition for public goods: Effects of
unequal resources and relative group size. Journal of Personality and Social Psychology, 56(5):
748–756, 1989.

Tabish Rashid, Mikayel Samvelyan, Christian Schroeder Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent
reinforcement learning. In International Conference on Machine Learning, pp. 4292–4301, 2018.

James K Rilling, David A Gutman, Thorsten R Zeh, Giuseppe Pagnoni, Gregory S Berns, and
Clinton D Kilts. A neural basis for social cooperation. Neuron, 35(2):395–405, 2002.

Bo Rothstein. The universal welfare state as a social dilemma. Rationality and society, 13(2):
213–233, 2001.

Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli,
Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon Whiteson. The
StarCraft Multi-Agent Challenge. CoRR, abs/1902.04043, 2019.

Thomas C Schelling. Hockey helmets, concealed weapons, and daylight saving: A study of binary
choices with externalities. Journal of Conflict resolution, 17(3):381–428, 1973.

Dirk Semmann, Hans Juergen Krambeck, and Manfred Milinski. Volunteering leads to rock-paperscissors dynamics in a public goods game. Nature, 425(6956):390, 2003.

Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning based on team reward. In Proceedings of the
_17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 2085–2087._
International Foundation for Autonomous Agents and Multiagent Systems, 2018.


-----

Francisco Úbeda and Edgar A Duéñez-Guzmán. Power and corruption. Evolution: International
_Journal of Organic Evolution, 65(4):1127–1139, 2011._

Eugene Vinitsky, Natasha Jaques, Joel Leibo, Antonio Castenada, and Edward Hughes. An
[open source implementation of sequential social dilemma games. https://github.com/](https://github.com/eugenevinitsky/sequential_social_dilemma_games)
[eugenevinitsky/sequential_social_dilemma_games, 2019. GitHub repository.](https://github.com/eugenevinitsky/sequential_social_dilemma_games)

Jane X Wang, Edward Hughes, Chrisantha Fernando, Wojciech M Czarnecki, Edgar A DuéñezGuzmán, and Joel Z Leibo. Evolving intrinsic motivations for altruistic behavior. arXiv preprint
_arXiv:1811.05931, 2018._

Tonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang. Rode:
Learning roles to decompose multi-agent tasks. In Proceedings of the International Conference on
_Learning Representations (ICLR), 2021._

Weiran Wang and Miguel Á. Carreira-Perpiñán. Projection onto the probability simplex: An efficient
algorithm with a simple proof, and an application, 2013.

Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229–256, 1992.

Jiachen Yang, Ang Li, Mehrdad Farajtabar, Peter Sunehag, Edward Hughes, and Hongyuan Zha.
Learning to incentivize other learning agents. arXiv preprint arXiv:2006.06051, 2020.

Hang Ye, Shu Chen, Jun Luo, Fei Tan, Yongmin Jia, and Yefeng Chen. Increasing returns to scale:
The solution to the second-order social dilemma. Scientific reports, 6(1):1–10, 2016.

A DETAILS OF MOTIVATING EXAMPLE

A.1 PROBLEM SETTINGS

From the perspective of multi-agent reinforcement learning, we provide a detailed motivating example
to explain both first- and second-order social dilemmas, and demonstrate that homophily can promote
cooperation by alleviating second-order social dilemmas. The motivating example is based on the
classic problem setting from Fowler (2005) for its modeling of the public goods dilemma.

A population of n agents has an opportunity to create a public good that will be equally distributed to
all participants. There are several possible strategies in this population. Contributors (C) pay a cost c
to increase the size of public goods by b. Defectors (D) do not contribute but benefit from the public
goods. Agents can also choose to neither contribute to nor benefit from the public goods, but receive
a fixed reward σ, as Nonparticipants (N ). To introduce altruistic incentives into the system, we add
Punishers (P ) as the fourth type of strategy. A punisher contributes to and benefits from the public
goods, which is the same as contributors, and importantly, it also pays a cost k to incur a punishment
_p on defectors. To guarantee that no agent can exploit altruistic incentives by being a pure contributor_
that does not spend energy punishing defectors, we consider unexploitable punishers (PA) who also
pay a cost αk to incur a punishment αp on the pure contributors for not punishing defectors.

In Sec. 3 Part A, to showcase the influence of first-order social dilemmas, we consider Contributors
(C), Defectors (D), and Nonparticipants (N ). Assume that the proportion of each strategy is pC, pD,
and pN, respectively, and pC + pD + pN = 1. Then the rewards at each step are


11bpbp−ppCCNN _[,][−]_ _[c,]_ ContributorsDefectors (D ()C) (7)

_−_
_σ,_ Nonparticipants (N )


_r =_


From Eq. 7 we can see that the rewards of defectors are always higher than those of contributors,
which leads to first-order social dilemmas (1st-SDs or SDs).

In Sec. 3 Part B, to show the effect of unexploitable altruistic incentives, we consider Contributors (C),
Defectors (D), Nonparticipants (N ), and Unexploitable Punishers (PA). Assume that the proportion


-----

of each strategy is pC, pD, pN, and pP A, respectively, and pC + pD + pN + pP A = 1. Then the
rewards at each step are


_b(p1C_ +ppNP A) _c_ _αppP A,_ Contributors (C)

_b(p1C−+ppNP A)_ _−_ _pp −P A,_ Defectors (D)

_−_ _−_
_σ,_ Nonparticipants (N )
_b(p1C_ +ppNP A) _c_ _kpD_ _αkpC,_ Unexploitable Punishers (PA)

_−_ _−_ _−_ _−_


(8)


_r =_


In Sec. 3 Part C, we restrict the punishment incurred by punishers, and they can only pay a cost k to
incur a punishment p on defectors. Therefore, in this part, we consider Contributors (C), Defectors
(D), Nonparticipants (N ), and Exploitable Punishers (P ). Assume that the proportion of each strategy
is pC, pD, pN, and pP, respectively, and pC + pD + pN + pP = 1. Then the rewards each step are


_b(p1C_ +pNpP ) _c,_ Contributors (C)

_b(p1C−+pNpP )_ _−_ _ppP,_ Defectors (D)

_−_ _−_
_σ,_ Nonparticipants (N )
_b(p1C_ +pNpP ) _c_ _kpD,_ Exploitable Punishers (P )

_−_ _−_ _−_


(9)


_r =_


From Eq. 9, we can find that the rewards of pure contributors are always higher than those of
exploitable punishers, which leads to the second-order social dilemmas (2nd-SDs).

In Sec. 3 Part D, to show the effect of homophily, based on the settings of Eq. 9 which suffer from
2nd-SDs, we further encourage agents with similar acting behaviors to have similar incentivizing
behaviors. In the considered setting, only contributors and punishers have the same acting behavior
of contributing to the public goods. Therefore, we encourage their incentivizing behaviors to be
the same by converting the minority of punishers and contributors to the majority of them with
probability 0.2. (For example, if 30% of the agents are punishers and 20% of them are contributors,
then contributors become punishers with probability 0.2.)

In all our simulations, we set n = 10, b = 3, c = 1, σ = 1, p = 2, k = 0.35, and α = 1. A sensitivity
analysis is provided in Sec. 3 of the main text to demonstrate that our observation and conclusion are
solid for a large range of parameters. Agents use REINFORCE policy gradients (Williams, 1992) to
learn independent local policies for selecting strategies (atomic actions).

A.2 VISUALIZATION AND COORDINATE TRANSFORMATION

In order to demonstrate the evolution of population, we employ
two visualization methods, the ternary plot (for pC, pD, pN
in Sec. 3 Part A) and quaternary plot (for pC, pD, pN _, pP A in_
Sec. 3 Part B and pC, pD, pN _, pP in Sec. 3 Part C, D)._


**Ternary plot, an equilateral triangle periphery with several tra-**
jectories inside, reveals the dynamics of population constituents
(three different strategies), as shown in Fig. 1(c) in the paper.
Each point X inside the equilateral triangle represents a distribution of the population (pC, pD, pN ), where pC + pD + pN = 1.
_pC, pD, and pN is represented by the distance from point X_
to the edge ND, CN, and CD, respectively. This corresponds
to the fact that the sum of distances from any interior points of
an equilateral triangle to three sides is a fixed value. Naturally,
we can then depict the evolution of the percentage of three
strategies as a curve inside the triangle.


Figure 8: Population policy space
in the 3-dimensional rectangular coordinate system.


However, ternary plot can only visualize the evolutionary trajectories of three strategies. When the
fourth strategy is introduced, we resort to the quaternary plot in 3-dimensional space. For any
interior point X of a regular tetrahedron CDNP (Fig. 8), the sum of distances from X to four faces
_NCD, PCN_, DPN, and PCD is a constant. Therefore, we can still use an interior point X to


-----

represent a population distribution and represent pC, pD, pN _, pP by the distances from X to different_
faces.

Point-to-face distances have to be transformed into rectangular coordinates before trajectories can be
plotted. We now describe the details of coordinate transformation.

We first plot a regular tetrahedron in the three-dimensional rectangular coordinate system, with
four vertices P (0, 0, 1), N ( _√22_ _[,][ 0][,][ 0)][,][ C][(][−]_ _√42_ _[,]_ _√46_ _[,][ 0)][,][ D][(][−]_ _√42_ _[,][ −]_ _√46_ _[,][ 0)][ as in Fig. 8. To simplify]_

notations, we denote the distance from an interior point X to faces DNP, CNP, CDP, and CDN
by pC, pD, pN, and pP, respectively.

It can be calculated that the equation for plane PNC is


2x +


6y + z = 1

6y + z = 1.


and the equation for plane PND is


_√2x −_ _√6y + z = 1._

Next, using formula in analytical geometry for calculating the distance from a point to a given plane,
we can derive

_pD =_ [1]3 _[|]√2x +_ _√6y + z −_ 1|,

_pC =_ [1]3 _[|]√2x_ _√6y + z_ 1 _,_ (10)

 _−_ _−_ _|_
pP = z.


Finally, we solve the above system of equations and get

_x =_ _√42_ [(2][ −] [3][p][D][ −] [3][p][C][ −] [2][p][P][ )][,]

y = _√46_ [(][p][D][ −] _[p][C][)][,]_ (11)
z = pP,




With Eq. 11, we can calculate the corresponding coordinate in the three-dimensional rectangular
coordinate system of any population distribution. Connecting all points into a curve gives the 3D
quaternary plot.

B MORE RESULTS AND ANALYSES

B.1 MOTIVATING EXAMPLE: LIMIT CYCLE

In Fig. 2 (d) of the paper, we visualize the gradient field within
the population policy space when exploitable punishers are
considered. There, we conclude that independent REINFORCE
learners can never converge to cooperative solutions in this
setting. As a special case, we show that a population may rotate
infinitely in the non-cooperative region of the policy space.


In Fig. 9, we present a limit cycle in the population policy
space. A limit cycle is a closed trajectory, and any population
initialized on the cycle would keep looping over it and never
converge to a stable solution. It is worth noting that although
homophilic incentives make cooperation possible (Fig. 3(b) in
the main text), they cannot eliminate limit cycles in the noncooperative (blue) region.

B.2 MOTIVATING EXAMPLE: DEGREE OF HOMOPHILY


Figure 9: Limit Cycle.


In Sec. 3 Part D, we encourage agents with similar acting behaviors to have similar incentivizing
behaviors by converting the minority of punishers and contributors to the majority of them with
probability λ (λ=0.2 in the paper). We call λ the degree of homophily.

To further show the influence of λ, in Fig. 10, we plot how the cooperation proportion changes
with respect to the degree of homophily. Cooperation proportion is the ratio of the volume of the


-----

Figure 10: The more homophilic a population is, the more possible cooperation would emerge. The
cooperation proportion and gradient field in the population policy space under different degrees of
homophily are shown. The increasing volume of the “safe region” and the cooperation proportion
increases with the degree of homophily.

Figure 12: Gradient fields of our method against baselines

cooperative (green) region to the volume of the regular tetrahedron. We observe that the cooperation
proportion increases with the degree of homophily, and gradually converges to a constant value 0.4.
We further present the gradient field under several degrees of homophily. As expected, the volume of
the “safe region” increases with the degree of homophily. These results demonstrate that the more
homophilic a population is, the more possible cooperation would emerge.

B.3 MOTIVATING EXAMPLE: SENSITIVITY ANALYSIS

We use one-at-a-time method and show the result
with a Tornado Diagram in Fig. 11. When homophily
is not introduced, with a 20% perturbation of default
parameters, 2nd-SDs persist and the “cooperative volume” (green region in Fig. 2(d)) is always 0 (Fig. 11
left). In contrast, when homophily is introduced, the
cooperative volume (green region in Fig. 3(b)) fluc
Figure 11: Sensitivity analysis.

tuates within a reasonable range and is always larger
than 0.1 (Fig. 11 right), which allows cooperation emergence. These results consolidate the robustness
of our observations and method across a wide range of conditions.

B.4 MOTIVATING EXAMPLE: GRADIENT FIELD OF INEQUITY AVERSION

In Sec. 3, we use the gradient field plot to show the effect of social dilemmas and homophily.
Here to demonstrate the mechanism behind other baselines, we plot the gradient field of Inequity


-----

_Homophily (Ours)_ _Homophily with restricted incentive range_

40 `Cleanup (n=3)` 50 `Cleanup (n=5)` `Cleanup (n=10)` `Harvest (n=10)`

125

30 40 100 100

20 30 75 75

20 50 50

10 10 25 25

Collective Return Mean Collective Return Mean Collective Return Mean Collective Return Mean

0 0 0 0

0.0 0.5 1.0 1.5 2.0 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5

T (mil) T (mil) T (mil) T (mil)


Figure 13: Performance of our method when the incentive range is restricted

Aversion (Hughes et al., 2018). We adopt the inequity aversion model used in its original paper as in
Equ. 12.


_ri[inequity aversion]_ =
_−_


max(ri _rj, 0)_ (12)

Xj≠ _i_ _−_


max(rj _ri, 0)_

Xj≠ _i_ _−_ _−_


_N −_ 1


_N −_ 1


where α = 5 and β = 0.05 in their default settings.

We compare the gradient field of our method and other baselines in Fig. 12. The cooperative area
volume of inequity aversion is smaller than our method (about 60%). Moreover, unlike our method,
there is no cooperative area around the vertex N. Once the population of agents is trapped in the plane
C-D-N, they can never escape from it.

B.5 SSDS: RESTRICTED INCENTIVE RANGE

In Sec. 4, we allow the agents to incentivize any other agents even if they are not in the view range.
Here we consider restricting the incentive range to the view size. We compare this restricted variant
with the original unrestricted version in Fig. 13. There is only a slightly performance decrease in the
restricted version, which shows the robustness of our method.

B.6 SSDS: FAIRNESS IN CLEANUP

In Sec. 5.4, we have observed the division of labor between agents in Cleanup (n=5). In real life,
fairness is a very important indicator of how good the division of labor is. Here we statistically
measure the fairness of the system, and find that the agents carrying out different tasks are roughly fair
in Cleanup (n=5), when we take the incentivizing rewards into consideration. We use the Equality
metric (Hughes et al., 2018) to measure the fairness:

_n_ _n_

_E = 1_ _i=1_ _j=1_ _[|][r][i][ −]_ _[r][j][|]_ (13)
_−_ P 2nP _i=1_ _[r][i]_

where ri is agent i’s reward and n is the number of agents.

[P][n]

The value of equlity metric is between 0 and 1. The closer the value to 1, the fairer the system is. We
measure the fairness of models learned with 5 random seeds and calculate the mean value as well
as 95% confidence intervals. The mean values and confidence intervals of equality when we only
consider environmental rewards and when we consider both environmental and incentivizing rewards
are 0.6693(0.1252) and 0.8900(0.0473), respectively. We conjecture that the fairness in this system
comes from the fact that apple-harvesters and waste-cleaners should have roughly equal total rewards,
otherwise no agents would choose to clean wastes.

B.7 RESULTS OF DIFFERENT INCENTIVE COEFFICIENTS

For single-stage social dilemmas, we have conducted a sensitivity analysis in Sec. 3, which shows
our method are quite robust to these hyperparameters. For SSDs, we also observe similar results.
However, when extreme values of these hyperparameters are used, the performance can severely


-----

degrade. Here we show the results when we change the incentive effect factor η[e], and incentive cost
factor η[c] in the following table.

Table 1: Results of Different Incentive Coefficients.

Incentivizing Action Incentives for
Configs (η[e],η[c]) Team Return
Frequency Cleaning Waste

(1.0, 0.1)(Ours) **46.80(2.85)** 0.1165(0.0002) 0.800(0.020)
(1.0, 1.0) 3.46(3.29) 0.0334(0.0004) -0.001(0.013)
(1.0, 0.0) 8.56(6.72) 0.4741(0.0009) 0.440(0.200)
(0.0, 0.0) 2.80(1.34) -  - 

Each item in the above table is formatted with mean(std). When the incentive cost η[c] is equal to
incentive effect η[e], agents tend to not use incentivizing actions, which makes it difficult to achieve
cooperation. On the contrary, when the incentive cost η[c] is zero, agents use incentivizing actions too
frequently, which interferes with the learning of environmental policies.

B.8 INTUITION OF HOMOPHILY

We have provided intuition for introducing homophily by our motivating example in Sec. 3. Here
we provide more explanations of why homophily is an effective heuristic by (1) using a real-world
example and (2) giving more explanations about the impact of homophily in SSDs in the experiment
section.

(1) In a real-world situation where there are multiple social infrastructure builders and taxpayers, the
taxpayers’ benefits depend on the builders’ contribution and the builders’ profits depend on gathered
taxes. In this case, taxpayers should have similar tax payment strategies. Otherwise, more and more
taxpayers will choose to be free-riders and refuse to pay the tax, then the cooperation will collapse.

(2) In the evolution of cooperation of Cleanup in Sec. 5.4, apple-harvesters under the constraint of
homophily tend to associate with other apple-harvesters, and tend not to be a second-order free-rider,
_i.e., refusing to give positive rewards to waste-cleaners. Then the altruistic apple-harvesters will not_
be exploited by second-order free-riders and the waste-cleaners will keep receiving rewards while
cleaning. Thus the emergent cooperation becomes stable.

B.9 DISCUSSION

**Is homophily the only possible solution for 2nd-SDs? No, it is not, but homophily has several**
advantages over other possible solutions. There are three types of solutions for (1st-) SDs (Kollock,
1998). (1) Motivational solutions assume that agents take their partners’ rewards into account when
making a decision (Kuhlman & Marshello, 1975), which build in a strong bias towards cooperation. (2)
_Strategic solutions, such as tit-for-tat (Axelrod & Hamilton, 1984) and partner selection (Anastassacos_
et al., 2020), typically require identifying cooperative and defective behaviors. Such approaches face
major challenges in temporally extended cases – the co-learning of cooperation/defection detection
and incentivizing mechanism is fragile and suffers from extremely postponed learning signals from
each other. (3) Structural solutions modify the game settings (Rapoport et al., 1989) and thus may
violate the assumption of the study of cooperation emergence. The advantage of homophily is that it
avoids higher-order SDs (e.g., 3rd-SDs) while not requiring identifying cooperation/defection.

C DETAILS OF EXPERIMENTS

Our method is built on the open-sourced codebase PyMARL (Samvelyan et al., 2019) and Sequential
Social Dilemma Games (SSDG) (Vinitsky et al., 2019). We now discuss the specific environmental
settings and implementation details.

C.1 SEQUENTIAL SOCIAL DILEMMAS


-----

In Cleanup game (Fig. 14(a)), the apple regrowth rate decreases linearly with the density of waste and would reduce to
zero as the amount of waste exceeds a saturation threshold. At
the start of each episode, the environment resets with waste just
beyond the saturation threshold. To enable apple reproduction,
agents can fire a cleaning beam to clear waste within the beam
range, but this action induces no immediate reward. Additionally, waste is produced with a certain probability during the
episode, so agents need to keep cleaning waste to maintain the
provision of apples. We have a dilemma in this game. There
is a distance between the waste field and the apple field. It is
personally more rewarding to stay in the apple field to wait for
the spawned apples, but some agents have to sacrifice, leave
the apple field, and contribute to the public goods by cleaning
waste.


Figure 14: (a) Cleanup and (b)
```
Harvest Game.

```

In Harvest game (Fig. 14(b)), the apple reproduction rate grows with the density of uncollected
apples around it. If all apples in a local area are harvested then no apple will re-spawn until the next
episode. There is a dilemma in the game. For individual short-term interests, agents tend to harvest
as rapidly as possible, which will deplete local resources and harm long-term collective interests.
However, agents who abstain from personal benefits for the good of the group are easily exploited by
defecting and over-harvesting agents.

C.2 ENVIRONMENTS

To focus on the problem of social dilemmas, for all experiments, including baselines and ablations,
we remove agent rotation actions in SSDG and set the orientation of all agents to face “up”, like in
previous works studying social dilemmas (Yang et al., 2020). For our method, we disable the “fining”
action and use incentive actions as the way to incentivize other agents. In both Cleanup and Harvest
game, eating an apple will provide a local reward of +1, and there are no other environmental rewards.

In Cleanup, agents are equipped with a cleaning beam, which allows them to remove waste. We test
our method on three Cleanup maps with different numbers of agents. In Table 2, we show the details
of each map.

Table 2: Environmental settings for Cleanup with different numbers of agents.

Parameter n=3 n=5 n=10

Map Size 10×10 25×18 48×18
View Size 7 7 7
Max Steps 50 100 100
Apple Respawn Probability 0.3 0.05 0.05
Depletion Threshold 0.4 0.99 0.99
Restoration Threshold 0.0 0.0 0.0
Waste Spawn Probability 0.5 0.05 0.05

In Harvest, the apple spawning rate at each point is related to the current number of apples within
an ℓ1-distance of 2, and the spawn probability is 0, 0.05, 0.08, and 0.1 when there are 0, 1, 2, and 3
_≥_
apples within the distance, respectively.

C.3 IMPLEMENTATION

**Network architecture There are two main components in our methods: environmental Q-functions**
and incentivizing Q-functions. In the study of social dilemmas, agents learn independently, so we
do not share parameters among different agents, except for the RGB preprocessing network, which
has the following architecture: 1 convolutional layer (6 filters, 3× 3 kernel with stride 1), 1 flatten
layer, and 1 dense layer (32 neurons) with LeakyReLU activation. The environmental Q-functions
and incentivizing Q-functions take the output of the RGB preprocessing network as input.


-----

The environmental Q-function consists of three layers: a fully-connected layer with LeakyReLU as
activation, followed by a 64 bit GRU, and followed by another fully-connected layer that outputs an
estimated utility for each action. The incentivizing Q-function has the same architecture as the acting
Q-function, except that the last layer additionally takes the target agent’s environmental action as
input.

**Optimization For all experiments, we set the loss scaling factor λ[inc]** = 1, λ[homo] = 0.01, discount
factor γ[env] = 0.95, γ[inc] = 0.995, incentive effect factor η[e] = 1.0, and incentive cost factor η[c] = 0.1.
The optimization is conducted using Adam with a learning rate of 0.0001. For exploration, we use
_ϵ-greedy with ϵ annealed linearly from 1.0 to 0.05 over 50K time steps and kept constant for the rest_
of the training. We run one environment each time to collect samples. Batches of 16 episodes are
sampled from the replay buffer (whose size is 5000), and the whole framework is trained end-to-end
on fully unrolled episodes.

Experiments are carried out on NVIDIA GTX 2080 Ti GPU. For a 10-agent environment (like
```
Cleanup (n=10)), our method requires approximately 60G of RAM and 0.9G of video memory, and

```
takes about 115 hours to finish 5M timesteps of training.

C.4 SIMILARITY OF ENVIRONMENTAL BEHAVIORS

In Sec. 4.2 of the paper, we discuss how to encourage homophily in temporally extended cases.
A challenge is to measure the similarity of environmental behaviors. In this paper, we use the
X-means (Pelleg et al., 2000) implementation provided by PyClustering (Novikov, 2019) to identify
similar behaviors. The behavioral features include the changes made to common information like
the amount of harvesting, the amount of cleanup, etc., in the last 10 timesteps. When clustering, the
minimum and maximum number of clusters are set to 2 and 4, respectively.

D DETAILS OF BASELINES AND ABLATIONS

We compare our method against various baselines and ablations. For LIO (Yang et al., 2020), Inequity
Aversion (Hughes et al., 2018), and Social Influence (Jaques et al., 2019), we use the codes provided
by the authors and the hyper-parameters that have been fine-tuned on the Sequential Social Dilemma
Games (SSDG) (Vinitsky et al., 2019). For Selfish Actor-Critic, we use the default implementation in
SSDG.

The ablation Cont. inc. actions, which learns continuous incentives, uses the same implementation
and hyperparameter settings as LIO, but with the only difference that the incentive cost is changed
from the default value of LIO to 0.1, which is the same as in our method. Ablation w/o homophily
and w/ inc use the identical network structure and hyper-parameter settings as our method. The
differences are that w/o homophily does not use the homophily loss by setting homophily loss scaling
factor λ[homo] to 0, and w/ inc additionally uses received incentives to train incentive Q-functions. For
all the experiments, we keep the game settings to be the same for fair comparisons.

E FORMAL ANALYSIS

In this section, we formally calculate the closed-form gradients in the policy space for the motivating
example problem, which are visualized as ternary plots or quaternary plots in Sec. 3 of the paper.

E.1 NOTATIONS

Suppose a team of n agents joins the public goods game introduced in Sec. 3, where the reward
functions can be found in Appendix A. The action set A is identical for all agents and depends on the
specific settings in the following subsections. Agent i chooses actions according to its own policy
_πi, which is parameterized by |A| parameters, and_ _X∈A_ _[π][i][(][a][i][ =][ X][) = 1][. In addition, we assume]_
agents choose actions independently, that is π(a1, · · ·, an) = _j=1_ _[π][j][(][a][j][)][.]_

[P]

We first define two frequently used equations. Let πk(ak = N ) = θk,N, where N represents the
action of non-participanting, θk,N [0, 1], and k = 1, 2, _n.[Q][n]_
_∈_ _· · ·_


-----

First we define


_E_ _i = Eπ_ _,_
_−_ " _n −_ [P]k≠ _i_ [I][(][a][k][ =][ N] [)] #

where _k≠_ _i_ [I][(][a][k][ =][ N] [)][ is the number of agents taking the non-participating action][ N] [. We use]

the notation d _i = (d1,_ _, di_ 1, di+1, _, dn), where dk_ I(ak = N ) 0, 1 is the indicator
variable, i.e.,[P] _d−k = 1 if a · · ·k = N and otherwise−_ _· · ·_ _dk = 0. We have ≡_ _∈{_ _}_


(1 − _θk,N_ )[1][−][d][k] _θk,N[d][k]_ _[.]_ (14)
_kY≠_ _i_


_E_ _i =_
_−_ **d−i∈{X0,1}[n][−][1]** _n −_ [P]k≠ _i_ _[d][k]_

_∂_

_∂θi,N_ _[E][−][i][ = 0][. Similarly, we define]_


Note that


1

_E_ _ij = Eπ_
_−_ " _n −_ [P]k/∈{i,j} [I][(][a][k][ =][ N] [)] #

and use the notation d _ij = (d1,_ _, di_ 1, di+1, _, dj_ 1, dj+1, _, dn), where dk_ I(ak =
_N_ ) ∈{0, 1}. We have _−_ _· · ·_ _−_ _· · ·_ _−_ _· · ·_ _≡_


1

(1 _θk,N_ )[1][−][d][k] _θk,N[d][k]_ _[.]_ (15)

_n −_ [P]k/∈{i,j} _[d][k]_ _k/∈{Yi,j}_ _−_


_E_ _ij =_
_−_


**d−ij** _∈{0,1}[n][−][2]_


Note that


_∂_

_∂θi,N_ _[E][−][ij][ = 0][.]_


E.2 SEC. 3 PART A

To showcace the influence of first-order social dilemmas, in Sec. 3 Part A we consider three atomic
actions, C, D, N . Assume that agent i chooses actions from A = {C, D, N _} with probabilities_
_θi,C, θi,D, θi,N_ . The reward function is defined in Eq. 7 in Appendix A.1. We can calculate the value
function of agent i:

_Vi[π]_ [=][E][π] [[][r][i][(][a][1][,][ · · ·][, a][n][)]]

1

=Eπ I(ai = C) _b_ I(aj = C) + 1 _c_

"  _n −_ [P]j≠ _i_ [I][(][a][j][ =][ N] [)]  _j≠_ _i_  _−_ 

 [X]  

1
+ I(ai = D)b I(aj = C)

_n −_ [P]j≠ _i_ [I][(][a][j][ =][ N] [)]  _j≠_ _i_ 

[X] 

+ I(ai = N )σ

#


=θi,C _b_ _θj,CE_ _ij + E_ _i_ _c_ + θi,Db _θj,CE_ _ij + θi,N_ _σ,_

  _−_ _−_  _−_  _−_

_j≠_ _i_ Xj≠ _i_

 [X]  

where E _i and E_ _ij are defined as in Eq. 14 and Eq. 15, respectively. Then we can calculate the_
_−_ _−_
derivatives of the value function w.r.t each parameter:


_θj,CE_ _ij + E_ _i_ _c,_ (16)

 _−_ _−_  _−_

_j≠_ _i_

[X] 


_∂Vi[π]_ =b

_∂θi,C_


-----

_∂Vi[π]_ =b

_∂θi,D_


_θj,CE−ij,_ (17)

Xj≠ _i_


_∂Vi[π]_ =σ. (18)

_∂θi,N_


The above equations are used to plot Fig. 1(d) of the paper.

E.3 SEC. 3 PART B

To introduce altruistic incentives into the system, we add the fourth type of atomic action, PA. We
consider unexploitable punishments here, and exploitable punishments in the next subsection. Agent
_i chooses actions from A =_ _C, D, N, PA_ with probabilities θi,C, θi,D, θi,N _, θi,P A. The reward_
_{_ _}_
function in this case is defined in Eq. 8 in Appendix A.1. We can calculate the value function of agent
_i,_

_Vi[π]_ [=][E][π] [[][r][i][(][a][1][,][ · · ·][, a][n][)]]


I(aj = C, PA) + 1 _c_

  _−_

_j≠_ _i_

[X] 


=Eπ


I(ai = C)


= C) _b_

_n −_ [P]j≠ _i_ [I][(][a][j][ =][ N] [)]

I(aj = PA)

Xj≠ _i_ !


_αp_ [1]
_−_ _n_


1

I(aj = C, PA) _p_ [1]

_n −_ [P]j≠ _i_ [I][(][a][j][ =][ N] [)]  _j≠_ _i_  _−_ _n_

[X] 


+ I(ai = D) _b_



+ I(ai = N )σ


I(aj = PA)

Xj≠ _i_


1

+ I(ai = PA) _b_ I(aj = C, PA) + 1 _c_

_n −_ [P]j≠ _i_ [I][(][a][j][ =][ N] [)]  _j≠_ _i_  _−_

[X] 

_αk_ [1] I(aj = C) _k_ [1] I(aj = D)
_−_ _n_ Xj≠ _i_ _−_ _n_ Xj≠ _i_ !#


=θi,C _b_ (θj,C + θj,P A)E _ij + E_ _i_

  _−_ _−_

_j≠_ _i_

 [X]

+ θi,D _b_ (θj,C + θj,P A)E _ij_ _p_ [1]

 Xj≠ _i_ _−_ _−_ _n_

+ θi,N _σ_


_c_ _α_ [1]

 _−_ _−_ _n_ _[p]_

_θj,P A_



Xj≠ _i_




_θj,P A_

Xj≠ _i_


(θj,C + θj,P A)E _ij + E_ _i_ _c_ _αk_ [1]

 _−_ _−_  _−_ _−_ _n_

_j≠_ _i_

[X] 


_θj,C_ _k_ [1]

Xj≠ _i_ _−_ _n_


+ θj,P A _b_






_,_






_θj,D_

Xj≠ _i_


where I(aj = C, PA) equals one if and only if aj = C or aj = PA, and E _i and E_ _ij are defined_
_−_ _−_
in Eq. 14 and Eq. 15, respectively. Then we can calculate the derivatives of the value function w.r.t
each parameter,


_∂Vi[π]_ =b (θj,C + θj,P A)E _ij + E_ _i_

_∂θi,C_  _−_ _−_

_j≠_ _i_

[X]


_c_ _αp_ [1]

 _−_ _−_ _n_


_θj,P A,_ (19)

Xj≠ _i_


-----

_∂Vi[π]_ =b

_∂θi,D_


(θj,C + θj,P A)E _ij_ _p_ [1]

Xj≠ _i_ _−_ _−_ _n_


_θj,P A,_ (20)

Xj≠ _i_


_∂Vi[π]_ =σ, (21)

_∂θi,N_


_∂Vi[π]_ =b (θj,C + θj,P A)E _ij + E_ _i_ _c_ _αk_ [1]

_∂θi,P A_  _−_ _−_  _−_ _−_ _n_

_j≠_ _i_

[X] 

The above equations are used to plot Fig. 3(a) of the paper.


_θj,C_ _k_ [1]

Xj≠ _i_ _−_ _n_


_θj,D._ (22)

Xj≠ _i_


E.4 SEC. 3 PART C

Now we consider exploitable punishments. Agent i chooses actions from A = {C, D, N, P _} with_
probabilities θi,C, θi,D, θi,N _, θi,P . The reward function is defined in Eq. 9 in Appendix A.1. We can_
calculate the value function of agent i:

_Vi[π]_ [=][E][π] [[][r][i][(][a][1][,][ · · ·][, a][n][)]]


I(aj = C, P ) + 1 _c_

 _j≠_ _i_  _−_ !

[X] 

I(aj = C, P ) _p_ [1] I(aj = P )

  _−_ _n_

_j≠_ _i_ Xj≠ _i_

[X] 


=Eπ


I(ai = C)


_n −_ [P]j≠ _i_ [I][(][a][j][ =][ N] [)]

1

_n −_ [P]j≠ _i_ [I][(][a][j][ =][ N] [)]


+ I(ai = D) _b_



+ I(ai = N )σ


I(aj = C, P ) + 1 _c_

  _−_

_j≠_ _i_

[X] 


+ I(ai = P )


= P ) _b_

_n −_ [P]j≠ _i_ [I][(][a][j][ =][ N] [)]

I(aj = D)

Xj≠ _i_ !#


_k_ [1]
_−_ _n_


=θi,C _b_ (θj,C + θj,P )E _ij + E_ _i_ _c_

  _−_ _−_  _−_ 

_j≠_ _i_

 [X]  

+ θi,D _b_ (θj,C + θj,P )E _ij_ _p_ [1] _θj,P_

 Xj≠ _i_ _−_ _−_ _n_ Xj≠ _i_

+ θi,N _σ_


+ θj,P _b_ (θj,C + θj,P )E _ij + E_ _i_ _c_ _k_ [1] _θj,D_ _,_

  _−_ _−_  _−_ _−_ _n_ 

_j≠_ _i_ Xj≠ _i_

 [X]  

where I(aj = C, P ) equals one if and only if aj = C or aj = P, and E _i and E_ _ij are defined in_
_−_ _−_
Eq. 14 and Eq. 15, respectively. Then we can calculate the derivatives of the value function w.r.t each
parameter,


(θj,C + θj,P )E _ij + E_ _i_ _c,_ (23)

 _−_ _−_  _−_

_j≠_ _i_

[X] 

(θj,C + θj,P )E _ij_ _p_ [1] _θj,P,_ (24)

Xj≠ _i_ _−_ _−_ _n_ Xj≠ _i_


_∂Vi[π]_ =b

_∂θi,C_

_∂Vi[π]_ =b

_∂θi,D_


-----

_∂Vi[π]_ =σ, (25)

_∂θi,N_

_∂Vi[π]_ =b (θj,C + θj,P )E _ij + E_ _i_ _c_ _k_ [1] _θj,D._ (26)

_∂θi,P_  _−_ _−_  _−_ _−_ _n_

_j≠_ _i_ Xj≠ _i_

[X] 

The above equations are used to plot Fig. 2(d) of the paper. It is worth noting that


_∂Vi[π]_ _i_ = _k_ [1]

_∂θi,P_ _−_ _∂θ[∂V]i,C[ π]_ _−_ _n_


_θj,D_ 0. (27)

Xj≠ _i_ _≤_


The gradient is non-positive for any θj,D, which indicates that second-order cooperators would be
taken over by second-order defectors.

E.5 SEC. 3 PART D

Based on the settings specified in Appendix E.4, we consider homophily here. Agent i chooses
actions from A = {C, D, N, P _} with probabilities θi,C, θi,D, θi,N_ _, θi,P . The reward function is_
defined in Eq. 9. We can calculate the value function of agent i:

_Vi[π]_ [=][E][π] [[][r][i][(][a][1][,][ · · ·][, a][n][)]]


I(aj = C, P ) + 1 _c_

 _j≠_ _i_  _−_ !

[X] 

I(aj = C, P ) _p_ [1] I(aj = P )

  _−_ _n_

_j≠_ _i_ Xj≠ _i_

[X] 


=Eπ


I(ai = C)


_n −_ [P]j≠ _i_ [I][(][a][j][ =][ N] [)]

1

_n −_ [P]j≠ _i_ [I][(][a][j][ =][ N] [)]


+ I(ai = D) _b_



+ I(ai = N )σ


I(aj = C, P ) + 1 _c_

  _−_

_j≠_ _i_

[X] 


+ I(ai = P )


= P ) _b_

_n −_ [P]j≠ _i_ [I][(][a][j][ =][ N] [)]

I(aj = D)

Xj≠ _i_ !#


_k_ [1]
_−_ _n_


=θi,C _b_ (θj,C + θj,P )E _ij + E_ _i_ _c_

  _−_ _−_  _−_ 

_j≠_ _i_

 [X]  

+ θi,D _b_ (θj,C + θj,P )E _ij_ _p_ [1] _θj,P_

 Xj≠ _i_ _−_ _−_ _n_ Xj≠ _i_

+ θi,N _σ_


+ θj,P _b_ (θj,C + θj,P )E _ij + E_ _i_ _c_ _k_ [1] _θj,D_ _,_

  _−_ _−_  _−_ _−_ _n_ 

_j≠_ _i_ Xj≠ _i_

 [X]  

where I(aj = C, P ) equals one if and only if aj = C or aj = P, and E _i and E_ _ij are defined in_
_−_ _−_
Eq. 14 and Eq. 15, respectively. Then we can calculate the derivatives of the value function w.r.t each
parameter. Besides, to show the effect of homophily, we encourage agents with similar environmental
behaviors to have similar incentivizing behaviors. Since only contributors and punishers have the
same environmental behavior of contributing to the public goods, we encourage their incentivizing
behaviors to be the same by converting the minority of P and C to the majority with a probability of


-----

_λ = 0.2. For each agent, the converting direction depends on all the other agents. Specifically, if_
other agents choose action P more often (sign([P]j=i[(][θ][j,P][ −] _[θ][j,C][))][ >][ 0][), agent][ i][ will increase the]_

_̸_
probability of choosing action P by λ · min(θi,C, θi,P ), and vice versa. Formally, taken homophily
into consideration, the gradients of each parameter of agent i are:

_θ˙i,C =b_ (θj,C + θj,P )E _ij + E_ _i_ _c_ (28)

 _−_ _−_  _−_

_j≠_ _i_

[X] 

+ λ `sign` (θj,C _θj,P )_ min(θi,C, θi,P ), (29)
_·_  _j≠_ _i_ _−_ 

[X] 

_θ˙i,D =b_ (θj,C + θj,P )E _ij_ _p_ [1] _θj,P,_ (30)
Xj≠ _i_ _−_ _−_ _n_ Xj≠ _i_

_θ˙i,N =σ,_ (31)


_θ˙i,P =b_ (θj,C + θj,P )E _ij + E_ _i_ _c_ _k_ [1] _θj,D_ (32)

 _−_ _−_  _−_ _−_ _n_

_j≠_ _i_ Xj≠ _i_

[X] 

+ λ `sign` (θj,P _θj,C)_ min(θi,C, θi,P ). (33)
_·_  _j≠_ _i_ _−_ 

[X] 

The above equations are used to plot Fig. 3(b) of the paper. And it is worth noting that

_θ˙i,P_ _θi,C =_ _k_ [1] _θj,D + 2λ_ `sign` (θj,P _θj,C)_ min(θi,C, θi,P ), (34)
_−_ [˙] _−_ _n_ Xj≠ _i_ _·_  _j≠_ _i_ _−_ 

[X] 

which is positive when the population is close to point P, in which situation the population chooses
action P more often than C and D (sign([P]j=i[(][θ][j,P][ −] _[θ][j,C][))][ >][ 0][ and][ P]j=i_ _[θ][j,D][ is small), and]_

_̸_ _̸_

thus the agents will not deviate to second-order free-riding.

E.6 PROJECT ONTO THE PROBABILITY SIMPLEX

After we have obtained the gradients of each action of agent i, we can calculate the new policy as
follows:


_θi,X[′]_ [=][ θ][i,X] [+][ β ∂V]i[ π] _,_ _X_ _A._ (35)

_∂θi,X_ _∀_ _∈_

where β is the learning rate. Now πi[′][(][a][i][ =][ X][) =][ θ]i,X[′] [. However, this new policy may not be in the]
valid probability space, i.e. _X_ _A_ _[π]i[′][(][a][i][ =][ X][) = 1][ may not hold anymore. To address this issue,]_

_∈_
we project it onto the probability simplex (Chen & Ye, 2011; Wang & Carreira-Perpiñán, 2013) after
each update. Let Π∆|A| : R[|][A][P][|] ∆[|][A][|] denotes the projection to the valid space:
_→_

Π∆|A| [x] = arg min (36)
**_z_** ∆[|][A][|][ ∥][x][ −] **_[z][∥][,]_**
_∈_

where ∥· ∥ denotes the regular Euclidean norm and ∆[|][A][|] is the canonical simplex defined by


_|A|_

_zi = 1_



_i=1_

X 

 _[.]_


∆[|][A][|] :=





[z][ = (][z][1][,][ · · ·][, z][|][A][|][)][⊤] _[∈]_ [R][|][A][|][ : 0][ ≤] _[z][i][ ≤]_ [1][, i][ = 1][,][ · · ·][, n,][ and]


-----

Using this projection, the valid new policy of agent i is

**_πi[′]_** _[←]_ [Π]∆[|][A][|] [[][π]i[′][]][,] (37)

where πi[′] [= (][θ]i,X[′] [)][⊤]X _A_ [is the parameter vector of agent][ i][. With enough updates and this projection]
_∈_
rule, we can determine the location of the “safe region” in Fig. 2(d), Fig. 3(a) and Fig. 3(b) of the
paper.

F LIMITATIONS

F.1 LIMITATIONS OF FORMAL ANALYSIS

In this work, we study the problem of cooperation emergence in both single-stage and sequential
social dilemmas. For single-stage social dilemmas, we provide formal analyses and numerical results
for the effect of free-riders and homophily in Sec. 3 and Appendix E. However, in the main text, for
sequential social dilemmas, we only provide empirical results in Sec. 5.

Our aim is to formally analyze the evolutionary dynamics of environmental behaviors and incentivizing behaviors on SSDs. However, the environmental policies and incentivizing policies are
interdependent, and it is difficult to accurately account for the effect of incentivizing behaviors when
considering environmental policy updates.

To better explain this challenge, we first give some notations. There are n agents participating in
the public goods game introduced in Sec. 3. The first-order cooperation contributes to the public
goods and benefits from it, while the first-order defection does not contribute but benefits from the
public goods. The second-order cooperation will pay a cost to punish those who choose first-order
defection, while the second-order defection will not punish such defections. Each agent i chooses
actions according to its own policy πi, which includes environmental policy πi[1][(][a]i[1][)][ parameterized]
by θi,1 and incentivizing policy πi[2][(][a]i[2][)][ parameterized by][ θ][i,][2][. We assume that the two policies are]
independent, i.e., πi(a[1]i _[, a]i[2][) =][ π]i[1][(][a]i[2][)][π]i[2][(][a]i[2][)][. Agent][ i][ chooses first-order cooperation (or defection)]_
with probability πi[1][(][a]i[1] [=][ C][) =][ θ][i,][1][ (or][ π]i[1][(][a]i[1] [=][ D][) = 1][ −] _[θ][i,][1][) and chooses second-order]_
cooperation (or defection) with probability πi[2][(][a]i[2] [=][ C][) =][ θ][i,][2][ (or][ π]i[2][(][a]i[2] [=][ D][) = 1][ −] _[θ][i,][2][). We also]_
assume different agents choose actions independently: π(a[1]1[, a][2]1[,][ · · ·][, a]n[1] _[, a][2]n[) =][ Q][n]j=1_ _[π][j][(][a]j[1][, a][2]j_ [)][.]
We can calculate the value function of agent i,

_Vi[π]_ [=][E][π][[][r][i][(][a]1[1][, a]1[2][,][ · · ·][, a]n[1] _[, a]n[2]_ [)]]


_b_

_j_ [=][ C][)][ −] _[c][I][(][a]i[1]_ [=][ C][)][ −] _[p][ 1]_ _i_ [=][ D][)]
_n_ [I][(][a][1] _n_ [I][(][a][1]


=Eπ


I(a[2]j [=][ C][)]

Xj≠ _i_


_j=1_


_k_ [1] _i_ [=][ C][)]
_−_ _n_ [I][(][a][2]


I(a[1]j [=][ D][)]

Xj≠ _i_


_n_

_b_

= _θj,2_ _k_ [1] (1 _θj,1)_

Xj=1 _n_ _[θ][j,][1][ −]_ _[cθ][i,][1][ −]_ _[p][ 1]n_ [(1][ −] _[θ][i,][1][)]_ Xj≠ _i_ _−_ _n_ _[θ][i,][2]_ Xj≠ _i_ _−_

_b_

=( _[b]_

_n_ _n_ _[θ][j,][1][ −]_ _[p][ 1]n_ [(1][ −] _[θ][i,][1][)][θ][j,][2][ −]_ _[k][ 1]n_ _[θ][i,][2][(1][ −]_ _[θ][j,][1][)]_

_[−]_ _[c][)][θ][i,][1][ +]_ Xj≠ _i_ 


The learning process of agent i’s incentivizing policy can be framed as a bi-level optimization
problem,

max _i_
_πi[2]_

_[∈][∆][2][V][ π][′]_


-----

subject to π[′] =


_πj[1][,][′][π]j[2]_
_j=1_

Y


_πj[1][,][′]_ = arg max _j_ _[, j][ = 1][,][ · · ·][, n]_
_πj[1]_

_[∈][∆][1][ V][ π]_

where ∆[1], ∆[2] is the valid space for environmental and incentivizing policies, respectively. The
environmental policies of all the agents are first updated under the influence of incentivizing policies,
and then the incentivizing policy of agent i is optimized under the new environmental policies.
It is challenging to get a closed-form solution for this optimization problem when other agents’
incentivizing policies also optimize this bi-level optimization problem. But we can still provide
insights into this problem. We first calculate the gradient of the value function Vj[π] [w.r.t][ θ][j,][1][:]


_∂Vj[π]_ = _[b]_

_∂θj,1_ _n_ _n_

_[−]_ _[c][ +][ p][ 1]_

With learning rate β, the updated θj,[′] 1 [becomes:]


_θk,2._ (38)

Xk≠ _j_


_θj,[′]_ 1 [=][ θ][j,][1] [+][ β ∂V]j[ π] = θj,1 + β

_∂θj,1_

Now the new value function for agent i is


 _n_ _n_

_[−]_ _[c][ +][ p][ 1]_

 _[b]_


_θk,2_ _._ (39)



Xk≠ _j_




_b_

_j,1_ _i,1[)][θ][j,][2]_ _j,1[)]_ _._ (40)
_n_ _[θ][′]_ _[−]_ _[p][ 1]n_ [(1][ −] _[θ][′]_ _[−]_ _[k][ 1]n_ _[θ][i,][2][(1][ −]_ _[θ][′]_

 


_Vi[π][′]_ = ( _[b]_ _i,1_ [+]

_n_

_[−]_ _[c][)][θ][′]_


_j≠_ _i_


To calculate updated incentivizing policies, we derive the gradient of Vi[π][′] w.r.t. θi,2. Note that agent
_i’s incentivizing policy does not directly influence its own environmental behaviors, so we have_

_∂θ∂θj,i,[′]_ 21 = 0βpn, _[,]_ _jj ̸ == i. i,_


With this derivative in hand, we can obtain the gradient of Vi[π][′] w.r.t θi,2:


_b_

_j,1[) +][ βpk][ 1]_

_n[2][ βp][ −]_ _[k][ 1]n_ [(1][ −] _[θ][′]_ _n[2][ θ][i,][2]_




_∂Vi[π][′]_

_∂θi,2_


_j≠_ _i_


_n_

_b_

= _k_ [1] (1 _θj,1) + β_ [1] (n 1) + pk [1] (n 2) _θj,2 + nθi,2_
_−_ _n_ _−_ _n_  _−_ _n_ [(][p][ +][ k][)][ −] _[kc]_ _n_  _−_

_j=i_   _j=1_

X̸ X

 

We find that the learning rate β has a decisive influence on the positivity and negativity of the above
equation, and the direction of the gradient w.r.t θi,2 cannot be determined at each step. An alternative
is to consider multiple gradient steps, calculate optimal Vi[π] [given incentivizing policies, and then]
derive the change direction of θi,2 based on convergent Vi[π][. However, it is challenging to formally]
calculate the global optimal Vi[π][.]

In addition, another problem arises when we consider the homophily constraint as a loss function.
Using the definition of Eq. 4 and Eq. 5 in the paper, the homophily loss of agent i under the updated
environmental policies is


_._






_i_ =
_L[π][′]_ _−_


(1−θk,[′] 1[)] _θi,[′]_ 1[θ]j,[′] 1 [+ (1][ −] _[θ]i,[′]_ 1[)(1][ −] _[θ]j,[′]_ 1[)] (θj,2 log θi,2 + (1 − _θj,2) log(1 −_ _θi,2)) ._
_j≠_ _i,k/X∈{i,j}_   

(41)


-----

With further calculation, we can find that the derivative of Li[π][′] [w.r.t][ θ][i,][2][ has a nonlinear dependency]
on others’ policies, which makes the dynamics of the system hard to capture, not to mention obtaining
the closed-form solution (Perko, 2013).

So far, we have presented the challenges of obtaining theoretical supports in temporally extended
cases. These challenges have perplexed the researchers for a long time (Gould et al., 2016; Hirsch
et al., 2012; Hughes et al., 2018), and we believe that the solution for these questions is an important
and promising direction for future work.

F.2 LIMITATIONS OF EMPIRICAL IMPLEMENTATIONS

In this work, we encourage homophily on the level of incentivizing behaviors and outperform other
works in SSDs. However, computing the homophily loss in Sec. 4 requires observing other agents’
environmental actions, as well as their incentivizing actions towards all other agents. It might be
a promising future work to restrict the agents’ observaton and only allow the agent to incentivize
nearby agents. Opponent modeling and communication might be critical techniques to achieve these
goals.

In addition, incentivizing actions are expected to positively influence the return given by the environment through incentivizing other agents. However, incentivizing actions mainly influence the
_policy update of other agents, so the effects of incentivizing actions could be delayed, which may_
result in slow learning. Yang et al. (2020) have studied this issue on the framework of actor-critic,
but how to effectively solve this problem on the framework of Q-learning is still an open question.
Fortunately, one advantage of homophily is that its influence is immediate. If homophily can help
cooperation emergence (and fortunately it can, as shown by our results), this timely feedback can
bypass the problem of delayed reward influence. In the meantime, we are aware that addressing the
problem of delayed reward influence may further improve learning performance. We will study this
more in-depth in future work.


-----

