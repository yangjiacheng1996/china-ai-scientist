# STRUCTURED ENERGY NETWORK
## AS A DYNAMIC LOSS FUNCTION
A CASE STUDY WITH MULTI-LABEL CLASSIFICATION

**Anonymous authors**
Paper under double-blind review

ABSTRACT

We propose SEAL which utilizes an energy network as a trainable loss function
for a simple feedfoward network. Structured prediction energy networks (SPENs)
(Belanger & McCallum, 2016; Gygli et al., 2017) have shown that a neural network
(i.e. energy network) can learn a reasonable energy function over the candidate
structured outputs. We find that rather than using SPENs as a prediction network,
using it as a trainable loss function is not only computationally more efficient but
also better performing. As the energy function is trainable, we propose SEAL
to be dynamic so it can adapt the energy function to focus on the region where
feed-forward models will be affected the most at the time point of model update.
We find this to be effective in an ablation study comparing SEAL to the static
version (§5) where the energy function is fixed after pretraining. We show the
relation to previous work on the joint optimization model of energy network and
feedforward model (INFNET) as we show that it is equivalent to SEAL using
margin-based loss if INFNET relaxes their loss function. Based on the unique
architecture of SEAL, we further propose a variant of SEAL that utilizes noise
contrastive ranking (NCE) loss that by itself does not perform well as a structured
energy network, but embodied in SEAL, it shows the greatest performance among
the variants we study. We demonstrate the effectiveness of SEAL on 7 feature-based
and 3 text-based multi-label classification datasets. The best version of SEAL that
uses NCE ranking method (SEAL-NCE) achieves close to 2.85, 2.23 respective F1
point gain on average over cross-entropy and INFNET for feature-based datasets,
excluding one dataset that has an excessively large gain. Lastly, examining whether
the proposed framework is effective on a large pre-trained model as well, we
observe SEAL-NCE achieving 0.87, 0.68 respective F1 point gain in average over
cross-entropy and INFNET with BERT-based adapter model on text datasets.

1 INTRODUCTION

Structured prediction is a popular machine learning task wherein the model learns a mapping function
from an input x to a multivariate-structured output y. Popular examples of this include image
segmentation (Muller, 2014), extracting parse trees or semantic role labels from a text (Palmer et al.,¨
2010), and multi-label classification (Belanger & McCallum, 2016; Gygli et al., 2017). In structured
prediction, the output space Y is often extremely large. For example, in multi-label classification, the
size of Y is 2[L] where model needs to predict output y ∈{0, 1}[L]. There are two key aspects that a
model for structured prediction needs to balance: statistical efficiency and computational efficiency.
Most models for structured prediction can be categorized into two categories: the feed-forward
approach, wherein one learns a neural network that models the direct mapping between the input
and the structured output, and the structured approach, in which the model explicitly models the
interactions (structure) in the output space.

The feed-forward approach learns all dimensions of y jointly in a conditionally independent manner
given input x, relying on the representational power of the network to capture dependencies in output
structure implicitly. While the feed-forward approach is computationally efficient, since it does
not capture relationships in the label space, it lacks statistical efficiency. The traditional structured
approaches, on the other hand, model a joint probability distribution P (x, y) that can capture the
label relationships. However, due to the intractability of modeling the full joint distribution which


-----

captures every possible interaction of output space, these approaches resort to limiting the interaction
terms to local subsets of the output space (Lafferty et al., 2001; Ghamrawi & McCallum, 2005). In
order to make inference for structured models more efficient, a recent line of work replaces the joint
probability with a structured energy function E(x, y) (LeCun et al., 2006), which can be thought of
as an unnormalized probability distribution, allowing the model to learn arbitrary global dependencies
in the output space. There have been several works that propose efficient approximate inference
procedures for structured energy networks using gradient based inference (Belanger et al., 2017;
Gygli et al., 2017; Rooshenas et al., 2019), hence also called ’prediction networks’. These models
showed noticeable gains in predictive performance over feed-forward models and graphical models
that assume partial structure such as full pairwise potential (Chen et al., 2015; Schwing & Urtasun,
2015). Despite these efforts, the inference for energy-based models still remains relatively inefficient
when compared to the feed-forward approach (Tu & Gimpel, 2019). Moreover, in our experience,
the models using gradient based inference (GBI) are finicky to train as the training also utilizes GBI,
requiring numerous hyperparameters: step size, number of iterations for GBI, and initial point to
begin GBI. This raises a question: Can the energy network be used in a way that is as expressive as a
_full joint probability, as efficient at inference as a feed-forward approach, and also stable and easy to_
_train? We believe that using structured energy networks as a parameterized dynamic loss function for_
feed-forward networks, instead of a prediction network, can fulfill all these requirements.

In this paper, we propose the Structured Energy As Loss (SEAL) framework that uses a trainable
structured energy network (SEN)[1] as a loss function guiding the training of a feed-forward network.
The key idea is to provide the feed-forward network access to rich relationships in the output space
through a learned loss function. We also propose to learn SEN in a dynamic fashion by adjusting
the energy function to be confident with the most up-to-date outputs of the feed-forward network.
We show that learning the loss function dynamically leads to more efficient, more stable, and better
performance. SEAL can be viewed as general-purpose framework where one can plugin various loss
functions and architectures to train SEN as well as the feed-forward network. Through experiments
(§4), we analyze the effect of applying different energy losses (e.g. margin-based, regression-based)
within SEAL framework. We also propose the noise-contrastive ranking loss (NCEranking) for SEN
which performs the best within the SEAL framework.

To summarize, we introduce a general framework SEAL that interprets structured energy networks
(SEN) as a dynamic loss functions. Through empirical evaluation on the task of multi-label classification, we analyse the impact of various loss functions for updating SEN. Finally, we propose
an NCE ranking loss that is uniquely suited for the SEAL framework, and demonstrate its superior
performance on 7 feature-based as well as 3 text-based multi-label classification datasets, when
compared to simple feed-forward approach and various energy based models.

Algorithm 1: SEAL Algorithm

**Require: (x, y): Training Instance**
**Require: FΦ: Feedforward Network**
**Require: sampling: True/False flag**
**Require: optimizerΘ, optimizerΦ**
**Require: T** : No. of steps

_t ←_ 0
Θwhile0, Φ t < T0 ← Random initialization do

**if sampling then**

_S_ **y[(][i][)], i = 1, . . ., K** **y[(][i][)]** _FΦt_ (x)
_←{_ _|_ _∼_ _}_

**else**

Figure 1: Overview of the SEAL framework. The _S_ _FΦt_ (x) _▷_ singleton set
figure on the right shows the update to the en- _←{_ _}_
ergy network (equation 4) and the figure on theleft shows the update to the feed-forward network UpdateUpdatet ← _t + 1 Θ Φ as Eqn. 4 using optimizer as Eqn. 3 using ˜y ∈_ _S and optimizerΦ_ Θ
(equation 3).

1The term comes from structured prediction energy network, minus ’prediction’, as we do not use it for
prediction anymore. For brevity, we use terms ’energy network’ and ’strcutured energy newtork’ interchangeably.


-----

2 STRUCTURED ENERGY NETWORK AS LOSS (SEAL)

This section first describes the proposed SEAL framework. Let X denote the input space, Y = {0, 1}[L]

the output space, and _Y[˜] = [0, 1][L]_ the continuous relaxation of Y. Then the structured energy
_EΘ : X ×_ _Y →[˜]_ R is defined to be a parameterized function of the input and the continuous relaxation
of the output. The feedforward network FΦ : X → _Y[˜] is defined to be a neural network that maps_
an input to the continuous relaxation of the output space. We denote j-th training instance pair as
(x[(][j][)], y[(][j][)]) and yi 0, 1 _, ˜yi_ [0, 1] to denote i-th label dimension where i 1, . . ., L .
_∈D_ _∈{_ _}_ _∈_ _∈{_ _}_

As shown in Figure 1, SEAL consists two loss functions: the energy loss LE that trains structured
energy network (Θ), and feedforward loss LF that guides the training of the feedforward network
(Φ). We first discuss how structured energy network implicitly affects training of the feedforward
network by defining LF (Φ). Given a training instance (x, y), the feedforward loss is defined as:


LF (Φ) = λ1EΘ(x, FΦ(x)) − _λ2_



[yi log FΦ(x)i + (1 − _yi) log (1 −_ _FΦ(x)i)] ._ (1)
_j=1_

X


Here, the first term involving the energy captures interaction across label space whereas binary
cross-entropy considers the predictions of each label independently (More details in Appendix B).

Since the training of feedforward network depends on the quality of the parameterized energy network,
it is critical to find the parameters Θ that produce the best loss surface for training the feedforward
network. This can be done in two ways: by first training the energy separately, or by training the
energy and the feedforward network simultaneously. SEAL uses the latter, which is shown to perform
better (see §5) than the former. We denote the former method as SEAL-static for distinction .

In SEAL-static, we first estimate the energy network parameter Θ over training data and optimize Φ
by plugging fixed Θ into LF . Given (x, y) ∈D, Φ is trained as equation 2.

1 1 [b]
minΦ[b] LF (Φ) s.t. Θ = arg minΘ LE(x, y, ˜y; Θ) (2)

_|D|_ XD _|D|_ XD

b

We finally present SEAL which learns Θt dynamically at step t. To do so, we alternate the optimization
steps of Θ and Φ. Given a training instance (x, y) _Bt, a complete training step is given as:_
_∈_

[b] 1

Θt _←_ Θt−1 −∇Θ _|Bt|_ _Bt_ LE **x, y, FΦt−1** (x); Θ (3)

X   

1
Φt Φt 1 Φ LF (Φ) (4)
_←_ _−_ _−∇_ _|Bt|_ _Bt_

X

Note that, from (2) to (3), arbitrary ˜y got replaced with FΦt−1 (x) in estimating Θt so that estimation
of energy surface depends on model output FΦt−1 (x). Further, the update step for feedforward
network Φ relies on Θt rather than static Θ. In both SEAL and SEAL-static, test-time predictions are
performed solely using the feedfoward network as ˆyi = 1(FΦ(x)i ≥ 0.5).

The motivation for proposing dynamic loss function in[b] SEAL is as following. It would be useful
to learn perfect energy surface, if possible, by estimating Θ that is static. However, doing so is
challenging as we are estimating energy surface on the joint space of x, ˜y, a high-dimensional
continuous space with limited data and resource. Instead, we hypothesize that it is more important

[b]
to concentrate resource in depicting accurate energy surface around the current input, output pair
(x, FΦ(x)). In SEAL, utilizing the fact that we have access to trainable loss function, we dynamically
adapt energy surface so that the region of interest for training feedforward network is well represented.

We now discuss various energy loss (margin-based, regression-based, and contrastive-sampling-based)
that can be plugged into LE in SEAL. For brevity, we use LE(Θ) and LE(x, y, ˜y; Θ) interchangeably
where again (x, y) denotes labeled data and ˜y denotes probability vector. We defer the description
of specific energy network structure to the experiment section (§4) as SEAL framework can work
with arbitrary network structures.


-----

**Margin-based (LE−margin)** SPEN (Belanger & McCallum, 2016) learned structured energy network with SSVM loss (Taskar et al., 2004; Tsochantaridis et al., 2004) so that energy functions learn
to have sufficient energy difference, larger than the margin ∆(˜y, y), between arbitrary output ˜y and
true output y. To examine the effect of the margin-based loss in SEAL, we follow SPEN and utilize
the SSVM loss as


LE margin =
_−_


max [∆(˜y, y) _EΘ (x, ˜y) + EΘ (x, y)]+_ (5)
**y˜** _−_
**x,y**

X


**Regression-based (LE−regression)** Deep Value Network (DVN) (Gygli et al., 2017) attempts to learn
an energy network which directly outputs a score that is similar to the metric s(˜y, y) of interest, such
as F1 score, that compares arbitrary ˜y with true output y. Following DVN, by making score s(·)[2]
and −E(·) to be between [0, 1], we express regression loss as cross entropy

LE regression = _s(˜y, y) log_ _EΘ(x, ˜y)_ (1 _s(˜y, y)) log (1 + EΘ(x, ˜y)) ._ (6)
_−_ _−_ _−_ _−_ _−_

**Noise-contrastive ranking (LE** **NCEranking and LE** **ranking)** In SEAL, we are interested in captur_−_ _−_
ing the output region that feedforward network has a high probability. We ask, rather than just taking a
single point ˜y = FΦ(x) from a feedforward network, whether sampling many discrete binary vectors
from ˜y and taking all those samples into consideration for learning EΘ could lead in better estimation
of energy surface. Motivated from noise contrastive estimation (NCE) (Ma & Collins, 2018), we ask
whether energy network EΘ trained to contrast the true output y from groups of K negative samples
drawn from feedforward output FΦ(x) can induce a good loss for teaching Φ. The intuition is as the
**y˜ = FΦ(x) becomes better and better, the loss surface teaching feedforward network should become**
more fine grained as ˜y and samples from ˜y will be already close to the true y.

Before we discuss LE NCEranking of our choice, we first review original form of NCE ranking loss
_−_
from Ma & Collins (2018). For K samples y[(][k][)] _PN_ _, k = 1, . . ., K drawn from noise distribution_
_∼_
_PN and rewriting y as y[(0)]_ without loss of generality, NCE ranking loss is defined as

exp _s(x, y[(0)]; Θ)_
log _K_ _,_ where _s(x, y; Θ) =_ _EΘ(x, y)_ log PN(y). (7)

_k=1_ [exp]  _s(x, y[(][k][)]; Θ)_ _−_ _−_

exp( _EΘ(x,y))_
Minimization of this loss makesP   _P_ (y _x; Θ) =_ _−_
_|_ _y∈Y_ [exp(][−][E][Θ][(][x,y][))][ as an unbiased estimator of true]

distribution P (Y _X). Thus, given perfect energyP E(_ ), minimizing _EΘ(x, y) with respect to y in_
_|_ _·_ _−_
LF would be equivalent to maximizing the estimate of − log P (y|x).

From the presented NCE ranking method, we propose to use P (y|x; Φt) = _i_ _[P]_ [(][y][i][|][x][; Φ][t][)][ in place]
of PN (y). The novelty of this proposal is that we view the output of feedforward network that we
train as a noise distribution, and that the energy model that contrasts noise distribution teaches the

[Q]
noise distribution as well through our SEAL framework (equation 3). In short, we get LE−NCEranking
when we plug in s(x, y; Θ) = _EΘ(x, y)_ log PΦ(y **x; Φt) into equation 7, i.e .**
_−_ _−_ _|_

exp _s(x, y[(0)]; Θ)_
LE−NCEranking = log _Kk=1_ [exp]  _s(x, y[(][k][)]; Θ)_ _, s(x, y; Θ) = −EΘ(x, y) −_ log PΦ(y|x; Φt). (8)

P   

This achieves two benefits that we have in mind in using NCE. First benefit is that it captures the
region of interest by sampling from the output distribution of Φt. Second benefit is that it brings more
efficient NCE method. NCE method is known to work best when the noise distribution is close to
the data distribution (Gutmann & Hirayama, 2012) but not exactly the same. In our SEAL learning
procedure, we hypothesize that P (y **x; Φt) becomes closer to the data distribution as training steps**
_|_
proceed.

Lastly, motivated that NCE can estimate the true distribution, we propose yet another loss function
that can estimate the difference between the true probability P (y|x) and feedforward network
probability. We show in appendix A, if we use plain ranking loss that sets s(x, y; Θ) = _EΘ(x, y)_
in equation 7, unbiased estimator of P (y **x) now becomes** _PN_ (y) exp(−EΘ(x,y)) _−_
_|_ **y∈Y** _[P][N]_ [(][y][) exp(][−][E][Θ][(][x][,][y][))][ (9)][. This]

2In this paper, we adopt soft F1 score s(˜y, y) from Gygli et al. (2017) that is defined on the continuousP
**y˜ ∈** [0, 1][L].


-----

means if we take a derivative of EΘwith respect to y, then we are taking derivative with respect to
log P (y **x) + log PN** (y). With the trick of replacing the PN (y) with P (y **x; Φt) in SEAL, we**
_−_ _|_ _|_
hypothesize that loss captured by energy network can focus only on the difference between true and
feedforward probability which may end up being more informative surface. In short, we get plain
ranking loss LE ranking by plugging s(x, y; Θ) = _EΘ(x, y) into equation 7, resulting in_
_−_ _−_

LE−ranking = log _Kexp_ _−E(x, y[(0)]; Θ)_ _._ (10)

_k=1_ [exp]  _−E(x, y[(][k][)]; Θ)_
P   

3 RELATED WORK


**Learning with dynamic loss:** Wu et al. (2018) and Huang et al. (2019) attempt to learn a dynamic
loss defined by neural network that is tailored to the task-specific metric m, such as BLEU or 0-1
accuracy. Both approaches try to to learn a loss function that can directly increase model’s metric
score on the validation set. Since m is usually non-differentiable, Huang et al. (2019) view this
problem as a reinforcement learning problem and attempts to learn a loss function that maximizes the
expected reward: higher m score in validation set. On the other hand, Wu et al. (2018) relax the m to
be differentiable and view the feed-forward model update and loss-model update as a connected chain
of computation graph, so that loss function parameter (Θ) gets updated in relation to feed-forward
model (Φ) improving the relaxed metric score.

There are three major differences between SEAL and the previous work. First, energy surface
of SEAL is not limited to a specific parametric form, σ(y[T] Θ log FΦ(x))[3], as previous papers do.
Instead, SEAL simply utilizes a neural network to express the loss function as _EΘ(_ ) which allows
_−_ _·_
it to represent a much larger function class. Second, previous works require manual design of state
vector. For example, the state vector used by Wu et al. (2018) consists of iteration number, validation
accuracy, and current loss-function parameter Θ. In contrast, SEAL only requires the input and
output pair (x, FΦ(x)) of the feed-forward network. Lastly, since these models were designed for
multi-class classification, it is non-trivial to extend them to the multi-label classification or to the
general structured prediction task wherein large number of labels interact.

**Structured Prediction Energy Networks:** Structured prediction energy networks (Belanger &
McCallum, 2016) and its variants (Gygli et al., 2017; Rooshenas et al., 2019) (referred to as SPENs
from here on) learn an energy network EΘ : X × _Y →[˜]_ R and predict the output using gradient-based
inference (GBI) with the objective y = arg miny E(x, y). To train energy network, SPENs apply
GBI adversarially in order to find x, y pair that violates the minimality of the energy the most. We
found this training procedure to be a bit unstable as its success depends on the initial point of GBI.
We discuss the loss function for SPEN (Belanger & McCallum, 2016) and DVN (Gygli et al., 2017)
in §2 and use these models as baselines in this work.

**Jointly learning Θ and Φ:** The Inference network (INFNET) proposed in Tu & Gimpel (2018)
is the first work that tried to learn structured energy network and feedforward network jointly. As
the name suggests, INFNET proposes a model that mimics the behaviour of SPEN by replacing the
expensive gradient-based inference with feedforward networks. However, just like SPENs, INFNET
also resorts to finding adversarial point that has the maximum energy violation, resulting in an
adversarial framework similar to GAN (Goodfellow et al., 2014), where INFNET maximizes, and
energy network minimizes the same margin based loss given in equation 5. INFNET has to serve as
the adversarial sampler for training energy network. While the end goal of INFNET is an efficient
inference for finding output with lowest energy, having to serve two roles, INFNET suffers from train-,
test-time objective mismatch and Tu et al. (2020) resolves this by augmenting INFNET structure; Two
separate feed-forward networks are used, one for prediction and the other as the adversarial sampler.
However, INFNET’s reliance on margin-based losses (marign-rescaled higne and perceptron) still
leaves the approach to be adversarial, i.e. minΘ maxΦ LE(x, y, FΦ(x); Θ).

In contrast, SEAL has completely independent loss functions LE and LF for updating the energy and
feed-forward network. On feed-forward (Φ) update, SEAL simply update the model in the direction

3The papers (Wu et al., 2018; Huang et al., 2019) use Φ instead of Θ. We change it to Θ to put notations in
consistent manner with this paper.


-----

#Instances Input Label
Dataset Domain #Labels
Type Taxonomy
Train Val Test

Expr FUN Gene Ontology 1636 849 1288 500 Continuous Forest
Spo FUN Gene Ontology 1600 837 1266 500 Continuous Forest
Bibtex Text 4407 1491 1497 159 Binary - 
Cal500 Music 283 105 114 174 Continuous - 
Delicious Text 9690 3207 3194 983 Binary - 
Genbase Biology 398 132 132 27 Binary - 
Eurlex-ev Text 11557 3876 3881 3993 Binary - 

BGC Text 58715 14785 18394 142 Raw Text Forest
RCV Text 13890 9260 781265 104 Raw Text DAG
NYT Text 175299 177067 180659 2109 Raw Text - 

Table 1: Statistics of the datasets used in the experiments.

that reduces energy defined by structured energy network; Energy network is only used as a tool for
evaluation of loss and SEAL framework is not interested in mimicking the test-time behaviour of
SPEN as INFNET tries to. Despite the difference in the end goal, mechanically speaking, INFNET is
a special case of SEAL (i.e. SEAL with LE−margin) if the margin is removed from the SSVM loss for
the feedforward model udpate. In fact, Tu et al. (2020) reports that this special case leads to more
efficient and stable learning of feedforward network in their ablation study. We use this special case
of INFNET to represent SEAL with LE−marginin the main experiment.

**Learning label embeddings for multi-label classification** Utilizing label representation has been
a popular approach for multi-label classification (Wang et al., 2018; Xiao et al., 2019; Zhang et al.,
2021). These works embed labels as vectors, allowing for richer representations, and then utilizing
cross-entropy loss which models individual label probability independently. While the energy network
in SEAL represents an output probability, its main purpose is to allow the gradient for each label to
depend on other label probabilities, a feature which is absent from these prior works, for which label
interactions must be latently encoded in the embedding space.

**Eenrgy-based models used for reranking** Recently, energy-based models (EBM) have been
applied to text generation problems to first generate and rerank the generated sentences by EBM
score (Deng et al., 2020; Bhattacharyya et al., 2021). The residual energy modeling of Deng et al.
(2020) shares some similarity to the equation 9,apart from the minor difference that they utilize frozen
language model for PN whereas SEAL has PN that constantly learns from the energy models. The
major difference is that these previous works have simply used the scores of the energy network,
whereas SEAL views the energy network as a loss function, leveraging the gradient from the energy
network to further train another neural network.

4 EXPERIMENTS

In our experiments, we use 10 multi-label classification datasets (shown in Table 1) covering varied
label space, size, as well as input characteristics. The first 7 are small and medium sized feature based
datasets, while the last three Blurb Genre Collection (BGC), RCV (Reuters 1), and New York Times
(NYT) are large datasets with raw text as the input.[4]

**Energy Network: We use the same structure for the energy network EΘ(x, y) as described in Be-**
langer & Mccallum (2016), wherein the energy is the sum of local EΘ[local](x, y) and global EΘ[global](x, y)
energies defined as:


_yib[⊤]i_ _[T][E][(][x][)][,]_ _EΘ[global](x, y) = v[⊤]σ(My),_ (11)
_i=1_

X


_EΘ[local](x, y) =_


where bi, v, M, TE contain learnable parameters, σ(z) = log(1 + e[z]) is the softplus activation
function, and TE is a feature network for structured energy network. Note that due to the presence of

4Due to their enormous size, we use a subset of NYT training set, which is still the largest dataset in our
experiments.


-----

Table 2: Performance of SEAL framework (row 5-9) compared to feedforward network trained with
cross-entropy (row 1) and sturctured energy networks (row 2-4) that are learned with LEdescribed
in earlier section. We observe feedforward network learned with SEAL is almost always better
than cross-entropy learned model. We also observe while LE−marginand LE−regressiondoes not show
significant difference, LE−NCErankingand LE−rankinghas stronger performance in general.

|method samples|discrete input datasets continuous input datasets bibtex delicious genbase cal500 eurlexev expr fun spo fun|
|---|---|

|cross-entropy x|42.40 30.16 47.37 33.58 42.19 37.48 27.97|
|---|---|

|energy only SPEN x DVN x NCE o|42.99 26.76 32.50 39.36 41.75 36.43 27.20 42.73 29.59 78.43 48.21 28.90 32.19 29.97 17.93 16.78 20.64 38.87 0.19 27.12 19.33|
|---|---|

|SEAL margin x regression x regression-s o NCEranking o ranking o|42.86 29.75 96.53 36.69 41.83 37.91 28.43 43.74 29.79 96.95 37.97 41.65 38.12 28.89 44.53 29.87 96.81 38.95 42.32 37.84 28.02 44.76 34.79 97.32 41.62 42.78 38.21 28.70 44.20 36.04 96.60 40.71 42.78 38.03 29.68|
|---|---|


_EΘ[global]_ term in LF, the gradient for feedforward network, _[∂]y[L]i[F]_ [, can capture the dependency of][ y][i][ to]

all L dimensions of y. The expression for this is provided in Appendix B.

**Feature Network: We use the same feature network structure for both the energy and feedforward**
network denoted as TE, TF respectively. For the the feature-based datasets, the feature network
_T : R[d]_ _→_ R[h] consists of multi-layer perceptron with with softplus nonlinearities. For the raw text
based datasets, we use pre-trained BERT (Devlin et al., 2019) with adapter (Houlsby et al., 2019;
Pfeiffer et al., 2020) as the feature network.

**Feedforward Network: Given a feature network TF that generates features in R[h], the feedforward**
network FΦ(x) = GTF (x), where G ∈ R[L][×][h] is a matrix consisting of learnable embeddings, one
row of embedding for each label.

**Training: The models for feature-based datasets are trained on a single TitanX GPU with 14GB CPU**
memory and the models for BCG, RCV and NYT are trained on m40 GPU. We use separte ADAM
optimizer (Kingma & Ba, 2014) for energy network (Θ) and for feedforward network (Φ) which
optimizes parameters in alternating fashion[5]. For each minimization loop, we take nE, nF gradient
steps respectively for estimating Θt and Φt. We leave these nE, nF as hyperparameters to be tuned.
The best hyper-parameters for each model are found using Bayesian search[6] and reported in Appendix
D. We also report train, inference time and parameter size of different methods in AppendixE.

4.1 RESULTS FOR FEATURE BASED DATASETS

The experiment results across 7 feature-based dataset are presented in Table 2. The table consists of
feedforward network only trained with cross-entropy (row 1), energy networks (row 2-4) described
in previous section that is evaluated with GBI, and lastly feedforward network trained with SEAL
framework with different types losses (row 5-9).

SEAL outperforms cross-entropy loss (CE) on almost all datasets with average gains over CE ranging
from +0.6 (SEAL-LE−margin) to +2.85 (SEAL-LE−NCEranking) F1 points. We excluded excessive gain
of genbase in computing the average as we believe it is an outlier. Further analysis on the performance
on genbase is provided in Appendix F.

[5The code we used to train and evaluate our models is available a https://anonymous.4open.](https://anonymous.4open.science/r/SEAL/README.md)
[science/r/SEAL/README.md.](https://anonymous.4open.science/r/SEAL/README.md)
6We use Weights & Biases (Biewald, 2020) for hyperparameter search


-----

Between LE types, LE NCEranking and LE ranking performs the best while LE margin and LE regression
_−_ _−_ _−_ _−_
performs similarly on average. LE NCEranking and LE ranking are the only ones that always outperforms
_−_ _−_
CE in all 7 datasets whereas the others sometimes perform slightly worse than CE.

With the high gains of LE NCEranking and LE ranking, we are likely to conclude using samples rather
_−_ _−_
than a single point can capture the output surface of the feedforward network better. To examine
this hypothesis in more detail, we test sampling approaches on regression-based loss as well. We
define LE−regression-s = **y˜∈S** [L][E][−][regression][(][x][,][ y][∗][,][ ˜]y; Θ) given sample set S. We take two approaches
in collecting these samples: 1. discrete binary vectors drawn from probability vector ˜y as done
in LE−NCEranking and 2. continuous perturbation of[P] ˜y with gaussian noise. We found that discrete
samples did not make any notable changes between LE regression and LE regression-s, however, found
_−_ _−_
that using continuous samples made +0.4 F1 improvements in average and report LE regression-s as
_−_
the version which uses continuous sample. We conlcude that using samples are helpful in capturing
better energy surface for traininig feedforward network, however, effect and characteristics of sample
might differ per types of LE. We believe the sampling is more effective on LE−NCEranking, LE−ranking
as groups of samples contribute in relative manner in learning surface in contrast to LE−regression.

To further study effect of different LE, we examine whether there is a correlation in performance
between the isolated structured energy network and SEAL which utilizes idential LE. For training
isolated structured energy network with margin-based and regression-based energy loss, we follow
training approaches of SPEN (Belanger & McCallum, 2016) and DVN (Gygli et al., 2017). For
NCE, we need access to noise distribution PN . we set pretrained feedforward network probability
_P_ (Y _X; Φ) as PN_ (Y ). We do not see much of correlation between the two, in other words, charac_|_
teristics of LE that trains best structured energy network and LE that trains best feedforward network
are different. While NCE seems to perform worst when utilizing LE NCEranking in isolated manner,
_−_
the constrastive characteristic of LE NCEranking and LE rankingthat is tailored to feedforward network
_−_ _−_
seems to be helpful. Regardless of LE types, we also obesreve that it is much more stable and usually
higher-performing to train with SEAL than to train energy network separately. In Appendix C, we
also show compare individual energy network as a prediction model compared to SEAL in detail.

4.2 RESULTS FOR LARGE TEXT DATASETS

Table 3: Test F1 for text datasets.

method \ datasets **BGC** **RCV** **NYT**

cross-entropy 81.15 87.18 77.4
SEAL-margin 81.14 87.01 78.13
SEAL-NCEranking **81.64** **87.82** **78.87**

In order to test the effectiveness of SEAL in the scope of large pre-trained models, we examine text
datasets with pre-trained BERT (Devlin et al., 2019) with adapter (Houlsby et al., 2019; Pfeiffer et al.,
2020) as the feature network TE and TF as shown in Table 3. Considering the computational expense
of running hyper-parameter searches on BERT, based on Table 2, we choose the best-performing
SEAL (NCEranking) to compare with baselines: SEAL-margin and cross-entropy. The NYT subset
we used has almost 17x larger training data than the largest feature-based data and SEAL framework
outperforms cross-entropy fine-tuning. Overall, the Table 3 shows that SEAL, especially SEALNCEranking, is effective on pre-trained models that utilizes large text datasets as well. On average,
SEAL-NCEranking gains 0.87, 0.68 F1 points over cross entropy and SEAL-margin, respectively, on
text datasets as shown in Table 3.

5 FURTHER ANALYSIS

**SEAL without cross entropy:** Figure 2a shows the performance of SEAL when the cross-entropy
loss (CE) is completely removed from equation 1. We first observe that surprisingly that SEAL itself
without resorting to CE learns a model that sometimes outperforms model trained with CE ( cal500
and spo fun). However, we also observe that SEAL without CE is unstable as performance varies a
lot across different datasets. This tells us that SEAL requires a signal that can bring the model FΦ( )

_·_
into reasonable region first in order to demonstrate stable performance shown in Table 2. Another


-----

cross-entropy margin regression NCEranking


regression-s NCEraking

|6 4.0|0.74 0.56|35 0.77|0.55 0.96|0.76 7|5 4|6 4.1|Col8|0.73 1.30|Col10|
|---|---|---|---|---|---|---|---|---|---|


avg.
increase


cal500 expr spo eurlex genbase bibtex delicious

(b)

|33.58 45 4 39.62|37.48 22.11 36.81 4.01|27.97 17.97 30.47 25.2|42.1 5.59 22.01 0.28|4 25.56 32.82 26.97|42.4 9.59 24.29 24.06|30.16 18.42 24.04 3.459|
|---|---|---|---|---|---|---|


cal500 expr spo eurlex genbase bibtex delicious

(a)


Figure 2: (a) The performance of SEAL variants without cross-entropy loss term on LF, i.e. when
_λ1 = 1, λ2 = 0 in equation 1, compared to the model that only learns with cross-entropy. (b) Change_
in test F1 when moving from SEAL-static to SEAL-dynamic.

interesting observation is that SEAL-NCEranking model when it is often the weakest without CE
can give the largest improvement when combined with CE together in SEAL as shown in Table 2.
In short, we find CE to bring a stable synergy with energy network loss within SEAL: surpassing
each individual’s performance significantly when combined together. While there could be multiple
techniques to stabilize learning of SEAL besides adding CE loss, such as pre-training techniques, we
leave those interesting studies as future work.


**Effect of dynamic loss computation:** We now compare the effect of dyanmic loss learning by
comparing SEAL to SEAL-static in Figure 2b We observe that pretrained energy networks with
SEAL-static also results in competitve results, often better performing than the simple cross-entropy
loss. However, we oberve that in average, the dynamic loss learnig of SEAL outperforms SEAL-static
for LE−regression, and LE−NCEranking.

**Effect of applying ranking loss directly on FΦ** With the large improvement that LE−NCEranking
and LE ranking has, we examine whether the gain is actually coming from SEAL framework or simply
_−_
from the power of ranking loss itself which can be applied to feedforward network FΦ directly. We
conduct this ablation study over relatively small datasets: genbase, cal500 and delicious. While
we observed a near +10 F1 point in genbase, we saw decrease in performance in cal500 (-1.0 F1)
and delicious (-2.3 F1) compared to plain cross-entropy model. With these experiments, we notice
two trends. First, we again observe that genbase is an outlier where capturing structure can greatly
increase the performance although not as much as SEAL framework helped. Two, we confirm that
the ranking loss without energy network is not very effective in training feedforward model FΦ. This
again confirms that the energy network EΘ plays a major role to train FΦ in the SEAL framework.


6 CONCLUSION

We propose SEAL: a framework that can adopt structured energy network as a trainable loss function
for training feedforward network. Through examining different energy losses that trains energy
network, we show that SEAL is a general framework which is effective over different loss functions
and different architectures, such as MLP and BERT-based adapter. Through extensive result on 7
feature-based and 3 text-based datasets, we show that SEAL brings synergy between isolated energy
newtork and cross entropy function: performing better and in more stable manner when combined
together. Lastly, through ablation study, we examine the benefit of dyanmic loss learing which in
average brings a gain over static version of SEAL, i.e. SEAL-static. This research opens up doors for
various future work, including but not limited to, application of learned energy loss in unsupervised
dataset and exploring different architectures of energy network. In the long horizon, the authors are
interested in whether general pretrained scoring neural networks, such as BLEURT (Sellam et al.,
2020) and BERT-score (Zhang et al., 2019), could serve as a loss function when it can provide a
backpropagatable gradients to the output space.


-----

REFERENCES

David Belanger and Andrew Mccallum. Structured Prediction Energy Networks. Technical report,
2016.

David Belanger and Andrew McCallum. Structured prediction energy networks. In International
_Conference on Machine Learning, pp. 983–992. PMLR, 2016._

David Belanger, Bishan Yang, and Andrew Mccallum. End-to-End Learning for Structured Prediction
Energy Networks. Technical report, 2017.

Sumanta Bhattacharyya, Pedram Rooshenas, Subhajit Naskar, Simeng Sun, Mohit Iyyer, and Andrew
McCallum. Energy-based reranking: Improving neural machine translation using energy-based
models. In Association for Computational Linguistics, 2021.

[Lukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.](https://www.wandb.com/)
[com/. Software available from wandb.com.](https://www.wandb.com/)

Liang-Chieh Chen, Alexander Schwing, Alan Yuille, and Raquel Urtasun. Learning deep structured
models. In International Conference on Machine Learning, pp. 1785–1794. PMLR, 2015.

Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc’Aurelio Ranzato. Residual energybased models for text generation. In International Conference on Learning Representations, 2020.
[URL https://openreview.net/forum?id=B1l4SgHKDH.](https://openreview.net/forum?id=B1l4SgHKDH)

Jacob Devlin, Ming Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. NAACL HLT 2019 - 2019 Conference of
_the North American Chapter of the Association for Computational Linguistics: Human Language_
_[Technologies - Proceedings of the Conference, 1:4171–4186, 2019. URL http://arxiv.org/](http://arxiv.org/abs/1810.04805)_
[abs/1810.04805.](http://arxiv.org/abs/1810.04805)

Nadia Ghamrawi and Andrew McCallum. Collective multi-label classification. In Proceedings of
_the 14th ACM international conference on Information and knowledge management, pp. 195–200,_
2005.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
_processing systems, 27, 2014._

Michael Gutmann and Jun-ichiro Hirayama. Bregman divergence as general framework to estimate
unnormalized statistical models. arXiv preprint arXiv:1202.3727, 2012.

Michael Gygli, Mohammad Norouzi, and Anelia Angelova. Deep value networks learn to evaluate
and iteratively refine structured outputs. In 34th International Conference on Machine Learning,
_[ICML 2017, volume 3, pp. 2160–2170, 2017. ISBN 9781510855144. URL https://goo.gl/](https://goo.gl/8OLufh)_
[8OLufh.](https://goo.gl/8OLufh)

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,
Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning
for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
_International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning_
_[Research, pp. 2790–2799. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.](https://proceedings.mlr.press/v97/houlsby19a.html)_
[press/v97/houlsby19a.html.](https://proceedings.mlr.press/v97/houlsby19a.html)

Chen Huang, Shuangfei Zhai, Walter Talbott, Miguel Bautista Martin, Shih-Yu Sun, Carlos Guestrin,
and Josh Susskind. Addressing the loss-metric mismatch with adaptive loss alignment. In
_International Conference on Machine Learning, pp. 2891–2900. PMLR, 2019._

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
_arXiv:1412.6980, 2014._

John Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. 2001.


-----

Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based
learning. Predicting structured data, 1(0), 2006.

Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional
models: Consistency and statistical efficiency. In Proceedings of the 2018 Conference on Empirical
_Methods in Natural Language Processing, pp. 3698–3707, Brussels, Belgium, October-November_
[2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1405. URL https:](https://aclanthology.org/D18-1405)
[//aclanthology.org/D18-1405.](https://aclanthology.org/D18-1405)

Andreas Christian Muller. Methods for learning structured prediction in semantic segmentation of¨
natural images. 2014.

Martha Palmer, Daniel Gildea, and Nianwen Xue. Semantic role labeling. Synthesis Lectures on
_Human Language Technologies, 3(1):1–103, 2010._

Jonas Pfeiffer, Andreas Ruckl¨ e, Clifton Poth, Aishwarya Kamath, Ivan Vuli´ c, Sebastian Ruder,´
Kyunghyun Cho, and Iryna Gurevych. Adapterhub: A framework for adapting transformers. In
_Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:_
_System Demonstrations, pp. 46–54, 2020._

Amirmohammad Rooshenas, Dongxu Zhang, Gopal Sharma, and Andrew McCallum. Searchguided, lightly-supervised training of structured prediction energy networks. Advances in Neural
_Information Processing Systems, 32:13522–13532, 2019._

Alexander G Schwing and Raquel Urtasun. Fully connected deep structured networks. arXiv preprint
_arXiv:1503.02351, 2015._

Thibault Sellam, Dipanjan Das, and Ankur P Parikh. Bleurt: Learning robust metrics for text
[generation. In Proceedings of ACL, 2020. URL https://arxiv.org/abs/2004.04696.](https://arxiv.org/abs/2004.04696)

Ben Taskar, Carlos Guestrin, and Daphne Koller. Max-Margin Markov Networks. Advances in neural
_information processing systems 16 (2004): 25, 2004._

Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. Support vector
machine learning for interdependent and structured output spaces. In Proceedings of the twenty-first
_international conference on Machine learning, pp. 104, 2004._

Lifu Tu and Kevin Gimpel. Learning approximate inference networks for structured prediction, 2018.
ISSN 23318422.

Lifu Tu and Kevin Gimpel. Benchmarking approximate inference methods for neural structured prediction. In Proceedings of the 2019 Conference of the North American Chapter of the Association
_for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),_
pp. 3313–3324, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
[doi: 10.18653/v1/N19-1335. URL https://aclanthology.org/N19-1335.](https://aclanthology.org/N19-1335)

Lifu Tu, Richard Yuanzhe Pang, and Kevin Gimpel. Improving joint training of inference networks
and structured prediction energy networks. In SPNLP, 2020.

Guoyin Wang, Chunyuan Li, Wenlin Wang, Yizhe Zhang, Dinghan Shen, Xinyuan Zhang, Ricardo
Henao, and Lawrence Carin. Joint embedding of words and labels for text classification. In
_Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume_
_1: Long Papers), pp. 2321–2331, Melbourne, Australia, July 2018. Association for Computational_
[Linguistics. doi: 10.18653/v1/P18-1216. URL https://aclanthology.org/P18-1216.](https://aclanthology.org/P18-1216)

Lijun Wu, Fei Tian, Yingce Xia, Yang Fan, Tao Qin, Lai Jian-Huang, and Tie-Yan Liu. Learning to
Teach with Dynamic Loss Functions. In Advances in Neural Information Processing Systems, vol[ume 31. Curran Associates, Inc., 2018. URL https://papers.nips.cc/paper/2018/](https://papers.nips.cc/paper/2018/hash/8051a3c40561002834e59d566b7430cf-Abstract.html)
[hash/8051a3c40561002834e59d566b7430cf-Abstract.html.](https://papers.nips.cc/paper/2018/hash/8051a3c40561002834e59d566b7430cf-Abstract.html)


-----

Lin Xiao, Xin Huang, Boli Chen, and Liping Jing. Label-specific document representation for
multi-label text classification. In Proceedings of the 2019 Conference on Empirical Methods in
_Natural Language Processing and the 9th International Joint Conference on Natural Language_
_Processing (EMNLP-IJCNLP), pp. 466–475, Hong Kong, China, November 2019. Association_
[for Computational Linguistics. doi: 10.18653/v1/D19-1044. URL https://aclanthology.](https://aclanthology.org/D19-1044)
[org/D19-1044.](https://aclanthology.org/D19-1044)

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating
text generation with bert. arXiv preprint arXiv:1904.09675, 2019.

Ximing Zhang, Qian-Wen Zhang, Zhao Yan, Ruifang Liu, and Yunbo Cao. Enhancing label correlation feedback in multi-label text classification via multi-task learning. In Findings of the
_Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 1190–1200, Online, August_
2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.101. URL
[https://aclanthology.org/2021.findings-acl.101.](https://aclanthology.org/2021.findings-acl.101)


-----

APPENDIX

A NCE RANKING METHOD WITH s(x, y; Θ) = _EΘ(x, y)_
_−_

As pointed out by Ma & Collins (2018) as well, If we set s(x, y; Θ) = _EΘ(x, y), we can regard_
_−_
this this learning with EΘ[′] [(][x][,][ y][) =][ E][Θ][(][x][,][ y][) + log][ P][N][(][y][)][. As pointed out earlier, this leads the]
model to estimate P (y|x with

exp (−EΘ[′] [(][x, y][))]

_y_ [exp (][−][E]Θ[′] [(][x, y][))] _[.]_
_∈Y_

P

Plugging in EΘ′ as EΘ(x, y) + log PN(y) once again, we can see that P (Y |X) is estimated by

_PN_ (y) exp ( _EΘ(x, y))_
_−_
**y** _[P][N]_ [(][y][) exp (][−][E][Θ][(][x][,][ y][))]
_∈Y_
P

B GRADIENT TO THE LABEL REPRESENTATIONS

As described in section 4, the energy component of loss LF, for the feedforward network, is the sum
of following two parts:


_yib[⊤]i_ _[T][E][(][x][)]_ and _EΘ[global](x, y) = v[⊤]σ(My)._ (12)
_i=1_

X


_EΘ[local](x, y) =_


The gradient of E[global] w.r.t y is given by the following vector of length L

_∂E[global]_

= M[T] Diag(σ[′](M **y))v.** (13)
_∂y_

While the gradient of cross entropy l(y, y[∗]) w.r.t. y is given by the following vector of length L


_∂l_

_∂y_ [=][ y][∗] _[⊗]_ **y[1]**

_[−]_ [(1][ −] **[y][∗][)][ ⊗]**


1

(14)
1 **y** _[,]_
_−_


where ⊗ is element-wise product.

Note the dependency of the gradient for a particular component yi of y on other component in the
expressions above. The gradient of cross entropy ( _∂y[∂l]i_ [) only depends on its label dimension][ i][:][ y][i][ and]

_yi[∗][. In contrast, in the case of the energy function as loss (SEAL), the gradient][ ∂]∂y[L][F]i_ [depends on all][ L]

dimensions of y due to the product term My in _[∂E]∂y[global]i_ [.]

C COMPARISON OF SEAL AND SEPARATE ENERGY NETWORK

In the following figures, we compare the F1 scores of SEAL framework with LE regression (Figure 3),
_−_
LE NCEranking (Figure 4) and LE margin (Figure 5) on validation set with its energy-only counterpart.
_−_ _−_
We further report validation F1 scores for the output of GBI starting from the samples drawn from ˆy
and randomly. We show this comparison to evaluate the performance of the energy network on points
around ˆy and random points.


-----

DVN _FΦ_ _FΦ+GBI_ random+GBI


DVN _FΦ_ _FΦ+GBI_ random+GBI


0.4

0.3


0.4

0.3


0.2

0.1


0.2

0.1


0.0

0.30

0.25

0.20

0.15

0.10

0.05


2000 4000 6000 8000

Step


100 200 300 400 500 600

Step


(a) bibtex


(b) cal500


DVN _FΦ_ _FΦ+GBI_ random+GBI


DVN _FΦ_ _FΦ+GBI_ random+GBI


1.0

0.8


0.6

0.4


0.2

0.0


200 400 600 800 1000 1200

Step


2500 5000 7500 10000 12500 15000 17500

Step


(d) delicious


(c) genbase


DVN _FΦ_ _FΦ+GBI_ random+GBI


DVN _FΦ_ _FΦ+GBI_ random+GBI


0.30

0.25

0.20

0.15

0.10

0.05


0.35

0.30

0.25

0.20

0.15

0.10

0.05


1000 2000 3000 4000 5000 6000 7000

Step


500 1000 1500 2000

Step


(e) expr fun


(f) spo fun


Figure 3: Validation F1 performance of SEAL framework with LE−regression(FΦ) compared to energyonly (DVN). Further, we compare validation F1 performance of GBI starting from samples drawn
from feedforward network’s output (FΦ+GBI) and samples drawn randomly (random + GBI).


-----

NCE _FΦ_ _FΦ+GBI_ random+GBI


NCE _FΦ_ _FΦ+GBI_ random+GBI


0.425

0.400

0.375

0.350

0.325

0.300

0.275

0.250

0.225

0.35

0.30

0.25

0.20

0.15

0.10

0.05


0.4

0.3


0.2

0.1


50 100 150 200 250 300 350 400

Step


5000 10000 15000 20000 25000

Step


(b) cal500


(a) bibtex


NCE _FΦ_ _FΦ+GBI_ random+GBI


NCE _FΦ_ _FΦ+GBI_ random+GBI


1.0

0.8


0.6

0.4


0.2

0.40

0.35

0.30

0.25

0.20

0.15

0.10

0.05


200 400 600 800 1000 1200

Step


5000 10000 15000 20000

Step


(d) delicious


(c) genbase


NCE _FΦ_ _FΦ+GBI_ random+GBI


NCE _FΦ_ _FΦ+GBI_ random+GBI


0.30

0.25

0.20

0.15

0.10

0.05


500 1000 1500 2000 2500

Step


500 1000 1500 2000 2500 3000 3500

Step


(e) expr fun


(f) spo fun


Figure 4: Validation F1 performance of SEAL framework with LE−NCEranking(FΦ) compared to
energy-only (NCE). Further, we compare validation F1 performance of GBI starting from samples
drawn from feedforward network’s output (FΦ+GBI) and samples drawn randomly (random + GBI).


-----

SPEN _FΦ_ _FΦ+GBI_ random+GBI


SPEN _FΦ_ _FΦ+GBI_ random+GBI


0.36

0.34

0.32

0.30

0.28

0.26

0.24

0.25


0.4

0.3


0.2

0.1


1000 2000 3000 4000 5000 6000

Step


100 200 300 400

Step


(b) cal500


(a) bibtex


SPEN _FΦ_ _FΦ+GBI_ random+GBI


SPEN _FΦ_ _FΦ+GBI_ random+GBI


1.0

0.8


0.20

0.15


0.6

0.4


0.10

0.05


0.2

0.35

0.30

0.25

0.20

0.15

0.10

0.05


200 400 600 800 1000

Step


5000 10000 15000 20000 25000

Step


(d) delicious


(c) genbase


SPEN _FΦ_ _FΦ+GBI_ random+GBI


SPEN _FΦ_ _FΦ+GBI_ random+GBI


0.25

0.20

0.15

0.10


0.05


500 1000 1500 2000 2500

Step


200 400 600 800 1000 1200

Step


(e) expr fun


(f) spo fun


Figure 5: Validation F1 performance of SEAL framework with LE−margin(FΦ) compared to energyonly (SPEN). Further, we compare validation F1 performance of GBI starting from samples drawn
from feedforward network’s output (FΦ+GBI) and samples drawn randomly (random + GBI).

D HYPERPARAMETERS


The following tables tabulate the hyper-parameters for SEAL framework for each dataset. We
further fix the number of layers, layer-dimensions and dropout in feed-forward network based on
the configuration of the best model trained using cross-entropy. We fix the λ2 = 1 in equation 1 to
reduce degree of freedom in hyperparameter search.


-----

|method|λ num samples Θ Φ 1 lr lr|
|---|---|


|margin regression regression-s NCEranking ranking|0.001 n/a 0.0002 0.001 0.002 n/a 0.0003 0.001 0.03 20 0.0004 0.001 0.002 100 0.0005 0.001 0.002 100 0.000015 0.001|
|---|---|


Table 4: genbase

|method|λ num samples Θ Φ 1 lr lr|
|---|---|


|margin regression regression-s NCEranking ranking|0.001 n/a 0.0005 0.008 1.4 n/a 0.006 0.007 9 5 0.008 0.006 0.4 40 0.007 0.005 8 80 0.003 0.0001|
|---|---|



Table 5: cal500

|ethod|λ num samples Θ Φ 1 lr lr|
|---|---|


|argin egression egression-s CEranking anking|0.0002 n/a 0.002 0.004 8 n/a 0.0001 0.0001 9 5 0.0001 0.003 0.9 20 0.002 0.001 1 60 0.0008 0.0001|
|---|---|



Table 6: delicious

method _λ1_ num samples Θlr Φlr

margin 0.001 n/a 0.0003 0.002
regression 0.002 n/a 0.0006 0.004
regression-s 0.01 20 0.0004 0.007
NCEranking 0.5 40 0.001 0.002
ranking 0.3 40 0.0003 0.002


Table 7: eurlex

|method|λ num samples Θ Φ 1 lr lr|
|---|---|


|margin regression regression-s NCEranking ranking|0.01 n/a 0.00003 0.001 0.02 n/a 0.0003 0.0006 2 30 0.007 0.002 4 80 0.00005 0.0002 1 40 0.008 0.0005|
|---|---|



Table 8: expr fun


-----

|method|λ num samples Θ Φ 1 lr lr|
|---|---|


|margin regression regression-s NCEranking ranking|0.08 n/a 0.0001 0.002 9 n/a 0.001 0.00015 0.02 10 0.0002 0.0009 0.7 60 0.008 0.0005 1.5 40 0.004 0.002|
|---|---|


Table 9: spo fun

|method|λ num samples Θ Φ 1 lr lr|
|---|---|


|margin regression regression-s NCEranking ranking|0.0002 n/a 0.00005 0.001 3 n/a 0.005 0.003 9 10 0.005 0.001 5 20 0.0004 0.001 9 40 0.00005 0.001|
|---|---|



Table 10: bibtex

E ANALYSIS ON PARAMETER SIZE AND SPEED OF INFERENCE AND TRAINING

Here, we present the analysis on number of parameters (Table 11), inference speed (Table 12)), and
training speed (Table 13) that different methods utilize per dataset.

In summary, SEAL requires approximately twice number of parameters compared to cross-entropy
loss on training time, however it requires the same amount of parameter on inference time. Similarly,
train time (sec) per epoch of SEAL is slower than that of cross-entropy and energy network as
SEAL requires multiple backpropagation steps for both energy network and feedforward network.
Nonetheless, again, at inference time, the speed of CE and SEAL is the same and is much faster
(2x-7x) than the inference of energy network.

The analysis we present below are on the seven feature-based dataset, however the trend is similar for
text-based dataset as well: twice parameter size on training time but equal parameter and runtime in
inference time with CE method. The only difference in the text-based MLC is, due to running on a
much larger BERT-baed models, that we only run one step for each update of feedforward network
and energy network. Thus, unlike Table 13, train time of SEAL is approximately twice that of CE.

|Dataset Methods \|Parameter size|Col3|Col4|Col5|
|---|---|---|---|---|
||CE|Energy network (SPEN, DVN, NCE)|SEAL (Train-time)|SEAL (Inference-time)|
|EXPR FUN|333000|533800|866800|333000|
|SPO FUN|447000|597600|1044600|447000|
|Bibtex|958800|991000|1949800|958800|
|Cal500|1158700|1123500|2282200|1158700|
|Delicious|754000|951000|1705000|754000|
|Genbase|485600|491400|977000|485600|
|Eurlex-ev|4747500|5546500|10294000|4747500|



Table 11: The number of parameters required during train time for SEAL is approximately double
the size of feedforward (CE column) while energy network and feedforward sizes are comparable.
However, in the inference time, SEAL has an equal amount of parameters to the CE column as only
feedforward network is utilized during inference time.


-----

|Dataset|Inference time (sec)|Col3|Inference speed (examples/sec)|Col5|Speed ratio (CE and SEAL/ EnergyNetwork)|
|---|---|---|---|---|---|
||Energy network|CE and SEAL|Energy network|CE and SEAL||
|EXPR FUN|1.33|0.22|638|3801|5.96|
|SPO FUN|1.18|0.16|709|5231|7.38|
|Bibtex|3.6|1.78|414|840|2.03|
|Cal500|0.24|0.10|438|1080|2.47|
|Delicious|5.35|1.39|599|2307|3.85|
|Genbase|0.23|0.13|574|1005|1.75|
|Eurlex-ev|24|12.22|162|317|1.96|


Table 12: We simply average inference time for CE and SEAL variants as they are very similar.
Likewise, we average the inference time of different energy networks. Here, inference time (sec) is
recorded for the whole validation set. We also present speed per sec (example/sec) and speed ratio.
The last column shows that CE and SEAL methods are 2x-7x faster in inference time than the energy
networks.

|Methods Datasets \|Train time (sec/epoch)|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
||EXPRFUN|SPO FUN|Bibtex|Cal500|Delicious|Genbase|Eurlex-ev|
|CE|6.29|5.46|22.67|1.00|33.01|2.12|114.84|
|SPEN|10.99|11.01|28.04|2.53|37.31|3.57|136.10|
|DVN|10.95|10.02|32.12|1.87|55.24|2.79|129.94|
|NCE|3.71|3.83|13.97|2.82|22.03|3.68|89.33|
|SEAL-margin|27.96|35.24|212.94|5.78|44.60|8.73|260.10|
|SEAL-regression|46.32|56.97|27.78|8.07|143.90|13.91|352.63|
|SEAL-regression-s|42.87|77.92|179.97|8.48|45.33|10.79|221.33|
|SEAL-NCEranking|72.43|40.34|131.73|16.58|158.97|17.77|431.92|
|SEAL-Ranking|41.24|26.83|218.37|7.04|317.63|10.74|408.79|


Train time (sec/epoch)
Methods Datasets
_\_ EXPRFUN SPO FUN Bibtex Cal500 Delicious Genbase Eurlex-ev

CE 6.29 5.46 22.67 1.00 33.01 2.12 114.84

SPEN 10.99 11.01 28.04 2.53 37.31 3.57 136.10

DVN 10.95 10.02 32.12 1.87 55.24 2.79 129.94

NCE 3.71 3.83 13.97 2.82 22.03 3.68 89.33

SEAL-margin 27.96 35.24 212.94 5.78 44.60 8.73 260.10

SEAL-regression 46.32 56.97 27.78 8.07 143.90 13.91 352.63

SEAL-regression-s 42.87 77.92 179.97 8.48 45.33 10.79 221.33

SEAL-NCEranking 72.43 40.34 131.73 16.58 158.97 17.77 431.92

SEAL-Ranking 41.24 26.83 218.37 7.04 317.63 10.74 408.79


Table 13: The training time per epoch is presented per dataset and per loss function used. Due to
different gpu types and node status, there are some outliers. Furthermore, as SEAL runs multiple
number of backpropagation steps for energy network and feedforward network in the alternating
optimization, direct comparison of training time is not really available. However, the general trend in
terms of training time per epoch is CE < Energy Networks < SEAL.

F DATA-SPECIFIC ANALYSIS

In this section, we analyze why certain datasets behaves differently.

F.1 GENBASE

Here, we analyze the factors behind SEAL achieving almost perfect F1 score on the genbase dataset.
It seems genbase stands out with its small label size (27 which is the smallest among Table 1) and
with very clear pattern in the label space. Upon analysis, out of 27 labels, we found that 6 labels
only occur as a singleton (by itself), 10 labels only occur as non-singleton, and 7 labels do not
participate at all. This leaves only 4 labels occurring by themselves as well as with others. Not only
that, only 7% (35/500) of training instances have more than one active label. In this peculiar setting,
we conjecture feedforward networks with cross-entropy (CE) loss can easily learn singletons, and
energy networks would easily learn co-occurrence, but learning both might be confusing for these
two models at the opposite end of the spectrum. We believe the synergy of CE and a loss from the
energy network enables capturing the best of both worlds and achieving a near-perfect score that
none of the approaches by itself achieves. As can seen in Table 2, neither cross-entropy nor energy
networks are nearly as strong as SEAL methods.


-----

F.2 DELICIOUS AND CAL500

In Table 2, it seems SEAL-NCEranking and SEAL-Ranking are particularly well performing on
cal500 and delicious. It turns out that cal500 and delicious have a very high diversity of 1 and 0.981
(Reference: MLC data repo). Diversity of 1 means that each data point holds a unique label set. It
seems that the exposure to multiple samples and evaluations of their relative scores in NCEranking
loss function can be helpful in a high-diversity setting.


-----

