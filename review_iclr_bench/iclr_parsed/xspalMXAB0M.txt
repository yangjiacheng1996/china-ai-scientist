# A BOOSTING APPROACH TO REINFORCEMENT LEARNING

**Anonymous authors**
Paper under double-blind review

ABSTRACT

We study efficient algorithms for reinforcement learning in Markov decision processes, whose complexity is independent of the number of states. This formulation
succinctly captures large scale problems, but is also known to be computationally
hard in its general form. Previous approaches attempt to circumvent the computational hardness by assuming structure in either transition function or the value
function, or by relaxing the solution guarantee to a local optimality condition.
We consider the methodology of boosting, borrowed from supervised learning, for
converting weak learners into an effective policy. The notion of weak learning we
study is that of sampled-based approximate optimization of linear functions over
policies. Under this assumption of weak learnability, we give an efficient algorithm that is capable of improving the accuracy of such weak learning methods
iteratively. We prove sample complexity and running time bounds on our method,
that are polynomial in the natural parameters of the problem: approximation guarantee, discount factor, distribution mismatch and number of actions. In particular,
our bound does not explicitly depend on the number of states.
A technical difficulty in applying previous boosting results, is that the value function over policy space is not convex. We show how to use a non-convex variant of
the Frank-Wolfe method, coupled with recent advances in gradient boosting that
allow incorporating a weak learner with multiplicative approximation guarantee,
to overcome the non-convexity and attain global optimality guarantees.

1 INTRODUCTION

The field of reinforcement learning, formally modelled as learning in Markov decision processes
(MDP), models the mechanism of learning from rewards, as opposed to examples. Although the
case of tabular MDPs is well understood, the main difficulty in applying RL to practice is the size
of the state space.

Various techniques have been suggested and applied to cope with very large MDPs. The most
common of which is function approximation of either the value or the transition function of the
underlying MDP, many times using deep neural networks. Training deep neural networks in the
supervised learning model is known to be computationally hard. Therefore reinforcement learning
with neural function approximation is also computationally hard in general, and for this reason lacks
provable guarantees.

This challenge of finding efficient and provable algorithms for MDPs with large state space is the
focus of our study. Previous approaches can be categorized in terms of the structural assumptions
made on the MDP to circumvent the computational hardness. Some studies focus on structured
dynamics, whereas others on structured value function or policy classes w.r.t. to the dynamics.

In this paper we study another methodology to derive provable algorithms for reinforcement learning: ensemble methods for aggregating weak or approximate algorithms into substantially more
accurate solutions. Our method can be thought of as extending the methodology of boosting from
supervised learning (Schapire & Freund, 2012) to reinforcement learning. Interestingly, however,
our resulting aggregation of weak learners is not linear.

In order to circumvent the computational hardness of solving general MDPs with function approximation, we assumes access to a weak learner: an efficient sample-based procedure that is capable


-----

of generating an approximate solution to any linear optimization objective over the space of policies. We describe an algorithm that iteratively calls this procedure on carefully constructed new
objectives, and aggregates the solution into a single policy. We prove that after sufficiently many
iterations, our resulting policy is provably near-optimal.

1.1 CHALLENGES AND TECHNIQUES

Reinforcement learning is quite different from supervised learning and several difficulties have to be
circumvented for boosting to work. Amongst the challenges that the reinforcement learning setting
presents, consider the following,

(a) The value function is not a convex or concave function of the policy. This is true even in
the tabular case, and even more so if we use a parameterized policy class.

(b) The transition matrix is unknown, or prohibitively large to manipulate for large state spaces.
This means that even evaluation of a policy cannot be exact, and can only be computed
approximately.

(c) It is unrealistic to expect a weak learner that attains near-optimal value for a given linear
objective over the policy class. At most one can hope for a multiplicative and/or additive
approximation of the overall value.

Our approach overcomes these challenges by applied several new as well as recently developed
techniques. To overcome the nonconvexity of the value function, we use a novel variant of the
Frank-Wolfe optimization algorithm that simultaneously delivers on two guarantees. First, it finds a
first order stationary point with near-optimal rate. Secondly, if the objective happens to admit a certain gradient domination property, an important generalization of convexity, it also guarantees near
optimal value. The application of the nonconvex Frank-Wolfe method is justified due to previous
recent investigation of the policy gradient algorithm (Agarwal et al., 2019; 2020a), which identified
conditions under which the value function is gradient dominated.

The second information-theoretic challenge of the unknown transition function is overcome by careful algorithmic design: our boosting algorithm requires only samples of the transitions and rewards.
These are obtained by rollouts on the MDP.

The third challenge is perhaps the most difficult to overcome. Thus far, the use of the Frank-Wolfe
method in reinforcement learning did not include a multiplicative approximation, which is critical
for our application. Luckily, recent work in the area of online convex optimization (Hazan & Singh,
2021) studies boosting with a multiplicative weak learner. We make critical use of this new technique
which includes a non-linear aggregation (using a 2-layer neural network) of the weak learners. This
aspect is perhaps of general interest to boosting algorithm design, which is mostly based on linear
aggregation.

1.2 OUR CONTRIBUTIONS

Our main contribution is a novel efficient boosting algorithm for reinforcement learning. The input
to this algorithm is a weak learning method capable of approximately optimizing a linear function
over a certain policy class.

The output of the algorithm is a policy which does not belong to the original class considered. It is
rather a non-linear aggregation of policies from the original class, according to a two-layer neural
network. This is a result of the two-tier structure of our algorithm: an outer loop of non-convex
Frank-Wolfe method, and an inner loop of online convex optimization boosting. The final policy
comes with provable guarantees against the class of all possible policies.

Our algorithm and guarantees come in four flavors, depending on the mode of accessing the MDP
(two options), and the boosting methodology for the inner online convex optimization problem (two
options).

It is important to point out that we study the question from an optimization perspective, and hence,
assume the availability of an efficient exploration scheme – either via access to a reset distribution
that has some overlap with the state distribution of the optimal policy, or constraining the policy


-----

_∞_

|Col1|Supervised weak learner|Online weak learner|Col4|
|---|---|---|---|
|Episodic model|C6 (Π)/α4ε5 ∞|C4 (Π)/α2ε3 ∞|C = max d dπ π∗ ∞ π∈Π|
|Rollouts w. ν-resets|6 /α4ε6 D ∞|4 /α2ε4 D ∞|∞ dπ ν∗ D = ∞|



Table 1: Sample complexity of the proposed algorithms for different α-weak learning models (supervised & online) and modes of accessing the MDP (rollouts & rollouts with reset distribution ν),
suppressing polynomial factors in |A|, 1/(1 − _γ). See Theorem 11 for details._

class to policies that explore sufficiently. Such considerations also arise when reducing reinforcement learning to a sequence of supervised learning problems, e.g. Conservative Policy Iteration
(Kakade & Langford, 2002) assumes the former. One contribution we make here is to quantitatively differentiate between these two modes of exploration in terms of the rates of convergence they
enable for the boosting setting.

1.3 RELATED WORK

To cope with prohibitively large MDPs, the method of choice to approximate the policy and transition space are deep neural networks, dubbed “deep reinforcement learning". Deep RL gave rise to
beyond human performance in games such as Go, protein folding, as well as near-human level autonomous driving. In terms of provable methods for deep RL, there are two main lines of work. The
first is a robust analysis of the policy gradient algorithm (Agarwal et al., 2019; 2020a). Importantly,
the gradient domination property of the value function established in this work is needed in order to
achieve global convergence guarantees of our boosting method.

The other line of work for provable approaches is policy iteration, which uses a restricted policy
class, making incremental updates, such as Conservative Policy Iteration (CPI) (Kakade & Langford,
2002; Scherrer & Geist, 2014), and Policy Search by Dynamic Programming (PSDP)(Bagnell et al.,
2003).

Our boosting approach for provable deep RL builds on the vast literature of boosting for supervised
learning (Schapire & Freund, 2012), and recently online learning (Leistner et al., 2009; Chen et al.,
2012; 2014; Beygelzimer et al., 2015; Jung et al., 2017; Jung & Tewari, 2018). One of the crucial
techniques important for our application is the extension of boosting to the online convex optimization setting, with bandit information (Brukhim & Hazan, 2021), and critically with a multiplicative
weak learner (Hazan & Singh, 2021). This latter technique implies a non-linear aggregation of the
weak learners. Non-linear boosting was only recently investigated in the context of classification
(Alon et al., 2020), where it was shown to potentially enable significantly more efficient boosting.

Perhaps the closest work to ours is boosting in the context of control of dynamical systems (Agarwal et al., 2020b). However, this work critically requires knowledge of the underlying dynamics
(transitions), which we do not, and cannot cope with a multiplicative approximate weak learner.

The Frank-Wolfe algorithm is extensively used in machine learning, see e.g. (Jaggi, 2013), references therein, and recent progress in stochastic Frank-Wolfe methods (Hassani et al., 2017; Mokhtari
et al., 2018; Chen et al., 2018; Xie et al., 2019). Recent literature has applied a variant of this algorithm to reinforcement learning in the context of state space exploration (Hazan et al., 2019).

2 PRELIMINARIES

**Optimization.** We say that a differentiable function f : K 7→ R over some domain K is L-smooth
with respect to some norm if for every x, y we have
_∥· ∥∗_ _∈K_

_f_ (y) _f_ (x) _f_ (x)[⊤](y _x)_
_−_ _−∇_ _−_ _≤_ _[L]2_ _∗[.]_

_[∥][x][ −]_ _[y][∥][2]_


-----

For constrained optimization (such as over ∆A), the projection Γ : R[|][A][|] _→_ ∆A of a point x to onto
a domain ∆A is
Γ[x] = arg miny∈∆A _∥x −_ _y∥._

An important generalization of convex function we use henceforth is that of gradient domination,
**Definition 1 (Gradient Domination). A function f : K →** R is said to be (κ, τ, K1, K2)-locally
gradient dominated (around K1 by K2) if for all x ∈K1, it holds that

max _f_ (x)[⊤](y _x)_ + τ.
_y_ _[f]_ [(][y][)][ −] _[f]_ [(][x][)][ ≤] _[κ][ ×][ max]y_ 2 _∇_ _−_
_∈K_ _∈K_



**Markov decision process.** An infinite-horizon discounted Markov Decision Process (MDP) M =
(S, A, P, r, γ, d0) is specified by: a state space S, an action space A, a transition model P where
_P_ (s[′]|s, a) denotes the probability of immediately transitioning to state s[′] upon taking action a at
state s, a reward function r : S × A → [0, 1] where r(s, a) is the immediate reward associated with
taking action a at state s, a discount factor γ ∈ [0, 1); a starting state distribution d0 over S. For any
infinite-length state-action sequence (hereafter, called a trajectory), we assign the following value


_γ[t]r(st, at)._
_t=0_

X


_V (τ = (s0, a0, s1, a1, . . . )) =_


The agent interacts with the MDP through the choice of stochastic policy π : S → ∆A it executes, where ∆A denotes the probability simplex over A. The execution of such a policy induces a
distribution over trajectories τ = (s0, a0, . . . ) as


(P (st+1 _st, at)π(at_ _st))._ (1)
_|_ _|_
_t=0_

Y


_P_ (τ _π) = d0(s0)_
_|_


Using this description we can associate a state V _[π](s) and state-action Q[π](s, a) value function with_
any policy π. For an arbitrary distrbution d over S, define:

_∞_

_Q[π](s) = E_ _γ[t]r(st, at)_ _π, s0 = s, a0 = a_ _,_

" _t=0_ #
X

_V_ _[π](s) = Ea∼π(·|s) [Q[π](s, a)|π, s],_ _Vd[π]_ [=][ E][s]0[∼][d] [[][V][ π][(][s][)][|][π][]][ .]
Here the expectation is with respect to the randomness of the trajectory induced by π in M . When
convenient, we shall use V _[π]_ to denote Vd[π]0 [, and][ V][ ∗] [to denote][ max][π][ V][ π][.]

Similarly, to any policy π, one may ascribe a (discounted) state-visitation distribution d[π] = d[π]d0 [.]

_∞_

_d[π]d_ [(][s][) = (1][ −] _[γ][)]_ _t=0_ _γ[t][ X]τ_ :st=s _P_ (τ _|π, s0 ∼_ _d)_

X

**Modes of Accessing the MDP.** We henceforth consider two modes of accessing the MDP, that are
standard in the reinforcement learning literature, and provide different results for each.

The first natural access model is called the episodic rollout setting. This mode of interaction allows
us to execute a policy, stop and restart at any point, and do this multiple times.

Another interaction model we consider is called rollout with ν-restarts. This is similar to the
episodic setting, but here the agent may draw from the MDP a trajectory seeded with an initial state
distribution ν = d0. This interaction model was considered in prior work on policy optimization
_̸_
Kakade & Langford (2002); Agarwal et al. (2019). The motivation for this model is two-fold:
first, ν can be used to incorporate priors (or domain knowledge) about the state coverage of the
optimal policy; second, ν provides a mechanism to incorporate exploration into policy optimization
procedures.

3 SETTING: POLICY AGGREGATION AND WEAK LEARNING

Our boosting algorithms henceforth call upon weak learners to generate weak policies, and aggregate these policies in a way that guarantees eventual convergence to optimality. In this section we
formalize both components.


-----

**A Policy Tree**
_π¯ ∈_ Π(Π, N, T )

_π¯_

_w1[′]_ _w2[′]_

Γ[λ2] Γ[λ2]

_λ1_ _λ2_

**A Shrub**

_w1_ _λt ∈_ Λ(Π, N ) _w6_

_π1_ _π2_ _π3_ _π4_ _π5_ _π6_


Figure 1: The figure illustrates a Policy Tree hierarchy (see Definition 5), obtained by setting N = 3
on the inner loop, andthe lower level. The middle level holds T = 2 on the outer loop, to overall get all base policies T = 2 Policy Shurbs (see Definition 4), where each Shrub π1, ..., π6 ∈ ΠW on
_projectedλt ∈_ Λ(Π shrubs, N ) is an aggregation of base policies. The top level is an weighted aggregation of the Γ[λt], which forms the overall Policy Tree ¯π ∈ Π(Π, N, T ).

3.1 POLICY AGGREGATION

For a base class of policies ΠW, our algorithm incrementally builds a more expressive policy class
by aggregating base policies via both linear combinations and non-linear transformations. In effect,
the algorithm produces a finite-width depth-2 circuit over some subset of the base policy class. We
start with the simpler linear aggregation.

**Definition 2 (Function Aggregation). Given some N0 ∈** Z+, w ∈ R[N][0], (f1, . . . fN0 ) ∈ (S →
R[|][A][|])[⊗][N][0], we define f = _n=1_ _[w][n][f][n][ to be the unique function][ f][ :][ S][ →]_ [R][A][ for which simultane-]
ously for all s ∈ _S, it holds_ _N0_

[P][N][0]

_f_ (s) = _wnf_ (s).

_n=1_

X

Next, the projection operation below may be viewed as a non-linear activation, such as ReLU, in
deep learning terms. Note that the projection of any function from S to R[|][A][|] produces a policy, i.e.
a mapping from states to distributions over actions.

**Definition 3 (Policy Projection). Given a function f : S →** R[|][A][|], define a projected policy π = Γ[f ]
to be a policy such that simultaneously for all s ∈ _S, it holds that π(·|s) = Γ [f_ (s)] .

The next definition defines the class of functions represented by circuits of depth 1 over a base
policy class. Note that these function do not necessarily represent policies since they take an affine
(vs. convex) combination of policies.

**Definition 4 (Shrub). For an arbitrary base policy class Π ⊆** _S →_ ∆A, define Λ(Π, N ) to be a set
such that λ ∈ Λ(Π, N ) if and only if there exists N0 ≤ _N, w ∈_ R[N][0] _, (π1, . . . πN0_ ) ∈ Π[⊗][N][0] such
that λ = _n=1_ _[w][n][π][n][.]_

The final definition describes the set of possible outputs of the boosting procedure.

[P][N][0]

**Definition 5 (Policy Tree). For an arbitrary base policy class Π ⊆** _S →_ ∆A, define Π(Π, N, T ) to be
a policy class such that π Π(Π, N, T ) if and only if there exists T0 _T, w_ ∆T0 _, (λ1, . . . λT0_ )
_∈_ _≤_ _∈_ _∈_
Λ(Π, N )[⊗][T][0] such that π = _t=1_ _[w][t][Γ[][λ][t][]][.]_

It is important that the policy that the boosting algorithm outputs can be evaluated efficiently. In the

[P][T][0]

appendix we show it is indeed the case (see Lemma 15).


-----

3.2 MODELS OF WEAK LEARNING

We consider two types of weak learners, and give different end results based on the different assumptions: weak supervised and weak online learners. In the discussion below, let πr be a uniformly
random policy, i.e. (s, a) _S_ _A, πr(a_ _s) = 1/_ _A_ .
_∀_ _∈_ _×_ _|_ _|_ _|_

**Supervised Learning.** The natural way to define weak learning is an algorithm whose performance is always slight better than that of random policy, one that chooses an action uniformly at
random at any given state. However, in general no learner can outperform a random learner over
all label distributions (this is called the “no free lunch" theorem). This motivates the literature on
agnostic boosting (Kanade & Kalai, 2009; Brukhim et al., 2020; Hazan & Singh, 2021) that defines
a weak learner as one that can approximate the best policy in a given policy class.
**Definition 6 (Weak Supervised Learner). Let α ∈** (0, 1). Consider a class L of linear loss functions
_ℓ_ : R[A] _→_ R, and D a family of distributions that are supported over S × L, policy classes ΠW, Π.
A weak supervised learning algorithm, for every ε, δ > 0, given m(ε, δ) samples Dm from any
distribution D ∈ D outputs a policy W(Dm) ∈ ΠW such that with probability 1 − _δ,_

E(s,ℓ) _ℓ(_ (Dm)) _α max_ _ℓ(π[∗](s))_ + (1 _α)E(s,ℓ)_ _ℓ(πr(s))_ _ε._
_∼D_ _W_ _≥_ _π[∗]∈Π_ [E][(][s,ℓ][)][∼D] _−_ _∼D_ _−_
     

Note that the weak learner outputs a policy in ΠW which is approximately competitive against
the class Π. As an additional relaxation, instead of requiring that the weak learning guarantee
holds for all distributions, in our setup, it will be sufficient that the weak learning assumption holds
over natural distributions. We define these below. Hereafter, we refer to Π(ΠW, N, T ) as Π for
_N, T = O(poly(|A|, (1 −_ _γ)[−][1], ε[−][1], α[−][1], log δ[−][1])) specified later._
**Assumption 1 (Weak Supervised Learning). The booster has access to a weak supervised learning**
_oracle (Definition 6) over the policy class Π, for some α ∈_ (0, 1). Furthermore, the weak learning
_condition holds only for a class of natural distributions D – D ∈_ D if and only if there exists some
_π ∈_ Π such that

_S(s) =_ (s, ℓ)dµ(ℓ) = d[π](s).
_D_ _ℓ_ _D_
Z

In particular, while a natural distribution may have arbitrary distribution over labels, its marginal
distribution over states must be realizable as the state distribution of some policy in Π over the MDP
_M_ . Therefore, the complexity of weak learning adapts to the complexity of the MDP itself. As an
extreme example, in stochastic contextual bandits where policies do not affect the distribution of
states (say d0), it is sufficient that the weak learning condition holds with respect to all couplings of
a single distribution d0.

**Online Learning.** The second model of weak learning we consider requires a stronger assumption,
but will give us better sample and oracle complexity bounds henceforth.
**Definition 7 (Weak Online Learner). Let α ∈** (0, 1). Consider a class L of linear loss functions
_ℓ_ : R[A] _→_ R. A weak online learning algorithm, for every M > 0, incrementally for each timestep
computes a policy Wm ∈ ΠW and then observes the state-loss pair (s, ℓt) ∈ _S × L such that_


_ℓm(π[∗](sm)) + (1_ _α)_
_−_
_m=1_

X


_ℓm(_ _m(sm))_ _α max_
_W_ _≥_ _π[∗]_ Π
_m=1_ _∈_

X


_ℓm(πr(sm)) −_ _RW_ (M ).
_m=1_

X


**Assumption 2 (Weak Online Learning). The booster has access to a weak online learning oracle**
_(Definition 7) over the policy class Π, for some α ∈_ (0, 1).
**Remark 8. A similar remark about natural distributions applies to the online weak learner. In par-**
ticular, it is sufficient the guarantee in 7 holds for arbitrary sequence of loss functions with high
probability over the sampling of the state from d[π] for some π ∈ Π. Although stronger than supervised weak learning, this oracle can be interpreted as a relaxation of the online weak learning
oracle considered in (Brukhim et al., 2020; Brukhim & Hazan, 2021; Hazan & Singh, 2021). A similar model of hybrid adversarial-stochastic online learning was considered in (Rakhlin et al., 2011;
Lazaric & Munos, 2009; Beygelzimer et al., 2011). In particular, it is known (Lazaric & Munos,
2009) that unlike online learning, the capacity of a hypothesis class for this model is governed by its
VC dimension (vs. Littlestone dimension).


-----

4 ALGORITHM & MAIN RESULTS

In this section we describe our RL boosting algorithm. Here we focus on the case where a supervised
weak learning is provided. The online weak learners variant of our result is detailed in the appendix.
We next define several definitions and algorithmic subroutines required for our method.

**The Extension Operator.** The extension operator (Hazan & Singh, 2021) operate overs functions
and modifies their value outside and near the boundary of the convex set ∆A to aid the boosting
algorithm.

_FG,β[f_ ](x) = min _f_ (y) + G min
_y∈∆A_  _z∈∆A_ _[∥][y][ −]_ _[z][∥]_ [+ 1]2β _[∥][x][ −]_ _[y][∥][2]_

To state the results, we need the following definitions. The first generalizes the policy completeness
notion from (Scherrer & Geist, 2014). It may be seen as the policy-equivalent analogue of inherent
bellman error (Munos & Szepesvári, 2008). Intuitively, it measures the degree to which a policy in Π
can best approximate the bellman operator in an average sense with respect to the state distribution
induced by a policy from Π.
**Definition 9 (Policy Completeness). For any initial state distribution µ, define**


_Eµ(Π, Π) = maxπ∈Π_ _π[min][∗]∈Π_ [E][s][∼][d]µ[π]


max
_a_ _A_ _[Q][π][(][s, a][)][ −]_ _[Q][π][(][s,][ ·][)][⊤][π][∗][(][·|][s][)]_
_∈_


The following notion of the distribution mismatch coefficient is often useful to characterize the
exploration problem faced by policy optimization algorithms.
**Definition 10 (Distribution Mismatch). Let π[∗]** = arg maxπ V _[π], and ν a fixed initial state distribu-_
tion (see section 2). Define the following distribution mismatch coefficients:[1]


_d[π][∗]_

_d[π]_


_d[π][∗]_
_D_ =
_∞_ _ν_


_C_ (Π) = max
_∞_ _π∈Π_


4.1 RL BOOSTING VIA WEAK SUPERVISED LEARNING

We give the main RL boosting algorithm, assuming supervised weak learners. We use a simple
sub-routine for choosing a step size, provided in the appendix.

**Algorithm 1 RL Boosting via Weak Supervised Learning**

2:1: for Input parameters t = 1 to T do T, N, M, P, µ. Initialize a policy π0 ∈ ΠW arbitrarily.
3: Set ρt,0 to be an arbitrary policy in ΠW .

4: **for n = 1 to N do**

5: Execute πt 1 for M episodes with initial state distribution µ via Algorithm 2, to get
_−_


6: Modify Dt,n to produce a new datasetDt,n D =t,n[′] {(s[=]i,[ {]Q[c][(][s]i)[i][m]i[, f]=1[i][)][}][}][.] _i[m]=1[, such that for all][ i][ ∈]_ [[][m][]][:]


_fi = −∇FG,β[−Q[b]i](ρt,n(·|si))_

.

7: Let At,n be the policy chosen by the weak learning oracle when given data set 1 _Dt,n[′]_ [.]

8:9: **end forUpdate ρt,n = (1 −** _η2,n)ρt,n−1 +_ _[η][2]α[,n]_ _[A][t,n][ −]_ _[η][2][,n]_ _α_ _[−]_ [1] _πr._

10: Declare πt[′] [= Γ [][ρ][t,N] []][.]   

11: Choose η1,t = min{1, [2][C]t[∞] _} if µ = d0 else η1,t = StepChooser(πt−1, πt[′][, µ, P]_ [)][.]

12: Update πt = (1 _η1,t)πt_ 1 + η1,tπt[′][.]
_−_ _−_

13: end for
14: Output ¯π=πT if µ = d0 else output πt 1 with the smallest ηt.
_−_

1For brevity, We use the shorthand C where clear from context.
_∞_


-----

**Theorem 11. Algorithm 1 samples T** (MN + P ) episodes of length _O[˜](_ 1 1 _γ_ [)][ with probability][ 1][ −] _[δ][.]_

2 _−_

_In the episodic model, for T = O_ (1 _Cγ∞[2])[3]ε_ _, N =_ (116|Aγ|)C[2]αϵ∞ _, M = m_ (1C−γ)A[2]αε _[,]_ _NTδ_ _,µ = d0,_

_−_ _−_ _∞|_ _|_

_P = 0, with probability 1_ _δ,_      
_−_

(Π, Π)
_V_ _V_ _[π]_ _C_ _E_ + ε.

_[∗]_ _−_ _≤_ _∞_ 1 _γ_

_−_

2

_In the ν-reset model, for T_ = (18Dγ∞)[2][6]ε[2][,][ N] = (116|Aγ|)D[3]αϵ∞ _, P_ = _O˜(_ [200](1 _[|][A]γ[|])[2][6][D]ε[2]∞[2][ )][,][ M]_ =

_−_ _−_ _−_

_m_ (18|−Aγ|D)[3]∞αε _[,]_ 2NTδ _,µ = ν, with probability 1 −_ _δ,_  
 

_ν(Π, Π)_
_V_ _V_ _[π]_ _D_ _E_

_[∗]_ _−_ _≤_ _∞_ (1 _γ)[2][ +][ ε.]_

_−_


4.2 TRAJECTORY SAMPLER

In Algorithm 2 we describe an episodic sampling procedure, that is used in our sample-based RL
boosting algorithms described above. For a fixed initial state distribution µ, and any given policy
_π, we apply the following sampling procedure: start at an initial state s0_ _µ, and continue to_
act thereafter in the MDP according to any policy π, until termination. With this process, it is ∼
straightforward to both sample from the state visitation distribution s ∼ _d[π], and to obtain unbiased_
samples of Q[π](s, ·); see Algorithm 2 for the detailed process.

**Algorithm 2 Trajectory Sampler: s** _d[π], unbiased estimate of Q[π]s_
_∼_

1: Sample state s0 _µ, and action a[′]_ (A) uniformly.
2: Sample s ∼ _d[π] ∼as follows: at every timestep∼U_ _h, with probability γ, act according to π; else,_
accept sh as the sample and proceed to Step 3.

3: Take action a[′] at state sh, then continue to execute π, and use a termination probability of 1 _γ._
_−_
Upon termination, set R(sh, a[′]) as the undiscounted sum of rewards from time h onwards.

4: Define the vector _Q[π]sh_ [, such that for all][ a][ ∈] _[A][,][ d]Q[π]sh_ [(][a][) =][ |][A][| ·][ R][(][s][h][, a][′][)][ ·][ I][a][=][a][′] [.]

5: return (sh, _Q[π]sh_ [)][.]

[d]

[d]

5 ANALYSIS – PROOF SKETCH

We sketch the high-level ideas of the proof of our main result, stated in Theorem 11, and refer the
reader to the appendix for the formal proof. Throughout the analysis, we use the notation _π[V][ π][ to]_
_∇_
denote the gradient of the value function with respect to the |S| × |A|-sized representation of the
policy π, namely the functional gradient of V _[π]._

We establish an equivalence between the outlined algorithm and an abstraction of the Frank-Wolfe
algorithm (Algorithm D) from optimization theory. This variant of the Frank-Wolfe (FW) algorithm operates over non-convex and gradient dominated functions to obtain the following novel
convergence guarantees. We establish the necessary gradient domination results from the policy
completeness results.
**Theorem 12. Let f :** R be L-smooth in some norm _, H-bounded, and the diameter of_
_K →_ _∥· ∥∗_ _K_
_in ∥· ∥∗_ _be D. Then, for a (ϵ0, K2)-linear optimization oracle, the output ¯x of Algorithm D satisfies_

2HLD[2]

max _x)[⊤](u_ _x¯)_ + 3ϵ + ϵ0.
_u_ 2 _−_ _≤_ _T_
_∈K_ r

_[∇][f]_ [(¯]

_Furthermore, if f is (κ, τ,_ 1, 2)-locally gradient-dominated and x0, . . . xT 1, then it holds
_K_ _K_ _∈K_

max _x)_ + τ + κϵ0.
_x[∗]_ _[f]_ [(][x][∗][)][ −] _[f]_ [(¯] _≤_ [2][κ][2][ max][{]T[LD][2][, H][}]
_∈K_

The Frank-Wolfe algorithm utilizes an inner gradient optimization oracle as a subroutine. To implement this oracle using approximate optimizers, we utilize yet another variant of the FW method as
“internal-boosting” for the weak learners (by employing an adapted analysis of Theorem 13).


-----

5.1 INTERNAL-BOOSTING WEAK LEARNERS

We utilize a variant of the Frank-Wolfe method as a form “internal-boosting” for the weak learners,
by employing an adapted analysis of previous work that is stated below.

Note that _Q[π](s, ·) produced by Algorithm 2 satisfies ∥Q[b][π](s, ·)∥_ = 1|−Aγ| [. We can now borrow the]

following result on boosting for statistical learning from (Hazan & Singh, 2021), specializing the
decision set to be[b] ∆A. Let _t be the distribution induced by the trajectory sampler in round t._
_D_

**Theorem 13 ((Hazan & Singh, 2021)). Let β =** _αN1_ _[, and][ η][2][,n][ = min][{][ 2]n_ _[,][ 1][}][. Then, for any][ t][,][ π]t[′]_

_produced by Algorithm 1 satisfies with probabilityq 1_ _δ that_
_−_

2 _A_ 2
max _Q[⊤]π(s)_ E(s,Q) _t_ _Q[⊤]πt[′][(][s][)]_ _|_ _|_ + ε
_π∈Π_ [E][(][s,Q][)][∼D][t] _−_ _∼D_ _≤_ (1 − _γ)α_  _√N_ 
   

5.2 FROM WEAK LEARNING TO LINEAR OPTIMIZATION

In the following Lemma, we give an important observation which allows us to re-state the guarantee
in the previous subsection in terms of linear optimization over functional gradients.
**Lemma 14. Applying Algorithm 2 for any given policy π, yields an unbiased estimate of the gradi-**
_ent, such that for any π[′],_

1
(∇π[V][ π]µ [)][⊤][π][′][ =] 1 − _γ_ [E][(][s,][ c]Q[π](s,·))∼DhQ[π](s, ·)[⊤]π[′](·|s)i, (2)

_where π[′](_ _s)_ ∆A, and _is the distribution induced on the outputs of Algorithm 2, for a givenc_

_·|_ _∈_ _D_
_policy π and initial state distribution µ._

_Proof. Recall ∇π[V][ π][ denotes the gradient with respect to the][ |][S][| × |][A][|][-sized representation of the]_
policy π – the functional gradient. Then, using the policy gradient theorem (Williams, 1992; Sutton
et al., 2000), it is given by,
_∂Vµ[π]_ 1 _µ[(][s][)][Q][π][(][s, a][)][.]_ (3)

_∂π(a_ _s) [=]_ 1 _γ [d][π]_
_|_ _−_

The following sources of randomness are at play in the sampling algorithm (Algorithm 2): the
distribution d[π] (which encompasses the discount-factor-based random termination, the transition
probability, and the stochasticity of π), and the uniform sampling over A. For a fixed s, π, denote by
_Qs[π]_ [as the distribution over][ c]Q[π](s, ·) ∈ R[A], induced by all the aforementioned randomness sources.
To conclude the claim, observe that by construction

E _π(s,_ )[Q[π](s, ) _π, s] = Q[π](s,_ ). (4)
_Q_ _·_ [c] _·_ _|_ _·_

6 CONCLUSIONS

Building on recent advances in boosting for online convex optimization and bandits, we have described a boosting algorithm for reinforcement learning over large state spaces with provable guarantees. We see this as a first attempt at using a tried-and-tested methodology from supervised learning
in RL, and many challenges remain.

First and foremost, our notion of weak learner optimizes a linear function over policy space. A more
natural weak learner would be an RL agent with multiplicative optimality guarantee, and it would
be interesting to extend our methodology to this notion of weak learnability.

Another important aspect that is not discussed in our paper is that of state-space exploration. Potentially boosting can be combined with state-space exploration techniques to give stronger guarantees
independent of distribution mismatch C _, D_ factors.
_∞_ _∞_

Finally, a feature of our method is that it produces nonlinear aggregations of weak learners as per a
two layer neural network. Are simpler aggregations with provable guarantees possible?


-----

REFERENCES

Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. arXiv preprint arXiv:1908.00261,
2019.

Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed exploration for provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020a.

Naman Agarwal, Nataly Brukhim, Elad Hazan, and Zhou Lu. Boosting for control of dynamical
systems. In International Conference on Machine Learning, pp. 96–103. PMLR, 2020b.

Noga Alon, Alon Gonen, Elad Hazan, and Shay Moran. Boosting simple learners. arXiv preprint
_arXiv:2001.11704, 2020._

J Andrew Bagnell, Sham Kakade, Andrew Y Ng, and Jeff G Schneider. Policy search by dynamic
programming. In Advances in Neural Information Processing Systems, 2003.

Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit
algorithms with supervised learning guarantees. In Proceedings of the Fourteenth International
_Conference on Artificial Intelligence and Statistics, pp. 19–26. JMLR Workshop and Conference_
Proceedings, 2011.

Alina Beygelzimer, Satyen Kale, and Haipeng Luo. Optimal and adaptive algorithms for online
boosting. In International Conference on Machine Learning, pp. 2323–2331, 2015.

Nataly Brukhim and Elad Hazan. Online boosting with bandit feedback. In Algorithmic Learning
_Theory, pp. 397–420. PMLR, 2021._

Nataly Brukhim, Xinyi Chen, Elad Hazan, and Shay Moran. Online agnostic boosting via regret
minimization. In Advances in Neural Information Processing Systems, 2020.

Lin Chen, Christopher Harshaw, Hamed Hassani, and Amin Karbasi. Projection-free online optimization with stochastic gradient: From convexity to submodularity. In International Conference
_on Machine Learning, pp. 814–823, 2018._

Shang-Tse Chen, Hsuan-Tien Lin, and Chi-Jen Lu. An online boosting algorithm with theoretical
justifications. In Proceedings of the 29th International Coference on International Conference on
_Machine Learning, pp. 1873–1880, 2012._

Shang-Tse Chen, Hsuan-Tien Lin, and Chi-Jen Lu. Boosting with online binary learners for the
multiclass bandit problem. In International Conference on Machine Learning, pp. 342–350, 2014.

John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efficient projections onto
the l 1-ball for learning in high dimensions. In Proceedings of the 25th international conference
_on Machine learning, pp. 272–279, 2008._

Hamed Hassani, Mahdi Soltanolkotabi, and Amin Karbasi. Gradient methods for submodular maximization. In Advances in Neural Information Processing Systems, pp. 5841–5851, 2017.

Elad Hazan. Introduction to online convex optimization. arXiv preprint arXiv:1909.05207, 2019.

Elad Hazan and Karan Singh. Boosting for online convex optimization. _arXiv preprint_
_arXiv:2102.09305, 2021._

Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum entropy
exploration. In International Conference on Machine Learning, pp. 2681–2691. PMLR, 2019.

Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In International
_Conference on Machine Learning, pp. 427–435. PMLR, 2013._

Young Hun Jung and Ambuj Tewari. Online boosting algorithms for multi-label ranking. In Inter_national Conference on Artificial Intelligence and Statistics, pp. 279–287, 2018._


-----

Young Hun Jung, Jack Goetz, and Ambuj Tewari. Online multiclass boosting. In Advances in neural
_information processing systems, pp. 919–928, 2017._

Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
_In Proc. 19th International Conference on Machine Learning. Citeseer, 2002._

Varun Kanade and Adam Kalai. Potential-based agnostic boosting. In Advances in neural informa_tion processing systems, pp. 880–888, 2009._

Alessandro Lazaric and Rémi Munos. Hybrid stochastic-adversarial on-line learning. In Conference
_on Learning Theory, 2009._

Christian Leistner, Amir Saffari, Peter M Roth, and Horst Bischof. On robustness of on-line
boosting-a competitive study. In IEEE 12th International Conference on Computer Vision Work_shops, ICCV Workshops, pp. 1362–1369. IEEE, 2009._

Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. Stochastic conditional gradient methods:
From convex minimization to submodular maximization. arXiv preprint arXiv:1804.09554, 2018.

Rémi Munos and Csaba Szepesvári. Finite-time bounds for fitted value iteration. Journal of Machine
_Learning Research, 9(5), 2008._

Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Stochastic and constrained adversaries. arXiv preprint arXiv:1104.5070, 2011.

Robert E Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press, 2012.

Bruno Scherrer and Matthieu Geist. Local policy search in a convex space and conservative policy iteration as boosted policy search. In Joint European Conference on Machine Learning and
_Knowledge Discovery in Databases, pp. 35–50. Springer, 2014._

Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. Solla, T. Leen,
and K. Müller (eds.), Advances in Neural Information Processing Systems, volume 12.
MIT Press, 2000. [URL https://proceedings.neurips.cc/paper/1999/file/](https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)
[464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf.](https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)

Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229–256, 1992.

Jiahao Xie, Zebang Shen, Chao Zhang, Hui Qian, and Boyu Wang. Stochastic recursive gradientbased methods for projection-free online learning. arXiv preprint arXiv:1910.09396, 2019.


-----

A APPENDIX

It is important that the policy that the boosting algorithm outputs can be evaluated efficiently. Towards that end, we give the following claim.

**Claim 15. For any π ∈** Π(Π, N, T ), π(·|s) for any s ∈ _S can be evaluated using TN base policy_
_evaluations and O(T × (NA + A log A)) arithmetic and logical operations._

_Proof. Since π ∈_ Π(Π, N, T ), it is composed of TN base policies. Producing each aggregated
function takes NA additions and multiplications; there are T of these. Each projection takes time
equivalent to sorting |A| numbers, due to a water-filling algorithm (Duchi et al., 2008); these are
also T in number. The final linear transformation takes an additional TA operations.

B STEP-SIZE SUBROUTINE

Below we give an algorithm for choosing step sizes used in both of the RL boosting methods (for
online, and supervised, weak learners).

**Algorithm 3 StepChooser(πt** 1, πt[′][, µ, P] [)]
_−_

1: Execute πt 1 for P episodes with initial state distribution µ via Algorithm 2, to get
_−_


_D = {(si,_ _Qi)[P]i=1[}][.]_

2: For any policy π, let _G[π]_ = _P[1]_ _Pp=1_ _Qi⊤π(_ _si).[c]_

_·|_

3: Return

2

P (1 _γ)_

[c]η1,t = clip[0[c],1] _−_ _G[π]t[′]_ _G[π][t][−][1]_

2 _−_ [\]

 d []

C RL BOOSTING VIA WEAK ONLINE LEARNING

**Algorithm 4 RL Boosting via Weak Online Learning**


1:2: Initialize a policy for t = 1 to T do π0 ∈ ΠW arbitrarily.
3: Initialize online weak learners 1, . . . .
_W_ _W_ _[N]_

4: **for m = 1 to M do**

5: Execute πt 1 once with initial state distribution µ via Algorithm 2, to get (st,m, _Qt,m)._
_−_

7:6: Choosefor n = 1 ρt,m, to N0 ∈ doΠW arbitrarily.

1 [b]

9:8: **end forSet ρt,m,n = (1 −** _η2,n)ρt,m,n−1 +_ _[η][2]α[,n]_ _[W]_ _[n][ −]_ _[η][2][,n]_ _α_ _[−]_ [1] _πr._

10: Pass to each the following loss linear ft,m,n:   
_W_ _[n]_

_ft,m,n = −∇FG,β[−Q[b]t,m](ρt,m,n(·|si))_

11: **end for**

12: Declare πt[′] [=] _M1_ _Mm=1_ [Γ [][ρ][t,m,N] []][.]

13: Choose η1,t = minP{1, [2][C][∞]t [(][Π][)] _} if µ = d0 else set η1,t = StepChooser(πt−1, πt[′][, µ, P]_ [)][.]

14: Update πt = (1 _η1,t)πt_ 1 + η1,tπt[′][.]
_−_ _−_

15: end for
16: Output ¯π= πT if µ = d0 else output πt 1 with the smallest ηt.
_−_


**Theorem 16. Algorithm 4 samples T** (M + _P_ ) episodes of length 1 1 _γ_ [log][ T][ (][M]δ[+][P][ )] _with probability_

_−_ 2

1−δ. In the episodic model, Algorithm 4 guarantees as long as T = [16](1[C]−∞[2]γ)[(][3][Π]ε[)] _[,][ N][ =]_ 16(1|A−|γC)∞[2]αϵ(Π) _,_

 


-----

_M = max_ 1000(1 _|Aγ|)[2][4]Cε[2]∞[2]α[(][2][Π][)]_ log[2] _Tδ,_ [8][|][A][|][C](1[∞][(]γ[Π])[)][2][R]αε[W] [(][M] [)] _,µ = d0, we have with probability 1_ _δ_

_−_ _−_ _−_

n o

_V_ _[∗]_ _−_ _V_ _[π]_ _≤_ _C∞(Π)_ _[E]1[(][Π][,][ Π)]γ_ + ε

_−_ 2

_In the ν-reset model, Algorithm 1 guarantees as long as T =_ (1100−γD)[6]∞[2]ε[2][,][ N][ =] (120−|Aγ|)D[3]αϵ∞ _,_

2

_P =_ [250](1 _[D]γ∞[2])[6][|][A]ε[2][|][ log][2]_ [2][ T]δ _[,][ M][ = max]_ (140|Aγ|)D[3]αε∞ [log][ T]δ _,_ [10][|][A](1[|][D][∞]γ)[R][3]αε[W] [(][M] [)] _,µ = ν, we have with_ 

_−_ _−_ _−_

_probability 1 −_ _δ_   

_ν(Π, Π)_
_V_ _V_ _[π]_ _D_ _E_

_[∗]_ _−_ _≤_ _∞_ (1 _γ)[2][ +][ ε]_

_−_

_If R_ (M ) = _M log_ _for some measure of weak learning complexity_ _, the algorithm_
_W_ _C[4]_ _|W|_ _D[4]_ _|W|_

_samples_ _O[˜]_ _∞p(1[(][Π][)][|]γ[A])[|][7][2]α[ log][2]ε[3][ |W|]_ _episodes in the episodic model, and_ _O[˜]_ (1∞[|][A]γ[|])[2][12][ log]α[2][ |W|]ε[4] _in the ν-_

_−_ _−_

_reset model._   

D NON-CONVEX FRANK-WOLFE

In this section, we give an abstract high-level procedural template that the previously introduced RL
boosters operate in. This is based on a variant of the Frank-Wolfe optimization technique, adapted
to non-convex and gradient dominated function classes (see Definition 1).

The Frank-Wolfe (FW) method assumes oracle access to a black-box linear optimizer, denoted O,
and utilizes it by iteratively making oracle calls with modified objectives, in order to solve the
harder task of convex optimization. Analogously, boosting algorithms often assume oracle access
to a ”weak” learner, which are utilized by iteratively making oracle calls with modified objective,
in order to obtain a ”strong” learner, with boosted performance. In the RL setting, the objective is
in fact non-convex, but exhibits gradient domination. By adapting Frank-Wolfe technique to this
setting, we will in subsequent section obtain guarantees for the algorithms given in Section 4.

**Setting.** Denote by O a black-box oracle to an (ϵ0, K2)-approximate linear optimizer over a convex set K ⊆ R[d] such that for any given v ∈ R[d], we have

_v[⊤]_ (v) max
_O_ _≥_ _u_ 2 _[v][⊤][u][ −]_ _[ϵ][0][.]_
_∈K_

**Algorithm 5 Non-convex Frank-Wolfe**

1: Input: T > 0, objective f, linear optimization oracle O
2: Choose x0 arbitrarily.
3: for t = 1, . . ., T do
4: Call zt = ( _t_ 1[)][, where][ ∇]t 1 [=][ ∇][f] [(][x]t 1[)][.]
_O_ _∇_ _−_ _−_ _−_

5: Choose ηt = min 1, [2]t[κ]
_{_ _[}][ in the gradient-dominated case, else choose][ η][t][ so that]_

_|LD[2]ηt −∇⊤t−1[(][z][t]_ _[−]_ _[x][t][−][1][)][| ≤]_ _[ϵ.]_


6: Set xt = (1 _ηt)xt_ 1 + ηtzt.
_−_ _−_

7: end for
8: return ¯x = xT in the gradient-dominated case, else xt 1 with the smallest ηt.
_−_

**Theorem 17. Let f :** R be L-smooth in some norm _, H-bounded, and the diameter of_
_K →_ _∥· ∥∗_ _K_
_in ∥· ∥∗_ _be D. Then, for a (ϵ0, K2)-linear optimization oracle, the output ¯x of Algorithm D satisfies_

2HLD[2]

max _x)[⊤](u_ _x¯)_ + 3ϵ + ϵ0.
_u_ 2 _−_ _≤_ _T_
_∈K_ r

_[∇][f]_ [(¯]

_Furthermore, if f is (κ, τ,_ 1, 2)-locally gradient-dominated and x0, . . . xT 1, then it holds
_K_ _K_ _∈K_

max _x)_ + τ + κϵ0.
_x[∗]_ _[f]_ [(][x][∗][)][ −] _[f]_ [(¯] _≤_ [2][κ][2][ max][{]T[LD][2][, H][}]
_∈K_


-----

E ANALYSIS FOR BOOSTING WITH SUPERVISED LEARNING (PROOF OF
THEOREM 11)

**Theorem (Formal version of Theorem 11). Algorithm 1 samples T** (MN + P ) episodes of length

1−1 _γ_ [log][ T][ (][MN]δ [+][P][ )] _with probability 12_ _−_ _δ. In the episodic model, Algorithm 1 guarantees as long as_

_T =_ [16](1[C]∞[2]γ)[(][3][Π]ε[)] _[,][ N][ =]_ 16(1|A|γC)∞[2]αϵ(Π) _, M = m_ 8(1C−γ(Π)[2])αεA _NTδ_ _,µ = d0, we have with probability_

_−_ _−_ _∞_ _|_ _|_ _[,]_

1 _δ_    
_−_

_V_ _[∗]_ _−_ _V_ _[π]_ _≤_ _C∞(Π)_ _[E]1[(][Π][,][ Π)]γ_ + ε

_−_ 2

_In the ν-reset model, Algorithm 1 guarantees as long as T =_ (1−8Dγ∞)[2][6]ε[2][,][ N][ =] (116−|Aγ|)D[3]αϵ∞ _, P =_

200(1 _|Aγ|)[2][6]Dε[2]∞[2][ log][ 2][T N]δ_ _[,][ M][ =][ m]_ (18−AγD)[3]αε 2NTδ _,µ = ν, we have with probability 1_ _δ_ 

_−_ _|_ _|_ _∞_ _[,]_ _−_

 

_ν(Π, Π)_
_V_ _V_ _[π]_ _D_ _E_

_[∗]_ _−_ _≤_ _∞_ (1 _γ)[2][ +][ ε]_

_−_

_If m(ε, δ) =_ [log]ε[ |W|][2] log [1]δ _[for some measure of weak learning complexity][ |W|][, the algorithm samples]_

_O˜_ _C∞[6](1[(][Π]−[)]γ[|][A])[11][|][4]α[ log][4]ε[ |W|][5]_ _episodes in the episodic model, and_ _O[˜]_ _D(1∞[6]_ _−[|][A]γ[|])[4][18][ log]α[4][ |W|]ε[6]_ _in the ν-reset model._
   

_Proof of Theorem 11. The broad scheme here is to utilize an equivalence between Algorithm 1 and_
Algorithm D on the function V _[π]_ (or Vν[π] [in the][ ν][-reset model), to which Theorem 17 applies.]

To this end, firstly, note V _[π]_ is 1 1 _γ_ [-bounded. Define a norm][ ∥· ∥][∞][,][1][ :][ R][|][S][|×|][A][|][ →] [R][ as][ ∥][x][∥][1][,][∞] [=]

_−_

following lemma specifies the smoothness ofmaxs∈S _a∈A_ _[|][x][s,a][|][. Further, observe that for any policy] V_ _[π]_ in this norm.[ π][ :][ S][ →] [∆][A][,][ ∥][π][∥][∞][,][1][ = 1][. The]

P

**Lemma 18. V** _[π]_ _is_ (1 2γγ)[3][ -smooth in the][ ∥· ∥][∞][,][1][ norm.]

_−_


To be able to interpret Algorithm 1 as an instantiation of the algorithmic template Algorithm D
presents, we advance two claims: one, the step-size choices of the two algorithms conincide; two,
_πt[′]_ [(Line 3-10) serves as an approximate linear optimizers for][ ∇][V][ π][t][−][1] [. Together, these imply that]
the iterates produced by the two algorithms conincide. The first of these, which provides a value of
_ϵ to use in the statement of Theorem 17, is established below._

**Claim 19. Upon every invocation of StepChooser, the output η1,t satisfies with probability 1** _−_ _δ_
2η1,t 16 _A_

(1 _γ)[3][ −]_ [(][∇][V][ π]µ _[t][−][1]_ )[⊤](πt[′] _[−]_ _[π][t][−][1][)]_ _[≤]_ (1 _γ|)[2]|[√]P_ log [1]δ
_−_ _−_

Next, we move onto the linear optimization equivalence. Indeed, Claim 20 demonstrates that πt[′]
serves a linear optimizer over gradients of the function V _[π]; the suboptimality specifies ϵ0._

**Claim 20. Let β =** _αN1_ _[, and][ η][2][,n][ = min][{][ 2]n_ _[,][ 1][}][. Then, for any][ t][,][ π]t[′]_ _[produced by Algorithm 1]_

_satisfies with probabilityq_ 1 _δ_
_−_

2 _A_ 2
max _µ_ )[⊤](π _πt[′][)][ ≤]_ _|_ _|_ + εW
_π∈Π_ [(][∇][V][ π][t][−][1] _−_ (1 − _γ)[2]α_  _√N_ 

is the class of all policies,Finally, observe that it is by construction that K1 = Π, K2 = Π. _πt ∈_ Π. Therefore, in terms of the previous section, K

In the episodic model, we wish to invoke the second part of Theorem 17. The next lemma establishes
gradient-domination properties of V _[π]_ to support this.


**Lemma 21. V** _[π]_ _is_ _C_ (Π),
_∞_



1−1 _γ_ _[C][∞][(][Π][)][E][(Π][,][ Π][)][,][ Π][,][ Π]_ _-gradient dominated, i.e. for any π ∈_ Π:



_V_ _[∗]_ _−_ _V_ _[π]_ _≤_ _C∞(Π)_


1

1 − _γ_ _[E][(Π][,][ Π][) + max]π[′]∈Π[(][∇][V][ π][)][⊤][(][π][′][ −]_ _[π][)]_


-----

Deriving κ, τ from the above lemma along with ϵ0 from Claim 20 and ϵ from Claim 19, as a consequence of the second part of Theorem 17, we have with probability 1 − _NTδ_


_V_ _[∗]_ _−_ _V_ _π[¯] ≤_ _C∞(Π)_ _[E]1[(] −[Π][,][ Π)]γ_ + (1[4][C] −∞[2]γ[(])[Π][3][)]T [+] (14 −|A|γC)∞[2]α(√Π)N


+ [2][|][A][|][C][∞][(][Π][)]

(1 _γ)[2]α [ε][W][ .]_
_−_


Similarly, in the ν-reset model, the first part of Theorem 17 provides a local-optimality guarantee for
_Vν[π][. Lemma 22 provides a bound on the function-value gap (on][ V][ π][) provided such local-optimality]_
conditions.

**Lemma 22. For any π ∈** Π, we have


_V_ _[∗]_ _−_ _V_ _[π]_ _≤_


1

_ν_ [)][⊤][(][π][′][ −] _[π][)]_
1 − _γ_ _[E][ν][(Π][,][ Π][) + max]π[′]∈Π[(][∇][V][ π]_


1 _γ [D][∞]_
_−_


Again, using the bound on maxπ′ Π( _Vνπ[¯][)][⊤][(][π][′][ −]_ _π[¯]) Theorem 17 provides, we have that with_
_∈_ _∇_
probability 1 − 2NTδ

_V_ _V_ _π[¯]_ + 2D∞ + [2][|][A][|][D][∞] 2 + εW + 48|A|D∞ log [1]

_[∗]_ _−_ _≤_ _[D][∞](1[E][ν][(]γ[Π])[,][2][ Π)]_ (1 _γ)[3][√]T_ (1 _γ)[3]α_ _√N_ (1 _γ)[3][√]P_ _δ_

_−_ _−_ _−_   _−_

F ANALYSIS FOR BOOSTING WITH ONLINE LEARNING (PROOF OF
THEOREM 16)

_Proof of Theorem 16. Similar to the proof of Theorem 11, we establish an equivalence between_
Algorithm 1 and Algorithm D on the function V _[π]_ (or Vν[π] [in the][ ν][-reset model), to which Theorem 17]
applies provided smoothness (see Lemma 18).

Indeed, Claim 23 demonstrates πt[′] [serves a linear optimizer over gradients of the function][ V][ π][, and]
provides a bound on ϵ0. Claim 19 ensures that that the step size choices (and hence iterates) of the
two algorithms coincide. As before, observe that it is by construction that πt ∈ Π.

**Claim 23. Let β =** _αN1_ _[, and][ η][2][,n][ = min][{][ 2]n_ _[,][ 1][}][. Then, for any][ t][,][ π]t[′]_ _[produced by Algorithm 4]_

_satisfies with probabilityq_ 1 _δ_
_−_


16 log δ[−][1]


2 _A_
max _µ_ )[⊤](π _πt[′][)][ ≤]_ _|_ _|_
_π_ Π [(][∇][V][ π][t][−][1] _−_ (1 _γ)[2]α_
_∈_ _−_


+ _[R][W]_ [(][M] [)]


In the episodic model, one may combine the second part of Theorem 17, which provides a bound
on function-value gap for gradient dominated functions, which Lemma 21 guarantees, to conclude
with probability 1 − _Tδ_


_V_ _V_ _π[¯]_ + [4][C]∞[2] [(][Π][)] 4|A|C∞(Π)

_[∗]_ _−_ _≤_ _[C][∞][(][Π]1 −[)][E][(]γ[Π][,][ Π)]_ (1 − _γ)[3]T_ [+] (1 − _γ)[2]α√_


+ [8][|][A][|][C][∞][(][Π][) log][ δ][−][1]

(1 − _γ)[2]α√M_


_R_ (M )

+ [2][|][A][|][C][∞][(][Π][)] _W_

(1 − _γ)[2]α_ _M_


Similarly, in the ν-reset model, Lemma 22 provides a bound on the function-value gap provided
local-optimality conditions, which the first part of Theorem 17 provides for. Again, with probability
1 − _Tδ_

_V_ _V_ _π[¯]_ + 2D∞ 1 + 2 + _[R][W]_ [(][M] [)] + [4 log][ δ][−][1] + [24][|][A][|] log [1]

_[∗]_ _−_ _≤_ _[D][∞](1[E][ν][(]γ[Π])[,][2][ Π)]_ (1 _γ)[3]_ _√T_ _[|][A]α[|]_ _√N_ _M_ _√M_ _√P_ _δ_

_−_ _−_   


-----

G PROOFS OF SUPPORTING CLAIMS

G.1 NON-CONVEX FRANK-WOLFE METHOD (THEOREM 17)

_Proof of Theorem 17. Non-convex case. Note that for any timestep t, it holds due to smoothness_
that
_f_ (xt) = f (xt 1 + ηt(zt _xt_ 1))
_−_ _−_ _−_

_L_
_≥_ _f_ (xt−1) + ηt∇⊤t−1[(][z][t] _[−]_ _[x][t][−][1][)][ −]_ _[η]t[2]_ 2 _[D][2]_

= f (xt−1) − 2LD1 [2] _LD[2]ηt −∇⊤t−1[(][z][t]_ _[−]_ _[x][t][−][1][)]_ 2 + (∇⊤t−1[(]2[z]LD[t] _[−]_ [2][x][t][−][1][))][2]

Using the step-size definition to bound on the middle term, and telescoping this inequality over  
function-value differences across successive iterates, we have


_T_

_t=1(∇⊤t−1[(][z][t]_ _[−]_ _[x][t][−][1][))][2][ ≤]_ [2][LD]T [2][H]

X


min _⊤t_ 1[(][z][t]
_t_ [(][∇] _−_ _[−]_ _[x][t][−][1][))][2][ ≤]_ _T[1]_


+ ϵ[2]


Let t[′] = arg mint ηt and t[∗] = arg mint( _⊤t_ 1[(][z][t]
_∇_ _−_ _[−]_ _[x][t][−][1][))][2][. Then]_

_⊤t[′]_ 1[(][z][t][′][ −] _[x][t][′][−][1][)][ ≤]_ _[LD][2][η][t][′][ +][ ϵ][ ≤]_ _[LD][2][η][t][∗]_ [+][ ϵ]
_∇_ _−_

_⊤t[∗]_ 1[(][z][t][∗] _[−]_ _[x][t][∗][−][1][) + 2][ϵ][ ≤]_
_≤∇_ _−_


2LD[2]H


+ ϵ[2] + 2ϵ


To conclude the claim for the non-convex part, observe _√a + b ≤_ _[√]a +_ _√b for a, b > 0, and that_

since zt′ = O(∇t[′]−1[)][, it follows by oracle definition that]

max _⊤t[′]_ 1[u][ ≤∇]⊤t[′] 1[z][t][′][ +][ ϵ][0][.]
_u_ 2 _−_ _−_
_∈K_

_[∇]_

**Gradient-Dominated Case.** Define x[∗] = arg maxx _f_ (x) and ht = f (x[∗]) _f_ (xt).
_∈K_ _−_


_ht ≤_ _ht−1 −_ _ηt∇⊤t−1[(][z][t]_ _[−]_ _[x][t][−][1][) +][ η]t[2]_


_L_

smoothness
2 _[D][2]_


_L_
_ht_ 1 _ηt max_ _⊤t_ 1[(][y][ −] _[x][t][−][1][) +][ η]t[2]_ oracle
_≤_ _−_ _−_ _y∈K2_ _[η][t][∇]_ _−_ 2 _[D][2][ +][ η][t][ϵ][0]_

_L_

_≤_ _ht−1 −_ _[η]κ[t]_ [(][f] [(][x][∗][)][ −] _[f]_ [(][x][t][−][1][)) +][ η]t[2] 2 _[D][2][ +][ η][t]_ _ϵ0 +_ _κ[τ]_ gradient domination

_L_  

= 1 − _[η]κ[t]_ _ht−1 + ηt[2]_ 2 _[D][2][ +][ η][t]_ _ϵ0 +_ _κ[τ]_

The theorem now follows from the following claim.   

**Claim 24. Let C ≥** 1. Let gt be a H-bounded positive sequence such that

_gt_ 1 _gt_ 1 + σt[2][D][ +][ σ][t][E.]
_≤_ _−_ _[σ]C[t]_ _−_

_Then choosing σt = min_ 1, [2]t[C]   2C[2] maxt{2D,H} + CE.
_{_ _[}][ implies][ g][t][ ≤]_


G.2 SMOOTHNESS OF VALUE FUNCTION (LEMMA 18)

_Proof of Lemma 18. Consider any two policies π, π[′]. Using the Performance Difference Lemma_
(Lemma 3.2 in (Agarwal et al., 2019), e.g.) and Equation 2, we have

_|V_ _[π][′]_ _−_ _V_ _[π]_ _−∇V_ _[π](π[′]_ _−_ _π)|_

1
= 1 _γ_ Es∼dπ′ _Q[π](·|s)[⊤](π[′](·|s) −_ _π(·|s)_ _−_ Es∼dπ _Q[π](·|s)[⊤](π[′](·|s) −_ _π(·|s)_

_−_

1  

[] []

_≤_ (1 _γ)[2][ ∥][d][π][′][ −]_ _[d][π][∥][1][∥][π][′][ −]_ _[π][∥][∞][,][1]_

_−_


-----

_dThe last inequality uses the fact that[π]_ 1 1 _γ_ _γ_ To establish this, consider the Markov operator maxs,a Q[π](s, a) _≤_ 1−1 _γ_ [.] It suffices to show P _[π]( ∥s[′]_ _ds[π]) =[′]_ _−_
_∥_ _≤_ _−_ _[∥][π][′][ −]_ _[π][∥][∞][,][1][.]_ _|_

_a_ _A_ _[P]_ [(][s][′][|][s, a][)][π][(][a][|][s][)][ induced by a policy][ π][ on MDP][ M] [. For any distribution][ d][ supported on]
_∈_
_S, we have_

P


_∥(P_ _[π][′]_ _−_ _P_ _[π])d∥1 =_


_P_ (s[′]|s, a)d(s)(π[′](a|s) − _π(a|s)_
_s,a_

X

_s[′]_ _P_ (s[′]|s, a)∥d∥1∥π[′] _−_ _π∥∞,1 ≤∥π[′]_ _−_ _π∥∞,1_

X


_s[′]_


Using sub-additivity of the l1 norm and applying the above observation t times, we have for any t


_∥((P_ _[π][′]_ )[t] _−_ (P _[π])[t])d∥1 ≤_ _t∥π[′]_ _−_ _π∥∞,1._


Finally, observe that


_d[π][′]_ _d[π]_ 1 (1 _γ)_
_∥_ _−_ _∥_ _≤_ _−_


_∞_

_γ[t]_ ((P _[π][′]_ )[t] (P _[π])[t])d0_ 1
_∥_ _−_ _∥_
_t=0_

X


_≤∥π[′]_ _−_ _π∥∞,1(1 −_ _γ)_

G.3 STEP-SIZE GUARANTEE (CLAIM 19)


_∞_

_tγ[t]_ =

_t=0_

X


_γ_

1 − _γ_ _[∥][π][′][ −]_ _[π][∥][∞][,][1]_


_Proof of Claim 19. Let D be the distribution induced by Algorithm 2 upon being given πt−1. Due_
to Lemma 14, it suffices to demonstrate that for any π ∈{πt[′][, π][t][−][1][}][ the following claim holds with]
probability 1 2 [. The claim in turn follows from Hoeffding’s inequality, while noting][ \]Q[π][t][−][1] (s, )

is 1|Aγ| [-bounded in the] − _[δ]_ _[ l][∞]_ [norm.] _·_

_−_
_G[π]_ _−_ E(s, \Q[πt][−][1] (s,·))∼DhQ\[π][t][−][1] (s, ·)[⊤]π(·|s)i ≤ (1 −8|γA)|√P log 2[1]δ

[c]

G.4 GRADIENT DOMINATION (LEMMA 21 AND LEMMA 22)

_Proof of Lemma 21. Invoking Lemma 4.1 from (Agarwal et al., 2019) with µ = d0, we have_


_d[π][∗]_
_V_ _[∗]_ _−_ _V_ _[π]_ _≤_ _d[π]_ maxπ0 [(][∇][V][ π][)][⊤][(][π][0][ −] _[π][)]_

_∞_

_C_ (Π)(max
_≤_ _∞_ _π0_ [(][∇][V][ π][)][⊤][π][0][ −] _π[max][′]_ Π[(][∇][V][ π][)][⊤][π][′][ + max]π[′] Π[(][∇][V][ π][)][⊤][(][π][′][ −] _[π][))]_
_∈_ _∈_

Finally, with the aid of Equation 2, observe that


1
max max _Q[π](s, a)_ _Q[π](_ _s)[⊤]π[′][i]_
_π0_ [(][∇][V][ π][)][⊤][π][0][ −] _π[max][′]∈Π[(][∇][V][ π][)][⊤][π][′][ = min]π[′]∈Π_ 1 − _γ_ [E][s][∼][d][π] h _a_ _−_ _·|_

1
_≤_ 1 _γ_

_−_ _[E][(Π][,][ Π][)]_

_Proof of Lemma 22. Invoking Lemma 4.1 from (Agarwal et al., 2019) with µ = ν, we have_


1 _d[π][∗]_

1 _γ_ _ν_ maxπ0 [(][∇][V][ π]ν [)][⊤][(][π][0] _[−]_ _[π][)]_
_−_ _∞_

1

1 − _γ [D][∞][(max]π0_ [(][∇][V][ π]ν [)][⊤][π][0] _[−]_ _π[max][′]∈Π[(][∇][V][ π]ν_ [)][⊤][π][′][ + max]π[′]∈Π[(][∇][V][ π]ν [)][⊤][(][π][′][ −] _[π][))]_


_V_ _[∗]_ _−_ _V_ _[π]_ _≤_


-----

Again, with the aid of Equation 2, observe that

1
maxπ0 [(][∇][V][ π]ν [)][⊤][π][0] _[−]_ _π[max][′]∈Π[(][∇][V][ π]ν_ [)][⊤][π][′][ = min]π[′]∈Π 1 − _γ_ [E][s][∼][d]ν[π] hmaxa _Q[π](s, a) −_ _Q[π](·|s)[⊤]π[′][i]_

1
_≤_ 1 _γ_

_−_ _[E][ν][(Π][,][ Π][)]_

G.5 SUPERVISED LINEAR OPTIMIZATION GUARANTEES (CLAIM 20)

_Proof of Claim 20. The subroutine presented in lines 3-10 (which culminate in πt[′][) is an instantiation]_
of Algorithm 3 from (Hazan & Singh, 2021), specializing the decision set to be ∆A. To note the
equivalence, note that in (Hazan & Singh, 2021) the algorithm is stated assuming that the center-ofmass of the decision set is at the origin (after a coordinate transform); correspondingly, the update
rule in Algorithm 1 can be written as


(ρt,n − _πr) = (1 −_ _η2,n)(ρt,n−1 −_ _πr) +_ _[η][2]α[,n]_ [(][A][t,n][ −] _[π][r][)][.]_

For any state s, πr(·|s) = _A1_ **[1][|][A][|][ corresponds to the center-of-masss of][ ∆][A][. Finally, note that]**

maximizing f _[⊤]x over x ∈K is equivalent to minimizing (−f_ )[⊤]x over the same domain. Therefore,
we can borrow the following result on boosting for statistical learning from (Hazan & Singh, 2021)
(Theorem 13). Note that _Q[π](s, ·) produced by Algorithm 2 satisfies ∥Q[c][π](s, ·)∥_ = 1|−Aγ| [. Let][ D][t][ be]

the distribution induced by the trajectory sampler in round t.

[c]

**Theorem 25 ((Hazan & Singh, 2021)). Let β =** _αN1_ _[, and][ η][2][,n][ = min][{][ 2]n_ _[,][ 1][}][. Then, for any][ t][,][ π]t[′]_

_produced by Algorithm 1 satisfies with probabilityq 1_ _δ that_
_−_

2 _A_ 2
max _Q[⊤]π(s)_ E(s,Q) _t_ _Q[⊤]πt[′][(][s][)]_ _|_ _|_ + ε
_π∈Π_ [E][(][s,Q][)][∼D][t] _−_ _∼D_ _≤_ (1 − _γ)α_  _√N_ 
   

Lemma 14 allows us to restate the guarantees in the previous subsection in terms of linear optimization over functional gradients. The conclusion thus follows immediately by combining Lemma 14
and Theorem 25.

G.6 ONLINE LINEAR OPTIMIZATION GUARANTEES (CLAIM 23)

_Proof of Claim 23. In a similar vein to the proof of Claim 20, here we state the a result on boosting_
for online convex optimization (OCO) from (Hazan & Singh, 2021) (Theorem 6), the counterpart of
Theorem 13 for the online weak learning case.


**Theorem 26 ((Hazan & Singh, 2021)). Let β =** _αN1_ _[, and][ η][2][,n][ = min][{][ 2]n_ _[,][ 1][}][. Then, for any][ t][,]_

Γ[ρt,m,N ] produced by Algorithm 4 satisfies q


2M
_√N_



2 _A_
_Qˆ[⊤]t,m[Γ[][ρ][t,m,N]_ [](][s][t,m][)] _|_ _|_
_≤_ (1 _γ)α_
i _−_


_Qˆ[⊤]t,m[π][(][s][t,m][)]_


max
_π∈Π_


+ R (M )
_W_


_m=1_


_m=1_


Next we invoke online-to-batch conversions. Note that in Algorithm 4, (st,m, _Q[ˆ]t,m) for any fixed_
_t is sampled i.i.d. from the same distribution. Therefore, we can apply online-to-batch results, i.e._
Theorem 9.5 in (Hazan, 2019), on Theorem 26 to get


2 _A_
max _Q[⊤]π(s)_ E(s,Q) _t_ _Q[⊤]πt[′][(][s][)]_ _|_ _|_
_π∈Π_ [E][(][s,Q][)][∼D][t] _−_ _∼D_ _≤_ (1 − _γ)α_
   

We finally invoke Lemma 14.


16 log δ[−][1]


+ _[R][W]_ [(][M] [)]


-----

G.7 REMAINING PROOFS (CLAIM 24)

_gProof of Claim 24.previous display. Now, assumet ≤_ _H ≤_ 2Ct[2]H Let. For T t[∗] ≥= arg max gTt[∗]−, we proceed by induction. The base case (1 ≤ _t{2tC :[2] tmax ≤t−{12D,H2C}}. For any+ CE for some t ≤_ _t > TT_ _[∗], we havet =[∗]. T_ _[∗]) is true by the σt = 1 and_

2C 2 max 2D, H

_gt_ 1 _{_ _}_ + CE + [4][C] [2][D] + [2][CE]
_≤_ _−_ [2]t _t_ 1 _t[2]_ _t_
   _−_ 

1
_CE + 2C_ [2] max 2D, H 1 + [1]
_≤_ _{_ _}_ _t_ 1 _−_ [2]t _t[2]_
 _−_   

= CE + 2C [2] max 2D, H
_{_ _}_ _[t][2][ −]t[2][2](t[t][ +][ t]1)[ −]_ [1]

_−_

_CE + 2C_ [2] max 2D, H _[t][(][t][ −]_ [1)]
_≤_ _{_ _}_ _t[2](t_ 1)

_−_


-----

