# PESSIMISTIC BOOTSTRAPPING FOR UNCERTAINTY- DRIVEN OFFLINE REINFORCEMENT LEARNING


**Chenjia Bai**
Harbin Institute of Technology
baichenjia255@gmail.com


**Lingxiao Wang** **Zhuoran Yang**
Northwestern University Princeton University


**Zhihong Deng** **Animesh Garg**
University of Technology Sydney University of Toronto
Vector Institute, NVIDIA

**Zhaoran Wang**
Northwestern University

ABSTRACT


**Peng Liu**
Harbin Institute of Technology


Offline Reinforcement Learning (RL) aims to learn policies from previously collected datasets without exploring the environment. Directly applying off-policy
algorithms to offline RL usually fails due to the extrapolation error caused by
the out-of-distribution (OOD) actions. Previous methods tackle such problems
by penalizing the Q-values of OOD actions or constraining the trained policy to
be close to the behavior policy. Nevertheless, such methods typically prevent the
generalization of value functions beyond the offline data and also lack a precise
characterization of OOD data. In this paper, we propose Pessimistic Bootstrapping for offline RL (PBRL), a purely uncertainty-driven offline algorithm without
explicit policy constraints. Specifically, PBRL conducts uncertainty quantification via the disagreement of bootstrapped Q-functions, and performs pessimistic
updates by penalizing the value function based on the estimated uncertainty. To
tackle the extrapolating error, we further propose a novel OOD sampling method.
We show that such OOD sampling and pessimistic bootstrapping yields a provable
uncertainty quantifier in linear MDPs, thus providing the theoretical underpinning
for PBRL. Extensive experiments on D4RL benchmark show that PBRL has better
performance compared to the state-of-the-art algorithms.

1 INTRODUCTION

Deep Reinforcement Learning (DRL) (Sutton & Barto, 2018) achieves remarkable success in a variety of tasks. However, in most successful applications, DRL requires millions of interactions with
the environment. In real-world applications such as navigation (Mirowski et al., 2018) and healthcare (Yu et al., 2019), acquiring a large number of samples by following a possibly suboptimal policy
can be costly and dangerous. Alternatively, practitioners seek to develop RL algorithms that learn
a policy based solely on an offline dataset, where the dataset is typically available. However, directly adopting online DRL algorithms to the offline setting is problematic. On the one hand, policy
evaluation becomes challenging since no interaction is allowed, which limits the usage of on-policy
algorithms. On the other hand, although it is possible to slightly modify the off-policy value-based
algorithms and sample solely from the offline dataset in training, such modification typically suffers
from a significant performance drop compared with their online learning counterpart (Levine et al.,
2020). An important reason for such performance drop is the so-called distributional shift. Specifically, the offline dataset follows the visitation distribution of the behavior policies. Thus, estimating
the Q-functions of the corresponding greedy policy with the offline dataset is biased due to the difference in visitation distribution. Such bias typically leads to a significant extrapolation error for
DRL algorithms since the estimated Q-function tends to overestimate the out-of-distribution (OOD)
actions (Fujimoto et al., 2019).


-----

To tackle the distributional shift issue in offline RL, previous successful approaches typically fall
into two categories, namely, policy constraints (Kumar et al., 2019; Fujimoto & Gu, 2021) and conservative methods (Kumar et al., 2020; Yu et al., 2020; 2021). Policy constraints aim to restrict
the learned policy to be close to the behavior policy, thus reducing the extrapolation error in policy evaluation. Conservative methods seek to penalize the Q-functions for OOD actions in policy
evaluation and hinge on a gap-expanding property to regularize the OOD behavior. Nevertheless,
since policy constraints explicitly confine the policy to be close to the behavior policy, such method
tends to be easily affected by the non-optimal behavior policy. Meanwhile, although the conservative algorithms such as CQL (Kumar et al., 2020) do not require policy constraints, CQL equally
penalizes the OOD actions and lacks a precise characterization of the OOD data, which can lead
to overly conservative value functions. To obtain a more refined characterization of the OOD data,
uncertainty quantification is shown to be effective when associated with the model-based approach
(Yu et al., 2020; Kidambi et al., 2020), where the dynamics model can be learned in static data thus
providing more stable uncertainty in policy evaluation. Nevertheless, model-based methods need
additional modules and may fail when the environment becomes high-dimensional and noisy. In
addition, the uncertainty quantification for model-free RL is more challenging since the Q-function
and uncertainty quantifier need to be learned simultaneously (Yu et al., 2021).

To this end, we propose Pessimistic Bootstrapping for offline RL (PBRL), an uncertainty-driven
model-free algorithm for offline RL. To acquire reliable Q-function estimates and their corresponding uncertainty quantification, two components of PBRL play a central role, namely, bootstrapping
and OOD sampling. Specifically, we adopt bootstrapped Q-functions (Osband et al., 2016) for uncertainty quantification. We then perform pessimistic Q-updates by using such quantification as a
penalization. Nevertheless, solely adopting such penalization based on uncertainty quantification is
neither surprising nor effective. We observe that training the Q-functions based solely on the offline
dataset does not regularize the OOD behavior of the Q-functions and suffers from the extrapolation
error. To this end, we propose a novel OOD sampling technique as a regularizer of the learned Qfunctions. Specifically, we introduce additional OOD datapoints into the training buffer. The OOD
datapoint consists of states sampled from the training buffer, the corresponding OOD actions sampled from the current policy, and the corresponding OOD target based on the estimated Q-function
and uncertainty quantification. We highlight that having such OOD samples in the training buffer
plays an important role in both the Q-function estimation and the uncertainty quantification. We
remark that, OOD sampling controls the OOD behavior in training, which guarantees the stability of
the trained bootstrapped Q-functions. We further show that under some regularity conditions, such
OOD sampling is provably efficient under the linear MDP assumptions.

We highlight that PBRL exploits the OOD state-action pairs by casting a more refined penalization
over OOD data points, allowing PBRL to acquire better empirical performance than the policyconstraint and conservatism baselines. As an example, if an action lies close to the support of offline
data but is not contained in the offline dataset, the conservatism (Kumar et al., 2020) and policy
constraint (Fujimoto et al., 2019) methods tend to avoid selecting it. In contrast, PBRL tends to
assign a small Q-penalty for such an action as the underlying epistemic uncertainty is small. Hence,
the agent trained with PBRL has a higher chance consider such actions if the corresponding value
estimate is high, yielding better performance than the policy constraint and conservatism baselines.
Our experiments on the D4RL benchmark (Fu et al., 2020) show that PBRL provides reasonable
uncertainty quantification and yields better performance compared to the state-of-the-art algorithms.

2 PRELIMINARIES

We consider an episodic MDP defined by the tuple (S, A, T, r, P), where S is the state space, A is
the action space, T ∈ N is the length of episodes, r is the reward function, and P is the transition
distribution. The goal of RL is to find a policy π that maximizes the expected cumulative rewards
E _Ti=0 −1_ _[γ][t][r][i][]][, where][ γ][ ∈]_ [[0][,][ 1)][ is the discount factor in episodic settings. The corresponding]
_Q-function of the optimal policy satisfies the following Bellman operator,_
 P
_Qθ(s, a) := r(s, a) + γEs′_ _T (_ _s,a)_ max _._ (1)
_T_ _∼_ _·|_ _a[′][ Q][θ][−]_ [(][s][′][, a][′][)]

where θ is the parameters of Q-network. In DRL, the Q-value is updated by minimizing the TD- 
error, namely E(s,a,r,s′)[(Q _Q)[2]]. Empirically, the target_ _Q is typically calculated by a separate_
_−T_ _T_
target-network parameterized by θ[−] without gradient propagation (Mnih et al., 2015). In online RL,


-----

one typically samples the transitions (s, a, r, s[′]) through iteratively interacting with the environment.
The Q-network is then trained by sampling from the collected transitions.

In contrast, in offline RL, the agent is not allowed to interact with the environment. The experiences
are sampled from an offline dataset in = (s[i]t[, a]t[i][, r]t[i][, s][i]t+1[)][}]i [m][. Naive off-policy methods such]
_D_ _{_ _∈_
as Q-learning suffer from the distributional shift, which is caused by different visitation distribution
of the behavior policy and the learned policy. Specifically, the greedy action a[′] chosen by the target
_Q-network in s[′]_ can be an OOD-action since (s[′], a[′]) is scarcely covered by the dateset in. Thus,
_D_
the value functions evaluated on such OOD actions typically suffer from significant extrapolation
errors. Such errors can be further amplified through propagation and potentially diverges during
training. We tackle such a challenge by uncertainty quantification and OOD sampling.

3 PESSIMISTIC BOOTSTRAPPING FOR OFFLINE RL

3.1 UNCERTAINTY QUANTIFICATION WITH BOOTSTRAPPING

In PBRL, we maintain K bootstrapped Q-functions in critic to quantify the epistemic uncertainty.
Formally, we denote by Q[k] the k-th Q-function in the ensemble. Q[k] is updated by fitting the
following target

_Q[k]θ_ [(][s, a][) :=][ r][(][s, a][) +][ γ]E[b]s′ _P (_ _s,a),a′_ _π(_ _s)_ _Q[k]θ[−]_ [(][s][′][, a][′][)] _._ (2)
_T_ _∼_ _·|_ _∼_ _·|_
h i

Here we denote the empirical Bellman operator byb _T, which estimates the expectation_
E[Q[k]θ[−] [(][s][′][, a][′][)][ |][ s, a][]][ empirically based on the offline dataset. We adopt such an ensemble technique]
from Bootstrapped DQN (Osband et al., 2016), which is initially proposed for the online explorationb
task. Intuitively, the ensemble forms an estimation of the posterior distribution of the estimated Qfunctions, which yields similar value on areas with rich data and diversely on areas with scarce data.
Thus, the deviation among the bootstrapped Q-functions yields an epistemic uncertainty estimation,
which we aim to utilize as a penalization in estimating the Q-functions. Specifically, we introduce
the following uncertainty quantification U (s, a) based on the Q-functions {Q[k]}k∈[K],

1 _K_ 2

(s, a) := Std(Q[k](s, a)) = _Q[k](s, a)_ _Q(s, a)_ _._ (3)
_U_ _K_ _k=1_ _−_ [¯]

r

X  

Here we denote by _Q[¯] the mean among the ensemble of Q-functions. From the Bayesian perspective,_
such uncertainty quantification yields an estimation of the standard deviation of the posterior of Qfunctions. To better understand the effectiveness of such uncertainty quantification, we illustrate
with a simple prediction task. We use 10 neural networks with identical architecture and different
initialization as the ensemble. We then train the ensemble with 60 datapoints in R[2] plane, where
the covariate x is generated from the standard Gaussian distribution, and the response y is obtained
by feeding x into a randomly generated neural network. We plot the datapoints for training and the
uncertainty quantification in Fig. 1(a). As shown in the figure, the uncertainty quantification rises
smoothly from the in-distribution datapoints to the OOD datapoints.

In offline RL, we perform regression (s, a) → _T[b] Q[k](s, a) in Din to train the bootstrapped Q-_
functions, which is similar to regress x → _y in the illustrative task. The uncertainty quantification_
allows us to quantify the deviation of a datapoint from the offline dataset, which provides more
refined conservatism compared to the previous methods (Kumar et al., 2020; Wu et al., 2019).

3.2 PESSIMISTIC LEARNING

We now introduce the pessimistic value iteration based on the bootstrapped uncertainty quantification. The idea is to penalize the Q-functions based on the uncertainty quantification. To this end, we
propose the following target for state-action pairs sampled from in,
_D_

[in]Q[k]θ [(][s, a][) :=][ r][(][s, a][) +][ γ]E[b]s′ _P (_ _s,a),a′_ _π(_ _s)_ _Q[k]θ[−]_ [(][s][′][, a][′][)][ −] _[β][in][ U][θ][−]_ [(][s][′][, a][′][)] _,_ (4)
_T_ _∼_ _·|_ _∼_ _·|_
h i

where _θb−_ (s[′], a[′]) is the uncertainty estimation at (s[′], a[′]) based on the target network, and βin is a
_U_
tuning parameter. In addition, the empirical mean Es′ _P (_ _s,a),a′_ _π(_ _s) is obtained by sampling the_
_∼_ _·|_ _∼_ _·|_

[b]


-----

1.0

|Main Q-nets|Col2|
|---|---|



0.8

Target Q-net 1

Offline Target Q-net 2
Dataset

…

Target Q-net K

Pessimistic

Actor

Gradients

PBRL for Main Q-nets TD-error (in)
offline data

PBRL for OOD Main Q-net 1
OOD data sampling

Pseudo TD-error (OOD)

Main Q-net 2

… Pessimistic

Main Q-net K


(a) Uncertainty (b) PBRL: overall architecture

Figure 1: (a) Illustration of the uncertainty estimations in the regression task. The white dots represent data points, and the color scale represents the bootstrapped-uncertainty values in the whole
input space. (b) Illustration of the workflow of PBRL. PBRL splits the loss function into two components. The TD-error (in) represents the regular TD-error for in-distribution data (i.e., from the
offline dataset), and pseudo TD-error (ood) represent the loss function for OOD actions. In the
update of Q-functions, both losses are computed and summed up for the gradient update.

transition (s, a, s[′]) from Din and further sampling a[′] _∼_ _π(· | s[′]) from the current policy π. We denote_
by _T_ [in]Q[k]θ [(][s, a][)][ the in-distribution target of][ Q][k]θ [(][s, a][)][ and write][ b]T [in] to distinguish the in-distribution
target from that of the OOD target, which we introduce in the sequel.

We remark that there are two options to penalize the[b] _Q-functions through the operator_ _T_ [in]. In
Eq. (4), we penalize the next-Q value Q[k]θ[−] [(][s][′][, a][′][)][ with the corresponding uncertainty][ U][θ][−] [(][s][′][, a][′][)][.]
Alternatively, we can also penalize the immediate reward by ˆr(s, a) := r(s, a) _θ(s, a) and use_
_−U_ [b]
_rˆ(s, a) in place of r(s, a) for the target. Nevertheless, since the datapoint (s, a) ∈Din lies on rich-_
data areas, the penalization _θ(s, a) on the immediate reward is usually very small thus having less_
_U_
effect in training. In PBRL, we penalize the uncertainty of (s[′], a[′]) in the next-Q value.

Nevertheless, our empirical findings in §D suggest that solely penalizing the uncertainty for indistribution target is insufficient to control the OOD performance of the fitted Q-functions. To
enforce direct regularization over the OOD actions, we incorporate the OOD datapoints directly in
training and sample OOD data of the form (s[ood], a[ood]) ∈Dood. Specifically, we sample OOD states
from the in-distribution dataset Din. Correspondingly, we sample OOD actions a[ood] by following
the current policy π(· | s[ood]). We highlight that such OOD sampling requires only the offline dataset
_Din and does not require additional generative models or access to the simulator._

It remains to design the target for OOD samples. Since the transition P (· | s[ood], a[ood]) and reward
_r(s[ood], a[ood]) are unknown, the true target of OOD sample is inapplicable. In PBRL, we propose a_
novel pseudo-target for the OOD datapoints,

[ood]Q[k]θ [(][s][ood][, a][ood][) :=][ Q][k]θ [(][s][ood][, a][ood][)][ −] _[β][ood]_ (5)
_T_ _[U][θ][(][s][ood][, a][ood][)][,]_

which introduces an additional uncertainty penalization _θ(s[ood], a[ood]) to enforce pessimistic Q-_

b _U_

function estimation, and βood is a tuning parameter. For OOD samples that are close to the indistribution data, such penalization is small and the OOD target is close to the Q-function estimation. In contrast, for OOD samples that are distant away from the in-distribution dataset, a larger
penalization is incorporated into the OOD target. In our implementation, we introduce an additional truncation to stabilize the early stage training as max{0, T [ood]Q[k]θ [(][s][ood][, a][ood][)][}][. In addition,]
we remark that βood is important to the empirical performance. Specifically,

-  At the beginning of training, both the Q-functions and the corresponding uncertainty quantifications are inaccurate. We use a large βood to enforce a strong regularization on OOD actions.

-  We then gradually decrease βood in the training process since the value estimation and uncertainty quantification becomes more accurate in training. We remark that a smaller βood requires
more accurate uncertainty estimate for the pessimistic target estimation _T_ [ood]Q[k]. In addition,
a decaying parameter βood stabilizes the convergence of Q[k]θ [(][s][ood][, a][ood][)][ in the training from]
the empirical perspective.

[b]


-----

Incorporating both the in-distribution target and OOD target, we conclude the loss function for critic
in PBRL as follows,

_Lcritic =_ E(s,a,r,s′)∼Din (T [in]Q[k] _−_ _Q[k])[2][]_ + Esood∼Din,aood∼π (T [ood]Q[k] _−_ _Q[k])[2][],_ (6)

where we iteratively minimize the regular TD-error for the offline data and the pseudo TD-error for 
the OOD data. Incorporated with OOD sampling, PBRL obtains a smooth and pessimistic value[b] [b] [b] [b]
function by reducing the extrapolation error caused by high-uncertain state-action pairs.

Based on the pessimistic Q-functions, we obtain the corresponding policy by solving the following
maximization problem,

_πϕ := maxϕ_ Es∼Din,a∼π(·|s) _k=1min,...,K_ _[Q][k][(][s, a][)]_ _,_ (7)
h i

where ϕ is the policy parameters. Here we follow the previous actor-critic methods (Haarnoja et al.,b
2018; Fujimoto et al., 2018) and take the minimum among ensemble Q-functions, which stablizes
the training of policy network. We illustrate the overall architecture of PBRL in Fig. 1(b).

3.3 THEORETICAL CONNECTIONS TO LCB-PENALTY

In this section, we show that the pessimistic target in PBRL aligns closely with the recent theoretical
investigation on offline RL (Jin et al., 2021; Xie et al., 2021a). From the theoretical perspective, an
appropriate uncertainty quantification is essential to the provable efficiency in offline RL. Specifically, the ξ-uncertainty quantifier plays a central role in the analysis of both online and offline RL
(Jaksch et al., 2010; Azar et al., 2017; Wang et al., 2020a; Jin et al., 2020; 2021; Xie et al., 2021a;b).
**Definition 1 (ξ-Uncertainty Quantifier (Jin et al., 2021)). The set of penalization** Γt _t_ [T ] forms a
_{_ _}_ _∈_
_ξ-Uncertainty Quantifier if it holds with probability at least 1 −_ _ξ that_

_Vt+1(s, a)_ _Vt+1(s, a)_ Γt(s, a)
_|T_ _−T_ _| ≤_

_for all (s, a) ∈S × A, where T[b]_ _is the Bellman equation and_ _T is the empirical Bellman equation_
_that estimates T based on the offline data._

[b]

In linear MDPs (Jin et al., 2020; Wang et al., 2020a; Jin et al., 2021) where the transition kernel and
reward function are assumed to be linear to the state-action representation φ : S × A → R[d], The
following LCB-penalty (Abbasi-Yadkori et al., 2011; Jin et al., 2020) is known to be a ξ-uncertainty
quantifier for appropriately selected {βt}t∈[T ],

1/2
Γ[lcb](st, at) = βt · _φ(st, at)[⊤]Λ[−]t_ [1][φ][(][s][t][, a][t][)] _,_ (8)

where Λt = _i=1_ _[φ][(][s]t[i][, a][i]t[)][φ][(][s][i]t[, a][i]t[)][⊤]_ [+][ λ][ ·][ I] [ accumulates the features of state-action pairs in] _[ D][in]_
and plays the role of a pseudo-count intuitively. We remark that under such linear MDP assumptions, the penalty proposed in PBRL and Γ[lcb](st, at) in linear MDPs is equivalent under a Bayesian

[P][m]
perspective. Specifically, we make the following claim.
_the LCB-penaltyClaim 1. In linear MDPs, the proposed bootstrapped uncertainty Γ[lcb](st, at) in Eq. (8) for an appropriately selected tuning parameter βt · U(st, at) is an estimation to βt._

We refer to §A for a detailed explanation and proof. Intuitively, the bootstrapped Q-functions estimates a non-parametric Q-posterior (Osband et al., 2016; 2018a). Correspondingly, the uncertainty
quantifier (st, at) estimates the standard deviation of the Q-posterior, which scales with the LCB_U_
penalty in linear MDPs. As an example, we show that under the tabular setting, Γ[lcb](st, at) is
approximately proportional to the reciprocal pseudo-count of the corresponding state-action pair in
the dataset (See Lemma 2 in §A). In offline RL, such uncertainty quantification measures how trustworthy the value estimations on state-action pairs are. A low LCB-penalty (or high pseudo-count)
indicates that the corresponding state-action pair aligns with the support of offline data.

Under the linear MDP or Bellman-consistent assumptions, penalizing the estimated value function
based on the uncertainty quantification is known to yield an efficient offline RL algorithm (Jin et al.,
2021; Xie et al., 2021a;b). However, due to the large extrapolation error of neural networks, we find
that solely penalizing the value function of the in-distribution samples is insufficient to regularize
the fitted value functions of OOD state-action pairs.


-----

A key to the success of linear MDP algorithms (Jin et al., 2020; Wang et al., 2020a; Jin et al., 2021)
is the extrapolation ability through L2-regularization in the least-squares value iteration (LSVI),
which guarantees that the linear parameterized value functions behave reasonably on OOD stateaction pairs. From a Bayesian perspective, such L2-regularization enforces a Gaussian prior on the
estimated parameter of linear approximations, which regularizes the value function estimation on
OOD state-action pairs with limited data available. Nevertheless, our empirical study in §D shows
that L2-regularization is not sufficient to regularize the OOD behavior of neural networks.

To this end, PBRL introduces a direct regularization over an OOD dataset. From the theoretical
perspective, we observe that adding OOD datapoint (s[ood], a[ood], y) into the offline dataset leads to an
equivalent regularization to the L2-regularization under the linear MDP assumption. Specifically, in
linear MDPs, such additional OOD sampling yields a covariate matrix of the following form,

_m_
Λ = _t[, a][i]t[)][φ][(][s][i]t[, a][i]t[)][⊤]_ [+] (9)

_i=1_ _[φ][(][s][i]_ (s[ood],a[ood],y) ood _[φ][(][s][ood][, a][ood][)][φ][(][s][ood][, a][ood][)][⊤][,]_

_∈D_

X X

where the matrixe Λood = (s[ood],a[ood],y) ood _[φ][(][s][ood][, a][ood][)][φ][(][s][ood][, a][ood][)][⊤]_ [plays the role of the][ λ][·][I][ prior]

_∈D_
in LSVI. It remains to design a proper target y in the OOD datapoint (s[ood], a[ood], y). The following
theorem show that setting[P] y = _Vh+1(s[ood], a[ood]) leads to a valid ξ-uncertainty quantifier under the_
_T_
linear MDP assumption.
(Theorem 1.s[ood], a[ood], y Let) ∈D Λoodood, if we set ⪰ _λ · I y. Under the linear MDP assumption, for all the OOD datapoint = T Vt+1(s[ood]1, a/2_ [ood]), it then holds for βt = O _T ·_ _√d · log(T/ξ)_

_that Γ[lcb]t_ [(][s][t][, a][t][) =][ β][t] _φ(st, at)[⊤]Λ[−]t_ [1][φ][(][s][t][, a][t][)] _forms a valid ξ-uncertainty quantifier. _ 

We refer to §A for a detailed discussion and proof.  Theorem 1 shows that if we set y =
_Vt+1(s[ood], a[ood]), the bootstrapped uncertainty based on disagreement among ensembles is a valid_
_T_
_ξ-uncertainty quantifier. However, such an OOD target is impossible to obtain in practice as it re-_
quires knowing the transition at the OOD datapoint (s[ood], a[ood]). In practice, if TD error is sufficiently
minimized, then Q(s[ood], a[ood]) should well estimate the target _Vt+1. Thus, in PBRL, we utilize_
_T_

_y = Q(s[ood], a[ood]) −_ Γ[lcb](s[ood], a[ood]) (10)

as the OOD target, where we introduce an additional penalization Γ[lcb](s[ood], a[ood]) to enforce pessimism. In addition, we remark that in theory, we require that the embeddings of the OOD sample
are isotropic in the sense that the eigenvalues of the corresponding covariate matrix Λood are lower
bounded. Such isotropic property can be guaranteed by randomly generating states and actions. In
practice, we find that randomly generating states is more expensive than randomly generating actions. Meanwhile, we observe that randomly generating actions alone are sufficient to guarantee
reasonable empirical performance since the generated OOD embeddings are sufficiently isotropic.
Thus, in our experiments, we randomly generate OOD actions according to our current policy and
sample OOD states from the in-distribution dataset.

4 RELATED WORKS

Previous model-free offline RL algorithms typically rely on policy constraints to restrict the learned
policy from producing the OOD actions. In particular, previous works add behavior cloning (BC)
loss in policy training (Fujimoto et al., 2019; Fujimoto & Gu, 2021; Ghasemipour et al., 2021),
measure the divergence between the behavior policy and the learned policy (Kumar et al., 2019;
Wu et al., 2019; Kostrikov et al., 2021), apply advantage-weighted constraints to balance BC and
advantages (Siegel et al., 2020; Wang et al., 2020b), penalize the prediction-error of a variational
auto-encoder (Rezaeifar et al., 2021), and learn latent actions (or primitives) from the offline data
(Zhou et al., 2020; Ajay et al., 2021). Nevertheless, such methods may cause overly conservative
value functions and are easily affected by the behavior policy (Nair et al., 2020; Lee et al., 2021b).
We remark that the OOD actions that align closely with the support of offline data could also be
trustworthy. CQL (Kumar et al., 2020) directly minimizes the Q-values of OOD samples and thus
casts an implicit policy constraint. Our method is related to CQL in the sense that both PBRL and
CQL enforce conservatism in Q-learning. In contrast, we conduct explicit uncertainty quantification
for OOD actions, while CQL penalizes the Q-values of all OOD samples equally.

In contrast with model-free algorithms, model-based algorithms learn the dynamics model directly
with supervised learning. Similar to our work, MOPO (Yu et al., 2020) and MOReL (Kidambi


-----

et al., 2020) incorporate ensembles of dynamics-models for uncertainty quantification, and penalize
the value function through pessimistic updates. Other than the uncertainty quantification, previous
model-based methods also attempt to constrain the learned policy through BC loss (Matsushima
et al., 2020), advantage-weighted prior (Cang et al., 2021), CQL-style penalty (Yu et al., 2021), and
Riemannian submanifold (Tennenholtz et al., 2021). Decision Transformer (Chen et al., 2021) builds
a transformer-style dynamic model and casts the problem of offline RL as conditional sequence
modeling. However, such model-based methods may suffer from additional computation costs and
may perform suboptimally in complex environments (Chua et al., 2018; Janner et al., 2019). In
contrast, PBRL conducts model-free learning and is less affected by such challenges.

Our method is related to the previous online RL exploration algorithms based on uncertainty quantification, including bootstrapped Q-networks (Bai et al., 2021; Lee et al., 2021a), ensemble dynamics (Sekar et al., 2020), Bayesian NN (O’Donoghue et al., 2018; Azizzadenesheli et al., 2018), and
distributional value functions (Mavrin et al., 2019; Nikolov et al., 2019). Uncertainty quantification
is more challenging in offline RL than its online counterpart due to the limited coverage of offline
data and the distribution shift of the learned policy. In model-based offline RL, MOPO (Yu et al.,
2020) and MOReL (Kidambi et al., 2020) incorporate ensemble dynamics-model for uncertainty
quantification. BOPAH (Lee et al., 2020) combines uncertainty penalization and behavior-policy
constraints. In model-free offline RL, UWAC (Wu et al., 2021) adopts dropout-based uncertainty
(Gal & Ghahramani, 2016) while relying on policy constraints in learning value functions. In contrast, PBRL does not require additional policy constraints. In addition, according to the study in
image prediction with data shift (Ovadia et al., 2019), the bootstrapped uncertainty is more robust to
data shift than the dropout-based approach. EDAC (An et al., 2021) is a concurrent work that uses
the ensemble Q-network. Specifically, EDAC calculates the gradients of each Q-function and diversifies such gradients to ensure sufficient penalization for OOD actions. In contrast, PBRL penalizes
the OOD actions through direct OOD sampling and the associated uncertainty quantification.

Our algorithm is inspired by the recent advances in the theory of both online RL and offline RL.
Previous works propose provably efficient RL algorithms under the linear MDP assumption for both
the online setting (Jin et al., 2020) and offline setting (Jin et al., 2021), which we follow for our
analysis. In addition, previous works also study the offline RL under the Bellman completeness
assumptions (Modi et al., 2021; Uehara et al., 2021; Xie et al., 2021a; Zanette et al., 2021) and the
model-based RL under the kernelized nonlinear regulator (KNR) setting (Kakade et al., 2020; Mania
et al., 2020; Chang et al., 2021). In contrast, our paper focus on model-free RL.

5 EXPERIMENTS

In experiments, we include an additional algorithm named PBRL-prior, which is a slight modification of PBRL by incorporating random prior functions (Osband et al., 2018b). The random
prior technique is originally proposed for online exploration in Bootstrapped DQN (Osband et al.,
2016). Specifically, each Q-function in PBRL-Prior contains a trainable network Q[k]θ [and a prior]
network pk, where pk is randomly initialized and is fixed in training. The prediction of each
_Q-function is the sum of the trainable network and the fixed prior, Q[k]p_ [=][ Q]θ[k] [+][ p][k][, where][ Q]θ[k]
and pk shares the same network architecture. The random prior function increases the diversity among ensemble members and improves the generalization of bootstrapped functions (Osband
et al., 2018b). We adopt SAC (Haarnoja et al., 2018) as the basic actor-critic architecture for both
_PBRL and PBRL-prior. We refer to §B for the implementation details. The code is available at_
[https://github.com/Baichenjia/PBRL.](https://github.com/Baichenjia/PBRL)

In D4RL benchmark (Fu et al., 2020) with various continuous-control tasks and datasets, we compare the baseline algorithms on the Gym and Adroit domains, which are more extensively studied in
the previous research. We compare PBRL and PBRL-Prior with several state-of-the-art algorithms,
including (i) BEAR (Kumar et al., 2019) that enforces policy constraints through the MMD distance,
(ii) UWAC (Wu et al., 2021) that improves BEAR through dropout uncertainty-weighted update,
(iii) CQL (Kumar et al., 2020) that learns conservative value functions by minimizing Q-values of
OOD actions, (iv) MOPO (Yu et al., 2020) that quantifies the uncertainty through ensemble dynamics in a model-based setting, and (v) TD3-BC (Fujimoto & Gu, 2021), which adopts adaptive BC
constraint to regularize the policy in training.


-----

BEAR UWAC CQL MOPO TD3-BC **PBRL** **PBRL-Prior**

HalfCheetah 2.3 ±0.0 2.3 ±0.0 17.5 ±1.5 35.9 ±2.9 11.0 ±1.1 11.0 ±5.8 13.1 ±1.2

Hopper 3.9 ±2.3 2.7 ±0.3 7.9 ±0.4 16.7 ±12.2 8.5 ±0.6 26.8 ±9.3 31.6 ±0.3

Walker2d 12.8 ±10.2 2.0 ±0.4 5.1 ±1.3 4.2 ±5.7 1.6 ±1.7 8.1 ±4.4 8.8 ±6.3

HalfCheetah 43.0 ±0.2 42.2 ±0.4 47.0 ±0.5 73.1 ±2.4 48.3 ±0.3 57.9 ±1.5 58.2 ±1.5

Hopper 51.8 ±4.0 50.9 ±4.4 53.0 ±28.5 38.3 ±34.9 59.3 ±4.2 75.3 ±31.2 81.6 ±14.5

Walker2d -0.2 ±0.1 75.4 ±3.0 73.3 ±17.7 41.2 ±30.8 83.7 ±2.1 89.6 ±0.7 90.3 ±1.2

HalfCheetah 36.3 ±3.1 35.9 ±3.7 45.5 ±0.7 69.2 ±1.1 44.6 ±0.5 45.1 ±8.0 49.5 ±0.8

Hopper 52.2 ±19.3 25.3 ±1.7 88.7 ±12.9 32.7 ±9.4 60.9 ±18.8 100.6 ±1.0 100.7 ±0.4

Walker2d 7.0 ±7.8 23.6 ±6.9 81.8 ±2.7 73.7 ±9.4 81.8 ±5.5 77.7 ±14.5 86.2 ±3.4


HalfCheetah 46.0 ±4.7 42.7 ±0.3 75.6 ±25.7 70.3 ±21.9 90.7 ±4.3 92.3 ±1.1 93.6 ±2.3

Expert Hopper 50.6 ±25.3 44.9 ±8.1 105.6 ±12.9 60.6 ±32.5 98.0 ±9.4 110.8 ±0.8 111.2 ±0.7
Medium Walker2d 22.1 ±44.9 96.5 ±9.1 107.9 ±1.6 77.4 ±27.9 110.1 ±0.5 110.1 ±0.3 109.8 ±0.2

HalfCheetah 92.7 ±0.6 92.9 ±0.6 96.3 ±1.3 81.3 ±21.8 96.7 ±1.1 92.4 ±1.7 96.2 ±2.3

Hopper 54.6 21.0 110.5 0.5 96.5 28.0 62.5 29.0 107.8 7 110.5 0.4 110.4 0.3

Expert _±_ _±_ _±_ _±_ _±_ _±_ _±_

Walker2d 106.6 ±6.8 108.4 ±0.4 108.5 ±0.5 62.4 ±3.2 110.2 ±0.3 108.3 ±0.3 108.8 ±0.2

**Average** 38.78 ±10.0 50.41 ±2.7 67.35 ±9.1 53.3 ±16.3 67.55 ±3.8 74.37 ±5.3 76.66 ±2.4

Table 1: Average normalized score and the standard deviation of all algorithms over five seeds in
Gym. The highest performing scores are highlighted. The score of TD3-BC is the reported scores
in Table 7 of Fujimoto & Gu (2021). The scores for other baselines are obtained by re-training with
the ‘v2’ dataset of D4RL (Fu et al., 2020).


**Results in Gym domain.** The Gym domain includes three environments (HalfCheetah, Hopper, and Walker2d) with five dataset types (random, medium, medium-replay, medium-expert,
and expert), leading to a total of 15 problem setups. We train all the baseline algorithms in the
latest released ‘v2’ version dataset, which is also adopted in TD3-BC (Fujimoto & Gu, 2021)
for evaluation. For methods that are originally evaluated on the ‘v0’ dataset, we retrain with
their respective official implementations on the ‘v2’ dataset. We refer to §B for the training details. We train each method for one million time steps and report the final evaluation performance through online interaction. Table 1 reports the normalized score for each task and the corresponding average performance. We find CQL and TD3-BC perform the best among all baselines, and PBRL outperforms the baselines in most of the tasks. In addition, PBRL-Prior slightly
outperforms PBRL and is more stable in training with a reduced variance among different seeds.

We observe that compared with the baseline algorithms, PBRL has strong advantages in the non- 80
optimal datasets, including medium, medium-replay, 70
and medium-expert. In addition, compared with the 60
policy-constraint baselines, PBRL exploits the opti- 50
mal trajectory covered in the dataset in a theoreti-cally grounded way and is less affected by the be- 40
havior policy. We report the average training curves UWAC
in Fig. 2. In addition, we remark that the perfor- CQL
mance of PBRL and PBRL-Prior are weaker than 10 TD3-BC
TD3-BC and CQL in the early stage of training, in- 0 PBRL-Prior (ours)
dicating that the uncertainty quantification is inaccu- 0 200 400 600 800 1000
rate initially. Nevertheless, PBRL and PBRL-Prior Gradient Steps (thousands)
converge to better policies that outperform the base- Figure 2: Average training curve in Gym.

Average Performance in D4RL

80

70

60

50

40

30 BEAR

UWAC

D4RL Normalized Score 20 MOPO

CQL

10 TD3-BC

PBRL (ours)

0 PBRL-Prior (ours)

0 200 400 600 800 1000

Gradient Steps (thousands)

lines in the learning of uncertainty quantifiers, demonstrating the effectiveness of uncertainty penalization and OOD sampling.

**Results in Adroit domain.** The adroit tasks are more challenging than the Gym domain in task
complexity. In addition, the use of human demonstration in the dataset makes the task even more
challenging in the offline setting. We defer the results to §E. We observe that CQL and BC have the
best average performance in all baselines, and PBRL outperforms baselines in most of the tasks.


-----

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
||||U|-Offline||
||||U U|-Policy -Rando|mAction|
||||U U|-Noise1 -Noise2||

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
||||U U|-Offline -Policy||
||||U|-Rando|mAction|
||||U U|-Noise1 -Noise2||

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
||||Q Q|-Offline -Policy||
||||Q|-Rando|mAction|
||||Q Q|-noise1 -noise2||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
|||||Q|-Offline||
|||||Q Q Q Q|-Policy -Rando -noise1 -noise2|mAction|
|0||200 400 600 800 10 Gradient Steps (thousands)|||||


16 walker2d-medium-replay-v2 (Uncertainty) 160 walker2d-medium-replay-v2 (Q-value) 7 walker2d-medium-v2 (Uncertainty) walker2d-medium-v2 (Q-value)

300

14 140 6

12 5 250

10 120 4 200

8 100 3 150

642 U-OfflineU-PolicyU-RandomActionU-Noise1 80 Q-OfflineQ-PolicyQ-RandomActionQ-noise1 21 U-OfflineU-PolicyU-RandomActionU-Noise1 10050 Q-OfflineQ-PolicyQ-RandomActionQ-noise1

0 U-Noise2 60 Q-noise2 0 U-Noise2 0 Q-noise2

0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000

Gradient Steps (thousands) Gradient Steps (thousands) Gradient Steps (thousands) Gradient Steps (thousands)


(a) Walker2d-Medium-Replay


(b) Walker2d-Medium


Figure 3: The uncertainty and Q-value for different state-action sets in the training process.

**Uncertainty quantification.** To verify the effectiveness of the bootstrapped uncertainty quantification, we record the uncertainty quantification for different sets of state-action pairs in training. In
our experiments, we consider sets that have the same states from the offline dataset but with different types of actions, including (i) aoffline, which is drawn from the offline in-distribution transition;
(ii) apolicy, which is selected by the training policy; (iii) arand, which is uniformly sampled from
the action space of the corresponding task; (iv) anoise1 = aoffline + N (0, 0.1), which adds a small
Gaussian noise onto the offline action to represent state-action pair that is close to in-distribution
data; and (v) anoise2 = aoffline + N (0, 0.5), which adds a large noise to represent the OOD action.

We compute the uncertainty and Q-value in ‘Walker2d’ task with two datasets (‘medium-replay’ and
‘medium’). The results are shown in Fig. 3. We observe that (i) PBRL yields large uncertainties for
(s, anoise2) and (s, arandom), indicating that the uncertainty quantification is high on OOD samples.
(ii) The (s, aoffline) pair has the smallest uncertainty in both settings as it is an in-distribution sample.
(iii) The (s, anoise1) pair has slightly higher uncertainty compared to (s, aoffline), showing that the
uncertainty quantification rises smoothly from the in-distribution actions to the OOD actions. (iv)
The Q-function of the learned policy (s, apolicy) is reasonable and does not deviate much from
the in-distribution actions, which shows that the learned policy does not take actions with high
uncertainty due to the penalty with uncertainty quantification. In addition, we observe that there is no
superior maximum for OOD actions, indicating that by incorporating the uncertainty quantification
and OOD sampling, the Q-functions obtained by PBRL does not suffer from the extrapolation error.

**Ablation study.** In the following, we briefly report the result of the ablation study. We refer to
§C and §D for the details. (i) Number of bootstrapped-Q. We attempt different numbers K of
bootstrapped-Q in PBRL, and find the performance to be reasonable for K ≥ 6. (ii) Penalization
_in_ _T_ [in]. We conduct experiments with the penalized reward discussed in §3.2 and find the penalized
reward does not improve the performance. (iii) Factor βin in _T_ [in]. We conduct experiments with
different[b] _βin from_ 0.1, 0.01, 0.001, 0.0001 to study the sensitiveness. Our experiments show that
_{_ _}_
PBRL performs the best for βin [0.0001, 0.01]. (iv) Factor βood. We use a large βood at the initial

[b]
training stage to enforce strong regularization over the OOD actions, and gradually decrease ∈ _βood_
in training. We conduct experiments by setting βood to different constants, and find our decaying
strategy generalizes better among tasks. (v) The learning target of OOD actions. We change the
learning target of OOD actions to the most pessimistic zero target y = 0 and find such a setting
leads to overly pessimistic value functions with suboptimal performance. (vi) Regularization types.
We conduct experiments with different regularization types, including our proposed OOD sampling,
_L2-regularization, spectral normalization, pessimistic initialization, and no regularization. We find_
OOD sampling the only reasonable regularization strategy and defer the complete report to §D.

6 CONCLUSION

In this paper, we propose PBRL, an uncertainty-based model-free offline RL algorithm. We propose bootstrapped uncertainty to guide the provably efficient pessimism, and a novel OOD sampling
technique to regularize the OOD actions. PBRL is closely related to the provable efficient offline RL
algorithm under the linear MDP assumption. Our experiments show that PBRL outperforms several
strong offline RL baselines in the D4RL environments. PBRL exploits the optimal trajectories contained in the suboptimal dataset and is less affected by the behavior policy. Meanwhile, we show
that PBRL produces reliable uncertainty quantifications incorporated with OOD sampling.


-----

REPRODUCIBILITY STATEMENT

[The code of our work is available at https://github.com/Baichenjia/PBRL. The en-](https://github.com/Baichenjia/PBRL)
vironments, datasets, and hyper-parameters of our experiments are are given in the appendix. All
settings, assumptions, lemmas, and theorems are proved and are discussed in detail in our appendix.

ACKNOWLEDGEMENTS

This work was supported in part by the National Natural Science Foundation of China under Grant
51935005, in part by the Fundamental Research Program under Grant JCKY20200603C010. The
authors thank the anonymous reviewers, whose invaluable comments and suggestions have helped
us to improve the paper.

REFERENCES

Yasin Abbasi-Yadkori, D´avid P´al, and Csaba Szepesv´ari. Improved algorithms for linear stochastic
bandits. In Advances in neural information processing systems, volume 24, pp. 2312–2320, 2011.

Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare.
Deep reinforcement learning at the edge of the statistical precipice. In Advances in neural infor_mation processing systems, 2021._

Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. Opal: Offline primitive discovery for accelerating offline reinforcement learning. In International Conference on
_Learning Representations, 2021._

Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. In Advances in neural information processing
_systems, 2021._

Mohammad Gheshlaghi Azar, Ian Osband, and R´emi Munos. Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, 2017.

Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient exploration
through bayesian deep q-networks. In 2018 Information Theory and Applications Workshop (ITA),
pp. 1–9. IEEE, 2018.

Chenjia Bai, Lingxiao Wang, Lei Han, Jianye Hao, Animesh Garg, Peng Liu, and Zhaoran Wang.
Principled exploration via optimistic bootstrapping and backward induction. In International
_Conference on Machine Learning, pp. 577–587. PMLR, 2021._

Catherine Cang, Aravind Rajeswaran, Pieter Abbeel, and Michael Laskin. Behavioral priors and
dynamics models: Improving performance and domain transfer in offline rl. _arXiv preprint_
_arXiv:2106.09119, 2021._

Jonathan D Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen Sun. Mitigating covariate shift in imitation learning via offline data without great coverage. arXiv preprint
_arXiv:2106.03207, 2021._

Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning
via sequence modeling. In Advances in neural information processing systems, 2021.

Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Advances in Neural Information
_Processing Systems, 31, 2018._

Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.

Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. In
_Advances in neural information processing systems, 2021._


-----

Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actorcritic methods. In International Conference on Machine Learning, pp. 1587–1596. PMLR, 2018.

Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019.

Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In International Conference on Machine Learning, pp. 1050–1059.
PMLR, 2016.

Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expectedmax q-learning operator for simple yet effective offline and online rl. In International Conference
_on Machine Learning, pp. 3682–3691. PMLR, 2021._

Florin Gogianu, Tudor Berariu, Mihaela C Rosca, Claudia Clopath, Lucian Busoniu, and Razvan
Pascanu. Spectral normalisation for deep reinforcement learning: An optimisation perspective.
In International Conference on Machine Learning, volume 139, pp. 3734–3744, 2021.

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer_ence on Machine Learning, pp. 1861–1870. PMLR, 2018._

Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 2010.

Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Modelbased policy optimization. _Advances in Neural Information Processing Systems, 32:12519–_
12530, 2019.

Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137–2143.
PMLR, 2020.

Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In
_International Conference on Machine Learning, pp. 5084–5096. PMLR, 2021._

Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun. Information theoretic regret bounds for online nonlinear control. arXiv preprint arXiv:2006.12466,
2020.

Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Modelbased offline reinforcement learning. In Advances in Neural Information Processing Systems,
volume 33, pp. 21810–21823, 2020.

Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning
with fisher divergence critic regularization. In International Conference on Machine Learning,
pp. 5774–5783. PMLR, 2021.

Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Sys_tems, volume 32, pp. 11784–11794, 2019._

Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
_Advances in Neural Information Processing Systems, volume 33, pp. 1179–1191, 2020._

Byungjun Lee, Jongmin Lee, Peter Vrancx, Dongho Kim, and Kee-Eung Kim. Batch reinforcement
learning with hyperparameter gradients. In International Conference on Machine Learning, pp.
5725–5735. PMLR, 2020.

Kimin Lee, Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning. In International Conference on Ma_chine Learning, pp. 6131–6141. PMLR, 2021a._


-----

Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-online
reinforcement learning via balanced replay and pessimistic q-ensemble. In Annual Conference on
_Robot Learning, 2021b._

Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.

Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li FeiFei, Silvio Savarese, Yuke Zhu, and Roberto Mart´ın-Mart´ın. What matters in learning from offline
human demonstrations for robot manipulation. In Annual Conference on Robot Learning, 2021.

Horia Mania, Michael I Jordan, and Benjamin Recht. Active learning for nonlinear system identification with guarantees. arXiv preprint arXiv:2006.10277, 2020.

Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deploymentefficient reinforcement learning via model-based offline optimization. In International Confer_ence on Learning Representations, 2020._

Borislav Mavrin, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang Yu. Distributional
reinforcement learning for efficient exploration. In International conference on machine learning,
pp. 4424–4434. PMLR, 2019.

Piotr Mirowski, Matt Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis
Teplyashin, Karen Simonyan, Andrew Zisserman, Raia Hadsell, et al. Learning to navigate in
cities without a map. Advances in Neural Information Processing Systems, 31:2419–2430, 2018.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
_Nature, 518:529–533, 2015._

Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free
representation learning and exploration in low-rank MDPs. arXiv preprint arXiv:2102.07035,
2021.

Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.

Nikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, and Andreas Krause. Informationdirected exploration for deep reinforcement learning. In International Conference on Learning
_Representations, 2019._

Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. In Advances in Neural Information Processing Systems, volume 29, pp. 4026–
4034, 2016.

Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement
learning. Advances in Neural Information Processing Systems, 31, 2018a.

Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement
learning. Advances in Neural Information Processing Systems, 31, 2018b.

Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating
predictive uncertainty under dataset shift. Advances in Neural Information Processing Systems,
32:13991–14002, 2019.

Brendan O’Donoghue, Ian Osband, Remi Munos, and Volodymyr Mnih. The uncertainty bellman
equation and exploration. In International Conference on Machine Learning, pp. 3836–3845,
2018.


-----

Shideh Rezaeifar, Robert Dadashi, Nino Vieillard, L´eonard Hussenot, Olivier Bachem, Olivier
Pietquin, and Matthieu Geist. Offline reinforcement learning as anti-exploration. arXiv preprint
_arXiv:2106.06431, 2021._

Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak.
Planning to explore via self-supervised world models. In International Conference on Machine
_Learning, pp. 8583–8592. PMLR, 2020._

Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what
worked: Behavioral modelling priors for offline reinforcement learning. In International Confer_ence on Learning Representations, 2020._

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

Guy Tennenholtz, Nir Baram, and Shie Mannor. GELATO: geometrically enriched latent model for
offline reinforcement learning. CoRR, abs/2102.11327, 2021.

Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. Representation learning for online and offline
RL in low-rank MDPs. arXiv preprint arXiv:2110.04652, 2021.

Ruosong Wang, Simon S Du, Lin F Yang, and Ruslan Salakhutdinov. On reward-free reinforcement learning with linear function approximation. In Advances in neural information processing
_systems, 2020a._

Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E
Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. Advances in Neural Information Processing Systems, 33, 2020b.

Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
_arXiv preprint arXiv:1911.11361, 2019._

Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua M. Susskind, Jian Zhang, Ruslan Salakhutdinov, and Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. In
_International Conference on Machine Learning, volume 139, pp. 11319–11328, 2021._

Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent
pessimism for offline reinforcement learning. In Advances in neural information processing sys_tems, 2021a._

Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. arXiv preprint arXiv:2106.04895,
2021b.

Lin Yang and Mengdi Wang. Sample-optimal parametric Q-learning using linearly additive features.
In International Conference on Machine Learning, pp. 6995–7004. PMLR, 2019.

Chao Yu, Jiming Liu, and Shamim Nemati. Reinforcement learning in healthcare: A survey. arXiv
_preprint arXiv:1908.08796, 2019._

Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. In Advances in Neural
_Information Processing Systems, volume 33, pp. 14129–14142, 2020._

Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.
Combo: Conservative offline model-based policy optimization. In Advances in neural information
_processing systems, 2021._

Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic methods for offline reinforcement learning. Advances in neural information processing systems, 2021.

Wenxuan Zhou, Sujay Bajracharya, and David Held. Plas: Latent action space for offline reinforcement learning. In Conference on Robot Learning, 2020.


-----

A THEORETICAL PROOF

A.1 BACKGROUND OF LCB-PENALTY IN LINEAR MDPS

In this section, we introduce the provably efficient LCB-penalty in linear MDPs (Abbasi-Yadkori
et al., 2011; Jin et al., 2020; 2021). We consider the setting of γ = 1 in the following. In linear
MDPs, the feature map of the state-action pair takes the form of φ : S × A → R[d], and the transition
kernel and reward function are assumed to be linear in φ. As a result, for any policy π, the stateaction value function is also linear in φ (Jin et al., 2020), that is,

_Q[π](s[i]t[, a][i]t[) =][ b]wt[⊤][φ][(][s]t[i][, a][i]t[)][.]_ (11)

The parameter wt can be solved in the closed-form by following the Least-Squares Value Iteration
(LSVI) algorithm, which minimizes the following loss function,

_m_

_wt = min_ _φ(s[i]t[, a][i]t[)][⊤][w][ −]_ _[r][(][s][i]t[, a][i]t[)][ −]_ _[V][t][+1][(][s][i]t+1[)]_ 2 + λ _w_ 22[,] (12)
_w∈R[d]_ _i=1_ _· ∥_ _∥_

X  

where Vt+1 is the estimated value function in theb (t + 1)-th step, and r(s[i]t[, a][i]t[) +][ V][t][+1][(][s][i]t+1[)][ is the]
target of LSVI. The explicit solution to (12) takes the form of


_wt = min_
_w∈R[d]_
b


_i=1_


_wt = Λ[−]t_ [1]


_φ(s[i]t[, a]t[i][)]_ _Vt+1(s[i]t+1[) +][ r][(][s]t[i][, a]t[i][)]_ _,_ Λt =
_i=1_

X   


_φ(s[i]t[, a]t[i][)][φ][(][s]t[i][, a]t[i][)][⊤]_ [+][ λ][ ·][ I][.] (13)
_i=1_

X


Here Λt accumulate the state-action features from the training buffer. Based on the solution of wt,
the action-value function can be estimated by Qt(st, at) ≈ _wt[⊤][φ][(][s][t][, a][t][)][. In addition, in offline RL]_
with linear function assumption, the following LCB-penalty yields an uncertainty quantification,
b 1/2

Γ[lcb](st, at) = βt · _φ(st, at)[⊤]Λ[−]t_ [1][φ][(][s][t][, a][t][)] _,_ (14)

which measures the confidence interval of the _Q-functions with the given training data (Abbasi-_
Yadkori et al., 2011; Jin et al., 2020; 2021). In offline RL, the pessimistic value function _Qt(st, at)_
penalizes Qt by the uncertainty quantification Γ[lcb](st, at) as a penalty as follows,

[b]

_Qt(st, at) = Qt(st, at)_ Γ[lcb](st, at)
_−_ (15)

= wt[⊤][φ][(][s][t][, a][t][)][ −] [Γ][lcb][(][s][t][, a][t][)][,]

b

where wt is defined in Eq. (12). Under the linear MDP setting, such pessimistic value iteration
is known to be information-theoretically optimal (Jin et al., 2021). In addition, exploration with
Γ[lcb](st, at) as a bonus is also provably efficient in the online RL setting (Abbasi-Yadkori et al.,
2011; Jin et al., 2020).

A.2 CONNECTION BETWEEN THE BOOTSTRAPPED UNCERTAINTY AND Γ[lcb]


In the sequel, we consider a Bayesian linear regression perspective of LSVI in Eq. (12). According
to the Bellman equation, the objective of LSVI is to approximate the Bellman target b[i]t [=][ r][(][s]t[i][, a][i]t[)+]
_Vt+1(s[i]t+1[)][ with the][ Q][-function][ Q][t][, where][ V][t][+1]_ [is the estimated value function in the][ (][t][+1)][-th step.]
In linear MDPs, We parameterize the Q-function by Qt(st, at) = _wt[⊤][φ][(][s][t][, a][t][)][. We further define]_
the noise ϵ in this least-square problem as follows,

_ϵ = b[i]t_ _[−]_ _[w]t[⊤][φ][(][s][t][, a][t][)][,]_ b (16)

where wt is the underlying true parameter. In the offline dataset with in = (s[i]t[, a][i]t[, s][i]t+1[)][}]i [m][,]
_D_ _{_ _∈_
we denote by _wt the Bayesian posterior of w given the dataset Din. In addition, we assume that_
we are given a Gaussian prior of the parameter w ∼N (0, I/λ) as a non-informative prior. The
following Lemma establishes connections between bootstrapped uncertainty and the LCB-penalty
b
Γ[lcb].
**Lemma 1 (Formal Version of Claim 1). We assume that ϵ follows the standard Gaussian distribution**
(0, 1) given the state-action pair (s[i]t[, a][i]t[)][. It then holds for the posterior][ w][t] _[given][ D][in]_ _[that]_
_N_

Varwt _Qt(s[i]t[, a][i]t[)]_ = Varwt _φ(s[i]t[, a][i]t[)][⊤]w[b]t_ = φ(s[i]t[, a][i]t[)][⊤][Λ]t[−][1][φ][(][s]t[i][, a][i]t[)][,] _∀(s[i]t[, a][i]t[)][ ∈S × A][.][ (17)]_
b    b   


-----

_Proof. The proof follows the standard analysis of Bayesian linear regression. Under the assumption_
that ϵ ∼N (0, 1), we obtain that

_b[i]t_ _[|][ (][s]t[i][, a][i]t[)][,][ b]w ∼N_ _wt[⊤][φ][(][s]t[i][, a][i]t[)][,][ 1]_ _._ (18)

Recall that we have the prior distribution w (0 , I/λ). Our objective is to compute the posterior
density _wt = w | Din. It holds from Bayes rule that ∼N_ b

log p(w in) = log p(w) + log p( in _w) + Const.,_ (19)

b _| D_ _D_ _|_

where p(·) denote the probability density function of the respective distributions. Plugging (18) and
the probability density function of Gaussian distribution into (19) yields b b b

_m_

log p(w | Din) = −∥w∥[2]/2 − _∥w[⊤]φ(s[i]t[, a][i]t[)][ −]_ _[y]t[i][∥][2][/][2 + Const][.]_

_i=1_

X

b = −(wb − _µt)[⊤]Λ[−]t_ [1][(][ b]wb − _µt)/2 + Const.,_

where we define

_m_ b _m_

_µt = Λ[−]t_ [1] _φ(s[i]t[, a][i]t[)][y]t[i][,]_ Λt = _φ(s[i]t[, a][i]t[)][φ][(][s][i]t[, a][i]t[)][⊤]_ [+][ λ][ ·][ I][.] (20)

_i=1_ _i=1_

X X

Then we obtain that _wt = w | Din ∼N_ (µt, Λ[−]t [1][)][. It then holds for all][ (][s][t][, a][t][)][ ∈S × A][ that]

Var _φ(s[i]t[, a]t[i][)][⊤]w[b]t_ = Var _Qt(s[i]t[, a]t[i][)]_ = φ(s[i]t[, a]t[i][)][⊤][Λ]t[−][1][φ][(][s]t[i][, a]t[i][)][,] (21)

b

which concludes the proof.     

In Lemma 1, we show that the standard deviation of the Q-posterior is equivalent to the LCBpenalty Var _Q(st, at)_ = φ(st, at)[⊤]Λ[−]t [1][φ][(][s][t][, a][t][)][ introduced in §A.1. Recall that our proposed]
bootstrapped uncertainty takes the form of
  

(st, at) Std _Q[k](st, at)_ _,_ (22)
_U_ _≈_

which is the standard deviation of the bootstrapped  Q-functions. Such bootstrapping serves as an
estimation of the posterior of Q-functions (Osband et al., 2016). Thus, our proposed uncertainty
quantification can be seen as an estimation of the LCB-penalty under the linear MDP assumptions.

In addition, under the tabular setting where the states and actions are finite, the LCB-penalty takes a
simpler form, which we show in the following lemma.
**Lemma 2. In tabular MDPs, the bootstrapped uncertainty U** (s, a) is approximately proportional to
_the reciprocal-count of (s, a), that is,_


_U_ (s, a) ≈ Γ[lcb](s, a)/βt =


1

(23)
_Ns,a + λ_ _[.]_


_Proof. In tabular MDPs, we consider the joint state-action space d = |S| × |A|. Then j-th state-_
action pair can be encoded as a one-hot vector as φ(s, a) ∈ R[d], where j ∈ [0, d _−_ 1], thus is a special
case of the linear MDP (Yang & Wang, 2019; Jin et al., 2020). Specifically, we define

0 0 ··· 0 ··· 0
. . .
.. .. [..]. ..

_φ(sj, aj) =_  1 _φ(sj, aj)φ(sj, aj)[⊤]_ =  0 1 0 (24)

. .

 ..  .. ... ...
 0  0 ··· 0 ··· 0
  _[∈]_ [R][d][,]   _[∈]_ [R][d][×][d][,]

where the value of φ(sj, aj) is 1 at the j-th entry and 0 elsewhere. Then the matrix Λj =
_m_
_i=0_ _[φ][(][s]j[i]_ _[, a][i]j[)][φ][(][s][i]j[, a][i]j[)][⊤]_ [+][ λ][ ·][ I][ is the sum of][ φ][(][s][j][, a][j][)][φ][(][s][j][, a][j][)][⊤] [over][ (][s][j][, a][j][)][ ∈D][in][, which]
takes the form of
P _n00+λ_ _n10+λ_ _······_ 00

. .

 .. ... .. 

Λj = 0 _···_ _nj_ +λ 0 _,_ (25)

 
 ... ... ... 
 0 _···_ _···_ _nd−1+λ_
 


-----

where the j-th diagonal element of Λj is the corresponding counts for state-action (sj, aj), i.e.,

_nj = Nsj_ _,aj_ _._

It thus holds that
1/2 1
_φ(sj, aj)[⊤]Λ[−]j_ [1][φ][(][s][j][, a][j][)] = (26)
_Nsj_ _,aj + λ,_
 

which concludes the proof.

p

A.3 REGULARIZATION WITH OOD SAMPLING

In this section, we discuss how OOD sampling plays the role of regularization in RL, which regularizes the extrapolation behavior of the estimated Q-functions on OOD samples.

Similar to §A.2, we consider the setup of LSVI-UCB (Jin et al., 2020) under linear MDPs. Specifically, we assume that the transition dynamics and reward function takes the form of

Pt(st+1 | st, at) = ⟨ψ(st+1), φ(st, at)⟩, _r(st, at) = θ[⊤]φ(st, at),_ _∀(st+1, at, st) ∈S × A × S(27),_
where the feature embedding φ : S ×A 7→ R[d] is known. We further assume that the reward function
_r :_ [0, 1] is bounded and the feature is bounded by _φ_ 2 1. Given the dataset in, LSVI
iteratively minimizes the least-square loss in Eq. (12). Recall that the explicit solution to Eq. (12) S ×A 7→ _∥_ _∥_ _≤_ _D_
takes the form of


_wt = Λ[−]t_ [1]


_φ(s[i]t[, a]t[i][)]_ _Vt+1(s[i]t+1[) +][ r][(][s]t[i][, a]t[i][)]_ _,_ Λt =
_i=1_

X   


_φ(s[i]t[, a]t[i][)][φ][(][s]t[i][, a]t[i][)][⊤]_ [+][ λ][ ·][ I][.] (28)
_i=1_

X


We remark that for the regression problem in Eq. (12), the L2-regularizer λ · ∥w∥2[2] [enforces a Gaus-]
sian prior under the notion of Bayesian regression. Such regularization ensures that the linear function approximation φ[⊤]wt[i] [extrapolates well outside the region covered by the dataset][ D][in][.]

However, as shown in §D, we observe that such L2-regularization is ineffective for offline DRL.
To this end, we propose OOD sampling in our proposed PBRL. To demonstrate the effectiveness of
OOD sampling as a regularizer, we consider the following least-squares loss with OOD sampling
and without the L2-regularizer,

_m_

_wt[i]_ [= min] _φ(s[i]t[, a]t[i][)][⊤][w][ −]_ _[r][(][s]t[i][, a]t[i][)][ −]_ _[V][t][+1][(][s]t[i]+1[)]_ 2 + _φ(s[ood], a[ood])[⊤]w_ _y_ 2.
_w∈R[d]_ Xi=1   (s[ood],a[ood]X,y)∈Dood  _−_ 

(29)

e

The explicit solution of Eq. (29) takes the form of


_m_

_wt[i]_ [=][ e]Λ[−]t [1] _φ(s[i]t[, a]t[i][)]_ _r(s[i]t[, a][i]t[) +][ V][t][+1][(][s]t[i]+1[)]_

 _i=1_
X  

e

where


_φ(s[ood], a[ood])y_ _,_ (30)
(s[ood],a[ood],y) ood 

X _∈D_


Λt = _φ(s[i]t[, a][i]t[)][φ][(][s][i]t[, a][i]t[)][⊤]_ [+] _φ(s[ood], a[ood])φ(s[ood], a[ood])[⊤]._ (31)

Xi=1 (s[ood],a[ood]X,y)∈Dood

e

Hence, if we further set y = 0 for all (s[ood], a[ood], y) ∈Dood, then (29) enforces a Gaussian prior with
the covariance matrix Λ[−]ood[1][, where we define]


_φ(s[ood], a[ood])φ(s[ood], a[ood])[⊤]._ (32)
(s[ood],a[ood]X,y)∈Dood


Λood =


Specifically, if we further enforce ood = (s[j], a[j], 0) _j_ [d] with φ(s[j], a[j]) = λ _e[j], where e[j]_ R[d]
_D_ _{_ _}_ _∈_ _·_ _∈_
is the unit vector with the j-th entry equals one, it further holds that Λood = λ · I and Eq. (29)
is equivalent to Eq. (12). In addition, under the tabular setting, by following the same proof as in
Lemma 2, having such OOD samples in the training is equivalent to setting the count in Eq. (26) to
be
_Nsj_ _,aj = Nsj_ _,aj + Ns[ood]j_ _,aj_ _[,]_


-----

where Nsj _,aj is the occurrence of (sj, aj) in the dataset and Ns[ood]j_ _,aj_ [is the occurrence of][ (][s][j][, a][j][)][ in]
the OOD dataset.

However, to enforce such a regularizer without affecting the estimation of value functions, we need
to set the target y of the OOD samples to zero. In practice, we find such a setup to be overly
pessimistic. Since the Q-network is smooth, such a strong regularizer enforces the Q-functions to
be zero for state-action pairs from both the offline data and OOD data, as show in Fig. 12 and Fig. 13
of §C. We remark that adopting a nonzero OOD target y does not hinder the effect of regularization
as it still imposes the same prior in the covariate matrix Λt. However, adopting nonzero OOD target
may introduce additional bias in the value function estimation and the corresponding uncertainty
quantification. To maintain a consistent and pessimistic estimation of value functions, one needs to

[e]
carefully design the nonzero OOD target y.

To this end, we recall the definition of a ξ-uncertainty quantifier in Definition 1 as follows.
**Definition 2 (ξ-Uncertainty Quantifier (Jin et al., 2021)). The set of penalization** Γt _t_ [T ] forms a
_{_ _}_ _∈_
_ξ-Uncertainty Quantifier if it holds with probability at least 1 −_ _ξ that_

_Vt+1(s, a)_ _Vt+1(s, a)_ Γt(s, a)
_|T_ _−T_ _| ≤_

_that estimatesfor all (s, a) ∈S × A T based on the data., where T[b]_ _is the Bellman operator and_ _T is the empirical Bellman operator_

[b]

We remark that here we slightly abuse the notation T of Bellman operator and write T V (s, a) =
E[r(s, a)+V (s[′]) | s, a]. Under the linear MDP setup, the empirical estimation _T Vt+1 is obtained via_
fitting the least-squares loss in Eq. (29). Thus, the empirical estimation _T Vt+1 takes the following_
explicit form, [b]
_Vt+1(st, at) = φ(st, at)[⊤]wt,_ [b]
_T_
where _wt is the solution to the least-squares problem defined in Eq. (30). We remark that such ξ-_
uncertainty quantifier plays an important role in the theoretical analysis of RL algorithms, both forb e
online RL and offline RL (Abbasi-Yadkori et al., 2011; Azar et al., 2017; Wang et al., 2020a; Jin
e
et al., 2020; 2021; Xie et al., 2021a;b). Our goal is therefore to design a proper OOD target y such
that we can obtain ξ-uncertainty quantifier based on the bootstrapped value functions. Our design is
motivated by the following theorem.
**Theorem 2.T Vt+1(s[ood], a Let[ood]) Λ, it then holds forood ⪰** _λ · I. For all the OOD datapoint βt = O_ _T ·_ _√d · log(T/ξ ()s[ood]that, a[ood], y) ∈Dood, if we set y =_

1/2
Γ[lcb]t [(][s][t][, a][t][) =][ β][t]  _φ(st, at)[⊤]Λ[−]t_ [1][φ][(][s][t][, a][t][)]

_forms a valid ξ-uncertainty quantifier._  

_Proof. Recall that we define the empirical Bellman operator_ _T as follows,_

_Vt+1(st, at) = φ(st, at)[⊤]wt,_
_T_ [b]

It suffices to upper bound the following difference between the empirical Bellman operator and
Bellman operator b e
_Vt+1(s, a)_ _Vt+1(s, a) = φ(s, a)[⊤](wt_ _wt)._
_T_ _−_ _T[b]_ _−_
Here we define wt as follows
e

_wt = θ +_ _Vt+1(st+1)ψ(st+1)dst+1,_ (33)
ZS

where θ and ψ are defined in Eq. (27). It then holds that


_Vt+1(s, a)_ _Vt+1(s, a) = φ(s, a)[⊤](wt_ _wt)_
_T_ _−_ _T[b]_ _−_ _m_

= φ(s, a)[⊤]wt _φe(s, a)[⊤]Λ[e][−]t_ [1] _φ(s[i]t[, a][i]t[)]_ _r(s[i]t[, a][i]t[) +][ V][ i]t+1[(][s][i]t+1[)]_
_−_ _i=1_

X   

_φ(s, a)[⊤]Λ[e]_ _[−]t_ [1] _φ(s[ood], a[ood])y._ (34)
_−_

(s[ood],a[ood]X,y)∈Dood


-----

where we plug the solution of _wt in Eq. (30). Meanwhile, by the definitions of_ Λt and wt in Eq. (31)
and Eq. (33), respectively, we have
e [e]

_φ(s, a)[⊤]wt = φ(s, a)[⊤]Λ[e]_ _[−]t_ [1]Λtwt

_m_

= φ(s, a)[⊤]Λ[e] _[−]t_ [1] _φ(s[i]te[, a][i]t[)][T][ V][t][+1][(][s][t][, a][t][) +]_ _φ(s[ood], a[ood])T Vt+1(s[ood], a[ood])_ _._

 _i=1_ (s[ood],a[ood],y) ood 
X X _∈D_

(35)
Plugging (35) into (34) yields


_Vt+1(s, a)_ _Vt+1(s, a) = (i) + (ii),_ (36)
_T_ _−_ _T[b]_


where we define


(i) = φ(s, a)[⊤]Λ[e][−]t [1] _φ(s[i]t[, a][i]t[)]_ _T Vt+1(s[i]t[, a][i]t[)][ −]_ _[r][(][s][i]t[, a][i]t[)][ −]_ _[V][ i]t+1[(][s][i]t+1[)]_ _,_

_i=1_

X   

(ii) = φ(s, a)[⊤]Λ[e][−]t [1] _φ(s[ood], a[ood])_ _Vt+1(s[ood], a[ood])_ _y_ _._

_T_ _−_
(s[ood],a[ood]X,y)∈Dood   

Following the standard analysis based on the concentration of self-normalized process (AbbasiYadkori et al., 2011; Azar et al., 2017; Wang et al., 2020a; Jin et al., 2020; 2021) and the fact that
Λood ⪰ _λ · I, it holds that_

1/2
_|(i)| ≤_ _βt ·_ _φ(st, at)[⊤]Λ[−]t_ [1][φ][(][s][t][, a][t][)] _,_ (37)

with probability at least 1 − _ξ, where βt = O_ _T ·_ _√d · log(T/ξ_ ) . Meanwhile, by setting y =

_Vt+1(s[ood], a[ood]), it holds that (ii) = 0. Thus, we obtain from (36) that_
_T_   

1/2
_|T Vt+1(s, a) −_ _T[b] Vt+1(s, a)| ≤_ _βt ·_ _φ(st, at)[⊤]Λ[−]t_ [1][φ][(][s][t][, a][t][)] (38)

for all (s, a) with probability at least 1 _ξ._  
_∈S × A_ _−_

Theorem 2 allows us to further characterize the optimality gap of the pessimistic value iteration. In
particular, the following corollary holds.
**Corollary 1 (Jin et al. (2021)). Under the same conditions as Theorem 2, it holds that**


_T_

Eπ∗ []Γ[lcb]t [(][s][t][, a][t][)][ |][ s][1]
_t=1_

X


_V_ (s1) _V_ _[π][1]_ (s1)

_[∗]_ _−_ _≤_


_Proof. See e.g., Jin et al. (2021) for a detailed proof._

We remark that the optimality gap in Corollary 1 is information-theoretically optimal under the linear
MDP setup with finite horizon (Jin et al., 2021). Intuitively, for an offline dataset with sufficiently
good coverage on the optimal trajectories such as the experience from experts, such gap is small.
For a dataset collected from random policy, such a gap can be large. Our experiments also support
such intuition empirically, where the score obtained by training with the expert dataset is higher than
that with the random dataset.

Theorem 2 shows that if we set y = _Vt+1(s[ood], a[ood]), then our estimation based on disagreement_
_T_
among ensembles is a valid ξ-uncertainty quantifier. However, such OOD target is impossible to
obtain in practice as it requires knowing the transition at the OOD datapoint (s[ood], a[ood]). In practice,
if TD error is sufficiently minimized, then Qt+1(s, a) should well estimate the target _Vt+1. Thus,_
_T_
in practice, we utilize
_y = Qt+1(s[ood], a[ood])_ Γt+1(s[ood], a[ood])
_−_
as the OOD target, where we introduce an additional penalization Γt+1(s[ood], a[ood]) to enforce the
pessimistic value estimation.

In addition, we remark that in theory, we require that the embeddings of the OOD sample are
isotropic, in the sense that the eigenvalues of the corresponding covariate matrix Λood are lower


-----

bounded. Such isotropic property can be guaranteed by randomly generating states and actions. In
practice, we find that randomly generating states is more expensive than randomly generating actions. Meanwhile, we observe that randomly generating actions alone are sufficient to guarantee
reasonable empirical performance since the generated OOD embeddings are sufficiently isotropic.
Thus, in our experiments, we randomly generate OOD actions according to our current policy and
sample OOD states from the in-distribution dataset.

B IMPLEMENTATION DETAIL

B.1 ALGORITHMIC DESCRIPTION

**Algorithm 1 PBRL algorithm**

1: Initialize: K bootstrapped Q-networks and target Q-networks with parameter θ and θ[−], policy
_π with parameter ϕ, and hyper-parameters βin, βood_

2: Initialize: total training steps H, current frame h = 0
3: while h < H do
4: Sample mini-batch transitions (s, a, r, s[′]) from the offline dataset in
_D_

5: _# Critic Training for offline data._

6: Calculate the bootstrapped uncertainty _θ−_ (s[′], a[′]) through the target-networks.
_U_

7: Calculate the Q-target in Eq. (4) and the resulting TD-loss |Q[k]θ [(][s, a][)][ −] _T[b]_ [in]Q[k](s, a)|[2].

8: _# Critic Training for OOD data_

9: Perform OOD sampling and obtains Nood OOD actions a[ood] for each s.

10: Calculate the bootstrapped uncertainty _θ(s, a[ood]) for OOD actions through Q-networks._
_U_

11: Calculate the Q-target in Eq. (5) and the pseudo TD-loss |Q[k]θ [(][s, a][ood][)][−]T[b] [ood]Q[k](s, a[ood])|[2].

12: Minimize |Q[k]θ [(][s, a][)][−]T[b] [in]Q[k](s, a)|[2] +|Q[k]θ [(][s, a][ood][)][−]T[b] [ood]Q[k](s, a[ood])|[2] to train θ by SGD.

13: _# Actor Training_

14: Improve πϕ by maximizing mink Q[k](s, aπ) − log πϕ(aπ|s) with entropy regularization.

15: Update the target Q-network via θ[−] _←_ (1 − _τ_ )θ[−] + τθ.

16: end while

B.2 HYPER-PARAMETERS

[Most hyper-parameters of PBRL follow the SAC implementations in https://github.com/](https://github.com/rail-berkeley/rlkit)
[rail-berkeley/rlkit. We use the hyper-parameter settings in Table 2 for all the Gym domain](https://github.com/rail-berkeley/rlkit)
tasks. We use different settings of βin and βood for the experiment for Adroit domain and fix the
other hyper-parameters the same as Table 2. See §E for the setup of Adroit. In addition, we use the
same settings for discount factor, target network smoothing factor, learning rate, and optimizers as
CQL (Kumar et al., 2020).

Table 2: Hyper-parameters of PBRL

Hyper-parameters Value Description

_K_ 10 The number of bootstrapped networks.
_Q-network_ FC(256,256,256) Fully Connected (FC) layers with ReLU activations.
_βin_ 0.01 The tuning parameter of in-distribution target _T_ [in].
_βood_ 5.0 0.2 The tuning parameter of OOD target [ood]. We perform
_→_ _T_
linearly decay within the first 50K steps, and perform expo-[b]
nentially decay in the remaining steps.

[b]
_τ_ 0.005 Target network smoothing coefficient.
_γ_ 0.99 Discount factor.
_lr of actor_ 1e-4 Policy learning rate.
_lr of critic_ 3e-4 Critic learning rate.
Optimizer Adam Optimizer.
_H_ 1M Total gradient steps.
_Nood_ 10 Number of OOD actions for each state.


-----

**Baselines.** We conduct experiments on D4RL with the latest ‘v2’ datasets. The dataset is released
[at http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2_](http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2_old/)
[old/.](http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2_old/) We now introduce the implementations of baselines. (i) The implementation of CQL
[(Kumar et al., 2020) is adopted from the official implementation at https://github.com/](https://github.com/aviralkumar2907/CQL)
[aviralkumar2907/CQL.](https://github.com/aviralkumar2907/CQL) In our experiment, we remove the BC warm-up stage since we
find CQL performs better without warm-up for ‘v2’ dataset. (ii) For BEAR (Kumar et al.,
2019), UWAC (Wu et al., 2021) and MOPO (Yu et al., 2020), we adopt their official implemen[tations at https://github.com/aviralkumar2907/BEAR, https://github.com/](https://github.com/aviralkumar2907/BEAR)
[apple/ml-uwac, and https://github.com/tianheyu927/mopo, respectively.](https://github.com/apple/ml-uwac) We
adopt their default hyper-parameters in training. (iii) Since the original paper of TD3-BC (Fujimoto
& Gu, 2021) reports the performance of Gym in ‘v2’ dataset in the appendix section, we directly cite
the reported scores in Table 1. The learning curve reported in Fig. 2 is trained by implementation
[released at https://github.com/sfujim/TD3_BC.](https://github.com/sfujim/TD3_BC)

**Computational cost comparison.** In the sequel, we compare the computational cost of PBRL
against CQL. We conduct such a comparison based on the Halfcheetah-medium-v2 task. We measure the number of parameters, GPU memory, and runtime per epoch (1K gradient steps) for both
PBRL and CQL in the training. We run experiments on a single A100 GPU. We summarize the result in Table 3. We observe that, similar to the other ensemble-based methods such as Bootstrapped
DQN (Osband et al., 2016), IDS (Nikolov et al., 2019), and Sunrize (Lee et al., 2021a), our method
requires extra computation to handle the ensemble of Q-networks. In addition, we remark that a
large proportion of computation for CQL is due to the costs of logsumexp over multiple sampled
actions (Fujimoto & Gu, 2021), which we do not require.

Table 3: Comparison of computational costs.

Runtime (s/epoch) GPU memory Number of parameters

CQL 30.3 1.1G 0.42M
PBRL 52.1 1.7G 1.52M

B.3 REMARKS ON FORMULATIONS OF PESSIMISM IN ACTOR AND CRITIC

We use different formulations to enforce pessimism in actor and critic. In the critic training, we
use the penalized Q-function as in Eq. (4) and Eq. (5). While in the actor training, we use the
minimum of ensemble Q-function as in Eq. (7). According to the analysis in EDAC [6], using the
minimum of ensemble Q-function minj=1,...,K Qj as the target is approximately equivalent to using
_Qdifferent factors (i.e.,¯ −_ _β0 · Std(Qj) with a fixed βin and β βood0 as the target. In contrast, in the critic training of PBRL, we tune) for the in-distribution target and the OOD target, which yields_
better performance for the critic estimation.

In the actor training, since we already have pessimistic Q-functions learned by the critic, it is not
necessary to enforce large penalties in the actor. To see such a fact, we refer to the ablation study
in Fig. 9, where utilizing the mean as the target achieves reasonable performances. We remark that
taking the minimum as the target avoids possible large values at certain state-action pairs, which
may arise due to the numerical computation in fitting neural networks. As suggested by our ablation study, taking the minimum among the ensemble of Q-functions as the target achieves the best
performance. Thus, we use the minimum among the ensemble of Q-functions as the target in PBRL.


-----

C ABLATION STUDY

In this section, we present the ablation study of PBRL. In the training, we perform online interactions to evaluate the performance for every 1K gradient steps. Since we run each method for 1M
gradient steps totally, each method is evaluated 1000 times in training. The experiments follow such
evaluation criteria, and each curve is drawn by 1000 evaluated scores through online interaction.

**Number of bootstrapped-Q.** We conduct experiments with different numbers of bootstrapped
_Q-networks in PBRL, and present the performance comparison in Fig. 4. We observe that the_
performance of PBRL is improved with the increase of the bootstrapped networks. We remark that
since the training of offline RL is more challenging than online RL, it is better to use sufficient
bootstrapped Q-functions to obtain a reasonable estimation of the non-parametric Q-posterior. We
observe from Fig. 4 that using K = 6, 8, and 10 yields similar final scores, and a larger K leads to
more stable performance in the training process. We adopt K = 10 for our implementation.

_K=2_ _K=4_ _K=6_ _K=8_ _K=10_

walker2d-medium-v2 hopper-medium-replay-v2

100

80

80

60

60

40

d4rl score

40

20

20

0

0

0 200 400 600 800 1000 0 200 400 600 800 1000

Gradient Steps (thousands)


Figure 4: The Ablation on the number of bootstrapped Q-functions.

**Uncertainty of in-distribution target.** We compare different kinds of uncertainty penalization in
_T_ [in] for in-distribution data. (i) Penalizing the immediate reward only. (ii) Penalizing the next-Q
value only, which is adopted in PBRL. (iii) Penalizing both the immediate reward and next-Q value.
We present the comparison in Fig. 5. We observe that the target-b _Q penalization performs the best,_
and adopt such penalization in the proposed PBRL algorithm.

Penalty only for reward Penalty only for next-Q Penalty for both reward and next-Q

walker2d-medium-v2 hopper-medium-replay-v2

100

80

80

60

60

40

d4rl score

40

20

20

0 0

0 200 400 600 800 1000 0 200 400 600 800 1000

Gradient Steps (thousands)


Figure 5: The Ablation on penalty strategy for the in-distribution data.

**Tuning parameterthe sensitivity of PBRL to the tuning parameter βin.** We conduct experiments with βin for the in-distribution target. We present the βin ∈{0.1, 0.01, 0.001, 0.0001} to study


-----

comparison in Fig. 6. We observe that in the ‘medium’ dataset generated by a single policy, the
performance of PBRL is insensitive to βin. One possible reason is that since the ‘medium’ dataset is
generated by a single policy, the offline dataset tends to concentrate around a few trajectories and has
low uncertainty. Thus, the magnitude of βin has a limited effect on the penalty. Nevertheless, in the
‘medium-replay’ dataset, since the data is generated by various policies, the uncertainty of offline
data is larger than that of the ‘medium’ dataset (as shown in Fig. 3). Correspondingly, the performance of PBRL is affected by the magnitude of βin. Our experiment shows that PBRL performs the
best for βin ∈ [0.0001, 0.01]. We adopt βin = 0.01 for our implementation.

_βin=0.0001_ _βin=0.001_ _βin=0.01_ _βin=0.1_

walker2d-medium-v2 hopper-medium-replay-v2

100

80

80

60

60

40

d4rl score

40

20

20

0

0 200 400 600 800 1000 0 200 400 600 800 1000

Gradient Steps (thousands)


Figure 6: The Ablation on the tuning parameter β[in] in the in-distribution target.

**Tuning parameter βood.** We use a large βood initially to enforce strong OOD regularization in
the beginning of training, and then decrease βood linearly while the training evolves. We conduct
experiments with constant settings of βood 0.01, 0.1, 1.0 . We observe that in the ‘medium’
dataset, a large βood = 1.0 performs the best since the samples are generated by a fixed policy with a ∈{ _}_
relatively concentrated in-distribution dataset. Thus, the OOD samples tend to have high uncertainty
and are less trustworthy. In contrast, in the ‘medium-replay’ dataset, a small βood 0.1, 0.01
performs reasonably well since the mixed dataset has larger coverage of the state-action space and ∈{ _}_
the uncertainty of OOD data is smaller than that of the ‘medium’ dataset. Thus, adopting a smaller
_βood for the ‘medium-replay’ dataset allows the agent to exploit the OOD actions and gain better_
performance. To match all possible situations, we propose the decaying strategy. Empirically, we
find such a decaying strategy performs well in both the ‘medium’ and ‘medium-replay’ datasets.

Constant-0.01 Constant-0.1 Constant-1.0 Decay (ours)

walker2d-medium-v2 hopper-medium-replay-v2

100

80

80

60

60

40

d4rl score 40

20

20

0

0

0 200 400 600 800 1000 0 200 400 600 800 1000

Gradient Steps (thousands)


Figure 7: The Ablation on the tuning parameter β[ood] in the OOD target.

We also record the Q-value for (s, a) ∼Din in the training process. As shown in Fig. 8, since both
the in-distribution actions and OOD actions are represented by the same Q-network, a large con

-----

stant βood makes the Q-value for in-distribution action overly pessimistic and leads to sub-optimal
performance. It is desirable to use the decaying strategy for βood in practice.

Constant-0.01 Constant-0.1 Constant-1.0 Decay (ours)

walker2d-medium-v2 hopper-medium-replay-v2

350

300 200

250

150

200

Q Offline 150 100

100

50

50

0 0
0 200 400 600 800 1000 0 200 400 600 800 1000

Gradient Steps (thousands)


Figure 8: With different settings of β[ood], we show the Q-value for state-action pairs sampled from
_Din in the training process._

**Actor training.** We evaluate different actor training schemes in Eq. (7), i.e., the actor follows the
gradients of ‘min’ (in PBRL), ‘mean’ or ‘max’ value among K Q-functions. The result in Fig. 9
shows training the actor by maximizing the ‘min’ among ensemble-Q performs the best. In the
‘medium-replay’ dataset, since the uncertainty estimation is difficult in mixed data, taking ‘mean’
in the actor can be unstable in training. Taking ‘max’ in the actor performs worse in both tasks due
to overestimation of Q-functions.

Actor: Max Actor: Mean Actor: Min

walker2d-medium-v2 hopper-medium-replay-v2

80 100

80

60

60

40

d4rl score

40

20

20

0

0 200 400 600 800 1000 0 200 400 600 800 1000

Gradient Steps (thousands)


Figure 9: The Ablation on action selection scheme of the actor.

**Number of OOD samples.** The pessimism in critic training is implemented by sampling OOD
actions and then performing uncertainty penalization, as shown in Eq. (6). In each training step,
we perform OOD sampling and sample Nood actions from the learned policy. We perform an ablation study with Nood 0, 2, 5, 10 . According to Fig 10, the performance is poor without OOD
sampling (i.e., Nood = 0) ∈{ . We find the performance becomes better with a small number of OOD}
actions (i.e., Nood = 2). Also, PBRL is robust to different settings of Nood.

**OOD Target.** In §A.3, we show that setting the learning target of OOD samples to zero enforces a
Gaussian prior to Q-function under the linear MDP setting. However, in DRL, we find such a setup
leads to overly pessimistic value function and performs poorly in practice, as shown in Fig. 11.
Specifically, we observe that such a strong regularizer enforces the Q-functions to be close to zero
for both in-distribution and OOD state-action pairs, as shown in Fig. 12 and Fig. 13. In contrast,


-----

N-OOD: 0 N-OOD: 2 N-OOD: 5 N-OOD: 10

walker2d-medium-v2 hopper-medium-replay-v2

100

80

80

60

60

40

d4rl score 40

20

20

0 0

0 200 400 600 800 1000 0 200 400 600 800 1000

Gradient Steps (thousands)


Figure 10: The Ablation on different number of OOD actions. the performance becomes better even
with very small amout of OOD actions.

our proposed PBRL target performs well and does not yield large extrapolation errors, as shown in
Fig. 12 and Fig. 13.

Zero target for OOD PBRL

walker2d-medium-v2 hopper-medium-replay-v2

100

80

80

60

60

40

d4rl score

40

20

20

0

0 200 400 600 800 1000 0 200 400 600 800 1000

Gradient Steps (thousands)


Figure 11: The Ablation on OOD target with y[ood] = 0 (normalized scores).

Zero target for OOD PBRL

walker2d-medium-v2 hopper-medium-replay-v2

300 200

175

250

150

200

125

150 100

Q Offline

75

100

50

50

25

0 200 400 600 800 1000 0 200 400 600 800 1000

Gradient Steps (thousands)


Figure 12: The Ablation on OOD target with y[ood] = 0. Q-offline is the Q-value for (s, a) pairs
sampled from the offline dataset, where a follows the behavior policy.


-----

Zero target for OOD PBRL

walker2d-medium-v2 hopper-medium-replay-v2

300 200

175

250

150

200

125

150 100

Q CurrPolicy

75

100

50

50

25

0
0 200 400 600 800 1000 0 200 400 600 800 1000

Gradient Steps (thousands)


Figure 13: The Ablation on OOD target with y[ood] = 0. Q-CurrPolicy is the Q-value for (s, aπ)
pairs, where aπ _π(a_ _s) follows the training policy π._
_∼_ _|_


-----

D REGULARIZATION FOR PBRL

In this section, we compare different regularization methods that are popular in other Deep Learning
(DL) literature to solve the extrapolation error in offline RL. Specifically, we compare the following
regularization methods.

-  None. We remove the OOD-sampling regularizer in PBRL and train bootstrapped Q-functions
solely based on the offline dataset through _T_ [in].

-  L2-regularizer. We remove the OOD-sampling regularizer and use L2 normalization instead.
As we discussed in §3.3, in linear MDPs, LSVI utilizes[b] _L2 regularization to control the ex-_
trapolation behavior of Q-functions in OOD samples. In DRL, we add the L2-norm of weights
in the Q-network in loss functions to conduct L2-regularization. We attempt two scale factors
_{1e −_ 2, 1e − 4} in our experiments.

-  Spectral Normalization (SN). We remove the OOD-sampling regularizer and use SN instead.
SN constrains the Lipschitz constant of layers, which measures the smoothness of the neural
network. Recent research (Gogianu et al., 2021) shows that SN helps RL training when applied
to specific layers of the Q-network. In our experiment, we follow Gogianu et al. (2021) and
consider two cases, namely, applying SN in the output layer (denoted by SN[-1]) and applying
SN in both the output layer and the one before it (denoted by SN[-1,-2]), respectively.



-  Pessimistic Initialization (PI). Optimistic initialization is simple and efficient for RL exploration, which initializes the Q-function for all actions with a high value. In online RL, such
initialization encourages the agent to explore all actions in the interaction. Motivated by this
method, we attempt a pessimistic initialization to regulate the OOD behavior of the Q-network.
In our implementation, we draw the initial value of weights and a bias of the Q-networks from
the uniform distribution Unif(a, b). We try two settings in our experiment, namely, (i) PI-small
that sets (a, b) to (−0.2, 0), and (ii) PI-large that sets (a, b) to (−1.0, 0). In both settings, we
remove the OOD sampling and use PI instead.

We illustrate (i) the normalized performance in the training process, (ii) the Q-value along the trajectory of the training policy, and (iii) the uncertainty quantification along the trajectory of the training
policy. We present the results in Fig. 14, Fig. 15, and Fig. 16, respectively. In the sequel, we discuss
the empirical results.

-  We observe that OOD sampling is the only regularization method with reasonable performance.
Though L2-regularization and SN yield reasonable performance in supervised learning, they
do not perform well in offline RL.

-  In the ‘medium-replay’ dataset, we observe that PI and SN can gain some score in the early
stage of training. Nevertheless, the performance drops quickly along with the training process.
We conjecture that both PI and SN have the potential to be effective with additional parameter
tuning and algorithm design.

In conclusion, previous regularization methods for DL and RL are not sufficient to handle the distribution shift issue in offline RL. Combining such regularization techniques with policy constraints
and conservatism methods may lead to improved empirical performance, which we leave for future
research.


-----

PI-large PI-small SN[-1,-2] SN[-1] _L2(1e-4)_ _L2(1e-2)_ None PBRL

walker2d-medium-v2 hopper-medium-replay-v2

100

80

80

60

60

40

d4rl score 40

20 20

0 0

0 200 400 600 800 1000 0 200 400 600 800 1000

Gradient Steps (thousands)


Figure 14: Comparision of different regularizers (normalized score)

PI-large PI-small SN[-1,-2] SN[-1] _L2(1e-4)_ _L2(1e-2)_ None PBRL

walker2d-medium-v2 hopper-medium-replay-v2

5000 400

350

4000

300

3000 250

200

2000

150

1000 100

Q value of the Current Policy 50

0 0

0 200 400 600 800 1000 0 200 400 600 800 1000

Gradient Steps (thousands)


Figure 15: Comparision of different regularizers (Q-value along trajectories of the training policy)


PI-large PI-small SN[-1,-2] SN[-1] _L2(1e-4)_ _L2(1e-2)_ None PBRL

walker2d-medium-v2 hopper-medium-replay-v2

5000 25

4000 20

3000 15

2000 10

1000 5

Uncertainty of the Current Policy

0 0

0 200 400 600 800 1000 0 200 400 600 800 1000

Gradient Steps (thousands)


Figure 16: Comparision of different regularizers (Uncertainty along trajectories of the training
_policy)_


-----

EXPERIMENTS IN ADROIT DOMAIN


In Adroit, the agent controls a 24-DoF robotic hand to hammer a nail, open a door, twirl a pen,
and move a ball, as shown in Fig. 17. The Adroit domain includes three dataset types, namely,
demonstration data from a human (‘human’), expert data from an RL policy (‘expert’), and fifty-fifty
mixed data from human demonstrations and an imitation policy (‘cloned’). The adroit tasks are more
challenging than the Gym domain in task complexity. In addition, the use of human demonstration
in the dataset also makes the task more challenging. We present the normalized scores in Table 4.
We set βin = 0.0001 and βood = 0.01. The other hyper-parameters in Adroit follows Table 2.

In Table 4, the scores of BC, BEAR, and CQL are adopted from the D4RL benchmark (Fu et al.,
2020). We do not include the scores of MOPO (Yu et al., 2020) as it is not reported and we cannot
find suitable hyper-parameters to make it work in the Adroit domain. We also attempt TD3-BC
(Fujimoto et al., 2018) with different BC weights and fail in getting reasonable score for most of
the tasks. In addition, since UWAC (Wu et al., 2021) has a different evaluation process, we re-train
UWAC with the official implementation and report the scores in Table 4.

In Table 4, we add the average score without ‘expert’ dataset since the scores of ‘expert’ dataset
dominate that of the rest of the datasets. We find CQL (Kumar et al., 2020) and BC have the best
average performance among all baselines. Meanwhile, we observe that PBRL slightly outperforms
CQL and BC. We remark that the human demonstrations stored in ‘human’ and ‘cloned’ are inherently different from machine-generated ‘expert’ data since (i) the human trajectories do not follow
the Markov property, (ii) human decision may be affected by unobserved states such as prior knowledge, distractors, and the action history, and (iii) different people demonstrate differently as their
solution policies. Such characteristic makes the ‘human’ and ‘cloned’ dataset challenging for offline RL algorithms. In addition, we remark that the recent study in robot learning from human
demonstration also encounter such challenges (Mandlekar et al., 2021).

Figure 17: Illustration of tasks in Adroit domain.


BC BEAR UWAC CQL MOPO TD3-BC **PBRL**

Pen 34.4 -1.0 10.1 ±3.2 37.5 -  0.0 35.4 ±3.3

Hammer 1.5 0.3 1.2 ±0.7 4.4 -  0.0 0.4 ±0.3

Door 0.5 -0.3 0.4 ±0.2 9.9 -  0.0 0.1 ±0.0

Relocate 0.0 -0.3 0.0 ±0.0 0.2 -  0.0 0.0 ±0.0

Pen 56.9 26.5 23.0 ±6.9 39.2 -  0.0 74.9 ±9.8

Hammer 0.8 0.3 0.4 ±0.0 2.1 -  0.0 0.8 ±0.5

Door -0.1 -0.1 0.0 ±0.0 0.4 -  0.0 4.6 ±4.8

Relocate -0.1 -0.3 -0.3 ±0.0 -0.1 -  0.0 -0.1 ±0.0

Pen 85.1 105.9 98.2 ±9.1 107.0 -  0.3 137.7 ±3.4

Hammer 125.6 127.3 107.7 ±21.7 86.7 -  0.0 127.5 ±0.2

Door 34.9 103.4 104.7 ±0.4 101.5 -  0.0 95.7 ±12.2

Relocate 101.3 98.6 105.5 ±3.2 95.0 -  0.0 84.5 ±12.2


Average 36.73 38.41 37.57 ±3.8 40.31 -  0.02 46.79 ±3.9

Average w/o expert 11.74 3.21 4.35 ±1.4 11.70 -  0.02 14.52 ±2.3

Table 4: Average normalized score over 3 seeds in Adroit domain. The highest performing scores
are highlighted. Among all methods, PBRL and CQL outperform the best of the baselines.


-----

F RELIABLE EVALUATION FOR STATISTICAL UNCERTAINTY

Recent research (Agarwal et al., 2021) proposes reliable evaluation principles to address the statistical uncertainty in RL. Since the ordinary aggregate measures like mean can be easily dominated by
a few outlier scores, Agarwal et al. (2021) presents several efficient and robust alternatives that are
not unduly affected by outliers and have small uncertainty even with a handful of runs. In this paper,
we adopt these evaluation methods for each method in Gym domain with Mtask ∗ _Nseed runs._

-  Stratified Bootstrap Confidence Intervals. The Confidence Intervals (CIs) for a finite-sample
score estimates the plausible values for the true score. Bootstrap CIs with stratified sampling
can be applied to small sample sizes and is better justified than sample standard deviations.

-  Performance Profiles. Performance profiles reveal performance variability through score distributions. A score distribution shows the fraction of runs above a certain score and is given by
_Fˆ(τ_ ) = F[ˆ](τ ; x1:M,1:N ) = _M1_ _Mm=1_ _N1_ _Nn=1_ [1][[][x][m,n][ ≥] _[τ]_ []][.]

-  Aggregate Metrics. Based on bootstrap CIs, we can extract aggregate metrics from score distri
P P

butions, including median, mean, interquartile mean (IQM), and optimality gap. IQM discards
the bottom and top 25% of the runs and calculates the mean score of the remaining 50% runs.
Optimality gap calculates the amount of runs that fail to meet a minimum score of η = 50.0.


The result comparisons are give in Fig. 18 and Fig. 19. Specifically, Fig. 18 shows aggregate metrics
based on 95% bootstrap CIs, and Fig. 19 shows performance profiles based on score distribution.
For both evaluations, our PBRL and PBRL-prior outperforms other methods with small variability.






Median IQM Mean Optimality Gap

PBRL-prior

PBRL

TD3-BC

CQL

MOPO
UWAC

BEAR

40 60 80 40 60 80 45 60 75 10 15 20


Normalized Score

Figure 18: Aggregate metrics on D4RL with 95% CIs based on 15 tasks and 5 seeds for each task.
Higher mean, median and IQM scores, and lower optimality gap are better. The CIs are estimated
using the percentile bootstrap with stratified sampling. Our methods perform better than baselines.

BEAR UWAC MOPO CQL TD3-BC PBRL PBRL-prior

1.00 1.00

_τ_ _τ_
>

0.75 0.75

0.50 0.50

0.25 0.25

Fraction of runs with score Fraction of tasks with score >

0.00 0.00

0 20 40 60 80 100 120 0 20 40 60 80 100 120

Normalized Score (τ) Normalized Score (τ)


Figure 19: Performance profiles on D4RL based on score distributions (left), and average score
distributions (right). Shaded regions show pointwise 95% confidence bands based on percentile
bootstrap with stratified sampling. The τ value where the profiles intersect y = 0.5 shows the
median, and the area under the performance profile corresponds to the mean.


-----

