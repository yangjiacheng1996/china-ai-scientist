# HOW TO TRAIN RNNS ON CHAOTIC DATA?

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Recurrent neural networks (RNNs) are wide-spread machine learning tools for
modeling sequential and time series data. They are notoriously hard to train because their loss gradients backpropagated in time tend to saturate or diverge during
training. This is known as the exploding and vanishing gradient problem. Previous solutions to this issue either built on rather complicated, purpose-engineered
architectures with gated memory buffers, or - more recently - imposed constraints
that ensure convergence to a fixed point or restrict (the eigenspectrum of) the recurrence matrix. Such constraints, however, convey severe limitations on the expressivity of the RNN. Essential intrinsic dynamics such as multistability or chaos
are disabled. This is inherently at disaccord with the chaotic nature of many, if
not most, time series encountered in nature and society. Here we offer a comprehensive theoretical treatment of this problem by relating the loss gradients during
RNN training to the Lyapunov spectrum of RNN-generated orbits. We mathematically prove that RNNs producing stable equilibrium or cyclic behavior have
bounded gradients, whereas the gradients of RNNs with chaotic dynamics always
diverge. Based on these analyses and insights, we adapt the old idea of teacher
forcing to yield an effective yet simple training technique for chaotic data, and
offer guidance on how to choose relevant hyperparameters according to the Lyapunov spectrum.

1 INTRODUCTION

Recurrent neural networks (RNNs) are widely used across various fields in engineering and science
for learning sequential tasks or modeling and predicting time series (Lipton et al., 2015). Yet, they
struggle when long-term temporal dependencies, very slow, or hugely varying time scales are involved (Hochreiter, 1991; Bengio et al., 1994; Schmidt et al., 2021; Li et al., 2018; Rusch & Mishra,
2021a). Time series or sequential data with such properties are, however, very common in fields
like climate physics (Thomson, 1990), neuroscience (Fusi et al., 2007; Russo & Durstewitz, 2017),
ecology (Turchin & Taylor, 1992), or language processing (Cho et al., 2014b). Training RNNs on
such data is hard because the loss gradients backpropagated in time easily saturate or diverge in this
process. This is commonly referred to as the exploding and vanishing gradient problem (EVGP)
(Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013).

One solution to the EVGP is based on specifically designed RNN architectures with gating mechanisms, such as long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) or gated recurrent units (GRU) (Cho et al., 2014a). These architectures allow states at earlier time steps to more
easily influence activity much later through a kind of protected memory buffer, thus alleviating the
EVGP by structural design. In practice, such models need to be backed up by further techniques
like gradient clipping to keep the gradients in check (Pascanu et al., 2013). The relatively complex
architectural design of these networks impedes their mathematical analysis and requires reverse engineering after training (Maheswaranathan et al., 2019; Monfared & Durstewitz, 2020a;b; Schmidt
et al., 2021). Partly to forego these complications, a variety of other solutions has been proposed recently, imposing restrictions on the recurrence matrix to bound the gradients (Arjovsky et al., 2016;
Chang et al., 2019), or enforcing global stability by design or regularization (Erichson et al., 2021;
Kolter & Manek, 2019). Often these procedures dramatically curtail the expressivity of the RNN
(Kerg et al., 2019; Orhan & Pitkow, 2020; Schmidt et al., 2021); in particular, they rule out chaotic
dynamics (see further discussion below).


-----

This is at odds with the plethora of chaotic phenomena in nature, engineering, and society. Chaotic
dynamics are commonplace, almost default in any complex physical or biological system. This includes scientific areas as diverse as neuroscience (Durstewitz & Gabriel, 2007; van Vreeswijk &
Sompolinsky, 1996), physiology (Kesmia et al., 2020), geophysics (Sivakumar, 2004), climate systems (Tziperman et al., 1997), astrophysics (Laskar & Robutel, 1993), ecology (Duarte et al., 2010),
chemical reactions (Field & Gy¨orgyi, 1993), cell (Olsen & Degn, 1977) or population (May, 1987)
biology. Chaotic phenomena are also crucial for the understanding of societal and epidemiological processes, such as the spread of diseases (Mangiarotti et al., 2020), or in economics (Faggini,
2014). They are further relevant in purely technical contexts such as electrical engineering (Tchitnga et al., 2019; Kamdjeu Kengne et al., 2021) or laser optics (Kantz et al., 1993). They have
even been suggested to play an up to now largely neglected, but potentially very significant role
in speech recognition (Sabanal & Nakagawa, 1996) and natural language processing (Inoue et al.,
2021). Hence, in almost any practical setting, chaotic phenomena abound. They cannot, in general,
be ignored when devising RNN training algorithms.

Here we offer a comprehensive theoretical treatment of the relation between RNN dynamics and the
behavior of the loss gradients during training. We find a close connection between an RNN’s loss
gradients and the largest Lyapunov exponent of its freely generated orbits. We mathematically prove
that RNNs producing stable fixed point or cyclic behavior have bounded gradients. Crucially, however, the loss gradients of RNNs producing chaotic dynamics always diverge. Hence, the chaotic
nature of many time series data induces a principle problem, and, despite significant efforts in the
past to solve the EVGP, training RNNs on such data remains an open issue. We illustrate the implications of our theory for RNN training on several simulated and empirical chaotic time series, and
adapt the old idea of sparsely forced BPTT as a simple yet effective remedy that enables to learn the
underlying dynamics despite exploding gradients.

2 RELATED WORKS

_Exploding and vanishing gradients. While ‘classical’ remedies of the EVGP (Hochreiter, 1991; Ben-_
gio et al., 1994; Pascanu et al., 2013) rest on purpose-tailored architectures with gating mechanisms,
which safeguard information flow across longer temporal distances (Hochreiter & Schmidhuber,
1997; Cho et al., 2014a), the focus has recently shifted to simpler RNNs that address the EVGP by
restricting the recurrence matrix to be orthogonal (Henaff et al., 2016; Helfrich et al., 2018; Jing
et al., 2019), unitary (Arjovsky et al., 2016), or antisymmetric (Chang et al., 2019), or by ensuring globally stable fixed point solutions (Kag et al., 2020; Kag & Saligrama, 2021a), for example
through co-trained Lyapunov functions (Kolter & Manek, 2019). However, all these approaches
impose strong limitations on the dynamical repertoire of the RNN, enforcing global convergence
to fixed points or simple cycles.[1] In doing so, they drastically reduce the expressiveness of these
models (Kerg et al., 2019; Orhan & Pitkow, 2020). To address this problem, Erichson et al. (2021)
somewhat relaxed the constraints on the recurrence matrix by introducing a skew-symmetric decomposition combined with a Lipschitz condition on the activation function. Another recent approach
discretizes oscillator ODEs to arrive at a stable system of coupled (Rusch & Mishra, 2021a) or
independent (Rusch & Mishra, 2021b) oscillators which increase the RNN’s expressiveness while
bounding its gradients. By design (and as acknowledged by the authors), neither of these architectures is capable of producing chaotic dynamics, however, as the underlying ODEs do not allow for
exponential divergence of close-by trajectories (a prerequisite for chaos). Given these often principle limitations of parametrically or dynamically strongly constrained models, a fruitful direction
may be to modify the training process itself, e.g. through modified or auxiliary loss functions (Trinh
et al., 2018; Schmidt et al., 2021), or special procedures for parameter updating (Kag & Saligrama,
2021b) or loss truncation (Williams & Zipser, 1989; Menick et al., 2021). Our empirical evaluation will follow up on such ideas, but also highlight that simple loss truncation, windowing, or
architectural solutions like LSTMs are not sufficient.

_Learning dynamical systems. Surprisingly disconnected from the work on the EVGP and learning_
long-term dependencies, a huge and long-standing literature deals with training RNNs on nonlinear
dynamical systems (DS) (Pearlmutter, 1990; Trischler & D’Eleuterio, 2016; Vlachas et al., 2020),
including chaotic systems like the famous Lorenz equations (Lorenz, 1963) or chaotic turbulence in
fluid dynamics (Li et al., 2021). Teacher forcing (TF; Williams & Zipser (1989), Pearlmutter (1990),

1We make this point more formal in Appx. A.1.6.


-----

Doya (1992), see also Goodfellow et al. (2016)) is one of the earliest techniques introduced to keep
RNN trajectories on track while training. The idea behind TF is to simply replace RNN states by
observations when available, thereby also effectively cutting off the gradients. TF essentially derives
from ideas in dynamical control theory, and adaptive schemes that increasingly hand over control to
the RNN throughout training have been devised (Abarbanel, 2013; Abarbanel et al., 2018; Bengio
et al., 2015). A related technique from the control theory literature is “multiple shooting” (Voss et al.,
2004): Here the whole observed time series is chopped into chunks, and for each chunk of trajectory
a new initial condition is estimated. Explicit constraints ensure continuity between the separate
trajectory bits during optimization. State space models and the Expectation-Maximization algorithm
became popular particularly in the 90es for uncovering the latent dynamics underlying a set of
time series observations (Ghahramani & Roweis, 1999), and remain an important tool until today
(Durstewitz, 2017; Koppe et al., 2019). Most recently, approaches based on variational inference
and the reparameterization trick (Kingma & Welling, 2014), like sequential variational autoencoders
(SVAE), gained in popularity for DS approximation (Hernandez et al., 2020). “Deterministic” RNNs
(i.e., with latent states not treated as random variables), like conventional LSTMs (Vlachas et al.,
2018), remain top choices for DS reconstruction, however.

Although connections between DS ideas and loss gradients have been drawn early on (Bengio et al.,
1994), so far only particular scenarios (like fixed point attractors) have been considered. Closest
to our work is recent work by Schmidt et al. (2021), where non-divergence of loss gradients is
established when RNNs converge to fixed points or cycles. However, this was done only for the
particular class of piecewise-linear RNNs (PLRNNs), more restrictive conditions for cycles were
imposed than assumed here, and - most importantly - the chaotic case on which we focus here was
not considered. Another recent study (Engelken et al., 2020) also points out the general connections
between Lyapunov exponents and loss gradients that we develop in sect. 3.1, but does not provide
any in-depth theoretical treatment, proofs, or empirical evaluation, as we do here. Thus, a systematic
theoretical framework that relates RNN dynamics more generally, and across a range of different
RNN architectures, to the behavior of its training gradients, is still lacking so far.

3 THEORETICAL ANALYSIS: RELATION BETWEEN RNN DYNAMICS AND
LOSS GRADIENTS

In our analysis, we will cover all major types of system dynamics (fixed points, cycles, chaos, and
quasi-periodicity), and mathematically investigate their implications for the loss gradients. We will
do this for all major classes of RNNs, including standard RNNs with largely arbitrary activation
function, LSTMs, GRUs, and PLRNNs. The next section will first develop and illustrate the basic
intuition behind the relations between RNN dynamics and loss gradients.

3.1 PRELIMINARIES: RNN DYNAMICS AND LOSS GRADIENTS

Formally, all popular RNN architectures, including LSTMs, GRUs, or PLRNNs, are discrete time
DS, defined by a (first-order-Markovian) recursive prescription for the temporal evolution of the
latent states zt ∈ R[M] of the form
**_zt = Fθ(zt_** 1, st), (1)
_−_

wherewe have st F ∈θ(Rzt[N] 1is the input at time, st) = f **_W zt_** 1 t + and Bs θt are RNN parameters. For instance, for standard RNNs + h, where f is an element-wise activation function
_−_ _−_
like tanh or a rectified linear unit (ReLU).
  

we can recursively rewrite eq. (1) asAssuming we start at some initial value z1 ∈ R[M], and given a sequence of external inputs S = {st},

**_zT = Fθ(Fθ(Fθ(...Fθ(z1, s2)...))) =: Fθ[T][ −][1](z1, s2)._** (2)
In DS theory, we characterize the long-term behavior of such sequences by its spectrum of Lyapunov
exponents. The Lyapunov exponents estimate the exponential growth rates in different local directions of the system’s state space, and the largest Lyapunov exponent gives the dominant exponential
behavior. Let us denote the system’s Jacobian at time t by

_∂zt_

**_Jt :=_** _[∂F][θ][(][z][t][−][1][,][ s][t][)]_ = _._ (3)

_∂zt−1_ _∂zt−1_


-----

For instance, for standard RNNs we would have Jt = W diag _f_ _[′][ ]W zt−1 + Bst + h_, where
_diagThen, the maximal Lyapunov exponent along an RNN trajectory denotes a diagonal matrix for which the i-th diagonal entry is the derivative of  {z1, z2, · · ·, zT, · · · } f w.r.t. is defined as zi,t−1._

1 _T −2_
_λmax :=_ lim **_JT_** _r_ (4)
_T_ _T_ [log] _−_ _[,]_
_→∞_ _r=0_

Y

where ∥· ∥ denotes the spectral norm (or any subordinate norm) of a matrix. If λmax < 0 nearby
trajectories will ultimately converge to a fixed point or cycle, while for λmax > 0 (a necessary
condition for chaos) initially nearby trajectories will exponentially separate, i.e. we will have divergence along one (or more) directions in state space. This accounts for the sensitive dependence on
initial conditions in chaotic systems.

Now let L(W, B, h) be some loss function employed for RNN training that decomposes in time
as = _t=1_
performed for RTRL), we recursively develop the loss gradients w.r.t. some RNN parameter L _[L][t][. Suppose we fancy BPTT as our training algorithm (similar derivations could be] θ in_
time (i.e., across layers of the RNN unrolled in time) as

[P][T]


_∂∂θ [+]zr_ _[,]_ (5)


_∂L_

_∂θ_ [=]


_∂Lt_ with _∂Lt_

_∂θ_ _∂θ_


_∂_ _t_
_L_

_∂zt_


_∂zt_
_∂zr_


_t=1_


_r=1_

_t−r−1_


and _∂∂zzrt_ = _∂∂zzt_ _t_ 1 _∂∂zztt−12_ _· · ·_ _[∂]∂[z]z[r][+1]r_ = _t−r−1_ _∂∂zzt_ _t−k_ _k_ 1 = _t−r−1_ **_Jt−k,_** (6)

_−_ _−_ _k=0_ _−_ _−_ _k=0_

Y Y

where ∂[+] denotes the immediate derivative. Now observe that the behavior of the loss gradients
crucially depends on the product series of Jacobians in eqn. (6) : If the maximum absolute eigenval
1/T
ues of the Jacobians Jt will, in the geometric mean, be larger than 1 (i.e., _r=0_ **_[J][T][ −][r]_** _> 1),_

1/T
gradients will explode as T →∞, while they will saturate if _r=0_ **_[J][T][ −][r]_** [Q][T][ −]<[2] 1. Thus, the key
point to note is that the same terms that occur in the definition of the Lyapunov spectrum, eqn. (4),
resurface in the loss gradients, eqn. (5) & (6). This accounts for the tight links between system[Q][T][ −][2]
dynamics and gradients.

3.2 FIXED POINTS AND CYCLIC DYNAMICS

Let us start by considering the simplest types of dynamics that can occur in RNNs (or any discretetime DS): fixed points and cycles. In fact, by far most of the literature on global stability in RNNs
and on loss gradients focused on just fixed points (Chang et al., 2019; Kolter & Manek, 2019;
Erichson et al., 2021), with only few authors who recently started to also connect cyclic behavior
to loss gradients (Schmidt et al., 2021; Rusch & Mishra, 2021a). Recall that a fixed point of a
recursive map zt = F (zt 1) is defined as a point z[∗] for which we have z[∗] = F (z[∗]).[2] Likewise,
_−_
a k-cycle (k > 1) is a set of temporally consecutive periodic points Pk := {zt1 _, zt2_ _, . . ., ztk_ _} =_
**_zt1_** _, F_ (zt1 ), . . ., F _[k][−][1](zt1_ ) that we obtain from recursive application of the map such that each of
_{_ _}_
positive integer for which this holds). To simplify the subsequent treatment, we will collectivelythe cyclic points ztr ∈ _Pk is a fixed point of the k times iterated map F_ _[k]_ (with k being the smallest
refer to fixed points and cycles as k-cycles (k ≥ 1). Further recall that a fixed point or k-cycle is
called stable if the maximum absolute eigenvalue of the Jacobian evaluated at that point is smaller
than 1, neutrally stable if exactly 1, and unstable otherwise. Although the results we develop in this
and the following sections will hold more widely, we will restrict our attention to recursive maps Fθ
from the class of RNNs R = {standardRNN, LSTM, GRU, PLRNN} (see Appx. A.1 for details).

Based on the observations made in the previous sections we can state the following theorem that
links RNN dynamics and loss gradients:
_fixed point orTheorem 1. Consider an RNN k-cycle Γk (k ≥ F1)θ with ∈R B parameterized byΓk as its basin of attraction θ, and assume that it converges to a stable[3]. Then for every z1 ∈BΓk (i)_

2From here on we will suppress the explicit dependence on external inputs st notation-wise, see Remark 2.
3The basin of attraction is defined as the set of all points from which orbits converge to the resp. attractor.


-----

_the Jacobian_ _[∂]∂[z]z[T]1_ _[exponentially vanishes as][ T][ →∞][; (ii) for][ Γ][k][ the tangent vectors][ ∂]∂θ[z][T]_ _[and thus]_

_the gradient of the loss function,_ _[∂]∂[L]θ[T]_ _[, will be bounded from above, i.e. will not diverge for][ T][ →∞][;]_

_and (iii) for the PLRNN (27) bothT →∞._ _[∂]∂θ[z][T]_ _and_ _[∂]∂[L]θ[T]_ _will remain bounded for every z1 ∈BΓk as_

_Proof. (i) Assume that Γk is a stable k-cycle (k ≥_ 1) denoted by

Γk = {z1, z2, · · ·, zT, · · · } = {zt∗k _, zt∗k−1, · · ·, zt∗k−(k−1), zt∗k_ _, zt∗k−1, · · ·, zt∗k−(k−1), · · · }._
(7)


Then, the largest Lyapunov exponent of Γk is given by

1 1 _k−1_ _j_
_λΓk = limt→∞_ _t_ [ln] _Jt[∗]_ _[J]t[∗]−1_ _[· · ·][ J]2[∗]_ = limj→∞ _jk_ [ln]  _s=0_ _Jt∗k−s_ _[.]_ (8)

Y

By assumption of stability of Γk we have λΓk < 0 and also ρ _sk=0−1_ _[J][t][∗][k][−][s]_ _< 1, which implies_

_k_ 1 _j_
_−_   Q 

_tlim→∞_ _[J]t[∗]_ _[J]t[∗]−1_ _[· · ·][ J]2[∗]_ [= lim]j→∞  _s=0_ _Jt∗k−s_ = 0. (9)

Y

the same largest Lyapunov exponent, we haveNow suppose that Oz1 is an orbit of (1) converging to Γk, i.e. z1 ∈BΓk . Since Oz1 and Γk have


1

(10)
_T_ [ln][ ∥][J][T][ J][T][ −][1][ · · ·][ J][2][∥] [=][ λ][Γ][k][ <][ 0][,]

[=] _Tlim →∞_ _[∥][J][T][ J][T][ −][1][ · · ·][ J][2][∥]_ [= 0][.] (11)


_λOz1 =_ _Tlim →∞_

and hence for z1 Γk
_∈B_

_∂zT_

lim
_T →∞_ _∂z1_

(ii) & (iii) See Appx. A.2.1.


**Remark 1. The result of Theorem 1 part (i) will be generally true for any first-order-Markovian**
_recursive map (1), but the conclusions in part (ii) may hinge on its specific definition._
**Remark 2. None of the results above and throughout sect. 3 require the dynamics to be autonomous,**
_the theory applies whether there is external input or not. In fact, mathematically, non-autonomous_
_(externally forced) systems can always be rewritten as autonomous dynamical systems (Alligood_
_et al., 1996; Perko, 2001; Zhang et al., 2009), see Appx. A.1.1 for details._

The results above ensure that loss gradients will not diverge (explode) as T →∞ in RNNs that are
“well-behaved” in the sense that they converge to a fixed point or cycle. This is a generalization of
the results given in Theorem 1 in Schmidt et al. (2021), where this was shown only a) for the specific
class of PLRNNs and b) for specific constraints imposed on the eigenvalue spectrum of the RNN’s
Jacobians which were relaxed in our theorem above.

While our treatment above is centered on the “exploding-gradients” case, various architectural modifications or regularization techniques can ensure that gradients do not vanish either, i.e. remain
bounded from below as well. This was established, for instance, in Schmidt et al. (2021) for
PLRNNs using ’manifold attractor regularization’. In Appx. A.2.1 we show that the results from
Theorem 2 from Schmidt et al. (2021) on doubly bounded (from below and above) loss gradients
can indeed be extended to the more general case covered by Theorem 1 above.

3.3 CHAOTIC DYNAMICS

We will now consider the all-important chaotic case. Let F be a recursive map and **_z1_** =
_O_
**_z1, z2, z3,_** be an orbit of F . The orbit is chaotic if (i) it is not asymptotically periodic and
_{_ _· · · }_
(ii) has at least one positive Lyapunov exponent (Glendinning & Simpson, 2021; Meiss, 2007). If
the system’s invariant set is bounded, condition (ii) is considered a standard signature of chaos, as
in this case two nearby orbits separate exponentially fast, but at the same time their mutual separation cannot go to infinity so that there are also folds. The following theorem states the sufficient
condition for exploding gradients:


-----

**Theorem 2.Γ∗** _as its basin of attraction. Then, for every orbit with Suppose that an RNN Fθ ∈R (parameterized by z1_ **_θ) has a chaotic attractorΓ∗_** _, (i) the Jacobians connecting Γ[∗]_ _with_
_temporally distal statesB_ **_zT and zt ( T ≫_** _t),_ _[∂]∂[z]z[T]t_ _[, will exponentially explode for] ∈B_ _[ T][ →∞][, and (ii)]_

_the tangent vector_ _[∂]∂θ[z][T]_ _[and so the gradients of the loss function,][ ∂]∂θ[L][T]_ _[, will diverge as][ T][ →∞][.]_

denoting byProof. Let the RNN JT[∗] [the Jacobian of (1) at] Fθ ∈R have a chaotic orbit denoted by[ z]T[∗] _[∈]_ [Γ][∗][, the largest Lyapunov exponent of] Γ[∗] = {z1[∗][,][ z]2[∗][,][ · · ·][,][ Γ][ z]T[∗][∗] _[,][is given by][ · · · }][. Then,]_

1
_λ =_ lim _JT[∗]_ _[J]T[∗]_ 1 2 _._ (12)
_T →∞_ _T_ [ln] _−_ _[· · ·][ J]_ _[∗]_

Since Γ[∗] is chaotic, so λ > 0. Hence, from (12), it is concluded that

_Tlim →∞_ _JT[∗]_ _[J]T[∗] −1_ _[· · ·][ J]2[∗]_ = _Tlim →∞_ _∂∂zzT[∗]t[∗]_ [=][ ∞] _[,]_ _T ≫_ _t._ (13)

Now, according to Oseledec’s multiplicative ergodic Theorem, nearly all the points in the basin of
attraction of Γ[∗] have the same largest Lyapunov exponent λ. Thus, (13) holds for every z1 Γ∗ .
_∈B_

(ii) See Appx. A.2.2.

**Remark 3. The first part of Theorem 2 holds for all first-order-Markovian recursive maps (1). Note**
_that for LSTMs,_ _[∂]∂[z]z[T]t_ _[(][z][ := (][h][,][ c][)][T][) denotes the full Jacobian of both hidden and cell states.]_

We collect some further mathematical results and remarks related to Theorem 2 in Appx. A.3.1.

Hence, the essential result is that for all popular RNNs R and activation functions, loss gradients
will inevitably diverge if the RNN latent states converge to a chaotic attractor.

3.4 QUASI-PERIODICITY

Quasi-periodicity is a long-term behavior which occurs on a torus and, superficially, bears some
similarity to chaos in the sense that, strictly speaking, orbits are also aperiodic. That is, as T →∞,
trajectories will never close up with themselves. Moreover, every trajectory becomes arbitrarily
close to any point on the torus, that is, it is dense. One important difference between quasi-periodic
and chaotic systems is, however, that in a quasi-periodic system, as time passes, two close initial
conditions are linearly diverging, while in a chaotic system the divergence is exponential.
**Theorem 3.with BΓ as its basin of attraction. Then, for every Assume that an RNN Fθ ∈R (parameterized by z1 ∈BΓ** **_θ) has a quasi-periodic attractor Γ_**

_∂zT_
0 < ϵ < 1 _T0 > 1 s.t._ _T_ _T0 =_ (1 _ϵ)[T][ −][1]_ _<_ _[<][ (1 +][ ϵ][)][T][ −][1][.]_ (14)
_∀_ _∃_ _∀_ _≥_ _⇒_ _−_ _∂z1_

_Proof. See Appx. A.2.3._

According to Theorem 3, for every orbit converging to a quasi-periodic attractor, the Jacobians _[∂]∂[z]z[T]t_

may diverge or vanish as T →∞, but this will not occur exponentially fast as T →∞. Thus, even
for bounded non-chaotic RNNs we may sometimes stumble into the problem of diverging gradients.
Although this may be a less common scenario, we point out it may occur if we train RNNs on
real data from oscillatory systems with incommensurate frequencies, as for instance encountered in
electronic engineering.

In Appx. A.3.2 we have collected further mathematical results on the connection between RNN
dynamics and loss gradients that hold regardless of the RNN’s limiting behavior.

4 EMPIRICAL EVALUATION

Our theoretical results imply that chaotic time series pose a principle challenge for RNN training
that cannot easily be circumvented through specifically designed architectures, constraints, or regularization criteria. If the underlying DS we aim to capture is chaotic, loss gradients propagated back
in time will inevitably explode. Here we will work out some implications for RNN training and
potential remedies empirically.


-----

4.1 TRAINING ON SYSTEMS WITH EXPLODING GRADIENTS BY SPARSE TEACHER FORCING

To illustrate the connections between theory and RNN training, we revive the old idea of TF
(Williams & Zipser, 1989) as a mechanism for truncating error gradients while training. However,
we would like to do this such that important information about the system dynamics does not get
lost, for which Lyapunov theory offers some guidance. Specifically, we should not force the system
back onto the true trajectory all or most of the time (as in “classical TF”), but should effectively
“re-calibrate” it only at certain time points chosen wisely according to the system’s local divergence
rates. This procedure will be referred to as sparsely forced BPTT in the following.

Assume we want to train an RNN with hidden statesa time-seriesB ∈ R[N] _[×][M] {, maps the RNN hidden states into the observation space. This allows us to modify thex1, x2, · · ·, xT } generated by a chaotic system. zt ∈_ R[M] [4]and linear (or affine) output layer onThe linear output layer ˆxt = Bzt,
original TF procedure by constructing a control series **_z˜1, ˜z2,_** _, ˜zT_ from the observations by
“inverting” the linear output mapping [5] _{_ _· · ·_ _}_
**_z˜t = (B[T]B)[−][1]B[T]xt._** (15)

The idea is to supply this control signal only sparsely, separated by the learning interval τ between
consecutive forcings. Hence, defining T = {nτ + 1}n∈N0 as the set of all time points at which we
force the RNN onto the ‘true’ values, the RNN updates can be written as

**_zt+1 =_** _RNN_ (˜zt) if t ∈T _._ (16)
_RNN_ (zt) else


This forcing is appliedwhether t is in T or not (and of course it is applied only during training, not at test time!). Replacing after calculation of the loss, such that Lt = ∥xt − **_Bzt∥2[2]_** [irrespective of]
hidden states zt with their teacher-forced signals ˜zt simply breaks divergence between true and
predicted trajectories at time points t ∈T, and also cuts off the Jacobians by breaking the temporal
contingency (for details see Appx. A.7). The learning interval τ hence controls how many time steps
are included in the gradient calculation and has to be chosen with care such as to balance the effects
of exploding gradients vs. those of loosing relevant time scales and long-term dependencies. While
it is general wisdom that an optimal batch size will facilitate training, the point here is thus much
more specific: Ideally τ should be chosen in accordance with the system’s Lyapunov spectrum, for
instance based on the predictability time (Bezruchko & Smirnov, 2010)

ln 2
_τpred =_ _._ (17)

_λmax_

We emphasize that such a simple recipe for addressing the exploding gradient problem is based on
modifying the training routine, and is thus in principle applicable to any model architecture.[6]

4.2 EXAMPLE 1: LORENZ SYSTEM AND EXTERNALLY FORCED DUFFING OSCILLATOR IN
CHAOTIC REGIME

Let us illustrate these ideas on two classical textbook examples of chaotic DS, the chaotic Lorenz
attractor as an autonomous system, and the chaotically forced Duffing oscillator as an example
with explicit external input (see Appx. A.4 for details). Trajectories were repeatedly drawn from
these systems, on which we trained a PLRNN, a vanilla RNN with tanh activation function, and a
LSTM by stochastic gradient descent (SGD) to minimize the MSE loss between predicted and actual
observations. As optimizer we used Adam (Kingma & Ba, 2015) from PyTorch (Paszke et al., 2017)
with a learning rate of 0.001. For all models, training proceeded solely by sparsely forced BPTT
and did not employ gradient clipping or any other technique that may interfere with optimal loss
truncation.

In nonlinear DS reconstruction, we are mainly interested in reproducing invariant properties of the
underlying system such as the attractor geometry (or topology; Takens (1981); Sauer et al. (1991)) or

4Note that in DS reconstruction one usually considers the data as observations (unsupervised problem);
according to Remark 2, however, it is mainly a matter of the scientific question addressed whether we include
certain variables as explicit inputs or as observations.
5To ensure invertibility, one could add a regularizer λI to BTB in eqn. (15), as in ridge regression, but we
did not find this necessary in any of our examples.
6All code produced here is available at [placeholder].


-----

the frequency composition (i.e., time-averaged properties), while measures like ahead-prediction errors are less meaningful especially on chaotic time series (Wood, 2010; Koppe et al., 2019). Thus, in
evaluating training performance, here we follow Koppe et al. (2019) in using a Kullback-Leibler divergence Dstsp to quantify the agreement between observed and generated probability distributions
across state-space to asses the overlap in attractor geometry (Appx. A.5). Moreover, we employ a
dimension-wise frequency correlation measure (PSC) to quantify the agreement of power-spectra of
the observed and generated time-series (Appx. A.5).


**(a)**


**(b)**


15

10


15

10



1.0

0.5


1.0

0.5


0.0


0.0


_τpred_ 200 400

30 øpred 100 150 200

learning interval ø

LSTM
RNN
PLRNN

learning interval τ

|Col1|Col2|
|---|---|
|ø|pred 100 2 learning interval ø|
|||
|τ|pred 100 20 learning interval τ|


Figure 1: Overlap in attractor geometry (Dstsp, lower = better) and dimension-wise power-spectra correlations
(PSC, higher = better) against learning interval τ for (a) the Lorenz and (b) the chaotically forced Duffing
oscillator. Continuous lines = sparsely forced BPTT. Dashed lines = classical BPTT with gradient clipping.
Prediction time indicated vertically in black.

Fig. 1 shows the dependence of the reconstruction quality on the learning interval τ for all RNN architectures on (a) the Lorenz and (b) the externally forced Duffing system. Fig. 2 provides particular
examples of reconstructions for τ chosen too small, too large, or about right. For all models we find
a system-dependent range for the optimal learning interval that agrees well with the predictability
time defined in eqn. (17), where estimates for the maximal Lyapunov exponent were taken from the
literature (Rosenstein et al., 1993; Gilpin, 2021). As a reference, dashed lines represent the reconstruction performance for all architectures when trained with classical BPTT and gradient clipping.
The training procedure was the same as for sparsely forced BPTT, except that instead of supplying a
control-signal, gradients were normalized to 1 prior to each parameter update. As evidenced by the
much worse performance, gradient clipping does not effectively address the EVGP, even for LSTMs.
As further shown in Fig. 9 in Appx. A.6.4, using the optimal window length τ but resetting the initial condition to zero (instead of its control value ˜zt) for each chunk equally destroys performance.
This suggests that neither mere gradient normalization nor simple windowing are sufficient, but will
wipe out essential information about the dynamics.

In Appx. A.6 we collect further results on the chaotic R¨ossler attractor (Fig. 5), high-dimensional
Mackey-Glass equations (Fig. 7), and the Lorenz attractor with partial observations (Fig. 8).

Figure 2: Lorenz attractor (blue) and example reconstructions by a LSTM (orange) trained with a learning
interval (a) chosen too small (τ = 5), (b) chosen optimally (τ = 30), and (c) chosen too large (τ = 200).


-----

4.3 EXAMPLE 2: CHAOTIC WEATHER DATA

As for an empirical example, we trained all RNNs (vanilla RNN, PLRNN, LSTM) on a tempera[ture time series recorded at the Weather Station at the Max Planck Institute for Biogeochemistry](https://www.bgc-jena.mpg.de/wetter/)
[in Jena, Germany. To expose the chaotic behavior and obtain a robust estimate of the maximal](https://www.bgc-jena.mpg.de/wetter/)
Lyapunov exponent, trends and yearly cycles were removed, and nonlinear noise-reduction was performed (Kantz et al. (1993); Appx. A.4). The maximal Lyapunov exponent was determined with
the TISEAN package (Hegger et al., 1999), as shown in Figure 3 (a). The value obtained is in close
agreement with the literature (Mill´an et al., 2010).

Figure 3 shows that also for these empirical data the optimal training interval τ agrees well with
the predictability time, eqn. (17), for all trained RNNs. Furthermore, as was the case for the DS
benchmarks, gradient clipping was not able to satisfactorily tackle the EVGP, even when paired
with architectures like LSTMs explicitly designed for alleviating this problem. Similar results are
reported for another real-world dataset, electroencephalogram (EEG) recordings, in Appx. A.11.


**(a)**


**(b)**


_°0.5_

_°1.0_

_°1.5_


1.0

0.5

0.0


15

10


_τpred_ 100 200

learning interval τ


m=6
m=5
m=4
_∏max = 0.016_


RNN
PLRNN
LSTM


50 100 150

time-steps ≤



|Col1|RNN|
|---|---|
|øpred lear|PLRNN LSTM 100 20 ning interval ø|


Figure 3: (a) The maximal Lyapunov exponent was determined as the slope of the average log-divergence of
nearest neighbors in embedding space (m = embedding dimension). (b) Reconstruction quality assessed by
attractor overlap (lower = better) and power-spectrum correlation (higher = better). Black vertical lines = τpred.

5 DISCUSSION AND CONCLUSIONS


In this paper we proved that RNN dynamics and loss gradients are intimately related for all major
types of RNNs and activation functions. If the RNN is “well behaved” in the sense that its dynamics
converges to a fixed point or cycle, loss gradients will remain bounded, and established remedies
(Hochreiter & Schmidhuber, 1997; Schmidt et al., 2021) can be used to refrain them from vanishing.
However, if the dynamics are chaotic, gradients will always explode. This constitutes a principle
problem in RNN training that cannot easily be mastered through architectural design or gradient
clipping. It is furthermore a practically highly relevant one, as most time series we encounter in
nature, and many from man-made systems as well, are inherently chaotic. While we do not offer
a full solution to this problem here, we suggest it might be tackled in training by taking a system’s
local divergence rates as measured through the Lyapunov spectrum into account. Hence, rather than
conquering the EVGP by structural design or specific constraints or regularization terms, we recommend to put the focus more on the training process itself. We illustrated this point empirically using
_sparsely forced BPTT, a training technique that pulls trajectories back on track at times determined_
by the maximal Lyapunov exponent. Doing so leads to optimal reconstruction results for a variety
of simulated and real-world benchmarks, regardless of the specific RNN architecture employed in
training. We stress that our goal above all was to provide a mathematically grounded perspective
on the problem, with the empirical section focused on elucidating the practical implications of the
theoretical results. Empirically, for instance, precise Lyapunov exponents may sometimes be hard to
obtain, although our empirical examples confirm that estimates based on log-divergence plots may
work sufficiently well.

ACKNOWLEDGMENTS


ETHICS STATEMENT

The current work deals with theoretical-mathematical aspects of RNN training and performs basic
research on limitations of training algorithms. As such it has no direct ethical implications. Since
many real world applications depend on accurate forward predictions of time series, however, this


-----

paper may raise awareness for an important issue that will also be relevant in practical settings,
including sensitive domains like medical time series, weather forecasts, or traffic control.

REPRODUCIBILITY STATEMENT

All theoretical results in this paper were carefully and thoroughly proven, with all proofs and detailed derivations available in the Appendix. Likewise, we will make available all code used in the
empirical section in a way that will allow others to easily reproduce the results from this paper. This
means we will include everything, starting with the code for benchmark simulations and simulated
time series data used for evaluation, code for our model and training algorithms, up to the metafiles that produce the actual figures in this work, on our lab github site. All of this will be clearly
documented.

REFERENCES

Henry D. I. Abarbanel. Predicting the Future: Completing Models of Observed Complex Systems.
Understanding Complex Systems. Springer-Verlag, New York, 2013. ISBN 978-1-4614-72179. doi: 10.1007/978-1-4614-7218-6. [URL https://www.springer.com/gp/book/](https://www.springer.com/gp/book/9781461472179)
[9781461472179.](https://www.springer.com/gp/book/9781461472179)

Henry D. I. Abarbanel, Paul J. Rozdeba, and Sasha Shirman. Machine Learning: Deepest Learning
as Statistical Data Assimilation Problems. Neural Computation, 30(8):2025–2055, August 2018.
ISSN 1530-888X. doi: 10.1162/neco a 01094.

Kathleen T. Alligood, Tim D. Sauer, and James A. Yorke. Chaos: An Introduction to Dynamical
_Systems. Springer, New York, NY, 1996._

Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks.
In Proceedings of the 33rd International Conference on International Conference on Machine
_Learning - Volume 48, ICML’16, pp. 1120–1128. JMLR.org, 2016._

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence
prediction with recurrent Neural networks. In Proceedings of the 28th International Conference
_on Neural Information Processing Systems - Volume 1, NIPS’15, pp. 1171–1179, Cambridge,_
MA, USA, December 2015. MIT Press.

Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient
descent is difficult. IEEE Transactions on Neural Networks, 5(2):157–166, 1994. doi: 10.1109/
72.279181.

Boris P. Bezruchko and Dmitry A. Smirnov. _Extracting Knowledge From Time Series: An In-_
_troduction to Nonlinear Empirical Modeling. Springer Series in Synergetics. Springer-Verlag,_
Berlin Heidelberg, 2010. ISBN 978-3-642-12600-0. doi: 10.1007/978-3-642-12601-7. URL
[https://www.springer.com/gp/book/9783642126000.](https://www.springer.com/gp/book/9783642126000)

Bo Chang, Minmin Chen, Eldad Haber, and Ed H. Chi. AntisymmetricRNN: A dynamical system
view on recurrent neural networks. In International Conference on Learning Representations,
[2019. URL https://openreview.net/forum?id=ryxepo0cFX.](https://openreview.net/forum?id=ryxepo0cFX)

Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties
of neural machine translation: Encoder–decoder approaches. In Proceedings of SSST-8, Eighth
_Workshop on Syntax, Semantics and Structure in Statistical Translation, pp. 103–111, Doha,_
Qatar, October 2014a. Association for Computational Linguistics. doi: 10.3115/v1/W14-4012.
[URL https://aclanthology.org/W14-4012.](https://aclanthology.org/W14-4012)

Kyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP
_2014), 2014b._

Kenji Doya. Bifurcations in the learning of recurrent neural networks. In [Proceedings] 1992 IEEE
_International Symposium on Circuits and Systems, volume 6, pp. 2777–2780 vol.6, May 1992._
doi: 10.1109/ISCAS.1992.230622.


-----

Jorge Duarte, Cristina Janu´ario, Nuno Martins, and Josep Sardany´es. Quantifying chaos for
ecological stoichiometry. _Chaos: An Interdisciplinary Journal of Nonlinear Science, 20(3):_
[033105, September 2010. ISSN 1054-1500. doi: 10.1063/1.3464327. URL https://aip.](https://aip.scitation.org/doi/full/10.1063/1.3464327)
[scitation.org/doi/full/10.1063/1.3464327.](https://aip.scitation.org/doi/full/10.1063/1.3464327) Publisher: American Institute of
Physics.

Georg Duffing. Erzwungene schwingungen bei ver¨anderlicher eigenfrequenz, 1918.

Daniel Durstewitz. Advanced Data Analysis in Neuroscience: Integrating Statistical and Compu_tational Models. Bernstein Series in Computational Neuroscience. Springer International Pub-_
lishing, 2017. ISBN 978-3-319-59974-8. doi: 10.1007/978-3-319-59976-2. [URL https:](https://www.springer.com/de/book/9783319599748)
[//www.springer.com/de/book/9783319599748.](https://www.springer.com/de/book/9783319599748)

Daniel Durstewitz and Thomas Gabriel. Dynamical Basis of Irregular Spiking in NMDA-Driven
Prefrontal Cortex Neurons. _Cerebral Cortex, 17(4):894–908, April 2007._ ISSN 1460-2199,
[1047-3211. doi: 10.1093/cercor/bhk044. URL https://academic.oup.com/cercor/](https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/bhk044)
[article-lookup/doi/10.1093/cercor/bhk044.](https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/bhk044)

Rainer Engelken, Fred Wolf, and Larry F. Abbott. Lyapunov spectra of chaotic recurrent neural net[works. arXiv:2006.02427 [nlin, q-bio], June 2020. URL http://arxiv.org/abs/2006.](http://arxiv.org/abs/2006.02427)
[02427. arXiv: 2006.02427.](http://arxiv.org/abs/2006.02427)

N. Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and Michael W.
Mahoney. Lipschitz recurrent neural networks. In International Conference on Learning Repre_[sentations, 2021. URL https://openreview.net/forum?id=-N7PBXqOUJZ.](https://openreview.net/forum?id=-N7PBXqOUJZ)_

Marisa Faggini. Chaotic time series analysis in economics: Balance and perspectives. Chaos: An
_Interdisciplinary Journal of Nonlinear Science, 24(4):042101, December 2014. ISSN 1054-1500._
[doi: 10.1063/1.4903797. URL https://aip.scitation.org/doi/full/10.1063/](https://aip.scitation.org/doi/full/10.1063/1.4903797)
[1.4903797. Publisher: American Institute of Physics.](https://aip.scitation.org/doi/full/10.1063/1.4903797)

Richard J Field and L´aszl´o Gy¨orgyi. Chaos in Chemistry and Biochemistry. WORLD SCIENTIFIC,
[1993. doi: 10.1142/1706. URL https://www.worldscientific.com/doi/abs/10.](https://www.worldscientific.com/doi/abs/10.1142/1706)
[1142/1706.](https://www.worldscientific.com/doi/abs/10.1142/1706)

Stefano Fusi, Wael F. Asaad, Earl K. Miller, and Xiao-Jing Wang. A neural circuit model of flexible
sensorimotor mapping: learning and forgetting on multiple timescales. Neuron, 54(2):319–333,
April 2007. ISSN 0896-6273. doi: 10.1016/j.neuron.2007.03.017.

Zoubin Ghahramani and Sam Roweis. Learning nonlinear dynamical systems using an em algorithm. In M. Kearns, S. Solla, and D. Cohn (eds.), Advances in Neural Information Process_[ing Systems, volume 11. MIT Press, 1999. URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/1998/file/0ebcc77dc72360d0eb8e9504c78d38bd-Paper.pdf)_
[paper/1998/file/0ebcc77dc72360d0eb8e9504c78d38bd-Paper.pdf.](https://proceedings.neurips.cc/paper/1998/file/0ebcc77dc72360d0eb8e9504c78d38bd-Paper.pdf)

William Gilpin. Chaos as an interpretable benchmark for forecasting and data-driven modelling.
In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks
_[Track (Round 2), 2021. URL https://openreview.net/forum?id=enYjtbjYJrf.](https://openreview.net/forum?id=enYjtbjYJrf)_

L. Glass and M. C. Mackey. Pathological conditions resulting from instabilities in physiological
control systems. Annals of the New York Academy of Sciences, 316:214–235, 1979. ISSN 00778923. doi: 10.1111/j.1749-6632.1979.tb29471.x.

Paul A. Glendinning and David J. W. Simpson. A constructive approach to robust chaos using
invariant manifolds and expanding cones. Discrete & Continuous Dynamical Systems, 41(7):
3367–3387, 2021.

A. L. Goldberger, L. A. N. Amaral, L. Glass, J. M. Hausdorff, P. Ch. Ivanov,
R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley. PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for
complex physiologic signals. _Circulation,_ 101(23):e215–e220, 2000. Circulation
Electronic Pages: http://circ.ahajournals.org/content/101/23/e215.full PMID:1085218; doi:
10.1161/01.CIR.101.23.e215.


-----

[Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:](http://www.deeplearningbook.org)
[//www.deeplearningbook.org.](http://www.deeplearningbook.org)

Alex Graves, Greg Wayne, Malcolm Reynolds, and et al. Hybrid computing using a neural network
with dynamic external memory. Nature, 538:471––476, 2016. doi: 10.1038/nature20101.

Rainer Hegger, Holger Kantz, and Thomas Schreiber. Practical implementation of nonlinear time
series methods: The TISEAN package. Chaos: An Interdisciplinary Journal of Nonlinear Sci_ence, 9(2):413–435, June 1999._ ISSN 1054-1500. doi: 10.1063/1.166424. [URL https:](https://aip.scitation.org/doi/citedby/10.1063/1.166424)
[//aip.scitation.org/doi/citedby/10.1063/1.166424.](https://aip.scitation.org/doi/citedby/10.1063/1.166424) Publisher: American
Institute of Physics.

Kyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal Recurrent Neural Networks with
Scaled Cayley Transform. In International Conference on Machine Learning, pp. 1969–1978.
PMLR, July 2018. [URL http://proceedings.mlr.press/v80/helfrich18a.](http://proceedings.mlr.press/v80/helfrich18a.html)
[html. ISSN: 2640-3498.](http://proceedings.mlr.press/v80/helfrich18a.html)

Mikael Henaff, Arthur Szlam, and Yann LeCun. Recurrent Orthogonal Networks and Long-Memory
Tasks. In International Conference on Machine Learning, pp. 2034–2042. PMLR, June 2016.
[URL http://proceedings.mlr.press/v48/henaff16.html. ISSN: 1938-7228.](http://proceedings.mlr.press/v48/henaff16.html)

Daniel Hernandez, Antonio Khalil Moretti, Ziqiang Wei, Shreya Saxena, John Cunningham, and
Liam Paninski. Nonlinear Evolution via Spatially-Dependent Linear Dynamics for Electrophysi[ology and Calcium Data. arXiv preprint arXiv:1811.02459, 2020. URL http://arxiv.org/](http://arxiv.org/abs/1811.02459)
[abs/1811.02459.](http://arxiv.org/abs/1811.02459)

Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen netzen, 1991.

Sepp Hochreiter and J¨urgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9
(8):1735–1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL
[https://doi.org/10.1162/neco.1997.9.8.1735.](https://doi.org/10.1162/neco.1997.9.8.1735)

Katsuma Inoue, Soh Ohara, Yasuo Kuniyoshi, and Kohei Nakajima. Transient chaos in bert.
_[arXiv:2106.03181 [nlin], June 2021. URL http://arxiv.org/abs/2106.03181. arXiv:](http://arxiv.org/abs/2106.03181)_
2106.03181.

Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua
Bengio. Gated Orthogonal Recurrent Units: On Learning to Forget. Neural Computation, 31(4):
765–783, April 2019. ISSN 1530-888X. doi: 10.1162/neco a 01174.

Anil Kag and Venkatesh Saligrama. Time Adaptive Recurrent Neural Network. In 2021 IEEE/CVF
_Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15144–15153, Nashville,_
TN, USA, June 2021a. IEEE. ISBN 978-1-66544-509-2. doi: 10.1109/CVPR46437.2021.01490.
[URL https://ieeexplore.ieee.org/document/9578651/.](https://ieeexplore.ieee.org/document/9578651/)

Anil Kag and Venkatesh Saligrama. Training recurrent neural networks via forward propagation
through time. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
_Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp._
[5189–5200. PMLR, 18–24 Jul 2021b. URL https://proceedings.mlr.press/v139/](https://proceedings.mlr.press/v139/kag21a.html)
[kag21a.html.](https://proceedings.mlr.press/v139/kag21a.html)

Anil Kag, Ziming Zhang, and Venkatesh Saligrama. Rnns incrementally evolving on an equilibrium manifold: A panacea for vanishing and exploding gradients? In International Confer_[ence on Learning Representations, 2020. URL https://openreview.net/forum?id=](https://openreview.net/forum?id=HylpqA4FwS)_
[HylpqA4FwS.](https://openreview.net/forum?id=HylpqA4FwS)

L´eandre Kamdjeu Kengne, Justin R. Mboupda Pone, and Hilaire B. Fotsin. On the dynamics
of chaotic circuits based on memristive diode-bridge with variable symmetry: A case study.
_Chaos, Solitons & Fractals, 145:110795, April 2021. ISSN 0960-0779. doi: 10.1016/j.chaos._
2021.110795. [URL https://www.sciencedirect.com/science/article/pii/](https://www.sciencedirect.com/science/article/pii/S0960077921001478)
[S0960077921001478.](https://www.sciencedirect.com/science/article/pii/S0960077921001478)

Holger Kantz and Thomas Schreiber. Nonlinear Time Series Analysis. Cambridge University Press,
2 edition, 2003. doi: 10.1017/CBO9780511755798.


-----

Holger Kantz, Thomas Schreiber, Ingo Hoffmann, Thorsten Buzug, Gerd Pfister, Leci G. Flepp,
Josef Simonet, Remo Badii, and Ernst Brun. Nonlinear noise reduction: A case study on experimental data. Physical Review E, 48(2):1529–1538, August 1993. doi: 10.1103/PhysRevE.48.
[1529. URL https://link.aps.org/doi/10.1103/PhysRevE.48.1529. Publisher:](https://link.aps.org/doi/10.1103/PhysRevE.48.1529)
American Physical Society.

Matthew B. Kennel, Reggie Brown, and Henry D. I. Abarbanel. Determining embedding dimension
for phase-space reconstruction using a geometrical construction. Phys. Rev. A, 45:3403–3411,
[Mar 1992. doi: 10.1103/PhysRevA.45.3403. URL https://link.aps.org/doi/10.](https://link.aps.org/doi/10.1103/PhysRevA.45.3403)
[1103/PhysRevA.45.3403.](https://link.aps.org/doi/10.1103/PhysRevA.45.3403)

Giancarlo Kerg, Kyle Goyette, Maximilian Puelma Touzel, Gauthier Gidel, Eugene Vorontsov,
Yoshua Bengio, and Guillaume Lajoie. Non-normal recurrent neural network (nnrnn):
learning long time dependencies while improving expressivity with transient dynamics.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso[ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/9d7099d87947faa8d07a272dd6954b80-Paper.pdf)
[9d7099d87947faa8d07a272dd6954b80-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/9d7099d87947faa8d07a272dd6954b80-Paper.pdf)

Mounira Kesmia, Soraya Boughaba, and Sabir Jacquir. Control of continuous dynamical systems
modeling physiological states. Chaos, Solitons & Fractals, 136:109805, July 2020. ISSN 09600779. doi: 10.1016/j.chaos.2020.109805. [URL https://www.sciencedirect.com/](https://www.sciencedirect.com/science/article/pii/S096007792030206X)
[science/article/pii/S096007792030206X.](https://www.sciencedirect.com/science/article/pii/S096007792030206X)

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
_[2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:](http://arxiv.org/abs/1412.6980)_
[//arxiv.org/abs/1412.6980.](http://arxiv.org/abs/1412.6980)

Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In Proceedings of the 2nd
_[International Conference on Learning Representations, 2014. URL http://arxiv.org/](http://arxiv.org/abs/1312.6114)_
[abs/1312.6114.](http://arxiv.org/abs/1312.6114)

J. Zico Kolter and Gaurav Manek. Learning Stable Deep Dynamics Models. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso[ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/0a4bbceda17a6253386bc9eb45240e25-Paper.pdf)
[0a4bbceda17a6253386bc9eb45240e25-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/0a4bbceda17a6253386bc9eb45240e25-Paper.pdf)

Georgia Koppe, Hazem Toutounji, Peter Kirsch, Stefanie Lis, and Daniel Durstewitz. Identifying
nonlinear dynamical systems via generative recurrent neural networks with applications to fMRI.
_PLOS Computational Biology, 15(8):1–35, 08 2019. doi: 10.1371/journal.pcbi.1007263. URL_
[https://doi.org/10.1371/journal.pcbi.1007263.](https://doi.org/10.1371/journal.pcbi.1007263)

Jacques Laskar and Philippe Robutel. The chaotic obliquity of the planets. Nature, 361(6413):
[608–612, February 1993. ISSN 1476-4687. doi: 10.1038/361608a0. URL https://www.](https://www.nature.com/articles/361608a0)
[nature.com/articles/361608a0.](https://www.nature.com/articles/361608a0)

Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural network
(indrnn): Building a longer and deeper rnn. In 2018 IEEE/CVF Conference on Computer Vision
_and Pattern Recognition, pp. 5457–5466, 2018. doi: 10.1109/CVPR.2018.00572._

Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial
differential equations. In International Conference on Learning Representations, 2021. URL
[https://openreview.net/forum?id=c8P9NQVtmnO.](https://openreview.net/forum?id=c8P9NQVtmnO)

Zachary C. Lipton, John Berkowitz, and Charles Elkan. A critical review of recurrent neural net[works for sequence learning. arXiv:1506.00019 [cs], October 2015. URL http://arxiv.](http://arxiv.org/abs/1506.00019)
[org/abs/1506.00019. arXiv: 1506.00019.](http://arxiv.org/abs/1506.00019)


-----

Edward N. Lorenz. Deterministic nonperiodic flow. Journal of the Atmospheric Sciences, 20(2):
[130–141, March 1963. ISSN 0022-4928, 1520-0469. doi: 10.1175/1520-0469(1963)020⟨0130:](https://journals.ametsoc.org/view/journals/atsc/20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.xml)
DNF⟩2.0.CO;2. [URL https://journals.ametsoc.org/view/journals/atsc/](https://journals.ametsoc.org/view/journals/atsc/20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.xml)
[20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.xml.](https://journals.ametsoc.org/view/journals/atsc/20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.xml) Publisher: American Meteorological Society Section: Journal of the Atmospheric Sciences.

Niru Maheswaranathan, Alex Williams, Matthew Golub, Surya Ganguli, and David Sussillo. Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso[ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/d921c3c762b1522c475ac8fc0811bb0f-Paper.pdf)
[d921c3c762b1522c475ac8fc0811bb0f-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/d921c3c762b1522c475ac8fc0811bb0f-Paper.pdf)

Sylvain Mangiarotti, Matthieu Peyre, Yixiao Zhang, Maciej Huc, Friederike Roger, and Yvonne
Kerr. Chaos theory applied to the outbreak of COVID-19: an ancillary approach to decision
making in pandemic context. Epidemiology and Infection, 148:e95, May 2020. ISSN 09502688. doi: 10.1017/S0950268820000990. [URL https://www.ncbi.nlm.nih.gov/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7231667/)
[pmc/articles/PMC7231667/.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7231667/)

Robert M. May. Chaos and the dynamics of biological populations. Proceedings of the Royal
_Society of London. Series A, Mathematical and Physical Sciences, 413(1844):27–44, 1987. ISSN_
[00804630. URL http://www.jstor.org/stable/2398225.](http://www.jstor.org/stable/2398225)

James D. Meiss. Differential Dynamical Systems. Society for Industrial and Applied Mathematics,
2007.

Jacob Menick, Erich Elsen, Utku Evci, Simon Osindero, Karen Simonyan, and Alex Graves.
Practical real time recurrent learning with a sparse approximation. In International Confer_[ence on Learning Representations, 2021. URL https://openreview.net/forum?id=](https://openreview.net/forum?id=q3KSThy2GwB)_
[q3KSThy2GwB.](https://openreview.net/forum?id=q3KSThy2GwB)

Humberto Mill´an, Behzad Ghanbarian-Alavijeh, and Ivan Garc´ıa-Fornaris. Nonlinear dynamics
of mean daily temperature and dewpoint time series at Babolsar, Iran, 1961–2005. _Atmo-_
_spheric Research, 98(1):89–101, October 2010._ ISSN 0169-8095. doi: 10.1016/j.atmosres.
2010.06.001. [URL https://www.sciencedirect.com/science/article/pii/](https://www.sciencedirect.com/science/article/pii/S0169809510001419)
[S0169809510001419.](https://www.sciencedirect.com/science/article/pii/S0169809510001419)

Zahra Monfared and Daniel Durstewitz. Existence of n-cycles and border-collision bifurcations in piecewise-linear continuous maps with applications to recurrent neural networks. _Nonlinear Dynamics, 101(2):1037–1052, July 2020a._ ISSN 0924-090X, 1573269X. doi: 10.1007/s11071-020-05841-x. [URL http://link.springer.com/10.](http://link.springer.com/10.1007/s11071-020-05841-x)
[1007/s11071-020-05841-x.](http://link.springer.com/10.1007/s11071-020-05841-x)

Zahra Monfared and Daniel Durstewitz. Transformation of ReLU-based recurrent neural networks
from discrete-time to continuous-time. In International Conference on Machine Learning, pp.
[6999–7009. PMLR, November 2020b. URL http://proceedings.mlr.press/v119/](http://proceedings.mlr.press/v119/monfared20a.html)
[monfared20a.html. ISSN: 2640-3498.](http://proceedings.mlr.press/v119/monfared20a.html)

Lars Folke Olsen and Hans Degn. Chaos in an enzyme reaction. Nature, 267(5607):177–178,
[May 1977. ISSN 1476-4687. doi: 10.1038/267177a0. URL https://www.nature.com/](https://www.nature.com/articles/267177a0)
[articles/267177a0.](https://www.nature.com/articles/267177a0)

Emin Orhan and Xaq Pitkow. Improved memory in recurrent neural networks with sequential
non-normal dynamics. In International Conference on Learning Representations, 2020. URL
[https://openreview.net/forum?id=ryx1wRNFvB.](https://openreview.net/forum?id=ryx1wRNFvB)

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In Proceedings of the 30th International Conference on International Conference on
_Machine Learning - Volume 28, ICML’13, pp. III–1310–III–1318. JMLR.org, 2013._

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zach DeVito, Zeming
Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
[URL https://openreview.net/forum?id=BJJsrmfCZ.](https://openreview.net/forum?id=BJJsrmfCZ)


-----

Barak Pearlmutter. Dynamic recurrent neural networks, 1990. [URL https://kilthub.](https://kilthub.cmu.edu/articles/journal_contribution/Dynamic_recurrent_neural_networks/6605018/1)
[cmu.edu/articles/journal_contribution/Dynamic_recurrent_neural_](https://kilthub.cmu.edu/articles/journal_contribution/Dynamic_recurrent_neural_networks/6605018/1)
[networks/6605018/1.](https://kilthub.cmu.edu/articles/journal_contribution/Dynamic_recurrent_neural_networks/6605018/1)

Lawrence Perko. Differential Equations and Dynamical Systems, volume 7. Springer, New York,
NY, 2001.

Michael T. Rosenstein, James J. Collins, and Carlo J. De Luca. A practical method for calculating
largest Lyapunov exponents from small data sets. _Physica D: Nonlinear Phenomena, 65(1):_
[117–134, May 1993. ISSN 0167-2789. doi: 10.1016/0167-2789(93)90009-P. URL https:](https://www.sciencedirect.com/science/article/pii/016727899390009P)
[//www.sciencedirect.com/science/article/pii/016727899390009P.](https://www.sciencedirect.com/science/article/pii/016727899390009P)

T. Konstantin Rusch and Siddhartha Mishra. Coupled oscillatory recurrent neural network (coRNN):
An accurate and (gradient) stable architecture for learning long time dependencies. In Interna_[tional Conference on Learning Representations, 2021a. URL https://openreview.net/](https://openreview.net/forum?id=F3s69XzWOia)_
[forum?id=F3s69XzWOia.](https://openreview.net/forum?id=F3s69XzWOia)

T. Konstantin Rusch and Siddhartha Mishra. Unicornn: A recurrent model for learning very long
time dependencies. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
_Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp._
[9168–9178. PMLR, 18–24 Jul 2021b. URL https://proceedings.mlr.press/v139/](https://proceedings.mlr.press/v139/rusch21a.html)
[rusch21a.html.](https://proceedings.mlr.press/v139/rusch21a.html)

Eleonora Russo and Daniel Durstewitz. Cell assemblies at multiple time scales with arbitrary lag
constellations. eLife, 6:e19428, January 2017. ISSN 2050-084X. doi: 10.7554/eLife.19428.
[URL https://doi.org/10.7554/eLife.19428. Publisher: eLife Sciences Publica-](https://doi.org/10.7554/eLife.19428)
tions, Ltd.

Otto E. R¨ossler. An equation for continuous chaos. _Physics Letters A, 57(5):397–398, July_
1976. ISSN 0375-9601. doi: 10.1016/0375-9601(76)90101-8. [URL https://www.](https://www.sciencedirect.com/science/article/pii/0375960176901018)
[sciencedirect.com/science/article/pii/0375960176901018.](https://www.sciencedirect.com/science/article/pii/0375960176901018)

Salvador Sabanal and Masahiro Nakagawa. The fractal properties of vocal sounds and their
application in the speech recognition model. _Chaos, Solitons & Fractals, 7(11):1825–1843,_
November 1996. ISSN 0960-0779. doi: 10.1016/S0960-0779(96)00043-4. [URL https:](https://www.sciencedirect.com/science/article/pii/S0960077996000434)
[//www.sciencedirect.com/science/article/pii/S0960077996000434.](https://www.sciencedirect.com/science/article/pii/S0960077996000434)

Tim Sauer, James A. Yorke, and Martin Casdagli. Embedology. Journal of Statistical Physics, 65
[(3):579–616, November 1991. ISSN 1572-9613. doi: 10.1007/BF01053745. URL https:](https://doi.org/10.1007/BF01053745)
[//doi.org/10.1007/BF01053745.](https://doi.org/10.1007/BF01053745)

Gerwin Schalk, Dennis J. McFarland, Thilo Hinterberger, Niels Birbaumer, and Jonathan R.
Wolpaw. BCI2000: a general-purpose brain-computer interface (BCI) system. _IEEE trans-_
_actions on bio-medical engineering, 51(6):1034–1043, June 2004._ ISSN 0018-9294. doi:
10.1109/TBME.2004.827072.

Dominik Schmidt, Georgia Koppe, Zahra Monfared, Max Beutelspacher, and Daniel Durstewitz. Identifying nonlinear dynamical systems with multiple time scales and long-range de[pendencies. In International Conference on Learning Representations, 2021. URL https:](https://openreview.net/forum?id=_XYzwxPIQu6)
[//openreview.net/forum?id=_XYzwxPIQu6.](https://openreview.net/forum?id=_XYzwxPIQu6)

Bellie Sivakumar. Chaos theory in geophysics: past, present and future. _Chaos,_
_Solitons & Fractals,_ 19(2):441–462, January 2004. ISSN 0960-0779. doi: 10.
1016/S0960-0779(03)00055-9. [URL https://www.sciencedirect.com/science/](https://www.sciencedirect.com/science/article/pii/S0960077903000559)
[article/pii/S0960077903000559.](https://www.sciencedirect.com/science/article/pii/S0960077903000559)

Floris Takens. Detecting strange attractors in turbulence. In David Rand and Lai-Sang Young
(eds.), Dynamical Systems and Turbulence, Warwick 1980, pp. 366–381, Berlin, Heidelberg,
1981. Springer Berlin Heidelberg. ISBN 978-3-540-38945-3.


-----

Robert Tchitnga, B. Anicet Mezatio, Theophile Fonzin Fozin, Romanic Kengne, Patrick H.
Louodop Fotso, and Anaclet Fomethe. A novel hyperchaotic three-component oscillator operating at high frequency. Chaos, Solitons & Fractals, 118:166–180, January 2019. ISSN 09600779. doi: 10.1016/j.chaos.2018.11.015. [URL https://www.sciencedirect.com/](https://www.sciencedirect.com/science/article/pii/S0960077918303047)
[science/article/pii/S0960077918303047.](https://www.sciencedirect.com/science/article/pii/S0960077918303047)

David J. Thomson. Time series analysis of Holocene climate data. Philosophical Transactions of
_the Royal Society of London. Series A, Mathematical and Physical Sciences, 330(1615):601–616,_
[April 1990. doi: 10.1098/rsta.1990.0041. URL https://royalsocietypublishing.](https://royalsocietypublishing.org/doi/abs/10.1098/rsta.1990.0041)
[org/doi/abs/10.1098/rsta.1990.0041. Publisher: Royal Society.](https://royalsocietypublishing.org/doi/abs/10.1098/rsta.1990.0041)

Trieu H. Trinh, Andrew M. Dai, Minh-Thang Luong, and Quoc V. Le. Learning longer-term de[pendencies in rnns with auxiliary losses, 2018. URL https://openreview.net/forum?](https://openreview.net/forum?id=Hy9xDwyPM)
[id=Hy9xDwyPM.](https://openreview.net/forum?id=Hy9xDwyPM)

Adam P. Trischler and Gabriele M.T. D’Eleuterio. Synthesis of recurrent neural networks for dynamical system simulation. Neural Networks, 80:67–78, 2016. ISSN 08936080. doi: 10.1016/
[j.neunet.2016.04.001. URL https://linkinghub.elsevier.com/retrieve/pii/](https://linkinghub.elsevier.com/retrieve/pii/S0893608016300314)
[S0893608016300314.](https://linkinghub.elsevier.com/retrieve/pii/S0893608016300314)

Peter Turchin and Andrew D. Taylor. Complex dynamics in ecological time series. Ecology, 73(1):
289–305, 1992. ISSN 1939-9170. doi: 10.2307/1938740.

Eli Tziperman, Harvey Scher, Stephen E. Zebiak, and Mark A. Cane. Controlling spatiotemporal
chaos in a realistic El Ni˜no prediction model. Physical Review Letters, 79(6):1034–1037, Au[gust 1997. doi: 10.1103/PhysRevLett.79.1034. URL https://link.aps.org/doi/10.](https://link.aps.org/doi/10.1103/PhysRevLett.79.1034)
[1103/PhysRevLett.79.1034. Publisher: American Physical Society.](https://link.aps.org/doi/10.1103/PhysRevLett.79.1034)

Carl van Vreeswijk and Haim Sompolinsky. Chaos in neuronal networks with balanced excitatory and inhibitory activity. Science, 274(5293):1724–1726, December 1996. ISSN 0036-8075,
[1095-9203. doi: 10.1126/science.274.5293.1724. URL https://science.sciencemag.](https://science.sciencemag.org/content/274/5293/1724)
[org/content/274/5293/1724. Publisher: American Association for the Advancement of](https://science.sciencemag.org/content/274/5293/1724)
Science Section: Reports.

Pantelis R. Vlachas, Wonmin Byeon, Zhong Y. Wan, Themistoklis P. Sapsis, and Petros Koumoutsakos. Data-driven forecasting of high-dimensional chaotic systems with long short-term memory
networks. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences,
474(2213):20170844, 2018. ISSN 1364-5021, 1471-2946. doi: 10.1098/rspa.2017.0844. URL
[https://royalsocietypublishing.org/doi/10.1098/rspa.2017.0844.](https://royalsocietypublishing.org/doi/10.1098/rspa.2017.0844)

Pantelis R. Vlachas, Georgios Arampatzis, Caroline Uhler, and Petros Koumoutsakos. Learning the
Effective Dynamics of Complex Multiscale Systems. arXiv:2006.13431 [nlin, physics:physics],
[July 2020. URL http://arxiv.org/abs/2006.13431. arXiv: 2006.13431.](http://arxiv.org/abs/2006.13431)

Henning U. Voss, Jens Timmer, and J¨urgen Kurths. Nonlinear dynamical system identification from uncertain and indirect measurements. _International Journal of Bifurcation_
_and Chaos, 14(06):1905–1933, June 2004._ ISSN 0218-1274, 1793-6551. doi: 10.1142/
S0218127404010345. [URL https://www.worldscientific.com/doi/abs/10.](https://www.worldscientific.com/doi/abs/10.1142/S0218127404010345)
[1142/S0218127404010345.](https://www.worldscientific.com/doi/abs/10.1142/S0218127404010345)

Joseph H. M. Wedderburn. Lectures on Matrices. New York: American mathematical society, New
York : Dover Publications, 1964.

Ronald J. Williams and David Zipser. A learning algorithm for continually running fully recurrent
neural networks. Neural Computation, 1(2):270–280, June 1989. ISSN 0899-7667, 1530-888X.
[doi: 10.1162/neco.1989.1.2.270. URL https://direct.mit.edu/neco/article/1/](https://direct.mit.edu/neco/article/1/2/270-280/5490)
[2/270-280/5490.](https://direct.mit.edu/neco/article/1/2/270-280/5490)

Simon N. Wood. Statistical inference for noisy nonlinear ecological dynamic systems. Nature,
[466(7310), August 2010. ISSN 1476-4687. doi: 10.1038/nature09319. URL https://www.](https://www.nature.com/articles/nature09319)
[nature.com/articles/nature09319.](https://www.nature.com/articles/nature09319)

Huaguang Zhang, Derong Liu, and Zhiliang Wang. Controlling Chaos. Springer, London, 2009.


-----

A APPENDIX

A.1 THEOREMS: PRELIMINARIES

A.1.1 TRANSFORMING NON-AUTONOMOUS INTO AUTONOMOUS DISCRETE-TIME DS

Following (Zhang et al., 2009), and based on similar reasoning as for continuous time (ODE-based)
DS (Alligood et al., 1996; Perko, 2001), let us consider the non-autonomous discrete-time DS
**_xt+1 = F_** (xt, t), **_x ∈_** R[n]. (18)

Defining zt = (xt, t)[T] and G(zt) = (F (xt, t), t + 1)[T], system (18) can be rewritten as the
autonomous system
**_zt+1 = G(zt),_** **_z ∈_** R[n][+1]. (19)
Hence, in all our theoretical treatment we can confine our attention to systems of the form eqn. 18.

A.1.2 RNN DERIVATIVES

Considering the loss function = _t=1_
_L_ _[L][t][ of an RNN]T_ _[ F][θ][ ∈R][ parameterized by][ θ][, we have]_

[P][T] _∂L_ _∂Lt_ (20)

_∂θ_ [=] _∂θ [,]_

_t=1_

X


where
_∂_ _t_
_L_ = _[∂][L][t]_

_∂θ_ _∂zt_


_∂∂θ zt_ _[.]_ (21)


The tangent vector _[∂]∂θ[z][T]_ [has the form]

_∂∂θzT_ = _[∂][+]∂θ[z][T]_ + _T −2_ _t−1_ **_JT −r_** _∂+∂θzT −t_ _,_ (22)

_t=1_  _r=0_ 

X Y

where ∂[+] denotes the immediate partial derivative. Since for an RNN Fθ the activation
function is element-wise, with θ the m-th element of a parameter vector θ (or belonging to the ∈R _m-th_
row of a parameter matrix θ), we have
_∂[+]∂θzT_ = 0 _· · ·_ 0 _∂[+]∂θzm,T_ 0 _· · ·_ 0 T _._ (23)

 

For instance, let θ = W be a weight matrix, then


_∂w∂L11_ _∂w∂L12_ _∂w∂L1M_

_· · ·_

_∂w∂L21_ _∂w∂L22_ _∂w∂L2M_

_· · ·_


_∂L_

_∂W_ [=]


(24)


 _∂w∂LM_ 1 _∂w∂LM_ 2 _· · ·_ _∂w∂MML_ 
 

In this case, for the standard RNN we have
_∂[+]zT_ = (0 0 _zk,T_ 1 ξmk(zT 1) 0 0)[T] = 1(m,k) ξmk(zT 1) zT 1, (25)
_∂wmk_ _· · ·_ _−_ _−_ _· · ·_ _−_ _−_

where ξmk(zT −1) = fw[′] _m,k_ _Mj=1_ _[w][mj][ z][j,T][ −][1][ +][ P]j[M]=1_ _[b][mj][ s][j,T][ +][ h][m]_, and fw[′] _m,k_ [stands for]
the derivative of f with respect to wm,k.
  P 

Therefore, for standard RNNs, (22) becomes

_∂zT_ _T −2_ _t−1_

= 1(m,k) ξmk(zT 1) zT 1 + **_JT_** _r_ **1(m,k) ξmk(zT** _t_ 1) zT _t_ 1. (26)
_∂wmk_ _−_ _−_ _−_ _−_ _−_ _−_ _−_

_t=1_  _r=0_ 

X Y


-----

A.1.3 PIECEWISE-LINEAR RNN (PLRNN)

The PLRNN has the generic form (Koppe et al., 2019; Schmidt et al., 2021)

**_zt = F_** (zt 1) = A zt 1 + W ϕ(zt 1) + Cst + h + εt, (27)
_−_ _−_ _−_

where ϕ(zt 1) = max(zt 1, 0) is the element-wise rectified linear unit (ReLU) function, zt R[M]
is the neural state vector,− **_A− ∈_** R[M] _[×][M]_ is a diagonal matrix of auto-regression weights, W ∈ R ∈[M] _[×][M]_
is a matrix of connection weights,by C ∈ R[M] _[×][K], and εt ∼N_ (0, Σ h) ∈ a Gaussian noise term with diagonal covariance matrixR[M] is the bias vector, st ∈ R[K] the external input weighted Σ.

Equation (27) can be rewritten as

**_zt = (A + W DΩ(t_** 1))zt 1 + Cst + h + εt =: WΩ(t 1) zt 1 + Cst + h + εt, (28)
_−_ _−_ _−_ _−_

where DΩ(t) := diag(dΩ(t)) is a diagonal matrix with dΩ(t) := (d1, d2, · · ·, dM ) an indicator
vector such that dm(zm,t) =: dm = 1 whenever zm,t > 0, and zeros otherwise.

For the PLRNN (28) we have

_∂zt_
**_Jt =_** = WΩ(t 1), (29)

_∂zt−1_ _−_


and **_WΩ(t−1)_** _≤∥A∥_ + ∥W ∥.

Furthermore, the derivatives (22) for the PLRNN (28) are


_j−1_

**_WΩ(T −i)_**

 _i=1_
Y


_T −1_

_j=2_

X


_∂zT_

= 1(m,k)DΩ(T 1) zT 1 +
_∂wmk_ _−_ _−_


**1(m,k)DΩ(T** _j) zT_ _j._ (30)
_−_ _−_


A.1.4 LONG SHORT-TERM MEMORY (LSTM)

The LSTM is defined by the equations

**_it = σ_** **_Wiist + Whiht_** 1 + bi
_−_

**_ft = σ Wif_** **_st + Whf_** **_ht_** 1 + bf
_−_

**_gt = tanh _** **_Wigst + Whght_** 1 + bg
_−_

**_ot = σ_** **_Wio _** **_st + Whoht_** 1 + bo 
_−_

**_ct = ft _** **_ct_** 1 + it **_gt_** 
_⊙_ _−_ _⊙_

**_ht = ot ⊙_** tanh (ct) (31)

where {st} is the input sequence, W denotes weight matrices, b bias terms, it, ft, gt, ot demonstrate the input, forget, cell, and output gates, ht and ct are the hidden and cell states at time t
respectively, σ is the sigmoid activation function, and ⊙ represents the element-wise (Hadamard)
product (see (Hochreiter & Schmidhuber, 1997; Graves et al., 2016; Vlachas et al., 2018) for further
information on LSTMs).


Defining zt := (ht, ct)[T], the LSTM (31) can be represented as the first-order recursive map

**_ot_** tanh (ft **_ct_** 1 + it **_gt)_**
**_zt = Fθ(zt_** 1) = _⊙_ _⊙_ _−_ _⊙_ _._
_−_ **_ft_** **_ct_** 1 + it **_gt_**
 _⊙_ _−_ _⊙_ 


(32)


-----

The term _[∂]∂θ[L][t]_ [in (20) for some LSTM parameter][ θ][ can be written as]


_∂_ _t_
_L_

_∂θ_


_∂_ _t_
_L_
_∂ht_


_∂ht_

_∂zt_


_∂_ _t_
_L_

_∂zt_


_∂zt_
_∂zr_


_∂∂θ zr_ _[.]_ (33)


_r=1_


A necessary condition for LSTMs to have a chaotic orbit is given by:

**Proposition 1. Let the LSTM given by (31) have a chaotic attractor Γ[∗]** _with_ Γ∗ _as its basin of_
_B_
_attraction. Then for every z1 = (h1, c1)[T]_ Γ∗
_∈B_


_∂hT_ _∂hT_

_∂h1_ _∂c1_

_∂cT_ _∂cT_
_∂h1_ _∂c1_


_> 1._ (34)


_γ :=_ lim
_T →∞_


_Proof. The Jacobian matrix of (32) for t > 1 can be written in the block form_


_∂ht_ _∂ht_

_∂ht−1_ _∂ct−1_

_∂ct_ _∂ct_

_∂ht−1_ _∂ct−1_


_∂zt_

= Jt =
_∂zt−1_

Further, due to the chain rule, we have


_._ (35)






_∂ht_ _∂ht−1_ _∂ht_ _∂ct−1_ _∂ht_ _∂ht−1_ _∂ht_ _∂ct−1_

_∂ht−1_ _∂ht−2_ [+] _∂ct−1_ _∂ht−2_ _∂ht−1_ _∂ct−2_ [+] _∂ct−1_ _∂ct−2_

_∂ct_ _∂ht−1_ _∂ct_ _∂ct−1_ _∂ct_ _∂ht−1_ _∂ct_ _∂ct−1_

_∂ht−1_ _∂ht−2_ [+] _∂ct−1_ _∂ht−2_ _∂ht−1_ _∂ct−2_ [+] _∂ct−1_ _∂ct−2_


_Jt Jt_ 1 =
_−_


_∂ht_ _∂ht_

_∂ht−2_ _∂ct−2_

=

 _∂ct_ _∂ct_

_∂ht−2_ _∂ct−2_



and by induction we obtain


_,_ (36)






_∂ht_ _∂ht_
_∂h1_ _∂c1_

_∂ct_ _∂ct_

_∂h1_ _∂c1_


_∂zt_
= Jt Jt 1 Jt 2 _J2 =_
_∂z1_ _−_ _−_ _· · ·_

Now assume that (32) has a chaotic orbit given by


_._ (37)






Γ[∗] = {z1[∗][,][ z]2[∗][,][ · · ·][,][ z]T[∗] _[,][ · · · }][.]_ (38)

According to (37), the largest Lyapunov exponent of Γ[∗] is given by


_∂h[∗]T_

_∂h[∗]1_

_∂c[∗]T_

_∂h[∗]1_


_∂h[∗]T_

_∂c[∗]1_

_∂c[∗]T_

_∂c[∗]1_ _[,]_


1

_JT[∗]_ _[J]T[∗]_ 1 2 = lim
_T_ [ln] _−_ _[· · ·][ J]_ _[∗]_ _T →∞_


_λΓ[∗]_ = lim
_T →∞_


_T_ [ln]


Since Γ[∗] is chaotic, so λΓ∗ _> 0, which gives_

lim _T_
_T →∞_ vu 

u
u
t 


_∂h[∗]T_

_∂h[∗]1_

_∂c[∗]T_

_∂h[∗]1_


_∂h[∗]T_

_∂c[∗]1_

_∂c[∗]T_

_∂c[∗]1_


_> 1._ (39)


Based on Oseledec’s multiplicative ergodic Theorem, (39) holds for every z1 Γ∗ . This completes the proof. _∈B_


-----

A.1.5 GATED RECURRENT UNIT (GRU)

A GRU network is defined by the equations

**_zt = σ_** **_Wz st + Uzht_** 1 + bz
_−_

**_rt = σ Wr st + Urht_** 1 + br 
_−_

**_ht = (1 _** **_zt)_** tanh **_Wh st + Uh(rt_** **_ht_** 1) + bh + zt **_ht_** 1, (40)
_−_ _⊙_ _⊙_ _−_ _⊙_ _−_
  

where rt represents the reset gate, zt the update gate, st and ht denote the inputs and the hidden state
Rrespectively,[M] are bias vectors, and Wz, Wr, Wh σ ∈ is the element-wise logistic sigmoid function (for more details aboutR[M] _[×][N]_ and Uz, Ur, Uh ∈ R[M] _[×][M]_ are weight matrices, bz, br, bh ∈
GRUs see Cho et al. (2014a)).

A.1.6 UNITARY EVOLUTION RNN (URNN)

The uRNN, proposed in (Arjovsky et al., 2016), is defined as the nonlinear DS

**_zt = σb_** **_W zt_** 1 + V st _,_ (41)
_−_
  

for which W ∈ _U_ (M ) is an unitary matrix, V ∈ C[M] _[×][N]_, b ∈ R[M] is the bias parameter, st is the
real- or complex-valued input of dimension N, and


_zi_ + bi _zzii_ if _zi_ + bi 0
_|_ _|_ _|_ _|_ _|_ _|_ _≥_ _._ (42)
( 0  if |zi| + bi < 0



[σb(z)]i = [σmodReLU(z)]i =


**Proposition 2. The uRNN given by (41) cannot have any chaotic orbit.**

_Proof. For any arbitrary orbit Oz1 of (41) we have_


_T −2_

**_DT_** _k W_ [T] (43)
_−_ _[,]_
_k=0_

Y


_JT JT_ 1 _J2_ =
_∥_ _−_ _· · ·_ _∥_


where Dt = diag _σb[′]_ **_W zt−1 + V st_** . Since W is unitary and so a norm preserving matrix, it
is concluded that    []


_T −2_

**_DT_** _k W_ [T]
_−_
_k=0_

Y


_T −2_

_k=0_

Y


_T −2_

_∥DT −k∥_ = 1, (44)
_k=0_

Y


**_DT_** _k W_ [T]
_−_


which implies


1

(45)
_T_ [ln][ ∥][J][T][ J][T][ −][1][ · · ·][ J][2][∥≤] [0][.]


_λmax =_ lim
_T →∞_


This rules out the existence of chaos (since λmax > 0 is a necessary condition for Oz1 to be
chaotic).

Note that, more generally, any RNN which is constrained such as to exhibit global convergence
to a fixed point or cycle, by definition must have a maximum Lyapunov exponent λmax 0 (in
accordance with Theorem 1), hence cannot exhibit chaotic behavior by definition. _≤_


-----

A.2 THEOREMS: PROOFS

A.2.1 PROOF OF THEOREM 1, PARTS (II) & (III)

_Proof. (ii) If J is the Jordan normal form of_ _s=0_ _[J][t][∗][k][−][s]_ [, then][ Q]s[k]=0[−][1] _[J][t][∗][k][−][s]_ [=][ P J P][ −][1][,]
where

[Q][k][−][1]

**_Jm1_** (λ1) 0 0 0

_· · ·_
0 **_Jm2_** (λ2) 0 0

_· · ·_

 . . 

**_J =_** .. _. . ._ ... _. . ._ .. _,_ (46)

 0 _. . ._ 0 **_Jmp_** 1 (λp 1) 0 
 _−_ _−_ 
 0 _. . ._ _. . ._ 0 **_Jmp_** (λp)
 
 

and mi is the algebraic multiplicity of each eigenvalue λi. Since ρ _sk=0−1_ _[J][t][∗][k][−][s]_ _< 1, so the_
eigenvalue λi associated with each Jordan block satisfies _λi_ _< 1 (i = 1,_ _, p). Moreover, every_
_|_ _|_   Q · · · 
_mi × mi Jordan block has the form_

_λi_ 1 0 0

_· · ·_
0 _λi_ 1 0

 . . _· · ·_ . 

**_Jmi_** (λi) = .. .. ... ... .. _._ (47)

 0 0 _. . ._ _λi_ 1 
 
 0 0 _. . ._ 0 _λi_
 
 


Accordingly

_k−1_ _j_

_Jt∗k−s_ [=] **_P J_** _[j]_ **_P_** _[−][1]_ _≤_ _p_ **_J_** _[j]_ _,_ (48)

 _s=0_ 
Y

in which p = ∥P ∥ **_P_** _[−][1]_ . Furthermore, for j ∈ N, J _[j]_ is a block diagonal matrix of the form

**_Jm[j]_** 1 [(][λ][1][)] 0 0 _· · ·_ 0

 0. **_Jm[j]_** 2 [(][λ][2][)] 0 _· · ·_ 0. 

**_J_** _[j]_ = .. _. . ._ ... _. . ._ .. _,_ (49)

 
 0 _. . ._ 0 **_Jm[j]_** _p_ 1 [(][λ][p][−][1][)] 0 
 _−_ 
 0 _. . ._ _. . ._ 0 **_Jm[j]_** _p_ [(][λ][p][)]
 
 

in which every mi × mi Jordan block has the form

_λ[j]i_ 1j _λ[j]i_ _[−][1]_ 2j _λ[j]i_ _[−][2]_ _mij_ 1 _λ[j]i_ _[−][m][i][+1]_

_· · ·_ _−_

 0   λ[j]i  1j _λ[j]i_ _[−][1]_  mij 2 _λ[j]i_ _[−][m][i][+2]_

_· · ·_ _−_
. . .

**_Jm[j]_** _i_ [(][λ][i][) =]  .. ..   ... ...    ..  _._ (50)

 _j_ 
 0 0 _. . ._ _λ[j]i_ 1 _λ[j]i_ _[−][1]_ 
 
 0 0 _. . ._ 0   λ[j]i 
 


In addition, for every block Jm[j] _i_ [(][λ][i][)][, we have]


_mi_

**_Jm[j]_** _i_ [(][λ][i][)] _≤_ _[√]mi_ **_Jm[j]_** _i_ [(][λ][i][)] _∞_ [=][ √][m][i] _q=1_ **_Jm[j]_** _i_ [(][λ][i][)]

X  

_mi_ _j_

= _mi_ _λi_ = _λi_ _mi_

_[√]_ _q_ 1 _|_ _|[j][−][q][+1]_ _|_ _|[j][√]_

_q=1_  _−_ 

X


1q

_mi_
_λi_
_|_ _|[1][−][m][i]_

_q=1_

X


_λi_
_|_ _|[m][i][−][q]_


_q −_ 1


_mi_
_λi_ _λi_ =: _λi_ _j_ _[m][i]_ _Nλi_ _._ (51)
_|_ _|[1][−][m][i]_ _|_ _|[m][i][−][q]_ _|_ _|[j]_

_q=1_ 

X


_λi_ _j_ _[m][i][ √]mi_
_≤|_ _|[j]_


-----

Moreover, for any 1 < ˜ri < _|λ1i|_ [, there exists some][ l][i][ such that][ j][ m][i][ <][ ˜]ri[j] [for][ j][ ≥] _[l][i][. This means]_

for j _li_
_≥_
**_Jm[j]_** _i_ [(][λ][i][)] _≤_ _Nλi |r˜i λi|[j],_ (52)

such that |r˜i λi| = ˜ri|λi| < 1.

Besides, for J _[j]_ = Jm[j] 1 [(][λ][1][)][ ⊕] **_[J]m[j]_** 2 [(][λ][2][)][ ⊕· · · ⊕] **_[J]m[j]_** _p_ [(][λ][p][)]
**_J_** _[j]_ = max **_Jm[j]_** _i_ [(][λ][i][)] =: **_Jm[j]_** [(][λ][)] _._ (53)
1≤i≤p

Hence, from (48), (52) and (53), it is deduced that for j ≥ _l_

_[k][−][1]_ _j_

_Jt∗k−s_ _r λ|[j]_ =: ¯p r[j], (54)

 _sY=0_  _[≤]_ _[p N][λ][ |][˜]_

in which r = |r λ˜ _| < 1._

Furthermore, let for Γk

max **_JT[∗]_** = max _Jt∗k_ _s_ = ¯m,
_T ≥1_ _∥_ _[∥]_ 0≤s≤k−1 _∥_ _−_ _∥_

n o n o

max _∂[+]zT_ = max _∂[+]zt∗k−s_ = ξ,
_T ≥1_ _∂θ_ 0≤s≤k−1 _∂θ_

n o n o

max **_zT_** = max **_zt∗k_** _s_ = ¯q. (55)
_T ≥1_ _∥_ _∥_ 0≤s≤k−1 _∥_ _−_ _∥_

n o n o


Hence, defining z0 = 0, for this k-cycle

_∂zT_ _∂[+]zT_


_T −2_ _t−1_ **_JT[∗] −r_** _∂+∂θzT −t_

_t=1_  _r=0_ 

X Y

_T −1_ _t−1_ **_JT[∗] −r_** _∂+∂θzT −t_

_t=1_  _r=0_ 

X Y


_∂θ_


_∂θ_

_∂[+]zT_

=

_∂θ_

_≤_ _q ξ¯_ 1 +



_T −1_

_t=1_

X


_t−1_

**_JT[∗] −r_**
_r=0_

Y


(56)

_t−1_

**_Jkj[∗]_** _−r_
_r=0_

Y


On the other hand, for T = kj, from (54) and (55) we have


_kj−1_

_t=1_

X


_T −1_

_t=1_

X


_t−1_

**_JT[∗] −r_**
_r=0_

Y


_t−1_

**_Jkj[∗]_** _−r_
_r=0_

Y


_k−1_

_t=1_

X


_t−1_

**_Jkj[∗]_** _−r_
_r=0_

Y


2k−1

_t=k_

X


_t−1_

**_Jkj[∗]_** _−r_
_r=0_

Y


(j−1)k−1

_t=(Xj−2)k_


_kj−1_

_t=(Xj−1)k_


3k−1

_t=2k_

X


_t−1_

**_Jkj[∗]_** _−r_
_r=0_

Y


_t−1_

**_Jkj[∗]_** _−r_
_r=0_

Y



[+][ · · ·][ +]

_t=(_

_j_ _ik−1_


_k−1_

_t=1_

X


_t−1_

**_Jkj[∗]_** _−r_
_r=0_

Y


_t−1_

**_Jkj[∗]_** _−r_
_r=0_

Y


_i=2_


_t=(i−1)k_


_m¯_ + ¯m[2] + + ¯m[k][−][1][]
_· · ·_


_j_

_p¯_ 1 + ¯m + ¯m[2] + · · · + ¯m[k][−][1][] _r[i][−][1]._
_i=2_

X  


(57)


-----

Thus, considering _m¯_ + ¯m[2] + · · · + ¯m[k][−][1][] = M, it is deduced that
 

lim _∂zT_ _∂zkj_ _q ξ_ 1 + + p r[¯] (1 + M) = ¯ _<_ _,_ (58)
_T_ _∂θ_ [= lim]j _∂θ_ _M_ 1 _r_ _M_ _∞_
_→∞_ _→∞_ _[≤]_ [¯] _−_

  

which, by (21), implies _[∂]∂θ[L][T]_ will be bounded for T .

_→∞_

(iii) Consider the PLRNN given by (27), where for simplicity we ignore the external inputs and
noise terms. Let **_zt1_** _, zt2_ _, zt3_ _, . . ._ be an orbit which converges to Γk. Hence
_{_ _}_

lim (59)
_n→∞_ _[d][(][z][t][n]_ _[,][ Γ][k][) = 0][,]_

which implies there exists a neighborhood U of Γk and k sub-sequences {ztkm _}m[∞]=1[,][ {][z][t]km+1_ _[}][∞]m=1[,]_

_· · ·, {ztkm+(k−1)_ _}m[∞]=1_ [of the sequence][ {][z][t]n _[}][∞]n=1_ [such that all these sub-sequences belong to][ U][ and]

a) ztkm+s = F _[k](ztk(m−1)+s_ ), s = 0, 1, 2, · · ·, k − 1,

b) lim
_m→∞_ **_[z][t][km][+][s][ =][ z][t][∗][k][−][s][, s][ = 0][,][ 1][,][ 2][,][ · · ·][, k][ −]_** [1][,]

c) for every ztn ∈ _U there is some s ∈{0, 1, 2, · · ·, k −_ 1} such that ztn ∈{ztkm+s _}m[∞]=1[.]_

In this case, for everyztn = ztkn˜ +s [and][ lim]n˜ **_ztn ∈n+sU with[=][ z]t[∗] z[k]−tsn ∈{[. Therefore, continuity of]ztkm+s_** _}m[∞]=1_ [, there exists some][ F][ results in][ ˜]n ∈ N such that
_→∞_ **_[z][t][k]_** [˜]

lim _n+s_ [) =][ F] [(][z]t[∗][k] _s[)][,]_ (60)
_n˜_ _−_
_→∞_ _[F]_ [(][z][t][k] [˜]

and so by (28)

lim **_WΩ(tkn˜_** +s[)][ z][t]kn˜ +s [+][ h] = WΩ(t∗k _s) zt∗k_ _s + h,_ (61)
_n˜_ _−_ _−_
_→∞_
  

which implies

lim _n+s[)][ z][t]kn˜_ +s [=][ W]Ω(t[∗][k] _s)_ **_[z]t[∗][k]_** _s[.]_ (62)
_n˜_ _−_ _−_
_→∞_ **_[W][Ω(][t][k]_** [˜]

Assuming lim _n+s[)][ =][ L][, since (62) holds for every][ z][t][∗][k][−][s][, substituting][ z][t][∗][k][−][s]_ [=][ e][T]1 [=]
_n˜→∞_ **_[W][Ω(][t][k]_** [˜]

(1, 0, · · ·, 0)[T] in (62), we can prove that the first column of L equals the first column of WΩ(t∗k−s).
Performing the same procedure for zt∗k _s = e[T]i_ [,][ i][ = 2][,][ 3][,][ · · ·][, M] [, yields]
_−_

lim _n+s[)][ =][ W][Ω(][t][∗][k][−][s][)][.]_ (63)
_n˜→∞_ **_[W][Ω(][t][k]_** [˜]

According to (59), U contains an infinite number of terms of the sequence {ztn _}n[∞]=1[, i.e.]_

_∃N ∈_ N s.t. _n ≥_ _N =⇒_ **_ztn ∈_** _U._ (64)

Suppose that ztn _U for some n_ _N_ . Thus, there exists some s 0, 1, 2, _, k_ 1 such that
**_ztn ∈{ztkm+s_** _}m[∞] ∈=1[. Without loss of generality let] ≥_ _[ s][ = 0][. Hence, there is some] ∈{_ _· · ·_ [ ˜]n ∈ − N} such that


-----

**_ztn = ztkn˜_** [and][ lim]n˜→∞ **_[z][t][k]n[˜]_** [=][ z]t[∗][k] [. In this case, moving forward in time gives]

**_ztn = ztkn˜_** **_ztn_** **_ztkm_** _m=1_ _,_ lim _n_ [=][ z]t[∗][k] _[,]_
_∈{_ _}[∞]_ _n˜→∞_ **_[z][t][k]_** [˜]
  

**_ztn+1 = ztkn˜_** +1 **_ztn+1_** **_ztkm+1_** _m=1_ _,_ lim _n+1_ [=][ z]t[∗][k] 1[,]
_∈{_ _}[∞]_ _n˜→∞_ **_[z][t][k]_** [˜] _−_
  

**_ztn+2 = ztkn˜_** +2 **_ztn+2_** **_ztkm+2_** _m=1_ _,_ lim _n+2_ [=][ z]t[∗][k] 2[,]
_∈{_ _}[∞]_ _n˜→∞_ **_[z][t][k]_** [˜] _−_

.   
.
.

**_ztn+k−1 = ztkn˜_** +k−1 **_ztn+(k−1) ∈{ztkm+k−1_** _}m[∞]=1_ _,_ _n˜lim→∞_ **_[z][t][k]n[˜]_** +k−1 [=][ z]t[∗][k]−(k−1)[,]
  

**_ztn+k = ztk(˜n+1)_** **_ztn+k ∈{ztkm_** _}m[∞]=1_ _,_ _n˜lim→∞_ **_[z][t][k][(˜]n+1)_** [=][ z][t][∗][k] _[,]_
  

**_ztn+k+1 = ztk(˜n+1)+1_** **_ztn+k+1 ∈{ztkm+1_** _}m[∞]=1_ _,_ _n˜lim→∞_ **_[z][t][k][(˜]n+1)+1_** [=][ z][t][∗][k][−][1][,]

.   
.
.

**_ztn+2k−1 = ztk(˜n+1)+k−1_** **_ztn+2k−1 ∈{ztkm+k−1_** _}m[∞]=1_ _,_ _n˜lim→∞_ **_[z][t][k][(˜]n+1)+k−1_** [=][ z][t][∗][k][−][(][k][−][1)][,]
  

**_ztn+2k = ztk(˜n+2)_** **_ztn+2k ∈{ztkm_** _}m[∞]=1_ _,_ _n˜lim→∞_ **_[z][t][k][(˜]n+2)_** [=][ z][t][∗][k] _[,]_

.   
.
. (65)

Consequently, for n ≥ _N and j ∈_ N, we can write

_kj−1_

**_WΩ(tn+kj−1−i)_**
_i=0_

Y

_k_ _k_ _k_
= **_WΩ(tk(˜n+j)+k−i[)]_** **_WΩ(tk(˜n+j−1)+k−i[)]_** _· · ·_ **_WΩ(tk(˜n)+k−i[)]_**

_i=1_ _i=1_ _i=1_

 Y  Y   Y 


**_WΩ(tk(˜n+j_** _l)+k_ _i[)][.]_ (66)
_−_ _−_
_i=1_

Y


_l=0_


On the other hand, in equation (28), there are different configurations for matrix DΩ(t 1) and
_−_
hence different forms for matrix WΩ(tkn˜ +s[)][ . In this case, the phase space of the system is divided]
into different sub-regions by some borders; see (Monfared & Durstewitz, 2020a;b) for more details.
Also, since the system (28) is a linear map in each sub-region, the k periodic points of Γk must
belong to different sub-regions (at least two different sub-regions). Accordingly, based on (63) and
(65), there exists some _N[˜]_ N such that for every ˜n _N both ztkn˜_ +s [and][ z]t[∗][k] _s_ [belong to the]
_∈_ _≥_ [˜] _−_
same sub-region and so the matrices WΩ(tkn˜ +s[)][ and][ W][Ω(][t][∗][k][−][s][)] [(][s][ ∈{][0][,][ 1][,][ 2][,][ · · ·][, k][ −] [1][}][) are]
identical. Hence, for n ≥ _N_, ˜n ≥ _N[˜] and j ∈_ N, equation (66) becomes

_kj−1_ _j_ _k_ _k−1_ _j_

**_WΩ(tn+kj−1−i) =_** **_WΩ(tk(˜n+j−l)+k−i[)][ =]_** **_WΩ(t∗k−s)_** _._ (67)
_i=0_ _l=0_ _i=1_  _s=0_ 

Y Y Y Y

Therefore, similar to the part (ii), we can prove for every z1 Γk, _[∂]∂θ[z][T]_ and _[∂]∂θ[L][T]_ will also remain

bounded. _∈B_

A.2.2 PROOF OF THEOREM 2, PART (II)

_Proof. (ii) Let for every T > 2_

**_LT := JT[∗]_** _[J]T[∗]_ 1 2 _[.]_ (68)
_−_ _[· · ·][ J]_ _[∗]_


-----

_{LT }T ∈N, T >2 is a sequence of matrices LT_ = _lij[(][T][ )]_ 1≤i,j≤M and, due to (13),

limT →∞ _∥LT ∥_ = _∞._ Hence, there is at least one sub-sequence  _{lmk[(][T][n][)][}][T][n][∈][N][, T][n][>][2][ (for]_
some m, k ∈{1, 2, · · ·, M _}) such that limTn→∞_ _lmk[(][T][n][)]_ = ∞.

On the other hand


_t−1_ **_JT[∗] −r_** _∂+∂θzT∗ −t_ _._ (69)
 _r=0_ 
Y


_T −2_

_t=1_

X


_∂zT[∗]_ = _[∂][+][z]T[∗]_

_∂θ_ _∂θ_


Moreover, there exists some N > 2 such that (for t = T − _N + 1)_

_∂[+]zN[∗]_ _−1_ = 0. (70)

_∂θ_ _̸_

For θ as the k-th element of a parameter vector θ (or belonging to the k-th row of a parameter matrix
**_θ), the term_**
_T −N_ **_JT[∗] −r_** _∂+z∂θN∗_ _−1_ (71)
 _r=0_ 
Y

is a vector in which the i-th element is lik[(][T][ )] _∂[+]z∂θk,N[∗]_ _−1_ .


Since limTn→∞ _lmk[(][T][n][)]_ = ∞, due to (70) limTn→∞ _lmk[(][T][n][)]_ _∂[+]z∂θk,N[∗]_ _−1_ = ∞, which implies _[∂]∂θ[z]T[∗]_ [will]

_T_
diverge as T →∞. Similarly, by (21), we can prove _[∂]∂θ[L][∗]_ [is divergent for][ T][ →∞][.]

By Oseledec’s multiplicative ergodic Theorem, the results also hold for every z1 Γ∗ .
_∈B_

A.2.3 PROOF OF THEOREM 3

_Proof. Let Γ = {z1, z2, . . . zT, · · · } be a quasi-periodic attractor. Then, the largest Lyapunov_
exponent of Γ is


1 _∂zT_

_T_ [ln] _∂z1_



[= 0][.] (72)


lim
_T_ [ln][ ∥][J][T][ J][T][ −][1][ · · ·][ J][2][∥] [=] _T_
_→∞_


_λ =_ lim
_T →∞_


We prove for every 0 < ϵ < 1

lim lim
_T →∞[(1][ −]_ _[ϵ][)][T][ −][1][ <]_ _T →∞_

For this purpose, we show ∀ 0 < ϵ < 1


_∂zT_

_∂z1_


lim (73)
_T →∞[(1 +][ ϵ][)][T][ −][1][.]_


(I) limT →∞(1 − _ϵ)[T][ −][1]_ _< limT →∞_ _[∂]∂[z]z[T]1_, and

(II) limT →∞ _[∂]∂[z]z[T]1_ _< limT →∞(1 + ϵ)[T][ −][1]._

Assume for the sake of contradiction that (I) does not hold. Then there exists some 0 < ϵ < 1 such
that


_∂zT_

_∂z1_


lim lim
_T →∞[(1][ −]_ _[ϵ][)][T][ −][1][ ≥]_ _T →∞_


(74)

(75)


Therefore


_∂zT_
_T0 > 1 s.t._ _T_ _T0 =_ (1 _ϵ)[T][ −][1]_
_∃_ _∀_ _≥_ _⇒_ _−_ _≥_ _∂z1_


-----

and so

_T0 > 1 s.t._ _T_ _T0 =_ ln _[∂]∂[z]z[T]1_ _._ (76)
_∃_ _∀_ _≥_ _⇒_ [ln(1]T[ −] _[ϵ][)]1[T][ −][1]_ _≥_ _T_ 1

_−_ _−_

Consequently, due to (72), for T →∞ we have ln(1 − _ϵ) ≥_ 0. This implies ϵ ≤ 0, which is a
contradiction.


Similarly if we assume (II) is not true, then there exists some 0 < ϵ < 1 such that


_∂zT_

_∂z1_


lim (77)
_T →∞[(1 +][ ϵ][)][T][ −][1][.]_


lim
_T →∞_


Thereby

and thus


_∂zT_
_T0 > 1 s.t._ _T_ _T0 =_
_∃_ _∀_ _≥_ _⇒_ _∂z1_



_[≥]_ [(1 +][ ϵ][)][T][ −][1][,] (78)


_T0 > 1 s.t._ _T_ _T0 =_ ln _[∂]∂[z]z[T]1_ _._ (79)
_∃_ _∀_ _≥_ _⇒_ _T_ 1 _≥_ [ln(1 +]T _[ ϵ][)]1[T][ −][1]_

_−_ _−_

This means ln(1 + ϵ) ≤ 0 as T →∞, i.e. ϵ ≤ 0, which is a contradiction.

Therefore (14) holds for Γ and also, according to Oseledec’s multiplicative ergodic Theorem, for
every z1 in the basin of attraction of Γ.

A.3 ADDITIONAL RESULTS ON RELATION BETWEEN DYNAMICS AND GRADIENTS

A.3.1 FURTHER RESULTS AND REMARKS RELATED TO THEOREM 2

**Remark 4. The result of Theorem 2 also holds for unstable orbits {z1, z2, z3, · · · } with positive**
_largest Lyapunov exponent. Trivially, for such orbits that diverge to infinity (unbounded latent states)_
_gradients of the loss function will explode as T →∞._
**Remark 5. For RNNs with ReLU activation functions there are finite compartments in the phase**
_space each with a different functional form. In such a case, to define the largest Lyapunov exponent_
_of Γ[∗], in the proof of Theorem 2 we assume that Γ[∗]_ _never maps to the points of the borders._

Based on Theorem 2, we can also formulate the necessary conditions for chaos and diverging gradients in standard RNNs with particular activation functions by considering the norms of their recurrence matrix, for which the following Corollary provides the basis:
**Corollary 1. Let for a standard RNN**
_diag_ _f_ _[′][ ]W zt−1 + Bst + h_ _≤_ _γ < ∞._ (80)

_If the RNN is chaotic, then_ **_W _** _γ > 1 ._ 
_∥_ _∥_

_Proof. Assume for the sake of contradiction that ∥W ∥_ _γ ≤_ 1 . From

**_W diag_** _f_ _[′][ ]W zt−1 + Bst + h_ _≤_ **_W diag_** _f_ _[′][ ]W zt−1 + Bst + h_
2<tY≤T    2<tY≤T   

_≤_ (∥W ∥ _γ)[T][ −][2],_ (81)

it is concluded that limT →∞ 2<t≤T **_[W][ diag]_** _f_ _[′][ ]W zt−1 + Bst + h_ _< ∞_, which con
tradicts (13). This means **_W_** _γ > 1 is a necessary condition for the standard RNN to be_

  

_∥_ _∥_
chaotic. [Q]

**Remark 6. For RNN with the tanh and sigmoid activation functions γ = 1 and γ =** [1]4 _[, respectively.]_

_Thus, by Corollary 1, the necessary conditions for chaos in these two cases are ∥W ∥_ _> 1 and_
_∥W ∥_ _> 4, respectively._


**_W diag_** _f_ _[′][ ]W zt−1 + Bst + h_
2<tY≤T   


2<t≤T


-----

A.3.2 OTHER CONNECTIONS BETWEEN DYNAMICS AND GRADIENTS

As sect. 3 elucidated, there is a direct link between the norms of the Jacobians of the RNN along
trajectories and the EVGP. By observing this link, we can formulate some general conditions that
will have implications for the behavior of the gradients regardless of the limiting behavior of the
RNN, as collected in the following theorem:
**Theorem 4.parameterized by Let O θz, and1 = P {Tz :=1, z2 J, . . .T − zTI,, T · · · } = 2 be a sequence (orbit) generated by an RNN, 3, · · · .** _Fθ ∈R_

_(i) Assume that Oz1 is an orbit for which_ _[∂][+]∂θ[z][T]_ _≤_ _ξ ∀t. If_ _T =2_ _[∥][J][T][ ∥]_ _[<][ ∞][, then the]_

_Jacobian_ _[∂]∂[z]z[T]1_ _[, the tangent vector][ ∂]∂θ[z][T]_ _[and thus the gradient of the loss function,][ ∂]∂θ[L][T]_ _[, will]_

_be bounded for T →∞._ [P][∞]

_(ii) If_ _T =2_ _[∥][P][T][ ∥]_ _[<][ ∞][, then the Jacobian][ ∂]∂[z]z[T]1_ _[will neither vanish nor explode as][ T][ →∞][.]_

_Proof. Let_ [P]. _[∞]be any matrix norm satisfying_ **_A1A2_** **_A1_** **_A2_** .
_∥_ _∥_ _∥_ _∥≤∥_ _∥∥_ _∥_

(i) By boundedness of _[∂][+]∂θ[z][T]_ we have

_∂∂θzT_ [=] _∂[+]∂θzT_ + _T −2_ _t−1_ **_JT −r_** _∂+∂θzT −t_

_t=1_  _r=0_ 

X Y

_T −2_ _t−1_ _T −2_ _t−1_

_≤_ _ξ_ 1 + **_JT −r_** _≤_ _ξ_ 1 + _∥JT −r∥_ _._ (82)
 _t=1_ _r=0_   _t=1_ _r=0_ 

X Y X Y


Moreover,


_T −2_

_t=1_

X


_t−1_

_∥JT −r∥≤_ 1 +
_r=0_

Y


**_Jp_** +
_∥_ _∥_


**_Jp_** **_Jq_** +
_∥_ _∥∥_ _∥_
_p<q_

X


**_Jp_** **_Jq_** **_Jr_** +
_∥_ _∥∥_ _∥∥_ _∥_ _· · ·_
_p<q<r_

X


1 +


_T_

= 1 + ∥JT ∥ 1 + ∥JT −1∥ _· · ·_ 1 + ∥J2∥ =: _t=2_ 1 + ∥Jt∥ _. (83)_
        Y   

Since _T =2_ _T =2_ 1 +

_[∥][J][T][ ∥]_ [converges, according to (Wedderburn, 1964), the infinite products][ Q][∞]
**_JT_** in (83) converge to a finite number [˜] = 0. Consequently, by (82) and (83)
_∥_ _∥_ _K ̸_  
[P][∞] _∂zT_

lim _<_ _,_ (84)
_T →∞_ _∂θ_ _[≤]_ _K[˜]_ _∞_

which implies _[∂]∂θ[L][T]_ [will be bounded for][ T][ →∞][.]

Furthermore


_∂zT_ _∞_ _∞_

_Tlim →∞_ _∂z1_ _[≤]_ _T =2_ _∥JT ∥_ := _Tlim →∞_  _∥JT ∥∥JT −1∥· · · ∥J2∥_  _≤_ _T =2_ 1 + ∥JT ∥ _≤_ _K[˜],_

Y Y    (85)

which completes the proof.

(ii) Since _T =1_

_[∥][P][T][ ∥]_ _[<][ ∞][, due to (Wedderburn, 1964) the infinite product]_

_∞_ _∞_

[P][∞] _T =2_ **_I + PT_** = _T =2_ **_JT :=_** _Tlim →∞_ **_[J][T][ J][T][ −][1][ · · ·][ J][2][,]_** (86)

Y    Y


converges to a matrix K ̸= O, which implies

_∂zT_

0 < lim
_T →∞_ _∂z1_



[=][ ∥][K][∥] _[<][ ∞][.]_ (87)


-----

Part (i) of Theorem 4 relaxes some of the conditions required in Theorem 1 for bounded gradients
by imposing a Lipschitz condition on the immediate derivatives. Part (ii) generalizes conditions
satisfied, for instance, in orthogonal (unitary) RNNs (Arjovsky et al., 2016; Henaff et al., 2016) or
fully regularized PLRNNs (Schmidt et al., 2021).

**Proposition 3. Let Oz1 = {z1, z2, . . . zT, · · · } be an orbit generated by an RNN Fθ ∈R (param-**
_eterized by θ), and_ **_JT_** = 0, T 2. If _T =2_ [ln][ ∥][J][T][ ∥] _[diverges to][ −∞][, then the Jacobian][ ∂]∂[z]z[T]1_
_∥_ _∦_ _≥_

_vanishes as T tends to infinity._

[P][∞]

_Proof. For_ **_JT_** = 0, T 2, we have
_∥_ _∦_ _≥_

0 _∂zT_ _Tt=2_ [ln][ ∥][J][t][∥].
_≤_ _∂z1_ _[≤∥][J][T][ ∥∥][J][T][ −][1][∥· · · ∥][J][2][∥]_ [=][ e][ln][ ∥][J][T][ ∥] _[e][ln][ ∥][J][T][ −][1][∥]_ _[· · ·][ e][ln][ ∥][J][2][∥]_ [=][ e]P

(88)

Hence if _T =2_ [ln][ ∥][J][T][ ∥→−∞][, then]

_∂zT_
lim = O. (89)

[P][∞] _T →∞_ _∂z1_


A.4 EMPIRICAL EVALUATION: DATASETS

**Lorenz attractor** The Lorenz system (Lorenz, 1963) is a simplified model for atmospheric convection, given by

dx

dt [=][ σ][(][y][ −] _[x][)][,]_


dy

(90)
dt [=][ x][(][ρ][ −] _[z][)][ −]_ _[y,]_

dz

dt [=][ xy][ −] _[βz.]_


The system is of particular interest for its chaotic regime and was studied here for σ = 16, ρ = 45.92
and β = 4. For these parameters the Lorenz system is known to have a maximal Lyapunov exponent
_λmax = 1.5 (Rosenstein et al., 1993). To generate a time series, the ODEs were integrated with_
a step size ∆t = 0.01 using scipy.integrate. Accordingly, the prediction time is τpred =
ln(2)

∆t λmax [= 46][.][2][.]

**Duffing oscillator** The Duffing oscillator (Duffing, 1918) is an example of a periodically forced
oscillator with nonlinear elasticity

_x¨ + δ ˙x + βx + αx[3]_ = γ cos(ωt). (91)

Note that this system is non-autonomous, that is externally forced due to the r.h.s. of eqn. 91. The
following parameters were chosen to arrive at a chaotically forced oscillator: α = 1.0, β = −1.0,
_δ = 0.1, γ = 0.35, and ω = 1.4. For these parameters the Duffing oscillator has a maximum_
Lyapunov exponent of λmax = 0.0995. The dataset used here was created with the code from
(Gilpin, 2021) as a three dimensional embedding with step size ∆t = 0.17. The prediction time is
_τpred = 39.28._

**R¨ossler system** Another prime textbook example for a chaotic system is the R¨ossler system
(R¨ossler, 1976) given by:

_dx_

_dt_ [=][ −][y][ −] _[z,]_


_dy_

(92)
_dt_ [=][ x][ +][ ay,]

_dz_

_dt_ [=][ b][ +][ z][(][x][ −] _[c][)][.]_


-----

For the parameters a = 0.15, b = 0.2 and c = 10, the maximal Lyapunov exponent is λmax =
0.09 (Rosenstein et al., 1993). To arrive at a time series, a step size of ∆t = 0.1 was chosen for
integration. This gives us a prediction time of τpred = 77.0 for this system.

**Mackey-Glass equation** The Mackey-Glass equation (Glass & Mackey, 1979) is a nonlinear time
delay differential equation

_xρ_
_x˙_ = β _γx_ with β, γ, ρ > 0. (93)

1 + x[n]ρ _−_

Here xρ represents the value of the variable x at time t − _ρ (note that strictly, mathematically, this_
makes the system infinite-dimensional). Choosing the parameters to be β = 2, γ = 1.0, n = 9.65,
and ρ = 2.0, leads to chaotic behavior with a maximum Lyapunov exponent of λmax = 0.21.
The dataset was created as a 10-dimensional embedding with the code from (Gilpin, 2021) using
∆t = 0.04. This yields a prediction time of τpred = 82.2.

**Empirical temperature time series** [This time series was recorded at the Weather Station at the](https://www.bgc-jena.mpg.de/wetter/)
[Max Planck Institute for Biogeochemistry in Jena, Germany, spanning the time period between 2009](https://www.bgc-jena.mpg.de/wetter/)
and 2016, and reassembled by Franc¸ois Chollet for the book Deep Learning with Python. The data
[set can be accessed at https://www.kaggle.com/pankrzysiu/weather-archive-jena.](https://www.kaggle.com/pankrzysiu/weather-archive-jena)
To expose the underlying chaotic dynamics of the time series, trends and yearly cycles were removed, and nonlinear noise-reduction was performed (using ghkss from TISEAN, see also Kantz
et al. (1993)). Fig. 4 (a) shows a snippet of the temperature data in comparison with the denoised time-series. High-frequency noise was further reduced through Gaussian kernel smoothing
(σ = 200), and the resulting time series was sub-sampled (every 5[th] data point was retained). Fig. 4
(b) clearly reveals a fractional dimension of Deff = 2.8 for the de-noised and smoothed time-series.
This strongly suggests that the dynamics governing the time series are chaotic. We created a time
delay embedding (Kantz & Schreiber, 2003) with m = 5 (estimated by the false nearest neighbor
technique, see Kennel et al. (1992)) and delay ∆t = 500 (obtained as the first minimum of the
mutual information). The first three embedding dimensions are shown in Fig. 4(c). The maximal
Lyapunov exponent of this time series was determined with lyap r from TISEAN (Hegger et al.,
1999) to be λmax = 0.016, see Fig. 3(a). This value is in close agreement with the literature (Mill´an
et al., 2010). The predictability time of this system is estimated to be τpred = 43.3.

Figure 4: (a) Snippet of the original temperature data and de-noised time series. (b) Blue lines show the
local slopes of the correlation sums for embedding dimensions m ∈{5, . . ., 10}. The convergence of these
estimates in m reveals a fractional dimension indicated by the plateau. (c) First three dimensions of the timedelay embedding series as used for training.

All datasets used were standardized (i.e., centered with unit variance) prior to training.

A.5 EMPIRICAL EVALUATION: MEASURES OF RECONSTRUCTION QUALITY

**Attractor overlap** To asses the geometrical similarity of the chaotic attractor produced by the
RNN to the one underlying the observations, we calculate the Kullback-Leibler divergence of the
ground truth distribution ptrue(x) and the distribution pgen(x|z) generated by RNN simulation. To
do so in practice, we employ a binning approximation (see (Koppe et al., 2019))


_pˆ[(]true[k][)]_ [(][x][)]

_pˆ[(]gen[k][)]_ [(][x][ |][ z][)]


_pˆ[(]true[k][)]_ [(][x][) log]
_k=1_

X


_Dstsp (ptrue(x), pgen(x | z)) ≈_


-----

where K is the total number of bins, and ˆp[(]true[k][)] [(][x][)][ and][ ˆ]p[(]gen[k][)] [(][x][ |][ z][)][ are estimates obtained as rela-]
tive frequencies through sampling trajectories from the observed time-series and the trained RNN,
respectively.

**Power-spectral correlations** Since in DS reconstruction we aim to capture invariant (timeindependent) properties of the underlying system, besides the geometrical agreement, we compare
the similarity in true and RNN-reconstructed power spectra. To do so, we generate a time series
of length 100, 000 from the RNN and calculate its power spectrum using the fast Fourier transform
(scipy.fft). To reduce the influence of noise we apply Gaussian kernel smoothing and cut off
the long high-frequency tails of the spectra. The dimension-wise correlation between observed and
generated spectra are then averaged to give the PSC.


A.6 FURTHER EMPIRICAL EVALUATIONS

A.6.1 RECONSTRUCTION: R ¨OSSLER SYSTEM


1.0

0.5


15

10


0.0

0

_τpred_ 200 400 _τpred_ 200 400

learning interval τ learning interval τ

Figure 5: Overlap in attractor geometry (Dstsp, lower = better) and dimension-wise power-spectra correlations

|Col1|pred 200 400 learning interval τ|
|---|---|
|τ||


|Col1|Col2|
|---|---|
||200 400 learning interval τ|
|τpred||

(PSC, higher = better) against learning interval τ for the R¨ossler attractor. Continuous lines = sparsely forced
BPTT. Dashed lines = classical BPTT with gradient clipping. Prediction time indicated vertically in black.


Figure 6: The R¨ossler attractor (blue) and reconstruction by a LSTM (orange) trained with a learning interval
(a) chosen too small (τ = 5), (b) chosen optimally (τ = 30), and (c) chosen too large (τ = 200).

A.6.2 RECONSTRUCTION: HIGH-DIMENSIONAL MACKEY-GLASS SYSTEM


10[2]


1.0

0.5


10[0]

10[−][2]


0.0




Figure 7: Overlap in attractor geometry (Dstsp, lower = better) and dimension-wise power-spectra correla
|Col1|RNN PLRNN LSTM|
|---|---|
|τpred le|200 300 40 arning interval τ|


|Col1|Col2|
|---|---|
|τp|red 200 300 40 learning interval τ|

tions (PSC, higher = better) against learning interval τ for the 10d Mackey-Glass system. Continuous lines =
sparsely forced BPTT. Dashed lines = classical BPTT with gradient clipping. Prediction time indicated vertically in black.


-----

A.6.3 RECONSTRUCTION: PARTIALLY OBSERVED LORENZ SYSTEM

For this evaluation we trained models only on the variables {y, z} of the Lorenz system, eqn. 90.
In order to compute the attractor overlap (Dstsp) in the true state space, however, after training the
observation matrix B was recomputed by linearly regressing the first 10 latent states onto the first
10 observations from all three Lorenz variables in eqn. 90.


1.0

0.5


15

10


0.0




Figure 8: Overlap in attractor geometry (Dstsp, lower = better) and dimension-wise power-spectra correlations

|RNN PLRNN|Col2|
|---|---|
|τpred le|LSTM 100 150 2 arning interval τ|


|Col1|Col2|
|---|---|
|τpred le|100 150 20 arning interval τ|

(PSC, higher = better) against learning interval τ for the partially observed Lorenz system. Continuous lines
= sparsely forced BPTT. Dashed lines = classical BPTT with gradient clipping. Prediction time indicated
vertically in black.

A.6.4 OTHER INITIALIZATION PROCEDURES: STANDARD BATCHING WITH ZERO RESETTING

A common procedure in training RNNs is partitioning the time series into chunks (as we did based
on the Lyapunov spectrum), but then simply resetting the hidden states at the beginning of each
chunk (window) to 0. Formally this would mean that we do not force the trajectory back on track as
in our approach, but instead may kick it off the track. To illustrate this, here we trained an LSTM
on chunks (windows) with a length given by the optimal τ (τopt = 30 for the Lorenz system), but
then initialized the hidden states to 0 at the beginning of each window. The performance obtained
this way is indicated by the green dashed line in Fig. 9 below.


15

10


1.0

0.5


0.0


LSTM


_τpred_ 100 150 200 _τpred_ 200 400

learning interval τ learning interval τ

Figure 9: Overlap in attractor geometry (Dstsp, lower = better) and dimension-wise power-spectra correla
|Col1|Col2|
|---|---|
||100 150 200 arning interval τ|
|τpred le||

tions (PSC, higher = better) against learning interval τ for the Lorenz system. Continuous lines = sparsely
forced BPTT. Dashed-dotted lines = windowing without forcing (choosing windows according to the optimal
prediction time, but resetting hidden states to zero rather than its TF control value). Prediction time indicated
vertically in black.


A.6.5 ELECTROENCEPHALOGRAM (EEG) DATA

We used EEG data recorded by Schalk et al. (2004) and provided on PhysioNet (Goldberger et al.,
2000), from which we took the baseline recording of the first patient for our analysis. Preprocessing
was performed as outlined above for the temperature time-series, i.e. we applied nonlinear
noise-reduction (see Fig.10 (a)) and Gaussian kernel smoothing (σ = 30). Fig. 10 (b) indicates a
fractional dimension Deff = 1.8 for the de-noised and smoothed times series. We created a time
delay embedding with an embedding dimension of m = 10 and a delay time of ∆t = 500. The
maximal Lyapunov exponent for this time series was determined to be λmax = 0.007, see Fig. 11
(a). To ease training, the time-series was sub-sampled and only every third data point was retained.
With this, we obtain a predictability time τpred = 33.01.


-----

**(a)**


**(b)**

) 20


**(c)**

2

0

EEG signal

_°2_


10


_°2_


2[°][6] 2[°][3] 2[0]

_Deff = 1.8_

log2 ≤


raw
de-noised

2500 5000 7500 10000

time-steps


m=1
m=2
m=3


5000 10000 15000

time steps


Figure 10: (a) Snippet of the original EEG data and de-noised time series. (b) Blue lines show the local slopes
of the correlation sums for embedding dimensions m ∈{5, . . ., 10}. The convergence of these estimates in m
reveals a fractional dimension indicated by the plateau. (c) First three dimensions of the time-delay embedding
series as used for training.

Full reconstruction of the dynamics from complex and noisy EEG signals is a very ambitious. Here
we therefore focused on the system’s short-term behavior and combined multiple shorter trajectory
bits (of length T = 70) in the calculation of our measure for geometrical agreement in state space.


**(a)**


**(b)**


1.0

0.5

0.0

_−0.5_


10[2]

10[0]


_°2_

_°4_


m=8
m=9
m=10
_∏max = 0.007_


200 400 600

time-steps ≤




RNN
PLRNN
LSTM

_øpred_ 100 200

learning interval ø

_τpred_ 100 200

learning interval τ

Figure 11: (a) The maximal Lyapunov exponent was determined as the slope of the average log-divergence
of nearest neighbors in embedding space (m = embedding dimension). (b) Reconstruction quality assessed by
attractor overlap (lower = better) and power-spectrum correlation (higher = better). Black vertical lines = τpred.

A.7 SPARSELY FORCED BPTT


**Loss truncation** One implicit consequence of the teacher forcing, eqn. (16), is the interruption
of the hidden-to-hidden connections at these time points. More specifically, if the system is forced
at time t ∈T, then there is no connection between zt and zt+1, that is

**_zt)_**

**_Jt+1 =_** _[∂][z][t][+1]_ = _[∂RNN]_ [(˜] = 0. (94)

_∂zt_ _∂zt_

To see how these vanishing Jacobians truncate the loss gradients w.r.t to some parameter θ, let us
focus on the loss gradients immediately after the forcing,


_t+1_

_∂zt+1_


_∂[+]zk_

_∂θ_


_∂_ _t+1_
_L_ = _[∂][L][t][+1]_

_∂θ_ _∂zt+1_


_∂zk_


_k=1_


= _[∂][L][t][+1]_ ( _[∂][+][z][t][+1]_

_∂zt+1_ _∂θ_


_∂[+]zk_

_∂θ_ [)]


_∂zt+1_


_∂θ_ _∂zk_ _∂θ_

_k=1_

X =0, because of (94)

_∂[+]zt+1_

_._ | {z } (95)
_∂θ_


= _[∂][L][t][+1]_

_∂zt+1_


Eqn. (95) shows that sparsely forced BPTT implicitly truncates the loss gradients because it interrupts the hidden-to-hidden connection from zt to zt+1 for t ∈T . More generally, defining
et := max{t[′] _∈T : t[′]_ _≤_ _t}, the overall loss gradients are truncated to_


_∂[+]zk_

_∂θ_


_∂L_

_∂θ_ [=]

tr.


_∂_ _t_
_L_

_∂zt_

_∂_ _t_
_L_

_∂zt_


_∂zt_

_∂zk_

_∂zt_

_∂zk_


_t=1_

_T_

_t=1_

X


_k=1_


_∂[+]∂θ zk_ _[.]_ (96)


_k=[e]t_


-----

