# TOWARDS MODEL-AGNOSTIC FEDERATED LEARNING USING KNOWLEDGE DISTILLATION


**Andrei Afonin**
EPFL
andrei.afonin@epfl.ch


**Sai Praneeth Karimireddy[∗]**
EPFL, UC Berkeley
sp.karimireddy@berkeley.edu

ABSTRACT


Is it possible to design an universal API for federated learning using which an
ad-hoc group of data-holders (agents) collaborate with each other and perform
federated learning? Such an API would necessarily need to be model-agnostic
i.e. make no assumption about the model architecture being used by the agents,
and also cannot rely on having representative public data at hand. Knowledge
_distillation (KD) is the obvious tool of choice to design such protocols. However,_
surprisingly, we show that most natural KD-based federated learning protocols
have poor performance.

To investigate this, we propose a new theoretical framework, Federated Kernel
ridge regression, which can capture both model heterogeneity as well as data
heterogeneity. Our analysis shows that the degradation is largely due to a fundamental limitation of knowledge distillation under data heterogeneity. We further
validate our framework by analyzing and designing new protocols based on KD.
Their performance on real world experiments using neural networks, though still
unsatisfactory, closely matches our theoretical predictions.

1 INTRODUCTION

“I speak and speak, but the listener retains only the words he is expecting... It is not
the voice that commands the story: it is the ear.” - Invisible Cities, Italo Calvino.

Federated learning (and more generally collaborative learning) involves multiple data holders (whom
we call agents) collaborating with each other to train their machine learning model over their collective
data. Crucially, this is done without directly exchanging any of their raw data (McMahan et al., 2017;
Kairouz et al., 2019). Thus, communication is limited to only what is essential for the training process
and the data holders (aka agents) retain full ownership over their datasets.

Algorithms for this setting such as FedAvg or its variants all proceed in rounds (Wang et al., 2021).
In each such round, the agents first train their models on their local data. Then, the knowledge from
these different models is aggregated by averaging the parameters. However, exchanging knowledge
via averaging the model parameters is only viable if all the agents use the same model architecture.
This fundamental assumption is highly restrictive. Different agents may have different computational
resources and hence may want to use different model architectures. Further, directly averaging the
model parameters can fail even when all clients have the same architecture (Wang et al., 2019b;
Singh & Jaggi, 2020; Yu et al., 2021). This is because the loss landscape of neural networks is
highly non-convex and has numerous symmetries with different parameter values representing the
same function. Finally, these methods are also not applicable when using models which are not
based on gradient descent such as random forests. To overcome such limitations, we would need to
take a functional view view of neural networks i.e. we need methods that are agnostic to the model
architecture and parameters. This motivates the central question investigated in this work:

Can we design model agnostic federated learning algorithms which would allow each agent to train
their model of choice on the combined dataset?

_∗Corresponding author._


-----

Specifically, we restrict the algorithms to access the models using only two primitives (a universal
model API): train on some dataset i.e. fit, and yield predictions on some inputs i.e. predict. Our goal
is to be able to collaborate with and learn from any agent which provides these two functionalities.

**Simple algorithms.** A naive such model agnostic algorithm indeed exists—agents can simply
transfer their entire training data to each other and then each agent can train any model of choice
on the combined dataset. However, transferring of the dataset is disallowed in federated learning.
Instead, we will replace the averaging primitive in federated learning with knowledge distillation
(KD) (Bucilua et al., 2006; Hinton et al., 2015). In knowledge distillation (KD), information is
transferred from model A to model B by training model B on the predictions of model A on some data.
Since we only access model A through its predictions, KD is a functional model-agnostic protocol.
The key challenge of KD however is that it is poorly understood and cannot be formulated in the
standard stochastic optimization framework like established techniques (Wang et al., 2021). Thus,
designing and analyzing algorithms that utilize KD requires developing an entirely new framework
and approach.

**Our Contributions. The main results in this work are**

-  We formulate the model agnostic learning problem as two agents with local datasets wanting
to perform kernel regression on their combined datasets. Kernel regression is both simple
enough to be theoretically tractable and rich enough to capture non-linear function fitting
thereby allowing each agent to have a different kernel (hence different models).

-  We analyze alternating knowledge distillation (AKD) and show that it is closely linked to
the alternating projection method for finding the intersection of convex sets. Our analysis
reveals that AKD sequentially loses information, leading to degradation of performance.
This degradation is especially severe when the two agents have heterogeneous data.

-  Using the connection to alternating projection, we analyze other possible variants such as
_averaged knowledge distillation (AvgKD) and attempt to construct an ‘optimal’ scheme._

-  Finally, we evaluate all algorithms on real world deep learning models and datasets, and
show that the empirical behavior closely matches our insights from the theoretical analysis.
This demonstrates the utility of our framework for analyzing and designing new algorithms.


2 RELATED WORK

**Federated learning (FL). In FL (Kairouz et al., 2019), training data is distributed over several agents**
or locations. For instance, these could be several hospitals collaborating on a clinical trial, or billions
of mobile phones involved in training a voice recognition application. The purpose of FL is to enable
training on the union of all agents’ individual data without needing to transmit any of the raw sensitive
data. Typically, the training is coordinated by some trusted server. One can also instead use direct
peer-to-peer communications (Nedic, 2020). A large body of work has designed algorithms for FL
under the identical model setting where we either learn a single global model (McMahan et al., 2017;
Reddi et al., 2020; Karimireddy et al., 2020b;a; Wang et al., 2021), or multiple personalized models
(Wang et al., 2019a; Deng et al., 2020; Mansour et al., 2020; Grimberg et al., 2021).

**Knowledge distillation (KD). Initially, KD was introduced as a way to compress models i.e. as a**
way to transfer the knowledge of a large model to a smaller model (Bucilua et al., 2006; Hinton
et al., 2015). Since then, it has found much broader applications such as improving generalization
performance via self-distillation, learning with noisy data, and transfer learning (Yim et al., 2017).
We refer to a recent survey (Gou et al., 2021) for progress in this vast area.

**KD in FL. Numerous works propose to use KD to transfer knowledge from the agent models to**
a centralized server model (Seo et al., 2020; Sattler et al., 2020; Lin et al., 2020; Li et al., 2020;
Wu et al., 2021). However, all of these methods rely on access to some common public dataset
which may be impractical. KD has also been proposed to combine personalization with model
compression (Ozkara et al., 2021), but our focus is for the agents to learn on the combined data. In
the closely related codistillation setting (Zhang et al., 2018; Anil et al., 2018; Sodhani et al., 2020),
an ensemble of students learns collaboratively without a central server model. While codistillation
does not need additional unlabelled data, it is only suitable for distributed training within a datacenter
since it assumes all agents have access to the same dataset. In FL however, there is both model and
data heterogeneity. Further, none of these methods have a theoretical analysis.


-----

**KD analysis. Despite the empirical success of KD, it is poorly understood with very little theoretical**
analysis. Phuong & Lampert (2021) explore a generalization bound for a distillation-trained linear
model, and Tang et al. (2021) conclude that by using KD one re-weights the training examples for the
student, and Menon et al. (2020) consider a Bayesian view showing that the student learns better if the
teacher provides the true Bayes probability distribution. Allen-Zhu & Li (2020) show how ensemble
distillation can preserve their diversity in the student model. Finally, Mobahi et al. (2020) consider
self-distillation in a kernel regression setting i.e. the model is retrained using its own predictions
on the training data. They show that iterative self-distillation induces a strong regularization effect.
We significantly extend their theoretical framework in our work to analyze KD in federated learning
where agents have different models and different datasets.

3 FRAMEWORK AND SETUP

**Notation.** We denote a set as A, a matrix as A, and a vector as a. A[i, j] is the (i, j)-th element of
matrix A, a[i] denotes the i’th element of vector a. ||a|| denotes the ℓ2 norm of vector a.

**Centralized kernel regression (warmup).** Consider, as a warm-up, the centralized setting with a
training dataset R[d] R. That is, = _i_
Given training set D ⊆ D, our aim is to find best function× _D_ _∪[N]_ _[{][(][x][i][, y] f[i][)][⋆][}]∈F[, where] : X →Y[ x][n][ ∈X ⊆]. To find[R][d][ and] f_ _[⋆][ y]we solve the[n][ ∈Y ⊆]_ [R][.]
following regularized optimization problem:


(f (xn) _yn)[2]_ + cRu(f ), with (1)
_−_

_u(x, x[′])f_ (x)f (x[′])dxdx[′] _._ (2)


_f_ _[⋆]_ := arg min
_f_ _∈F_

_Ru(f_ ) :=


Here, F is defined to be the space of all functions such that (2) is finite, c is the regularization
parameter, and u(x, x[′]) is a kernel function. That is, u is symmetric u(x, x[′]) = u(x[′], x) and
positive with Ru(f ) = 0 only when f = 0 and Ru(f ) > 0 o.w. Further, let k(x, t) be the function
s.t.

_u(x, x[′])k(x[′], t)dx[′]_ = δ(x − **_t),_** where δ(·) is the Dirac delta function. (3)

ZX

Now, we can define the positive definite matrix K ∈ R[N] _[×][N]_ and vector kx ∈ R[N] as:

**_K[i, j] := [1]_** and **_kx[i] := [1]_** for **_xi_** _,_ _i_ [N ] . (4)

_N [k][(][x][i][,][ x][j][)]_ _N [k][(][x][,][ x][i][)][,]_ _∈D_ _∀_ _∈_


Note that kx is actually a vector valued function which takes any x ∈X as input, and both kx and
**_K depend on the training data D. We can then derive a closed form solution for f_** _[⋆]._

**Proposition I (Schölkopf et al. (2001)). The f** _[⋆]_ _which minimizes (1) is given by_

_f_ _[⋆](x) = kx[⊤][(][c][I][ +][ K][)][−][1][y][,]_ _for_ **_y[i] := yi, ∀i ∈_** [N ] .

Note that on the training data X ∈ R[N] _[×][d]_ with X[i, :] = xi, we have f _[⋆](X) = K(cI + K)[−][1]y._
Kernel regression for an input x outputs a weighted average of the training _yn_ . These weights are
_{_ _}_
computed using a learned measure of distance between the input x and the training **_xn_** . Intuitively,
_{_ _}_
the choice of the kernel u(x, x[′]) creates an inductive bias and corresponds to the choice of a model
in deep learning, and the regularization parameter c acts similarly to tricks like early stopping, large
learning rate, etc. which help in generalization. When c = 0, we completely fit the training data
and the predictions exactly recover the labels with f _[⋆](X) = K(K)[−][1]y = y. When c > 0 the_
predictions f _[⋆](X) = K(cI + K)[−][1]y ̸= y and they incorporate the inductive bias of the model. In_
knowledge distillation, this extra information carried by the predictions about the inductive bias of
the model is popularly referred to as “dark knowledge” (Hinton et al., 2015).

**Federated kernel regression (our setting).** We have two agents, with agent 1 having dataset
_D1 = ∪i[N][1]_ _[{][(][x]i[1][, y]i[1][)][}][ and agent 2 with dataset][ D][2][ =][ ∪]i[N][2]_ _[{][(][x]i[2][, y]i[2][)][}][. Agent 1 aims to find the best]_


-----

approximation mapping g[1][⋆] _∈F1 : X →Y using a kernel u1(x, x[′]) and objective:_


(g(x[1]n[)][ −] _[y]n[1]_ [)][2][ +]


(g(x[2]n[)][ −] _[y]n[2]_ [)][2][ +][ cR][u]1 [(][g][)][, with]

(5)


_g[1][⋆]_ := arg min
_g∈F1_


_N1 + N2_


_N1 + N2_


_u1(x, x[′])g(x)g(x[′])dxdx[′]_ _._ (6)


_Ru1_ (g) :=


Note that the objective of agent 1 is defined using its individual kernel u1(x, x[′]), but over the joint
_dataset (_ 1, 2). Correspondingly, agent 2 also uses its own kernel function u2(x, x[′]) to define the
_D_ _D_
regularizer Ru2 (g[2]) over the space of functions g[2] _∈F2 and optimum g[2][⋆]. Thus, our setting has_
model heterogeneity (different kernel functions u1 and u2) and data heterogeneity (D1 and D2 are
not i.i.d.). Given that the setting is symmetric between agents 1 and 2, we can focus solely on error in
terms of agent 1’s objective (5) without loss of generality.

Proposition I can be used to derive a closed form form for function g[1][⋆] minimizing objective (5).
However, computing this requires access to the datasets of both agents. Instead, we ask “can we
design an iterative federated learning algorithm which can approximate g[1][⋆]”?

4 ALTERNATING KNOWLEDGE DISTILLATION

In this section, we describe a popular iterative knowledge distillation algorithm and analyze its
updates in our framework. Our analysis leads to some surprising connections between KD and
projection onto convex sets, and shows some limitations of the current algorithm.

**Algorithm.** Denote the data on agent 1 as D1 = (X [1], y[1]) where X [1][i, :] = x[1]n [and][ y][1][[][i][] =][ y]i[1][.]
Correspondingly, we have D[2] = (X [2], y[2]). Now starting from ˆy0[1] [=][ y][1][, in each rounds][ t][ and][ t][ + 1][:]

a. Agent 1 trains their model on dataset (X [1], ˆyt[1][)][ to obtain][ g]t[1][.]

b. Agent 2 receives gt[1] [and uses it to predict labels][ ˆ]yt[2]+1 [=][ g]t[1][(][X] [2][)][.]

c. Agent 2 trains their model on dataset (X [2], ˆyt[2]+1[)][ to obtain][ g]t[1]+1[.]

d. Agent 1 receives a model gt[1]+1 [from agent 2 and predicts][ ˆ]yt[1]+2 [=][ g]t[1]+1[(][X] [1][)][.]

Thus the algorithm alternates between training and knowledge distillation on each of the two agents.
We also summarize the algorithm in Figure 1a. Importantly, note that there is no exchange of raw data
but only of the trained models. Further, each agent trains their choice of a model on their data with
agent 1 training {gt[1][}][ and agent 2 training][ {][g]t[1]+1[}][. Superscript 1 means we start AKD from agent 1.]

4.1 THEORETICAL ANALYSIS

Similar to (3), let us define functions k1(x, x[′]) and k2(x, x[′]) such that they satisfy

_ua(x, x[′])ka(x[′], t)dx[′]_ = δ(x **_t)_** for a 1, 2 _._
_−_ _∈{_ _}_

ZX

For such functions, we can then define the following positive definite matrix L ∈ R[(][N][1][+][N][2][)][×][(][N][1][+][N][2][)]:


**_L =_** **_L11_** **_L12_**
**_L21_** **_L22_**



1

_k1(x[a]i_ _[,][ x]j[b][)][ for][ a, b][ ∈{][1][,][ 2][}][ and][ i][ ∈]_ [[][N][1][]][, j][ ∈] [[][N][2][]][ .]
_N1 + N2_


**_La,b[i, j] =_**


Note L is symmetric (with L[⊤]12 [=][ L][21][) and is also positive definite. Further, each component]
**_La,b measures pairwise similarities between inputs of agent a and agent b using the kernel k1._**
Correspondingly, we define M ∈ R[(][N][1][+][N][2][)][×][(][N][1][+][N][2][)] which uses kernel k2:


**_M =_** **_M11_** **_M12_**
**_M21_** **_M22_**



1

_k2(x[a]i_ _[,][ x]j[b][)][ and][ i][ ∈]_ [[][N][1][]][, j][ ∈] [[][N][2][]][ .]
_N1 + N2_


**_Ma,b[i, j] =_**


We can now derive the closed form of the AKD algorithm repeatedly using Proposition I.
**Proposition II. The model in round 2t learned by the alternating knowledge distillation algorithm is**

_g2[1]t[(][x][) =][ l]x[⊤][(][c][I][ +][ L][11][)][−][1][  ]M12(cI + M22)[−][1]L21(cI + L11)[−][1][][t]_ **_y[1]_** _,_


-----

_where lx ∈_ R[N][1] _is defined as lx[i] =_


1

_N1+N2_ _[k][1][(][x][,][ x]i[1][)][. Further, for any fixed][ x][ we have]_

_tlim→∞_ _[g]t[1][(][x][) = 0][ .]_


First, note that if agents 1 and 2 are identical with the same data and same model, we have M12 =
**_M22 = L11 = L12. This setting corresponds to self-distillation where the model is repeatedly_**
retrained on its own predictions. Proposition II shows that after 2t rounds of self-distillation, we

obtain a model is of the form g2[1]t[(][x][) =][ l]x[⊤] [(][c][I][ +][ L][11][)][−][1][ ]L11 (cI + L11)[−][1][][2][t] **_y. Here, the effect_**
of c is amplified as t increases. Thus, this shows that repeated self-distillation induces a strong
regularization effect, recovering the results of (Mobahi et al., 2020).

Perhaps more strikingly, Proposition II shows that not only does AKD fail to converge to the actual
optimal solution g[1][⋆] as defined in (5), it will also slowly degrade and eventually converges to 0. We
next expand upon this and explain this phenomenon.

4.2 DEGRADATION AND CONNECTION TO PROJECTIONS

While mathematically, Proposition II completely describes the AKD algorithm, it does not provide
much intuition. In this section, we rephrase the result in terms of projections and contractions which
provides a more visual understanding of the method.

**Oblique projections.** A projection operator P linearly maps (projects) all inputs onto some linear
subspace A = Range(A). In general, we can always rewrite such a projection operation as

**_P x =_** min
**_y∈Range(A)[(][y][ −]_** **_[x][)][⊤][W][ (][y][ −]_** **_[x][) =][ A][(][A][⊤][W A][)][−][1][A][⊤][W x][ .]_**

Here, A defines an orthonormal basis of the space A and W is a positive definite weight matrix
which defines the geometry ⟨x, y⟩W := x[⊤]W y. When W = I, we recover the familiar orthogonal
projection. Otherwise, projections can happen ‘obliquely’ following the geometry defined by W .

**Contractions.** A contraction is a linear operator C which contracts all inputs towards the origin:
_∥Cx∥≤∥x∥_ for any x .

Given these notions, we can rewrite Proposition II as follows.
**Proposition III. There exist oblique projection operators P1 and P2, contraction matrices C1 and**
**_C2, and orthonormal matrices V1 and V2 such that the model in round 2t learned by the alternating_**
_knowledge distillation algorithm is_

_t_ 1
_g2[1]t[(][x][) =][ l]x[⊤][(][c][I][ +][ L][11][)][−][1][V][ ⊤]1_ **_C1P2[⊤][C][2][P][ ⊤]1_** **_V1y_** _._
_In particular, the predictions on agent 2’s inputs are_
  

_g2[1]t[(][X]_ [2][) =][ V][ ⊤]2 **_[P][ ⊤]1_** **_C1P2[⊤][C][2][P][ ⊤]1_** _t V1y1 ._

_Further, the projection matrices satisfy P1[⊤][P][ ⊤]2_ **_[x] [ =][ x][ only if][ x][ = 0]_** _[ .]_

_t_
The term **_C1P2[⊤][C][2][P][ ⊤]1_** is the only one which depends on t and so captures the dynamics of the
algorithm. Two rounds of AKD (first to agent 1 and then back to 2) correspond to a multiplication
with C1P 2[⊤][C][2][P][ ⊤]1 [i.e.][ alternating projections] [ interspersed by contractions. Thus, the dynamics]
of AKD is exactly the same as that of alternating projections interspersed by contractions. The
projections P1[⊤] [and][ P][ ⊤]2 [have orthogonal ranges whose intersection only contains the origin 0. As]
Fig. 1b shows, non-orthogonal alternating projections between orthogonal spaces converge to the
origin. Contractions further pull the inputs closer to the origin, speeding up the convergence.
**Remark 1. To understand the connection to projection more intuitively, suppose that we had a 4-way**
_classification task with agent 1 having data only of the first two classes, and agent 2 with the last two_
_classes. Any model trained by agent 1 will only learn about the first two classes and its predictions_
_on the last two classes will be meaningless. Thus, no information can be transferred between the_
_agents in this setting. More generally, the transfer of knowledge from agent 1 to agent 2 is mediated_
_by its data X2. This corresponds to a projection of the knowledge of agent 1 onto the data of agent 2._
_If there is a mismatch between the two, knowledge is bound to be lost._


-----

|Train Train|Col2|
|---|---|
|||


**2 rounds**

(a) Alternating KD starting from agent 1. We predict and train
using predictions, alternating between the two agents.


0


(b) Alternating oblique projections starting
from x converges to 0.


1 3 4 2 0


(b) Intuition behind ensemble scheme. In each round,
AKD alternates between overfitting the data of agent 1
or of agent 2. We can construct an ensemble out of these
models to correct for this bias and quickly converge to
the true optima.


**1 round**

|Train Train|Col2|
|---|---|
|||



(a) AvgKD scheme

**Speed of degradation.** The alternating projection algorithm converges to the intersection of the
subspaces corresponding to the projection operators (their range) (Boyd & Dattorro, 2003). In our
particular case, the fixed point of P1[⊤] [and][ P][ ⊤]2 [is 0, and hence this is the point the algorithm will]
converge to. The contraction operations C1 and C2 only speed up the convergence to the origin 0
(also see Fig. 1b). This explains the degradation process notes in Proposition II. We can go further
and examine the rate of degradation using known analyses of alternating projections (Aronszajn,

1950).

**Proposition IV (Informal). The rate of convergence to gt[1][(][x][)][ to][ 0][ gets faster if:]**

-  a stronger inductive bias is induced via a larger regularization constant c,

-  the kernels k1(x, y) and k2(x, y) are very different, or

-  the difference between the datasets D1 and D2 as measured by k1(x, y) increases.

In summary, both data and model heterogeneity may speed up the degradation defeating the purpose
of model agnostic FL. All formal proofs and theorem statements are moved to the Appendix.

5 ADDITIONAL VARIANTS

In the previous section, we saw that the alternating knowledge distillation (AKD) algorithm over
multiple iterations suffered slow degradation, eventually losing all information about the training
data. In this section, we explore some alternative approaches which attempt to correct this. We first
analyze a simple way to re-inject the training data after every KD iteration which we call averaged
distillation. Then, we show an ensemble algorithm that can recover the optimal model g[1][⋆].

5.1 AVERAGED KNOWLEDGE DISTILLATION

As we saw earlier, each step of knowledge distillation seems to lose some information about the
training data, replacing it with the inductive bias of the model. One approach to counter this slow
loss of information is to recombine it with the original training data labels such as is commonly done
in co-distillation (Sodhani et al., 2020).

**Algorithm.** Recall that agent 1 has data D1 = (X [1], y[1]) and correspondingly agent 2 has data
_D[2]_ = (X [2], y[2]). Now starting from ˆy0[1] [=][ y][1][,][ ˆ]y0[2] [=][ y][2][, in each round][ t][ ≥] [0][:]

a. Agents 1 and 2 train their model on datasets (X [1], ˆyt[1][)][ and][ (][X] [2][,][ ˆ]yt[2][)][ to obtain][ g]t[1] [and][ g]t[2][.]


-----

b. Agents exchange their models gt[1] [and][ g]t[2] [between each other.]

c. Agents use exchanged models to predict labels ˆyt[1]+1 [=][ y][1][+][g]2t[2][(][X] [1][)], ˆyt[2]+1 [=][ y][2][+][g]2t[1][(][X] [2][)] .

The summary the algorithm is depicted in Figure 2a. Again, notice that there is no exchange of
raw data but only of the trained models. The main difference between AKD and AvgKD (averaged
knowledge distillation) is that we average the predictions with the original labels. This re-injects
information y[1] and y[2] at every iteration, preventing degradation. We theoretically characterize its
dynamics next in terms of the afore-mentioned contraction and projection operators.
**Proposition V. There exist oblique projection operators P1 and P2, contraction matrices C1 and**
**_C2, and orthonormal matrices V1 and V2 such that the model of agent 1 in round t learned by the_**
_averaged knowledge distillation (AvgKD) algorithm is_


_t−1_

2 **_[C][2][P][ ⊤]1_**

_gt[1][(][x][) =][ F]_ ( **_[C][1][P][ ⊤]_**

2 [(] 4

_i=0_

X


_t−2_

1 **_[C][1][P][ ⊤]2_**
( **_[C][2][P][ ⊤]_**

4

_i=0_

X


)[i])z1 + **_[F C][1][P][ ⊤]2_**


)[i])z2 .


_where F = lx[⊤][(][c][I][ +][ L][11][)][−][1][V][ ⊤]1_ _[. Further, in the limit of rounds for any fixed][ x][ we have]_


2 **_[C][2][P][ ⊤]1_**

_tlim→∞_ _[g]t[1][(][x][) =][ F]2 [(][I][ −]_ **_[C][1][P][ ⊤]4_**


)[†]z1 + **_[F C][1][P][ ⊤]2_**


1 **_[C][1][P][ ⊤]2_**
(I
_−_ **_[C][2][P][ ⊤]4_**


)[†]z2 .


This shows that the model learned through AvgKD does not degrade to 0, unlike AKD. Instead, it
converges to a limit for which we can derive closed-form expressions. Unfortunately, this limit model
is still not the same as our desired optimal model g[1][⋆]. We next try to overcome this using ensembling.

5.2 ENSEMBLED KNOWLEDGE DISTILLATION

We first analyze how the limit solution of AvgKD differs from the actual optimum g[1][⋆]. We will build
upon this understanding to construct an ensemble that approximates g[1][⋆]. For simplicity, we assume
that the regularization coefficient c = 0.

_Understanding AvgKD. Consider AvgKD algorithm, then_

**Proposition VI. There exist matrices A1 and A2 such that the minimizer g[1][⋆]** _of objective (5) predicts_
_g[1][⋆](Xi) = A1β1 + A2β2 for i ∈{1, 2}, where β1 and β2 satisfy_
**_L11_** **_M12_** **_β1_** = **_y1_** _._ (7)
**_L21_** **_M22_** **_β2_** **_y2_**
     

_In contrast, for the same matrices A1 and A2, the limit models of AvgKD predict g∞[1]_ [(][X][i][) =][ 1]2 **_[A][1][β][1]_**

_and g[2]_ 2 **_[A][2][β][2][, for][ i][ ∈{][1][,][ 2][}][ for][ β][1][ and][ β][2][ satisfying]_**
_∞[(][X][i][) =][ −]_ [1] **_LL22111_** **_MM21222_** **_ββ12_** = **_yy12_** _._ (8)

    − 

By comparing the equations (7) and (8), the output 2(g[1]
_∞[(][x][)][ −]_ _[g]∞[2]_ [(][x][))][ is close to the output of]
_g[1][⋆](x), except that the off-diagonal matrices are scaled by_ [1]2 [and we have][ −][y][2][ on the right hand side.]

We need two tricks if we want to approximate g[1][⋆]: first we need an ensemble using differences of
models, and second we need to additionally correct for the bias in the algorithm.

_Correcting bias using infinite ensembles. Consider the initial AKD (alternating knowledge distillation)_
algorithm illustrated in the Fig. 1a. Let us run two simultaneous runs, ones starting from agent 1 and
another starting from agent 2, outputting models _gt[1]_ _t_
using the final models, we construct the following infinite ensemble. For an input { _[}][, and][ {][g][2][}][ respectively. Then, instead of just] x, we output:_


(−1)[t](gt[1][(][x][) +][ g]t[2][(][x][))] (9)
_t=0_

X


_f_ (x) =
_∞_


That is, we take the models from odd steps t with positive signs and from even ones with negative
signs and sum their predictions. We call this scheme Ensembled Knowledge Distillation (EKD). The
intuition behind our ensemble method is schematically visualized in 1-dimensional case in the Fig.
2b where numbers denote the t variable in the equation (9). We start from the sum of both agents
models obtained after learning from ground truth labels (0-th round). Then we subtract the sum of


-----

Figure 3: AKD, AvgKD, and EKD methods for linear regression on synthetic data with same data

Train loss

Centralized

0.10 Agent 1

Agent 2
AKD

0.08 AvgKD

EKD

MSE Loss0.06

0.04

0.02

0.00

0 40 80 120 160 200 240

Rounds

Train loss

0.12 Centralized

Agent 1

0.10 Agent 2AKD

AvgKD

0.08 EKD

0.06

MSE Loss

0.04

0.02

0.00

0 40 80 120 160 200 240

Rounds

Train loss

0.14 CentralizedAgent 1

Agent 2

0.12 AKD

AvgKD

0.10 EKD

0.08

MSE Loss

0.06

0.04

0.02

0 40 80 120 160 200 240

Rounds

(left), different data (middle), and strong regularization (right). EKD (black) eventually matches
centralized performance (dashed green), whereas AvgKD (solid blue) is worse than only local training
(dashed blue and red). AKD (in solid red) performs the worst and degrades with increasing rounds.

both agents models obtained after the first round of KD. Then we add the sum of both agents models
obtained after the second round of KD and so on. From the section 4.2 we know that in the AKD
process with regularization model gradually degrades towards 0, in other words, intuitively each next
round obtained model adds less value to the whole sum in the EKD scheme. Although, in the lack of
regularization models degradation towards 0 is not always the case (see App. C). But under such an
assumption we we gradually converge to the limit model, which is the point ∞ in the Fig. 2b. We
formalize this and prove the following.

**Proposition VII. The predictions of f** _using (9) satisfies f_ (Xi) = g[1][⋆](Xi) for i 1, 2 _._
_∞_ _∞_ _∈{_ _}_

Thus, not only did we succeed in preventing degeneracy to 0, but we also managed to recover the
predictions of the optimal model. However, note that this comes at a cost of an infinite ensemble.
While we can approximate this using just a finite set of models (as we explore experimentally next),
this still does not recover a single model which matches g[1][⋆].

6 EXPERIMENTS

6.1 SETUP

We consider three settings corresponding to the cases Proposition IV with the agents having

-  the same model architecture and close data distributions (Same model, Same data)

-  different model architectures and close data distributions (Different model, Same data)

-  the same model architecture and different data distributions (Same model, Different data).

The toy experiments solve a linear regression problem of the form Ax[⋆] = b. The data A and b is
split between the two agents randomly in the ‘same data’ case, whereas in the ‘different data’ case
the data is sorted according to b before splitting to maximize heterogeneity.

The real world experiments are conducted using Convolutional Neural Network (CNN), Multi-Layer
Perceptron network (MLP), and Random Forest (RF). We use squared loss since it is closer to the
theoretical setting. In the ‘same model’ setting both agents use the CNN model, whereas agent 2
instead uses an MLP in the ‘different model’ setting. Further, we split the training data randomly
in the ’same data’ setting. For the ’different data’ setting, we split the data by labels and take some
portion Alpha of data from each agent and randomly shuffle taken points between agents. By varying
hyperparameter Alpha we control the level of data heterogeneity between agents. Notice that if
_Alpha = 0 then datasets are completely different between agents, if Alpha = 1 then we have the_
’same data’ setting with i.i.d. split of data between two agents. All other details are presented in the
Appendix A. We next summarize and discuss our results.

6.2 RESULTS AND DISCUSSION

**AvgKD > AKD. In all settings (both synthetic in Fig. 3 and real world in Fig. 4), we see that with**
the increasing number of rounds the performance of AKD significantly degrades whereas that of
AvgKD stabilizes (Fig. 5) regardless of regularization, model and data heterogeneity. Moreover,
from the experiments on MNIST in Fig. 4, we see the following: AKD degrades faster if there is
regularization used, model heterogeneity or data heterogeneity between agents, and the last plays the
most significant role. AvgKD, on the other hand, quickly converges in a few rounds and does not
degrade. However, it fails to match the centralized accuracy as well.


-----

Test Acc. of agents in AKD

Same model, Same data Different model, Same data


Same model, Same data, No Reg.

0 6 12 18 24 30 36

|Col1|Col2|Col3|Centralized Agent 1 Agent 2|Centralized Agent 1 Agent 2|
|---|---|---|---|---|


Centralized
Agent 1
Agent 2

Rounds


Same model, Different data

Centralized
Agent 1
Agent 2

6 12 18 24 30 36

Rounds


0.975

0.950

0.925

0.900

0.875

0.850


12 18 24 30 36

Rounds


12 18 24 30 36

Rounds


Figure 4: Test accuracy of centralized (dashed green), and AKD on MNIST using model starting
from agent 1 (blue) and agent 2 (red) with varying amount of regularization, model heterogeneity,
and data heterogeneity. In all cases, performance degrades with increasing rounds with degradation
speeding up with the increase in regularization, model heterogeneity, or data heterogeneity.


Test Acc. of agents in AvgKD

Different model, Same data


Same model, Same data


Same model, Different data

5 10 15 20

|Col1|Col2|Centralized Agent 1 Agent 2|Centralized Agent 1 Agent 2|
|---|---|---|---|


Centralized
Agent 1
Agent 2

Rounds


0.98

Accuracy0.960.94

0.92


10 15 20

Rounds


10 15 20

Rounds


Figure 5: Test accuracy of AvgKD on MNIST using model starting from agent 1 (blue) and agent
2 (red) with varying model heterogeneity, and data heterogeneity. During training regularization is
used. In all cases, there is no degradation of performance, though the best accuracy is obtained by
agent 1 in round 1 with only local training.


est Acc


|Alpha = 0|Alpha = 0.01|Alpha = 0.1|Alpha = 1|Col5|
|---|---|---|---|---|
||||Centralized Agent 1 Agent 2 AvgKD PKD AKD FedAvg EKD||
|||||Centralized Agent 1 Agent 2 AvgKD PKD AKD FedAvg EKD|


8

Centralized
Agent 1
Agent 2
AvgKD
PKD
AKD
FedAvg
EKD

Rounds


Alpha = 0.01

8

Rounds


Alpha = 0.1

8

Rounds


Alpha = 1

8

Rounds


0.96

0.88

0.80

0.72

0.64

0.56

0.48


16


16


16


16


Figure 6: Test accuracy on MNIST with varying data heterogeneity in the setting of ’same model’. In
case of high data heterogeneity (small Alphas), both agents benefit from the AvgKD, PKD and EKD
schemes. Moreover, AvgKD and EKD consistently outperform FedAvg scheme.

**EKD works but needs large ensembles. In the synthetic setting (Fig. 3), in 250 rounds (ensemble**
of 500 models) it even matches the centralized model. However, in the real world setting (Fig. 6) the
improvement is slower and it does not match centralized performance. This might be due to the small
number of rounds run (only 20). EKD is also the only method that improves with subsequent rounds.
Finally, we observed that increasing regularization actually speeds up the convergence of EKD in the
synthetic setting (Fig. 3).

**Data heterogeneity is the main bottleneck. In all our experiments, both data and model hetero-**
geneity degraded the performance of AKD, PKD (Parallel KD introduced in App. E) and AvgKD.
However, data heterogeneity has a much stronger effect. This confirms our theory that mismatch
between agents data leads to loss of information when using knowledge distillation. Overcoming
this data heterogeneity is the crucial challenge for practical model agnostic FL. Fig. 6 shows how
all schemes behave in dependence on data heterogeneity. Indeed, the higher data heterogeneity the
faster speed of degradation for the AKD and PKD schemes. In the case of the AvgKD scheme, we
see that agents do improve over their local models and this improvement is larger with more data
heterogeneity.


-----

6.3 ADDITIONAL EXTENSIONS AND EXPERIMENTS

In App. G, we extend our algorithms to M agents and show experimentally in App. H.5 for AvgKD
algorithm that the same trends hold there as well. Our conclusions also hold for the cross-entropy
loss (Figs. 11, 12), for highly heterogeneous model case with MLP and Random Forests (Figs. 13,

14), as well on other datasets and models (VGG on CIFAR10 in Figs. 9, 10). In the latter, we see
the same trends as on MNIST for all the schemes except EKD which is probably due to the use of
cross-entropy loss function and, as a result, models being further from the kernel regime. Moreover,
speed of degradation is higher if there is the model heterogeneity (Figs. 8, 10) and EKD does not help
even on MNIST dataset (Fig. 8). Finally, all the schemes are compared to the more standard FedAvg
that is not applicable in the ’different model’ setting and is outperformed by the AvgKD scheme
at highly data heterogeneous regimes. That is, AvgKD consistently outperforms all the methods at
highly data heterogeneous regimes indicating it is the most promising variant.

7 CONCLUSION

While the stochastic optimization framework has been very useful in analyzing and developing new
algorithms for federated learning so far, it fundamentally cannot capture learning with different
models. We instead introduced the federated kernel regression framework where we formalized
notions of both model and data heterogeneity. Using this, we analyzed different knowledge distillation
schemes and came to the conclusion that data heterogeneity poses a fundamental challenge limiting
the knowledge that can be transmitted. Further, these theoretical predictions were exactly reflected in
our deep learning experiments as well. Overcoming this data heterogeneity will be crucial to making
KD based model agnostic federated algorithms practical.

We also utilized our framework to design a novel ensembling method motivated by our theory.
However, this method could require very large ensembles (up to 500 models) in order to match the
centralized performance. Thus, we view our method not as a practical algorithm, but more of a demo
on how our framework can be leveraged. Similarly, our experiments are preliminary and are not on
real world complex datasets. We believe there is great potential in further exploring our results and
framework, especially in investigating how to mitigate the effect of data heterogeneity in knowledge
distillation.

ACKNOWLEDGEMENTS

We are really grateful to Martin Jaggi for his insightful comments and support throughout this work.
SPK is partly funded by an SNSF Fellowship and AA is funded by a research scholarship from MLO
lab, EPFL headed by Martin Jaggi. SPK also thanks Celestine Dünner for conversations inspiring
this project.

REFERENCES

Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and
self-distillation in deep learning. arXiv 2012.09816, 2020.

Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E Dahl, and Geoffrey E Hinton. Large scale distributed neural network training through online distillation. arXiv 1804.03235,
2018.

Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical
_society, 68(3):337–404, 1950._

Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for federated
learning on user-held data. arXiv 1611.04482, 2016.

Stephen Boyd and Jon Dattorro. Alternating projections. 01 2003.

Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings
_of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp._
535–541, 2006.


-----

Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated
learning. arXiv 2003.13461, 2020.

Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A
survey. International Journal of Computer Vision, 129(6):1789–1819, 2021.

Thore Graepel, Kristin Lauter, and Michael Naehrig. Ml confidential: Machine learning on encrypted
data. In International Conference on Information Security and Cryptology, pp. 1–21. Springer,
2012.

Felix Grimberg, Mary-Anne Hartley, Sai Praneeth Karimireddy, and Martin Jaggi. Optimal model
averaging: Towards personalized collaborative learning. FL ICML workshop, 2021.

Israel Halperin. The product of projection operators. Acta Sci. Math.(Szeged), 23(1):96–99, 1962.

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
_1503.02531, 2015._

Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv 1912.04977, 2019.

Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U
Stich, and Ananda Theertha Suresh. Mime: Mimicking centralized stochastic algorithms in
federated learning. arXiv 2008.03606, 2020a.

Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and
Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for on-device federated
learning. In 37th International Conference on Machine Learning (ICML), 2020b.

Adrian Lewis, Russell Luke, and Jerome Malick. Local convergence for alternating and averaged
nonconvex projections. arXiv 0709.0109, 2007.

Qinbin Li, Bingsheng He, and Dawn Song. Practical one-shot federated learning for cross-silo setting.
_arXiv 2010.01017, 2020._

Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model
fusion in federated learning. arXiv 2006.07242, 2020.

Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for
personalization with applications to federated learning. arXiv 2002.10619, 2020.

Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Proceedings of
_AISTATS, pp. 1273–1282, 2017._

Aditya Krishna Menon, Ankit Singh Rawat, Sashank J. Reddi, Seungyeon Kim, and Sanjiv Kumar.
Why distillation helps: a statistical perspective. arXiv 2005.10419, 2020.

Hossein Mobahi, Mehrdad Farajtabar, and Peter L. Bartlett. Self-distillation amplifies regularization
in hilbert space. arXiv 2002.05715, 2020.

Angelia Nedic. Distributed gradient methods for convex machine learning problems in networks:
Distributed optimization. IEEE Signal Processing Magazine, 37(3):92–101, 2020.

Kaan Ozkara, Navjot Singh, Deepesh Data, and Suhas Diggavi. Quped: Quantized personalization
via distillation with applications to federated learning. Advances in Neural Information Processing
_Systems, 34, 2021._

Mary Phuong and Christoph H. Lampert. Towards understanding knowledge distillation. arXiv
_2105.13093, 2021._

Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcny,`
Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv 2003.00295,
2020.


-----

Felix Sattler, Arturo Marban, Roman Rischke, and Wojciech Samek. Communication-efficient
federated distillation. arXiv 2012.00632, 2020.

Bernhard Schölkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theorem. In
_International conference on computational learning theory, pp. 416–426. Springer, 2001._

Hyowoon Seo, Jihong Park, Seungeun Oh, Mehdi Bennis, and Seong-Lyun Kim. Federated knowledge
distillation. arXiv 2011.02367, 2020.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition, 2015.

Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. Advances in Neural Information
_Processing Systems, 33, 2020._

Kennan T Smith, Donald C Solmon, and Sheldon L Wagner. Practical and mathematical aspects of
the problem of reconstructing objects from radiographs. Bulletin of the American Mathematical
_Society, 83(6):1227–1270, 1977._

Shagun Sodhani, Olivier Delalleau, Mahmoud Assran, Koustuv Sinha, Nicolas Ballas, and Michael
Rabbat. A closer look at codistillation for distributed training. arXiv 2010.02838, 2020.

Jiaxi Tang, Rakesh Shivanna, Zhe Zhao, Dong Lin, Anima Singh, Ed H. Chi, and Sagar Jain.
Understanding and improving knowledge distillation. arXiv 2002.03532, 2021.

Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat,
Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to federated
optimization. arXiv 2107.06917, 2021.

Kangkang Wang, Rajiv Mathews, Chloé Kiddon, Hubert Eichner, Françoise Beaufays, and Daniel
Ramage. Federated evaluation of on-device personalization. arXiv 1910.10252, 2019a.

Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K. Leung, Christian Makaya, Ting He, and
Kevin Chan. Adaptive federated learning in resource constrained edge computing systems. IEEE
_Journal on Selected Areas in Communications, 37(6):1205–1221, 2019b._

Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. arXiv preprint arXiv:1705.08292, 2017.

Chuhan Wu, Fangzhao Wu, Ruixuan Liu, Lingjuan Lyu, Yongfeng Huang, and Xing Xie. Fedkd:
Communication efficient federated learning via knowledge distillation. arXiv 2108.13323, 2021.

Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast
optimization, network minimization and transfer learning. In Proceedings of the IEEE Conference
_on Computer Vision and Pattern Recognition, pp. 4133–4141, 2017._

Fuxun Yu, Weishan Zhang, Zhuwei Qin, Zirui Xu, Di Wang, Chenchen Liu, Zhi Tian, and Xiang
Chen. Fed2: Feature-aligned federated learning. In Proceedings of the 27th ACM SIGKDD
_Conference on Knowledge Discovery & Data Mining, pp. 2066–2074, 2021._

Fuzhen Zhang. The Schur complement and its applications, volume 4. Springer Science & Business
Media, 2006.

Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. In
_Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4320–4328,_
2018.


-----

A EXPERIMENTS DETAILS

In all real world experiments we use the Adam optimizer with a default regularization (weight decay)
of 3 × 10[−][4], unless in the ‘no regularization’ case when it is set to 0. We split the data between 2
agents by giving a bigger part of data to agent 1 at all ’same data’ experiments. That is, for ’different
data’ experiments and heterogeneous data experiments where we vary Alpha hyperparameter we
split data equally between 2 agents. The non-equal split can help us to see another effect in the
experiments: if agent 2 (with less data) benefits from the communication with agent 1 (with more
data). In heterogeneous data experiments, we explore the significance of data heterogeneity effect on
the behavior of KD scheme. That is why we design an almost ideal setting at all such experiments:
the ’same model’ setting (except experiments with RF and MLP), the equal split of data in terms of
its amount between agents.

**Toy experiments.** The toy experiments solve a linear regression problem of the form Ax[⋆] = b
where A ∈ R[n][×][d] and x[⋆] _∈_ R[d] is randomly generated for n = 1.5d and d = 100. Then, the data A
and b is split between the two agents at proportion 0.6/0.4. This is done randomly in the ‘same data’
case, whereas in the ‘different data’ case the data is sorted according to b before splitting to maximize
heterogeneity. This experiment with the linear kernel is supposed to show if our theory is correct,
especially if the EKD scheme really works.

**Real world experiments.** The real world experiments are conducted using CNN and MLP networks
on MNIST, MLP network and RF model on MNIST, and VGG16[1] (Simonyan & Zisserman, 2015)
and CNN models on CIFAR10 datasets. Further, we split the training data randomly at proportion
0.7/0.3 in the ’same data’ setting. For the ’different data’ setting, we split the data by labels: agent
1 has ’0’ to ’4’ labeled data points, agent 2 has ’5’ to ’9’. Then we take randomly from each agent
some Alpha = 0.1 portion of data, combine it and randomly return data points to both agents from
this combined dataset.

B ALTERNATING KD WITH REGULARIZATION

**Different models.** Keeping the notation of section 4 we can construct the following matrix:


**_L11_** **_L12_** 0 0
**_L21_** **_L22_** 0 0
0 0 **_M11_** **_M12_**
0 0 **_M21_** **_M22_**


**_K =_** = 21 22 _._
0 **_M_** 0 0 **_M11_** **_M12_**
 

0 0 **_M21_** **_M22_**

 
 

Notice that each of the sub-blocks L and M are symmetric positive semi-definite matrices. This
makes matrix K symmetric positive semi-definite matrix and it has eigendecomposition form:

**_K = V_** _[⊤]DV_ and **_V =_** **_V1V2V[˜]1V[˜]2_** _,_

where D and V are R[2(][N][1][+][N][2][)][×][2(][N][1][+][N][2][)] diagonal and orthogonal matrices correspondingly. This  
means that the R[2(][N][1][+][N][2][)][×][2(][N][1][+][N][2][)] matrices V1, V2, **_V[˜]1,_** **_V[˜]2 are also orthogonal to each other. Then_**
one can deduce:

**_Lij = Vi[⊤][DV][j][,]_** and **_Mij = V[˜]i[⊤][D][ ˜]Vj_** _∀i, j = 1, 2 ._

In AKD setting only agent 1 has labeled data. The solution of learning from the dataset D1 evaluated
at set X2 is the following:

_g0[1][(][X][2][) =][ L][21][(][c][I][ +][ L][11][)][−][1][y][1]_ [=][ V][ ⊤]2 [((][V][ ⊤]1 [(][c][I][ +][ D][)][V][1][)][−][1][V][ ⊤]1 **_[D][)][⊤][y][1]_** _[.]_ (10)

Let us introduce notation:

**_P1 = V1(V1[⊤][(][c][I][ +][ D][)][V][1][)][−][1][V][ ⊤]1_** [(][c][I][ +][ D][)] and **_P˜2 = V[˜]2( V[˜]2[⊤][(][c][I][ +][ D][) ˜]V2)[−][1][ ˜]V2[⊤][(][c][I][ +][ D][)][ .]_**

These are well known oblique (weighted) projection matrices on the subspaces spanned by the
columns of matrices V1 and **_V[˜]2 correspondingly, where the scalar product is defined with Gram_**
matrix G = cI + D. Similarly one can define P2 and **_P[˜]1._**

1This model is also a convolutional neural network, but in our experiments it is bigger than CNN model.


**_K =_**


-----

Given the introduced notation and using the fact that V2[⊤][V][1] [=][ 0][, we can rewrite equation][ 10][ as:]

_g0[1][(][X][2][) =][ V][ ⊤]2_ **_[P][ ⊤]1_** **_[V][1][y][1]_** _[.]_ (11)

by agent 2 can be written as:Similarly, given X1 ⊆D1 and agent 1 learned from _Y[ˆ]2 = g0[1][(][X][2][)][, inferred prediction][ ˆ]Y1 = g1[1][(][X][1][)]_

_g1[1][(][X][1][) = ˜]V1[⊤]P˜2[⊤]V[˜]2V2[⊤][P][ ⊤]1_ **_[z][1][,]_** where **_z1 = V1y1 ._**

At this point we need to introduce additional notation:

**_C1 = V1V[˜]1[⊤]_** and **_C˜2 = V[˜]2V2[⊤]_** _[.]_

These are matrices of contraction linear mappings.

One can now repeat the whole process again with replacement Y1 by _Y[ˆ]1. The predictions by agent 1_
and agent 2 after such 2t rounds of AKD are:

_t_ _t_
_g2[1]t[(][X][2][) =][ V][ ⊤]2_ **_[P][ ⊤]1_** **_C1P[˜]2[⊤]C[˜]2P1[⊤]_** **_z1_** and _g2[1]t+1[(][X][1][) = ˜]V1[⊤]P˜2[⊤]C[˜]2P1[⊤]_ **_C1P[˜]2[⊤]C[˜]2P1[⊤]_** **_z1 ._**
   (12)

If one considers the case where agents have the same kernel u1 = u2 = u then one should ’remove
all tildas’ in the above expressions and the obtained expressions are applied. This means M = L,

_t_
**_V1 = V[˜]1 andt_** **_V2 = V[˜]2. Crucial changes in this case aret_** **_C1 = C[˜]2 →_** **_I and_** **_C1P[˜]2[⊤]C[˜]2P1[⊤]_** _→_
**_P2[⊤][P][ ⊤]1_** . The matrix **_P2[⊤][P][ ⊤]1_** is an alternating projection (Boyd & Dattorro, 2003) operator
after t steps. Given 2 closed convex sets, alternating projection algorithm in the limit finds a point in
     
the intersection of these sets, provided they intersect (Boyd & Dattorro, 2003; Lewis et al., 2007). In
our case matrices operators P1 and P2 project onto linear spaces spanned by columns of matrices
**_V1 and V2 correspondingly. These are orthogonal linear subspaces, hence the unique point of their_**
_t_
intersection is the origin point 0. This means that **_P2[⊤][P][ ⊤]1_** _−t−−→_
_→∞_ **[0][ and in the limit of such AKD]**
procedure both agents predict 0 for any data point.
  

**Speed of degradation.** The speed of convergence for the alternating projections algorithm is known
and defined by the minimal angle φ between corresponding sets (Aronszajn, 1950) (only non-zero
elements from sets one has to consider):

(P2P1)[t] **_v_** (cos(φ))[2][t][−][1] **_v_** = (cos(φ))[2][t][−][1] as **_v_** = 1,
_||_ _|| ≤_ _||_ _||_ _||_ _||_

where v is one of the columns of matrix V1. In our case we can write the expression for the cosine:


**_v1[⊤][(][c][I][ +][ D][)][v][2][|]_** **_v1[⊤][Dv][2][|]_**
cos(φ) = max _|_ ) = max _|_ ) .
**_v1,v2[(]_** (v1[⊤][(][c][I][ +][ D][)][v][1][)][ ·][ (][v]2[⊤][(][c][I][ +][ D][)][v][2][)] **_v1,v2[(]_** (c + v1[⊤][Dv][1][)][ ·][ (][c][ +][ v]2[⊤][Dv][2][)]

where v1 and v2p are the vectors from subspaces spanned by columns of matricesp **_V1 and V2 corre-_**
spondingly. Hence the speed of convergence depends on the elements of the matrix L12. Intuitively
these elements play the role of the measure of ’closeness’ between data points. This means that the
’closer’ points of sets X1 and X2 the higher absolute values of elements in matrix L12. Moreover,
we see inversely proportional dependence on the regularization constant c. All these sums up in the
following proposition which is the formal version of the proposition IV:
**Proposition VIII (Formal). The rate of convergence of gt[1][(][x][)][ to][ 0][ gets faster if:]**

-  larger regularization constant c is used during the training,

-  smaller the eigenvalues of the matrix V1V[˜]1[⊤][, or]

-  smaller absolute value of non-diagonal block L12


C ALTERNATING KD WITHOUT REGULARIZATION

Before we saw that models of both agents degrade if one uses regularization. A natural question to
ask if models will degrade in the lack of regularization. Consider the problem (1) with c → 0, so


-----

the regularization term cancels out. In the general case, there are many possible solutions as kernel
matrix K may have 0 eigenvalues. Motivated by the fact that Stochastic Gradient Descent (SGD)
tends to find the solution with minimal norm (Wilson et al., 2017), we propose to analyze the minimal
norm solution in the problem of alternating knowledge distillation with 2 agents each with private
dataset. For simplicity let us assume that agents have the same model architecture. Then the agent I
solution evaluated at set X2 with all the above notation can be written as:

_g0[⋆][(][X][2][) =][ K][21][K]11[†]_ **_[y][1][ =][ V][ ⊤]2_** **_[DV][1][(][V][ ⊤]1_** **_[DV][1][)][†][y][1][,]_** (13)

where † stands for pseudoinverse.
In this section let us consider 3 possible settings one can have:

1. Self-distillation: The datasets and models of both agents are the same.
2. Distillation with K > 0: The datasets of both agents are different and private. Kernel matrix
**_K is positive definite._**

3. Distillation with K ≥ 0: The datasets of both agents are different and private. Kernel matrix
**_K is positive semi-definite._**


**Self-distillation.** Given the dataset D = X × Y, the solution of the supervised learning with
evaluation at any x ∈ R[d] is:

_g0[⋆][(][x][) =][ l]x[⊤][(][V][ ⊤][DV][ )][†][y][.]_

Then the expression for the self-distillation step with evaluation at any x ∈ R[d] is:

_g1[⋆][(][x][) =][ l]x[⊤][(][V][ ⊤][DV][ )][†][V][ ⊤][DV][ (][V][ ⊤][DV][ )][†][y][ =][ l]x[⊤][(][V][ ⊤][DV][ )][†][y][ =][ g]0[⋆][(][x][)][,]_

where property of pseudoinverse matrix was used: A[†]AA[†] = A[†].
That is, self-distillation round does not change obtained model. One can repeat self-distillation step t
times and there is no change in the model:

_gt[⋆][(][x][) =][ l]x[⊤][(][V][ ⊤][DV][ )][†][y][ =][ g]0[⋆][(][x][)][,]_

Hence in the no regularization setting self-distillation step does not give any change in the obtained
model.

**Distillation with K > 0.** In this setting we have the following identity K[†] = K[−][1] and results are
quite similar to the setting with regularization. But now the Gram matrix of scalar product in linear
space is D instead of cI + D in the regularized setting. By assumption on matrix K it follows that
**_D is of full rank and the alternating projection algorithm converges to the origin point 0. Therefore_**
in the limit of AKD steps predictions by models of two agents will degrade towards 0.

**Distillation with K ≥** 0. In this setting kernel matrix K has at least one 0 eigenvalue and we take
for the analysis the minimal norm solution (13). We can rewrite this solution:

_g0[⋆][(][X][2][) =][ V][ ⊤]2_ **_[DV][1][(][V][ ⊤]1_** **_[DV][1][)][†][y][1]_** [=][ V][ ⊤]2 **_Pˆ1[⊤][z][1][,]_**

where **_P[ˆ]1 = V1(V1[⊤][DV][1][)][†][V][ ⊤]1_** **_[D][ and][ z][1]_** [=][ V][1][y][1][.]

One can notice the following projection properties of matrix **_P[ˆ]1:_**

**_Pˆ1[2]_** [= ˆ]P1 and **_Pˆ1V1(V1[⊤][DV][1][)][†][ =][ V][1][(][V][ ⊤]1_** **_[DV][1][)][†][ .]_**

That is, **_P[ˆ]1 is a projection matrix with eigenspace spanned by columns of the matrix V1(V1[⊤][DV][1][)][†][.]_**
Similarly, one can define **_P[ˆ]2. Then the solution for the first AKD step evaluated at set X1 is:_**

_g1[⋆][(][X][1][) =][ V][ ⊤]1_ **_[DV][2][(][V][ ⊤]2_** **_[DV][2][)][†][V][ ⊤]2_** **_Pˆ1[⊤][z][1]_** [=][ V][ ⊤]1 **_Pˆ2[⊤]P[ˆ]1[⊤][z][1][,]_**

and after t rounds we obtain for agent I and agent II correspondingly:

_g2[⋆]t[(][X][2][) =][ V][ ⊤]2_ **_Pˆ1[⊤][( ˆ]P2[⊤]P[ˆ]1[⊤][)][t][z][1]_** and _g2[⋆]t+1[(][X][1][) =][ V][ ⊤]1_ [( ˆ]P2[⊤]P[ˆ]1[⊤][)][t][+1][z][1][.]

In the limit of the distillation rounds operator ( P[ˆ]2P[ˆ]1)[t] tends to the projection on the intersection
of 2 subspaces spanned by the columns of matrices V1(V1[⊤][DV][1][)][†][ and][ V][2][(][V][ ⊤]2 **_[DV][2][)][†][. We should]_**


-----

**Train**

**Train**

**1 round**


(b) PKD scheme


0


(a) Illustrative projection 3D


highlight that in general, the intersection set in this case consists not only from the origin point
**0 but some rays that lie simultaneously in the eigenspaces of both projectors** **_P[ˆ]1 and_** **_P[ˆ]2. The_**
illustrative example is shown in the Fig. 7a. We perform the alternating projection algorithm between
orthogonal linear spaces that have a ray in the intersection and we converge to some non-zero point
that belongs to this ray. The intersection of eigenspaces of both projectors **_P[ˆ]1 and_** **_P[ˆ]2 is the set of_**
points x ∈ Span(V1(V1[⊤][DV][1][)][†][)][ s.t.][ ˆ]P1P[ˆ]2x = x.

D AVERAGED KD

In this section we present the detailed analysis of the algorithm presented in the section 4.2 keeping
the notation of the section B. As a reminder, models of both agents after round t of AvgKD algorithm
are as follows:

_gt[1][(][X][2][) = 1]_ _t_ 1[(][X][1][)) = 1] 2 **_[P][ ⊤]1_** [(][z][1] [+][ g]t[2] 1[)][,]

2 **_[L][21][(][c][I][ +][ L][11][)][−][1][(][y][1][ +][ g][2]−_** 2 **_[V][ ⊤]_** _−_


_gt[2][(][X][1][) = 1]_ _t_ 1[(][X][2][) +][ y][2][) = 1]

2 **_[M][12][(][c][I][ +][ M][22][)][−][1][(][g][1]−_** 2


**_V˜1[⊤]P˜2[⊤][(][g]t[1]−1_** [+][ z][2][)][.]


where

_gt[1]_ 1 [= 1] _t_ 2[(][X][1][))][,] _t_ 2 (14)
_−_ 2 [(][c][I][ +][ D][)][V][1][(][c][I][ +][ L][11][)][−][1][(][y][1][ +][ g][2]− _∀_ _≥_

_gt[2]_ 1 [= 1] **_V2(cI + M22)[−][1](gt[1]_** 2[(][X][2][) +][ y][2][)][,] _t_ 2 (15)
_−_ 2 [(][c][I][ +][ D][) ˜] _−_ _∀_ _≥_

_t = 1 :_ _g0[1]_ [=][ P][ ⊤]1 **_[z][1]_** and _g0[2]_ [= ˜]P2[⊤][z][2][,] where **_z1 = V1y[1],_** **_z2 = V[˜]2y[2]_** _._ (16)


The illustration of this process is presented in the Fig. 2a.

Consider the sequence of solutions for the agent 1 with evaluation at X2:

-  Supervised Learning:
_g0[1][(][X][2][) =][ V][ ⊤]2_ **_[P][ ⊤]1_** **_[z][1]_**



-  1st round of KD:

-  2nd round of KD:


_g1[1][(][X][2][) =][ V][ ⊤]2_ **_P2 1[⊤]_** [(][z][1][ +][ C][1][ ˜]P2[⊤][z][2][)]


_g2[1][(][X][2][) =][ V][ ⊤]2_ **_P1[⊤]_** **_P2[⊤]_**

2 [(][z][1][ +][ C][1]2[ ˜]

-  3rd round of KD:


**_z2 +_** **_[C][1][ ˜]P2[⊤]C[˜]2P1[⊤]_**


**_z1)_**


_g3[1][(][X][2][) =][ V][ ⊤]2_ **_P1[⊤]_** **_P2[⊤]_**

2 [(][z][1][ +][ C][1]2[ ˜]


**_z2 +_** **_[C][1][ ˜]P2[⊤]C[˜]2P1[⊤]_**


**_z1 +_** **_[C][1][ ˜]P2[⊤]C[˜]2P1[⊤][C][1]P[˜]2[⊤]_**


**_z2)_**


-----

-  t-th round of KD:

_t_

_gt[1][(][X][2][) =][ V][ ⊤]2_ **_P2 1[⊤]_** [(] ( **_[C][1][ ˜]P2[⊤]4C[˜]2P1[⊤]_**

_i=0_

X


_t−1(_ **_C˜2P1[⊤][C][1]P[˜]2[⊤]_**

4

_i=0_

X


)[i])z1 + V2[⊤] **_P1[⊤][C]4[1]P[˜]2[⊤]_**


)[i])z2

)[†]z2,



-  The limit of KD steps:


_g∞[1]_ [(][X][2][) =][ V][ ⊤]2 **_P2 1[⊤]_** [(][I][ −] **_[C][1][ ˜]P2[⊤]4C[˜]2P1[⊤]_** )[†]z1 + V2[⊤] **_P1[⊤][C]4[1]P[˜]2[⊤]_** (I − **_C˜2P1[⊤]4[C][1]P[˜]2[⊤]_** )[†]z2,

where † stands for pseudoinverse.

Let us analyze the limit solution and consider the first term of its expression. One can deduce the
following identity: [2]


**_V2[⊤]_** **_P˜2 1[⊤]_** [(][I][ −] **_[C][1][ ˜]P2[⊤]4C[˜]2P1[⊤]_**


)[†]z1 = (17)


(cI + V1[⊤][DV][1] _[−]_ **_V˜1[⊤]2[D][ ˜]V2_**


**_V2[⊤][DV][1]_**


2 **_[DV][1]_**
(cI + V[˜]2[⊤][D][ ˜]V2)[−][1][ V][ ⊤]2


)[†]y1 (18)


In a similar manner, we can deal with the second term:


**_V2[⊤]_** **_P1[⊤][C]4[1]P[˜]2[⊤]_**



[1]P[˜]2[⊤] (I **_C˜2P1[⊤][C][1]P[˜]2[⊤]_** )[†]z2 =

4 _−_ 4

(cI + V1[⊤][DV][1] _[−]_ **_V˜1[⊤]2[D][ ˜]V2_** (cI + V[˜]2[⊤][D][ ˜]V2)[−][1][ V][ ⊤]2 2[DV][1]


)[†][ ˜]V1[⊤][D][ ˜]V2


**_V2[⊤][DV][1]_**


(cI + V[˜]2[⊤][D][ ˜]V2)[−][1]y2


Now, we notice Schur complement expression in the equation (18):

(cI + V1[⊤][DV][1] _[−]_ **_V˜1[⊤]2[D][ ˜]V2_** (cI + V[˜]2[⊤][D][ ˜]V2)[−][1][ V][ ⊤]2 2[DV][1] ) .

This means that we can consider the following problem:
L11L +221 cI **_M22M2 +12_** _cI ββ12_ = **_V1[⊤]V[DV]2[⊤]2[DV][1]_** [+][1] _[ c][I]_ **_V˜2[⊤]V[D]˜1[⊤][ ˜]V2[D]2[ ˜]V +2_** _cI! ββ12_


**_y1_**
**_y2_**
_−_


_, (19)_


and derive that g∞[1] [(][X][2][) =][ V][ ⊤]2 2[DV][1] **_β1 and g∞[2]_** [(][X][1][) =][ −] **_V˜1[⊤]2[D][ ˜]V2_** **_β2, where β1, β2 are defined as_**

the solution to the problem (19). Given this, we can derive the following:


**_V1[⊤][DV][1][β][1]_** [+] **_V˜1[⊤]2[D][ ˜]V2_**


**_β2 = y1_** _cβ1 = 2g[1]_ (20)
_−_ _∞[(][X][1][)][ −]_ _[g]∞[2]_ [(][X][1][)][,]


2 **_[DV][1]_**
_−_ **_[V][ ⊤]2_** **_β1 −_** **_V[˜]2[⊤][D][ ˜]V2β2 = y2 + cβ2 = 2g∞[2]_** [(][X][2][)][ −] _[g]∞[1]_ [(][X][2][)][.] (21)

As one can see, there is a strong relation between the limit KD solutions and the solution of a linear
system of equations with modified matrix K and right-hand side. Mainly, we take the kernel matrix
and divide its non-diagonal blocks by 2, which intuitively shows that our final model accounts for
the reduction of the ’closeness’ between datasets D1 and D2. And on the right-hand side, we see
_−y2 instead of y2 which is quite a ’artificial’ effect. Overall these results in the fact that both limit_
solutions (for each agent) do not give a ground truth prediction for X1 and X2 individually, which
one can see from equation (20) with c = 0. That is, we need combine the predictions of both agents
in a specific way to get ground truth labels for datasets D1 and D2. Moreover, the way we combine
the solutions differs between datasets D1 and D2 that one can see from comparison of right-hand
sides of expressions (20) and (21). Given all the above, to predict optimal labels we need to change
we way we combine models of agents in dependence on a dataset, but an usual desire is to have one
model that predicts ground truth labels for at least both training datasets.

To obtain the expressions for the case of identical models one should ’remove all tildas’ in the above
expressions and by setting V1 = V[˜]1, V2 = V[˜]2, C1 = C[˜]2 → **_I and (C1P[˜]2[⊤]C[˜]2P1[⊤][)][t][ →]_** [(][P][ ⊤]2 **_[P][ ⊤]1_** [)][t]

2In case of c = 0, the whole analysis can be repeated by replacing inverse sign with † sign and using the
following fact for positive semidefinite matrices (Zhang, 2006):


**_V2[⊤][DV]1[(][V][ ⊤]1_** **_[DV]1[)][†][(][V][ ⊤]1_** **_[DV]1[) =][ V][ ⊤]2_** **_[DV]1_** _[.]_


-----

PARALLEL KD


In this section, we theoretically analyze a slight modification of the AvgKD algorithm which we
call Parallel KD (PKD). Keeping the notation of sections B and C, denote the data on agent 1
as D1 = (X [1], y[1]) where X [1][i, :] = x[1]n [and][ y][1][[][i][] =][ y]i[1][. Correspondingly, for agent 2 we have]
_D[2]_ = (X [2], y[2]). Now starting from ˆy0[1] [=][ y][1][,][ ˆ]y0[2] [=][ y][2][, in each round][ t][ ≥] [0][:]

a. Agents 1 and 2 train their model on datasets (X [1], ˆyt[1][)][ and][ (][X] [2][,][ ˆ]yt[2][)][ to obtain][ g]t[1] [and][ g]t[2][.]

b. Agents exchange gt[1] [and][ g]t[2] [between each other.]

c. Agents use exchanged models to predict labels ˆyt[1]+1 [=][ ˆ]yt[1][+][g]2t[2][(][X] [1][)], ˆyt[2]+1 [=][ ˆ]yt[2][+][g]2t[1][(][X] [2][)] .

The summary of the algorithm is depicted in Figure 7b. That is, we learn from the average of 2
agents’ predictions. For simplicity, we are going to analyze the scheme without regularization and we
take always a minimum norm solution. Notice that there is no exchange of raw data but only of the
trained models.

Let us analyse KD algorithm where solutions for agent I (g[1]) and agent II (g[2]) obtained as:

_gt[1][(][X][2][) = 1]2_ **_[K][21][(][K][11][)][†][(][g]t[1]−1[(][X][1][) +][ g]t[2]−1[(][X][1][)) = 1]2_** **_[V][ ⊤]2_** **_Pˆ1[⊤][(][g]t[1]−1_** [+][ g]t[2]−1[)][,] (22)

_gt[2][(][X][1][) = 1]2_ **_[K][12][(][K][22][)][†][(][g]t[1]−1[(][X][2][) +][ g]t[2]−1[(][X][2][)) = 1]2_** **_[V][ ⊤]1_** **_Pˆ2[⊤][(][g]t[1]−1_** [+][ g]t[2]−1[)][,] (23)


where


_gt[1]_ 1 [= 1] _t_ 2[(][X][1][) +][ g]t[2] 2[(][X][1][))][,] _t_ 2 (24)
_−_ 2 **_[DV][1][(][K][11][)][†][(][g][1]−_** _−_ _∀_ _≥_

_gt[2]_ 1 [= 1] _t_ 2[(][X][2][) +][ g]t[2] 2[(][X][2][))][,] _t_ 2 (25)
_−_ 2 **_[DV][2][(][K][22][)][†][(][g][1]−_** _−_ _∀_ _≥_

_t = 1 :_ _g0[1]_ [= ˆ]P1[⊤][z][1] and _g0[2]_ [= ˆ]P2[⊤][z][2][,] where **_zi = Viyi,_** _i = 1, 2 ._ (26)

Consider gt[1] [for][ t][ ≥] [1][:]


_gt[1]_ [= 1]

2



[= 1]2 **_Pˆ1[⊤][(][g]t[1]−1_** [+][ g]t[2]−1[) = 1]2 **_Pˆ1[⊤][(1]2_** **_Pˆ1[⊤][(][g]t[1]−2_** [+][ g]t[2]−2[) + 1]2 **_Pˆ2[⊤][(][g]t[1]−2_** [+][ g]t[2]−2[)) =]

**_Pˆ1[⊤][(1]2_** [( ˆ]P1[⊤] [+ ˆ]P2[⊤][)(][g]t[1]−2 [+][ g]t[2]−2[)) =][ ...][ = 1]2[t][ ˆ]P1[⊤][( ˆ]P1[⊤] [+ ˆ]P2[⊤][)][t][−][1][(][g]0[1] [+][ g]0[2][) =]

= 2[1][t][ ˆ]P1[⊤][( ˆ]P1[⊤] [+ ˆ]P2[⊤][)][t][−][1][( ˆ]P1[⊤][z][1] [+ ˆ]P2[⊤][z][2][)]


= [1]


The form of the solutions reminds the method of averaged projection (Lewis et al., 2007) with
operator **_P[ˆ]1 + P[ˆ]2, which is similar to alternating projection converges to the intersection point of 2_**
subspaces[3]. That is, similarly to AKD in the case with the regularization we expect the solution to
converge to the origin point 0 in the limit of the distillation steps. As a result, after some point, one
expects steady degradation of the predictions of both agents in the PKD scheme.

F ENSEMBLED KD


One can consider the following problem:

**_L11_** **_M12_** = **_V1[⊤][DV][1]_** **_V˜1[⊤][D][ ˜]V2_** **_β1_**
L21 **_M22_** V2[⊤][DV][1] **_V˜2[⊤][D][ ˜]V2 β2_**

with the following identities:


**_y1_**
**_y2_**


(27)


**_V1[⊤][DV][1][β][1]_** [+ ˜]V1[⊤][D][ ˜]V2β2 = y1 and **_V2[⊤][DV][1][β][1]_** [+ ˜]V2[⊤][D][ ˜]V2β2 = y2 . (28)

3Actually, there is an explicit relation between alternating and averaged projections (Lewis et al., 2007).


-----

One can find β1, β2 and deduce the following prediction by the model associated with the system
(27) for i = 1, 2:

**_Vi[⊤][DV][1][β][1]_** [+ ˜]Vi[⊤][D][ ˜]V2β2 =

**_Vi[⊤][P][ ⊤]1_** [(][I][ −] **_[C][1]P[˜]2[⊤]C[˜]2P1[⊤][)][†][z][1]_** _[−]_ **_[V][ ⊤]i_** **_[P][ ⊤]1_** **_[C][1]P[˜]2[⊤][(][I][ −]_** **_C[˜]2P1[⊤][C][1]P[˜]2[⊤][)][†][z][2][+]_**
**_V˜i[⊤]P˜2[⊤][(][I][ −]_** **_C[˜]2P1[⊤][C][1]P[˜]2[⊤][)][†][z][2]_** _[−]_ **_V[˜]i[⊤][P][ ⊤]2_** **_C[˜]2P1[⊤][(][I][ −]_** **_[C][1]P[˜]2[⊤]C[˜]2P1[⊤][)][†][z][1]_** [=]

_∞_ _∞_ (29)

**_Vi[⊤][P][ ⊤]1_** _t=0(C1P[˜]2[⊤]C[˜]2P1[⊤][)][t][z][1]_ _[−]_ **_[V][ ⊤]i_** **_[P][ ⊤]1_** **_[C][1]P[˜]2[⊤]_** _t=0( C[˜]2P1[⊤][C][1]P[˜]2[⊤][)][t][z][2][+]_

X X

_∞_ _∞_

**_V˜i[⊤][P][ ⊤]2_** _t=0( C[˜]2P1[⊤][C][1]P[˜]2[⊤][)][t][z][2]_ _[−]_ **_V[˜]i[⊤][P][ ⊤]2_** **_C[˜]2P1[⊤]_** _t=0(C1P[˜]2[⊤]C[˜]2P1[⊤][)][t][z][1][.]_

X X

From this one can easily deduce (28) which means that for datasets of both agents this model predicts
ground truth labels. To obtain the expressions for the case of identical models one should ’remove
all tildas’ in all the above expressions and by setting V1 = V[˜]1, V2 = V[˜]2, C1 = C[˜]2 → **_I and_**
(C1P[˜]2[⊤]C[˜]2P1[⊤][)][t][ →] [(][P][ ⊤]2 **_[P][ ⊤]1_** [)][t]

The last question is how one can construct the scheme of iterative KD rounds to obtain the above
expression for the limit model. From the form of the prediction, we conclude that one should use
models obtained in the process of AKD. There are many possible schemes how one can combine
these models to obtain the desired result. One of the simplest possibilities is presented in the section
5.2.

G M-AGENT SCHEMES

The natural question to ask is how one could extend the discussed schemes to the setting of M agents.
In this section, we address this question and explicitly provide the description of each algorithm for
the setting of M agents.

**Scalability and privacy.** Before diving into particular algorithms we investigate some important
concerns about our KD based framework. A naive implementation of our methods will require each
agent sharing their model with all other agents. This will incur significant communication costs (M [2]),
storage costs (each agent has to store M models), and is not compatible with secure aggregation
(Bonawitz et al., 2016) potentially leaking information. One potential approach to alleviating these
concerns is to use an server and homomorphic encryption (Graepel et al., 2012). Homomorphic
encryption allows agent 1 to compute predictions on agent 2’s data f1(X [2]) without learning anything
about X [2] i.e. there exists a procedure Hom such that given encrypted data Enc(X [2]), we can compute

Hom(f1, Enc(X [2])) = Enc(f1(X [2])) .

Given access to such a primitive, we can use a dedicated server (agent 0) to whom all models f1, . . ., fM are sent. Let us define some weighted sum of the predictions as fα(X) :=
_M_
_i=1_ _[α][i][f][i][(][X][)][ .][ Then, using homomorphic encryption, each agent][ i][ can compute][ Enc][(][f][α][(][X]_ _[i][)))]_
in a private manner without leaking any information to agent 0. This makes the communication
Pcost linear in M instead of quadratic, and also makes it more private and secure. A full fledged
investigation of the scalability, privacy, and security of such an approach is left for future work. With
this caveat out of the way, we next discuss some concrete algorithms.

**AKD with M agents.** To extend AKD scheme to a multi-agent setting and corresponding theory
we need to start by understanding what the alternating projection algorithm is in the case of M convex
sets. Suppose we want to find the intersection point of M affine sets _i, for i = 1, ..., M_ . In terms of
_C_
alternating projection algorithm we can write the following extension of it (Halperin, 1962):

**_P_** _i=1[C][i]_ [(][x][) = (][P][C][M][ P][C][M] _[−][1]_ _[...][P][C][1]_ [)][∞][(][x][)] (30)
_∩[n]_

For our algorithm, it means that agent 1 passes its model to agent 2, then agent 2 passes its model
to agent 3 and so until agent M that passes its model to agent 1, and then the cycle repeats. That is,
as before, we denote the data on agent 1 as D1 = (X [1], y[1]) where X [1][i, :] = x[1]n [and][ y][1][[][i][] =][ y]i[1][.,]
for all other agents we have D[i] = (X _[i], y[i]), for i = 2, ..., M_ . Now starting from ˆy0[1] [=][ y][1][, in each]
rounds t, ..., t + M − 1, t ≥ 0:


-----

a. Agent 1 trains their model on dataset (X [1], ˆyt[1][)][ to obtain][ g]t[1][.]

b. for i = 2, ..., M :

b.1 Agent i receives gt[1]+i 2 [and uses it to predict labels][ ˆ]yt[i]+i 1 [=][ g]t[1]+i 2[(][X] _[i][)][.]_
_−_ _−_ _−_

b.2 Agent i trains their model on dataset (X _[i], ˆyt[i]+i_ 1[)][ to obtain][ g]t[1]+i 1[.]
_−_ _−_

c. Agent 1 receives a model gt[1]+M 1 [from agent M and predicts][ ˆ]yt[1]+M [=][ g]t[1]+M 1[(][X] [1][)][.]
_−_ _−_

As before, there is no exchange of raw data but only of the trained models. And given all the results
deduced before, all the models from some point will start to degenerate. The rate of convergence
of such an algorithm is defined similarly to alternating projection in case 2 sets and can be found in
Smith et al. (1977).

**PKD with M agents.** PKD scheme can be easily extended to the multi-agent setting analogously
to how averaged projection algorithm can be extended to the multi-set setting (Lewis et al., 2007).
Suppose we want to find the intersection point of M affine sets _i, for i = 1, ..., M then the in terms_
_C_
of averaged projection we have the following:


**_P_** _i=1[C][i]_ [(][x][) = ( 1]
_∩[n]_ _M_


**_PCi_** )[∞](x) (31)
_i=1_

X


This expression easily translates into the PKD algorithm for M agents. Denote the data on all agents
as D[i] = (X _[i], y[i]), for i = 2, ..., M_ . Now starting from ˆy0[i] [=][ y][i][,][ for][ i][ = 2][, ..., M] [, in each round]
_t ≥_ 0:

a. for i = 1, ..., M :

a.1 Agent i trains their model on dataset (X _[i], ˆyt[i][)][ to obtain][ g]t[i][.]_

b. for i = 1, ..., M :


b.1 Agent i receives models gt[j][,][ for][ j][ = 1][, ..., M, j][ ̸][=][ i][ from all other agents.]

b.2 Agent i use received models to predict ˆyt[i]+1 [=] **_yˆt[i][+][P][M]j=1M,j≠_** _i_ _[g]t[j]_ [(][X] _[i][)]_ .

As in the case of 2 agents, there is no exchange of data between agents, but only models. This scheme
requires all to all communication.

**AvgKD with M agents.** Similarly to PKD algorithm we can extend AvgKD algorithm as follows:
starting from ˆy0[i] [=][ y][i][,][ for][ i][ = 2][, ..., M] [, in each round][ t][ ≥] [0][:]

a. for i = 1, ..., M :

a.1 Agent i trains their model on dataset (X _[i], ˆyt[i][)][ to obtain][ g]t[i][.]_

b. for i = 1, ..., M :


b.1 Agent i receives models gt[j][,][ for][ j][ = 1][, ..., M, j][ ̸][=][ i][ from all other agents.]

b.2 Agent i use received models to predict ˆyt[i]+1 [=] **_y[i]+[P][M]j=1M,j≠_** _i_ _[g]t[j]_ [(][X] _[i][)]_ .

That is, there is no exchange of data between agents, but only models. This scheme as well as PKD
requires all to all communication. That means that the scheme can not be scaled, but it is still useful
for small numbers of agents (e.g collaboration of companies). The last is motivated by the simplicity
of the scheme without any need for hyperparameter tuning, the non-degrading behavior as well as its
superiority over the FedAvg scheme at highly heterogeneous data regimes.

**EKD with M agents.** EKD scheme in the case of 2 agents is based on models obtained in the
process of 2 simultaneous runs of the AKD algorithm. This means that the extension of EKD to
the multi-agent setting, in this case, is straightforward by using the M simultaneous runs of AKD
algorithm starting from each agent in the multi-agent setting and summing the obtained models in the
process as follows:


(−1)[t]( _gt[i][(][x][))]_ (32)
_t=0_ _i=1_

X X


_f_ (x) =
_∞_


-----

H ADDITIONAL EXPERIMENTS

H.1 MNIST WITH VARYING DATA HETEROGENEITY


In this section, we present the results for MNIST dataset with varying data heterogeneity in the setting
of ’different model’. The results one can see in Fig. 8. There is a faster degradation trend for both
AKD and PKD schemes if different models for agents are used (Fig. 8) comparing to rhe ’same
model’ setting (Fig. 6) at all data heterogeneity regimes. The PKD scheme is a slight modification of
the AvgKD scheme which is proven to degrade through rounds of distillation. We see the degradation
trend for PKD scheme which is suggested by our theory presented in App. E. EKD does not improve
with subsequent rounds in the setting of ’different models’. AvgKD scheme outperforms both PKD
and AKD in all settings. However, its convergence is not stable in extremely high heterogeneous
settings, showing large oscillations. Investigating and mitigating this could be interesting future work.


est Acc


|Alpha = 0|Alpha = 0.01|Alpha = 0.1|Alpha = 1|Col5|
|---|---|---|---|---|
||||Centralized Agent 1 Agent 2 AvgKD PKD AKD EKD||
|||||Centralized Agent 1 Agent 2 AvgKD PKD AKD EKD|


8

Centralized
Agent 1
Agent 2
AvgKD
PKD
AKD
EKD

Rounds


Alpha = 0.01

8

Rounds


Alpha = 0.1

8

Rounds


Alpha = 1

8

Rounds


0.90

0.75

0.60

0.45

0.30

0.15


16


16


16


16


Figure 8: Test accuracy of on MNIST with varying data heterogeneity in the setting of ’different
model’. Performance of PKD and AKD degrade with degradation speeding up with the increase in
data heterogeneity. Performance of AvgKD scheme converges to steady behavior at any regime of
data heterogeneity. Both agents benefit from AvgKD, PKD and EKD schemes in the early rounds of
communication.

H.2 CIFAR10 WITH VARYING DATA HETEROGENEITY


In this section, we present the results for CIFAR10 dataset with varying data heterogeneity. Note that
we use cross-entropy loss function here. The results one can see in Fig. 9 and 10 that again show
data heterogeneity plays key role in the behavior of all the schemes. All the trends we saw on the
MNIST dataset are repeated here except one: EKD does not improve in subsequent rounds in the
’same model’ setting.


est Acc


|Alpha = 0|Alpha = 0.01|Alpha = 0.1|Alpha = 1|Col5|
|---|---|---|---|---|
||||Centralized Agent 1 Agent 2 AvgKD PKD AKD FedAvg EKD||
|||||Centralized Agent 1 Agent 2 AvgKD PKD AKD FedAvg EKD|


8

Centralized
Agent 1
Agent 2
AvgKD
PKD
AKD
FedAvg
EKD

Rounds


Alpha = 0.01

8

Rounds


Alpha = 0.1

8

Rounds


Alpha = 1

8

Rounds


0.88

0.80

0.72

0.64

0.56

0.48

0.40

0.32


16


16


16


16


Figure 9: Test accuracy of on CIFAR10 with varying data heterogeneity in the setting of ’same
model’. As on MNIST: performance of PKD and AKD degrade with degradation speeding up with
the increase in data heterogeneity; performance of AvgKD scheme converges to steady behavior;
both agents benefit from AvgKD, PKD and EKD schemes in early rounds of communication.

H.3 CROSS-ENTROPY OBJECTIVE


In this section, we present the results of experiments on MNIST with Cross-Entropy (CE) loss for 2
main schemes under investigation: AKD and AvgKD. In Fig. 11 and 12 one can see the results of
AKD and AvgKD schemes correspondingly for CE loss. The results are aligned with our theory: in


-----

est Acc


|Alpha = 0|Alpha = 0.01|Alpha = 0.1|Alpha = 1|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
||||Centralized Agent 1 Agent 2 AvgKD PKD AKD EKD||||||
|||||||||Centralized Agent 1 Agent 2 AvgKD PKD AKD EKD|


8

Centralized
Agent 1
Agent 2
AvgKD
PKD
AKD
EKD

Rounds


Alpha = 0.01

8

Rounds


Alpha = 0.1

8

Rounds


Alpha = 1

8

Rounds


0.8

0.7

0.6

0.5

0.4

0.3


16


16


16


16


Figure 10: Test accuracy of on CIFAR10 with varying data heterogeneity in the setting of ’different
model’. All the schemes behave similarly to the ’same model’ setting.

Fig. 11 we see the degradation trend for AKD which is dependent on the amount of the regularization,
model and data heterogeneity. In Fig. 12 we see steady behavior of AvgKD scheme for both agents
models: there is no degradation even if model and data are different.


Test Max Acc. of agents in AKD

Same model, Same data Different model, Same data


Same model, Same data, No Reg.

0 6 12 18 24 30 36

|Col1|Col2|Col3|Centralized Agent 1 Agent 2|Centralized Agent 1 Agent 2|
|---|---|---|---|---|


Centralized
Agent 1
Agent 2

Rounds


Same model, Different data

Centralized
Agent 1
Agent 2

6 12 18 24 30 36

Rounds


1.00

0.95

0.90

0.85

0.80

Max Accuracy

0.75

0.70

0.65


12 18 24 30 36

Rounds


12 18 24 30 36

Rounds


Figure 11: Test accuracy of AKD on MNIST using CE loss and model starting from agent 1 (blue)
and agent 2 (red) with varying amount of regularization, model heterogeneity, and data heterogeneity.
In all cases, performance degrades with increasing rounds with degradation speeding up with the
increase in regularization, model heterogeneity, or data heterogeneity.


Test Max Acc. of agents in AvgKD

Different model, Same data


Same model, Same data


Same model, Different data

5 10 15 20

|Col1|Col2|Centralized Agent 1 Agent 2|Centralized Agent 1 Agent 2|
|---|---|---|---|


Centralized
Agent 1
Agent 2

Rounds


0.96

0.92


0.88

0.84


10 15 20

Rounds


10 15 20

Rounds


Figure 12: Test accuracy of AvgKD on MNIST using CE loss and model starting from agent 1 (blue)
and agent 2 (red) with varying model heterogeneity, and data heterogeneity. In all cases, there is no
degradation of performance, though the best accuracy is obtained by agent 1 in round 1 with only
local training.

H.4 COLLABORATION BETWEEN MLPS AND RANDOM FORESTS


In this section, we present the results for MNIST dataset for Random Forests (RF) and MLP models
with MSE loss. That is, the experiments are in the setting ’different model’, where agent 1 has MLP
model and agent 2 has RF model. These experiments can show how AKD and AvgKD schemes
behave in the setting of fundamentally different models. In the Figs. 13 and 14 the results for accuracy


-----

are presented. We see the alignment of these results with theory and other experiments with deep
learning models. Mainly, there is a degradation trend for AKD scheme which is speeding up with the
increase in data heterogeneity, there is no degradation for AvgKD scheme, and the performance of
both agents in AvgKD scheme is highly dependent on data heterogeneity.


Test Acc. of agents


|AKD, Same data|AKD, Different data|AvgKD, Same data|AvgKD, Different data|Col5|
|---|---|---|---|---|
||||Centralized MLP Centralized RF Agent 1 (MLP) Agent 2 (RF)||
|||||Centralized MLP Centralized RF Agent 1 (MLP) Agent 2 (RF)|


10 20 30 40

Centralized MLP
Centralized RF
Agent 1 (MLP)
Agent 2 (RF)

Rounds


AvgKD, Different data

Centralized MLP
Centralized RF
Agent 1 (MLP)
Agent 2 (RF)

5 10 15 20

Rounds


0.90

0.75

0.60

0.45

0.30

0.15


10 20 30 40

Rounds


10 15 20

Rounds


Figure 13: Test accuracy of AKD and AvgKD on MNIST using models MLP (blue) and RF (red) with
varying data heterogeneity. For AKD performance degrades with increasing rounds. Degradation
is speeding up with the increase in data heterogeneity. For AvgKD there is no degradation of
performance.


Test Acc. of agents in AvgKD

|Alpha = 0|Alpha = 0.01|Alpha = 0.05|Alpha = 0.1|Alpha = 1.0|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||Centralized MLP Centralized RF Agent 1 (MLP) Agent 2 (RF)||
||||||Centralized MLP Centralized RF Agent 1 (MLP) Agent 2 (RF)|



8 0 4 8 0

Centralized MLP
Centralized RF
Agent 1 (MLP)
Agent 2 (RF)

Rounds


Alpha = 0

4

Rounds


Alpha = 0.01

4

Rounds


Alpha = 0.1

4

Rounds


Alpha = 1.0

Centralized MLP
Centralized RF
Agent 1 (MLP)
Agent 2 (RF)

4

Rounds


0.96

0.88

0.80

0.72

0.64

0.56

0.48


Figure 14: Test accuracy of AvgKD on MNIST using models MLP (blue) and RF (red) with varying
data heterogeneity. The increase in data heterogeneity lowers the performance of both agents without
degradation trend through rounds.
H.5 AVGKD WITH M AGENTS

The AvgKD scheme does not degrade in comparison with AKD and PKD schemes that degrade
already in the case of 2 agents. In this section, we present the results of the AvgKD scheme with M
agents and use 5 agents on the MNIST dataset in the setting of the ’same model’ with varying data
heterogeneity. In case of full data heterogeneity (Alpha = 0) we assign the labels (2(i − 1), 2i − 1)
to the actor number i, for i = 1...5. The results are presented in the Fig. 15. We see that all the
agents repeat the behavior pattern in all cases of data heterogeneity. In cases of Alpha < 0.05 (high
data heterogeneity) early stopping is beneficial.


Test Acc. of N agents in AvgKD


|Alpha = 0|Alpha = 0.01|Alpha = 0.05|Alpha = 0.1|Alpha = 1.0|Col6|
|---|---|---|---|---|---|
|||||||
|||||Centralized Agent 1 Agent 2 Agent 3 Agent 4 Agent 5||
||||||Centralized Agent 1 Agent 2 Agent 3 Agent 4 Agent 5|


4

Centralized
Agent 1
Agent 2
Agent 3
Agent 4
Agent 5

Rounds


Alpha = 0.1

4

Rounds


Alpha = 1.0

4

Rounds


0.90

0.75

0.60

0.45

0.30

0.15


Rounds


Rounds


Figure 15: Test accuracy of AvgKD with M agents on MNIST with varying data heterogeneity in
the setting of ’same model’. All agents can benefit from the distilled knowledge in early rounds of
communication.


-----

