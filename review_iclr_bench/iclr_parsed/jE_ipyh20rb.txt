# FEDPROF: SELECTIVE FEDERATED LEARNING WITH REPRESENTATION PROFILING

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Federated Learning (FL) has shown great potential as a privacy-preserving solution to learning from decentralized data that are only accessible to end devices
(i.e., clients). In many scenarios however, a large proportion of the clients are
probably in possession of low-quality data that are biased, noisy or even irrelevant. As a result, they could significantly slow down the convergence of the
global model we aim to build and also compromise its quality. In light of this, we
propose FEDPROF, a novel algorithm for optimizing FL under such circumstances
without breaching data privacy. The key of our approach is a data representation
profiling and matching scheme that uses the global model to dynamically profile
data representations and allows for low-cost, lightweight representation matching.
Based on the scheme we adaptively score each client and adjust its participation
probability so as to mitigate the impact of low-value clients on the training process. We have conducted extensive experiments on public datasets using various
FL settings. The results show that FEDPROF effectively reduces the number of
communication rounds and overall time (up to 4.5x speedup) for the global model
to converge and provides accuracy gain.

1 INTRODUCTION

With the advances in Artificial Intelligence (AI), we are seeing a rapid growth in the number of
AI-driven applications as well as the volume of data required to train them. However, a large proportion of data used for machine learning are often generated outside the data centers by distributed
resources such as mobile phones and IoT (Internet of Things) devices. It is predicted that the data
generated by IoT devices will account for 75% of the total in 2025 (Meulen, 2018). Under this
circumstance, it will be very costly to gather all the data for centralized training. More importantly,
moving the data out of their local devices (e.g., mobile phones) is now restricted by law in many
countries, such as the General Data Protection Regulation (GDPR)[1] enforced in EU.

We face three main difficulties to learn from decentralized data: i) massive scale of end devices; ii)
limited communication bandwidth at the network edge; and iii) uncertain data distribution and data
quality. As an promising solution, Federated Learning (FL) (McMahan et al., 2017) is a framework
for efficient distributed machine learning with privacy protection (i.e., no data exchange). A typical
process of FL is organized in rounds where the devices (clients) download the global model from
the server, perform local training on their data and then upload their updated local models to the
server for aggregation. Compared to traditional distributed learning methods, FL is naturally more
communication-efficient at scale (Koneˇcn`y et al., 2016; Wang et al., 2019). Nonetheless, several
issues stand out.

1.1 MOTIVATION

_1) FL is susceptible to biased and low-quality local data. Only a fraction of clients are selected for_
a round of FL (involving too many clients leads to diminishing gains (Li et al., 2019)). The standard FL algorithm (McMahan et al., 2017) selects clients randomly, which implies that every client
(and its local data) is considered equally important. This makes the training process susceptible
to local data with strong heterogeneity and of low quality (e.g., user-generated texts (Hard et al.,

1https://gdpr.eu/what-is-gdpr/


-----

1

0.8

0.6

0.4

unbiased and noiseless
biased local data

Accuracy of global model 0.2 noisy local data

biased and noisy

0

2 4 6 8 10 12 14 16 18 20
Communication rounds


Figure 1: (Preliminary experiment) The global model’s convergence under different data conditions.
We ran the FL process with 100 clients to learn a CNN model on the MNIST dataset, which is
partitioned and allocated to clients in four different ways where the data are 1) original (black
line): noiseless and evenly distributed across the clients, 2) biased (magenta line): locally classimbalanced, 3) noisy (blue line): blended with noise, or 4) biased and noisy (red line). The noise (if
applied) covers 65% of the clients; the dominant class accounts for >50% of the samples for biased
local data. The fraction of selected clients is 0.3 for each round.

2018) and noisy photos). In some scenarios, local data may contain irrelevant or even adversarial
samples (Bhagoji et al., 2019; Bagdasaryan et al., 2020) from malicious clients (Fang et al., 2020;
Bagdasaryan et al., 2020; Tolpegin et al., 2020). Traditional solutions such as data augmentation
(Yoo et al., 2020) and re-sampling (Lin et al., 2017) prove useful for centralised training but applying them to local datasets may introduce extra noise (Cui et al., 2019) and increase the risk of
information leakage (Yu et al., 2021). Another naive solution is to directly exclude those low-value
clients with low-quality data, which, however, is often impractical because i) the quality of the data
depends on the learning task and is difficult to gauge; ii) some noisy or biased data could be useful
to the training at early stages (Feelders, 1996); and iii) sometimes low-quality data are very common
across the clients.

In Fig. 1 we demonstrate the impact of involving ”low-value” clients by running FL over 100 clients
to learn a CNN model on MNIST using the standard FEDAVG algorithm. From the traces we can
see that training over clients with problematic or strongly biased data can compromise the efficiency
and efficacy of FL, resulting in an inferior global model that takes more rounds to converge.

_2) Learned representations can reflect data distribution and quality. Representation learning is_
vital to the performance of deep models because learned representations can capture the intrinsic
structure of data and provide useful information for the downstream machine learning tasks (Bengio
et al., 2013). In ML research, The value of representations lies in the fact that they characterize the
domain and learning task and provide task-specific knowledge (Morcos et al., 2018; Kornblith et al.,
2019). In the context of FL, the similarity of representations are used for refining the model update
rules (Li et al., 2021; Feng & Yu, 2020), but the distributional difference between representations of
heterogeneous data is not yet explored.

Our study is also motivated by a key observation that representations from neural networks tend to
have Gaussian patterns. As a demonstration we trained two different models (LeNet-5 and ResNet18) on two different datasets (MNIST and CIFAR-100) separately. Fig. 2a shows the neuron-wise
distribution of representations extracted from the first dense layer (FC-1) of LeNet-5. Fig. 2b
shows the distribution of fused representations (in a channel-wise manner) extracted from a plain
convolution layer and a residual block of ResNet-18.

These observations motivate us to study the distributional property of data representations and use
it as a means to differentiate clients’ value.


-----

3000 5000 3000 2500 1800 3000 3000 3000 3000 7000

epoch 1 3000200010000-20 0 20 2000150010005000-10 0 10 3000200010000-20 0 20 3000200010000 -5 0 5 3000200010000 -10 -5 0 5 Conv1 2500200015001000500 4000300020001000 2500200015001000500 200015001000500 1600140012001000800600400200 2500200015001000500 2500200015001000500 2500200015001000500 2500200015001000500 600050004000300020001000

3000 3000 3000 3000 0 -0.04 0 0 0 0.4 0 0.3 0.4 0.150 0.2 0.25 0 0.1 0.15 0.2 0 0 0.1 0 -0.2 0 0-1 -0.5 0 0 -0.2 0 0.2 0-2 0 2

2000

2000 1500 2000 2000 2000

epoch 6 10003000200010000 -20 0 20 20001500100010005005000-10 0 10 -10 10003000200010000-20 0 20 30002000100010000 -10 0 10 30002000100010000 -10 -5 0 5 Res. block 50004000300020001000 350030002500200015001000500 4000350030002500200015001000500 45004000350030002500200015001000500 4000350030002500200015001000500 350030002500200015001000500 4000350030002500200015001000500 4000350030002500200015001000500 45004000350030002500200015001000500 50004000300020001000

epoch 10 0 -20 0 20 0-10 0 10 0-20 0 20 0-20 0 20 0-10 -5 0 5 -0.1 -0.050 0 0 0 0.2 0.4 0 0 0.1 0.2 0 0 0.2 0 -0.1 0 0 0 0.2 0 0 0.2 0-0.2 0 0.2 0 0 0.20.4 0 0 1 2


(b) Fused representations from a standard convolution layer (1st row) and a residual block (2nd row)
of a ResNet-18 model trained for 100 epochs on
CIFAR-100.


(a) Representations from FC-1 of a LeNet-5
model after being trained for 1, 6 and 10 epochs
on MNIST.


Figure 2: (Preliminary experiment) Demonstration of learned representations from a distributional
perspective. The representations are generated by forward propagation in model evaluation. Each
box corresponds to a randomly sampled element in the representation vector.

1.2 CONTRIBUTIONS

Our contributions are summarized as the following:

-  We first provide theoretical proof for the observation that data representations from neural
networks tend to follow Gaussian distribution, based on which we propose a representation
profiling and matching scheme for fast, low-cost comparison between different representation profiles.

-  We present a novel FL algorithm FEDPROF that adaptively adjusts clients’ participation
probability based on representation profile dissimilarity.

-  Results of extensive experiments show that FEDPROF reduces the number of communication rounds by up to 77%, shortens the overall training time (up to 4.5x speedup) while
increasing the accuracy of the global model by up to 2.5%.

2 RELATED WORK

Different from traditional distributed training methods (e.g., Alistarh et al. (2017); Wu et al. (2018);
Zheng et al. (2017)), Federated Learning assumes strict constraints of data locality and limited communication capacity (Koneˇcn`y et al., 2016). Much effort has been made in optimizing FL and covers
a variety of perspectives including communication (Koneˇcn`y et al., 2016; Niknam et al., 2020; Cui
et al., 2021), update rules (Li et al., 2020; Wu et al., 2021a; Leroy et al., 2019; Luping et al., 2019),
flexible aggregation (Wang et al., 2019; Wu et al., 2021b) and personalization (Fallah et al., 2020;
Tan et al., 2021; Deng et al., 2020).

The control of device participation is imperative in cross-device FL scenarios (Kairouz et al., 2019;
Yang et al., 2020) where the quality of local data is uncontrollable and the clients show varied
value for the training task (Tuor et al., 2020). To this end, the selection of clients is pivotal to the
convergence of FL over heterogeneous data and devices (Nishio & Yonetani, 2019; Wang et al.,
2020b; Chai et al., 2019; Acar et al., 2020). Non-uniform client selection is widely adopted in
existing studies (Li et al., 2020; Goetz et al., 2019; Cho et al., 2020; Li et al., 2019; Chen et al.,
2020b; Wang et al., 2020a) and has been theoretically proven with convergence guarantees (Chen
et al., 2020b; Li et al., 2019). Many approaches sample clients based on their performance (Nishio
& Yonetani, 2019; Chai et al., 2020) or aim to jointly optimize the model accuracy and training time
(Shi et al., 2020; Chen et al., 2020a; 2021). A popular strategy is to use loss as the information to
guide client selection (Goetz et al., 2019; Lai et al., 2021; Sarkar et al., 2020). For example, AFL
(Goetz et al., 2019) prioritizes the clients with high loss feedback on local data, but it is potentially
susceptible to noisy and unexpected data that yield illusive loss values. Data representations are


-----

useful in the context of FL for information exchange (Feng & Yu, 2020) or objective adaptation. For
example, Li et al. (2021) introduces representation similarities into local objectives. This contrastive
learning approach guides local training to avoid model divergence. Nonetheless, the study on the
distribution of data representations is still lacking whilst its connection to clients’ training value is
hardly explored either.

3 DATA REPRESENTATION PROFILING AND MATCHING

In this paper, we consider a typical cross-device FL setting (Kairouz et al., 2019), in which multiple
end devices collaboratively perform local training on their own datasets Di, i = 1, 2, ..., n. The
server owns a validation dataset D[∗] for model evaluation. Every dataset is only accessible to its
owner.

Considering the distributional pattern of data representations (Fig. 2) and the role of the global
model in FL, we propose to profile the representations of local data using the global model. In this
section, we first provide theoretical proof to support our observation that representations from neural
network models tend to follow Gaussian distribution. Then we present a novel scheme to profile data
representations and define profile dissimilarity for fast and secure representation comparison.

3.1 GAUSSIAN DISTRIBUTION OF REPRESENTATIONS

We first make the following definition to facilitate our analysis.
**Definition 1 (The Lyapunov’s condition). A set of random variables {Z1, Z2, . . ., Zv} satisfy the**
_Lyapunov’s condition if there exists a δ such that_

_v_

1
lim E _Zk_ _µk_ = 0, (1)
_v→∞_ _s[2+][δ]_ _k=1_ _|_ _−_ _|[2+][δ][]_

X 

_where µk = E[Zk], σk[2]_ [= E[(][Z][k][ −] _[µ][k][)][2][]][ and][ s][ =]_ _vk=1_ _[σ]k[2][.]_

The Lyapunov’s condition can be intuitively explained as a limit on the overall variation (withpP _Zk_
_µk_ being the (2 + δ)-th moment of Zk) of a set of random variables. _|_ _−_
_|[2+][δ]_

Now we present Proposition 1 and Proposition 2. The Propositions provide theoretical support for
our representation profiling and matching method to be introduced in Section 3.2.
**Proposition 1. The representations from linear operators (e.g., a pre-activation dense layer or a**
_plain convolutional layer) in a neural network tend to follow the Gaussian distribution if the layer’s_
_weighted inputs satisfy the Lyapunov’s condition._
**Proposition 2. The fused representations[2]** _from non-linear operators (e.g., a hidden layer of LSTM_
_or a residual block of ResNet) in a neural network tend to follow the Gaussian distribution if the_
_layer’s output elements satisfy the Lyapunov’s condition._

The proofs of Propositions 1 and 2 are provided in Appendices A.1 and A.2, respectively.

We base our proof on the Lyapunov’s CLT which assumes independence between the variables. The
assumption theoretically holds by using the Bayesian network concepts: let X denote the layer’s
input and Hk denote the k-th component in its output. The inference through the layer produces
dependencies X → _Hk for all k. According to Local Markov Property, we have Hi independent of_
any Hj (j ̸= i) given X. Also, the Lyapunov’s condition is typically met when the model is properly
initialized and batch normalization is applied. Next, we discuss the proposed representation profiling
and matching scheme.

3.2 DISTRIBUTIONAL PROFILING AND MATCHING

Based on the Gaussian pattern of representations, we compress the data representations statistically
into a compact form called representation profiles. The profile produced by the global model w on

2Fused representations refer to the sum of elements in the original representations produced by a single
layer (channel-wise for a residual block).


-----

a dataset D, denoted by RP (w, D), has the following format:

_RP_ (w, D) = {N (µi, σi[2][)][}]i[q]=1[,] (2)

where q is the profile length determined by the dimensionality of the representations. For example, q
is equal to the number of kernels for channel-wise fused representations from a convolutional layer.
The tuple (µi, σi[2][)][ contains the mean and the variance of the][ i][-th representation element.]

Local representation profiles are generated by clients and sent to the server for comparison (the cost
of transmission is negligible considering each profile is only q × 8 bytes). Let RPk denote the local
profile from client k, and RP _[∗]_ denote the baseline profile (generated in model evaluation) on the
server. The dissimilarity between RPk and RP _[∗], denoted by div(RPk, RP_ _[∗]), is defined as:_


_div(RPk, RP_ ) = [1]

_[∗]_ _q_


KL(Ni[(][k][)]||Ni[∗][)][,] (3)
_i=1_

X


where KL(·) denotes the Kullback–Leibler (KL) divergence. An advantage of our profiling scheme
is that a much simplified KL divergence formula can be adopted because of the Gaussian distribution
property (see Roberts & Penny (2002, Appendix B) for details), which yields:

_i_ _i_ )[2] + (µ[(]i[k][)] _µ[∗]i_ [)][2]
KL(Ni[(][k][)]||Ni[∗][) = log][ σ]σi[(][k][∗][)] + [(][σ][(][k][)] 2(σi[∗][)][2] _−_ _,_ (4)


Eq. (4) computes the KL divergence without calculating any integral, which is computationally
cost-efficient. Besides, the computation of profile dissimilarity can be performed under the Homomorphic Encryption for minimum knowledge disclosure (see Appendix D for details).

4 THE TRAINING ALGORITHM FEDPROF

Our research aims to optimize the global model over a large group of clients (datasets) of disparate
training value. Given the client set U (|U _| = N_ ), let Dk denote the local dataset on client k and D[∗]
the validation set on the server, We formulate the optimization problem in (5) where the coefficient
_ρk differentiates the importance of the local objective functions Fk(w) and depends on the data in_
_Dk. Our global objective is in a sense similar to the agnostic learning scenario (Mohri et al., 2019)_
where a non-uniform mixture of local data distributions is implied.


_ρkFk(w),_ (5)

_k=1_

X


arg min
_w_ _[F]_ [(][w][) =]


where w is the parameter set of the global model. The coefficients {ρk}k[N]=1 [add up to 1.][ F][k][(][w][)][ is]
client k’s local objective function of training based on the loss function ℓ(·):


_ℓ(xi, yi; w),_ (6)
(xi,yXi)∈Dk


_Fk(w) =_


_Dk_
_|_ _|_


Involving the ”right” clients facilitates the convergence. With this motivation we score each client
with λk each round based on the representation profile dissimilarity:

_λk = exp_ _−_ _α · div(RPk(vk), RP_ _[∗](vk)_ _,_ (7)

where vk is the version of RPk (i.e., the version of the global model that client   _k receives) and_
_α is a preference factor deciding how biased the selection strategy needs to be towards the clients_
with small profile dissimilarity (i.e., higher training value). With α = 0, our strategy is equivalent
to random selection. The scores connect the representation profiling and matching scheme to the
design of the selective client participation strategy adopted in our FL training algorithm FEDPROF,
which is outlined in Algorithm 1 (see Appendix B for a detailed version with both client and server
processes). The key steps of our algorithm are local representation profiling (line 5), baseline representation profiling (line 9) and client scoring (line 3). Fig. 3 illustrates the workflow of the proposed
algorithm from representation profiling, matching to scheduling.


-----

**benchmark data** **baseline footprints**

**_RP*_**

**_D*_**

**...** [

**_RP1_** 0.75

**_D1_**

**the global model** **_RP2_** 0.49

**_D2_**

**...** **...** match **...**

& rate

**_RPn_** 0.15

**_Dn_** **...** [

**client credits**

**local data** **representa"ons** **local profiles**

Credit-based Model Update footprints Local training & Model
client selec!on distribu!on and credits model uploading aggrega!on

Round + 1


Figure 3: The workflow of the proposed FEDPROF algorithm.

**Algorithm 1: the FEDPROF algorithm**
**Input: maximum number of rounds Tmax, iterations per round E, fraction C**


**1 Initialize global model w and generate baseline profile RP** _[∗]_

**2 Collect initial representation profiles** _RPk_ _k_ _U from all clients_
_{_ _}_ _∈_
**for round T ←** 1 to Tmax do

**3** Update client scores {λk}k∈U and compute Λ = _k∈U_ _[λ][k]_

**4** _S_ Choose K = N _C clients by probability distribution_ _[λ]Λ[k]_

**for ← client k in S in parallel do ·** [P] _{_ _[}][k][∈][U]_

**5** _RPk_ _updateProfile(k, w, T_ 1)

**6** _wk_ _←localTraining(k, w, E)_ _−_
_←_
**end**

**7** Collect local profiles from the clients in S

**8** Update w via model aggregation

**9** Evaluate w and update RP _[∗]_

**end**

**10 return w**

The convergence rate of FL algorithms with opportunistic client selection (sampling) has been extensively studied in the literature (Li et al., 2019; Chen et al., 2020b; Wang et al., 2019). Inspired by
these studies, we present Theorem 1 to guarantee the global model’s convergence for our algorithm.
Similar to Stich et al. (2018); Zhang et al. (2013); Li et al. (2019), we assume that {Fk}k[N]=1 [are]
_L-smooth and µ-strongly convex and that in expectation, the variance of local stochastic gradients_
are bounded by ϵ[2] and their squared norms are bounded by G[2]. For each local update step t we
define S(t) as the set of selected clients in the associated round.
**Theorem 1. Using partial aggregation and a strategy π that selects clients by the probability distri-**
_intervalbution { Eqk ≥}k[N]=11 and a decreasing step size (learning rate)[, the global model][ w][(][t][)][ converges in expectation by having] ηt =_ _µ(t2+γ)_ _[.]_ _[ q][k][ =][ ρ][k][, an aggregation]_

_L_ 2( + )
E _F_ (w(t)) _F_ _B_ _C_ + _[γ][ + 1]_ ∆1 _,_ (8)
_−_ _[∗]_ _≤_ (γ + t) _µ[2]_ 2
   

_where t ∈_ _TA = {nE|n = 1, 2, . . .}, γ = max{_ [8]µ[L] _[, E][}−][1][,][ B][ =][ P]k[N]=1_ _[ρ]k[2][ϵ][2]k_ [+6][L][Γ+8(][E] _[−][1)][2][G][2][,]_

_C =_ _K[4]_ _[E][2][G][2][,][ Γ =][ F][ ∗]_ _[−]_ [P]k[N]=1 _[ρ][k][F][ ∗]k_ _[,][ ∆][1][ = E][∥]w[¯](1) −_ _w[∗]∥[2], K = |S(t)| = N · C._

The proof of Theorem 1 is provided in Appendix C.


-----

5 EXPERIMENTS

We conducted extensive experiments to evaluate FEDPROF under various FL settings. Apart from
FEDAVG (McMahan et al., 2017), we also reproduced several state-of-the-art FL algorithms for
comparison. For fair comparison, the algorithms are grouped by the aggregation method (i.e., full
aggregation and partial aggregation) and configured following the hyper-parameter settings in their
papers (if any). Table 1 summarizes these FL algorithms. Note that our algorithm can adapt to both
aggregation methods.

Table 1: The implemented FL algorithms for comparison

Algorithm Aggregation method Rule of selection

FEDAVG (McMahan et al., 2017) full aggregation random selection
CFCFM (Wu et al., 2021b) full aggregation submission order
FEDAVG-RP (Li et al., 2019) partial (Scheme II) random selection
FEDPROX (Li et al., 2020) partial aggregation weighted random by data ratio
FEDADAM (Leroy et al., 2019) partial with momentum random selection
AFL (Goetz et al., 2019) partial with momentum local loss valuation
FEDPROF (ours) full/partial aggregation weighted random by score

5.1 EXPERIMENT SETUP

We built our simulated FL system and implemented the algorithms based on the Pytorch framework
(Build 1.7.0). We first set up a Small-scale Task (S-Task) to learn a multi-layer feed-forward network
model from decentralized sensor data for predicting carbon monoxide (CO) and nitrogen oxides
(NOx) emissions using the GasTurbine[3] dataset. Next, we set up a Large-scale Task (L-Task) to
train a CNN over a large population of user devices for image classification (EMNIST [4]). In both
tasks, data sharing is not allowed between any parties. The data are non-IID across the end devices.[5]
We introduce a diversity of noise into the local datasets on end devices to simulate the discrepancy
in data quality. The S-Task is performed over 50 sensor clients and 50% of them produce noisy data
(including 10% invalid). The population for L-Task contains 1000 end devices across which the data
spread with strong class imbalance – roughly 60% of the samples on each device fall into the same
class. 15% of the local datasets in the L-Task are irrelevant images whereas another 45% of them
are low-quality images (blurred or affected by salt-and-pepper noise). Considering the population
of the clients, the setting of the selection fraction C is based on the scale of the training participants
suggested by Kairouz et al. (2019). For both tasks, the clients are heterogeneous in terms of both
performance and communication bandwidth. More experimental settings are listed in Table 4 in
Appendix E where the environment setup is also given in details.

5.2 EVALUATION RESULTS

We evaluate the performance of our FEDPROF algorithm in terms of the efficacy (best accuracy
achieved) and efficiency (costs for convergence) in establishing a global model for the two tasks.
Tables 2 and 3 report the average results of multiple runs with standard deviations for our algorithm.
Figs. 4 and 5 plot the accuracy traces from the round-wise evaluations of the global model.

_1) Convergence in different aggregation modes: Our results show a great difference in convergence_
rate under different aggregation modes. From Figs. 4 and 5, we observe that partial aggregation
facilitates faster convergence of the global model than full aggregation, which is consistent with the
observations made by Li et al. (2019). The advantage is especially obvious in the L-Task where
partial aggregation requires much fewer communication rounds to reach the 90% accuracy. Our
FEDPROF algorithm yields the fastest convergence in both groups of comparison for both tasks
because selective participation benefits both aggregation methods.

3https://archive.ics.uci.edu/ml/datasets/Gas+Turbine+CO+and+NOx+Emission+Data+Set
4https://www.nist.gov/itl/products-and-services/emnist-dataset. We use the digits subset of EMNIST.
5In the S-Task, local datasets are of different sizes that follow a Gaussian distribution. In the L-Task, local
data are largely imbalanced where around 60% of the samples on each client have the same class label.


-----

Table 2: The results of running the S-Task. The best accuracy is achieved by running for long
enough. Other metrics are recorded upon reaching the target accuracy (80% for the S-Task).

**Full aggregation**
**C=0.2** **C=0.3**

For accuracy@0.8 For accuracy@0.8
Best acc Best acc
Rounds Time(s) E(Wh) Rounds Time(s) E(Wh)

FEDAVG 0.805 56 2869.66 2.87 0.806 52 3160.01 4.12
CFCFM 0.806 39 1230.81 1.61 0.802 42 1495.34 2.91
Ours **0.824** **16** **803.74** **0.80** **0.827** **12** **701.18** **0.94**
(std.) 6.13E-03 2.06 139.76 0.14 8.01E-03 2.94 122.63 0.20


**Partial aggregation**
**C=0.2** **C=0.3**

For accuracy@0.8 For accuracy@0.8
Best acc Best acc
Rounds Time(s) E(Wh) Rounds Time(s) E(Wh)

FEDAVG-RP 0.819 13 735.48 0.71 0.817 8 466.90 0.64
FEDPROX 0.821 16 899.93 0.79 0.810 16 841.65 1.08
FEDADAM 0.818 8 438.20 0.42 0.819 12 667.00 0.94
AFL 0.816 6 313.81 0.30 0.813 6 298.81 0.42
Ours **0.844** **5** **283.76** **0.27** **0.841** **4** **235.22** **0.35**
(std.) 1.70E-03 0.47 30.69 0.03 7.07E-03 1.70 90.12 0.14

Table 3: The results of running the L-Task. The best accuracy is achieved by running for long
enough. Other metrics are recorded upon reaching the target accuracy (90% for the L-Task).

**Full aggregation**
**C=0.05** **C=0.1**

For accuracy@0.9 For accuracy@0.9
Best acc Best acc
Rounds Time(s) E(Wh) Rounds Time(s) E(Wh)

FEDAVG 0.906 213 10407.30 60.01 0.929 76 3894.26 43.16
CFCFM 0.923 251 8645.17 61.37 0.932 75 2675.56 37.62
Ours **0.926** **98** **4846.15** **28.22** **0.945** **45** **2295.03** **25.98**
(std.) 8.16E-05 1.89 96.57 0.43 4.71E-04 0.82 29.46 0.01


**Partial aggregation**
**C=0.05** **C=0.1**

For accuracy@0.9 For accuracy@0.9
Best acc Best acc
Rounds Time(s) E(Wh) Rounds Time(s) E(Wh)

FEDAVG-RP 0.937 12 572.90 3.29 0.938 12 603.94 6.57
FEDPROX 0.936 13 640.01 3.58 0.942 11 559.16 5.91
FEDADAM 0.940 12 599.96 3.47 0.939 12 608.85 6.76
AFL 0.952 10 479.73 2.73 0.944 9 476.62 5.26
Ours **0.962** **8** **383.94** **2.26** **0.962** **8** **413.16** **4.64**
(std.) 9.43E-04 0.471 17.67 0.15 8.16E-04 0.47 11.64 0.03

_2) Best accuracy of the global model: Through the training process of FL, the global model is_
evaluated each round on the server. The best global model obtained thus far is stored on the server.
As shown in the 2nd column of Tables 2 and 3, our FEDPROF algorithm improves the accuracy
achieved by 2.5% when compared against the baselines (FEDAVG and FEDAVG-RP). The AFL
algorithm uses a loss-oriented client selection strategy, which shows the closest performance to our
algorithm in the L-Task but the worst accuracy in the S-Task.

_3) Total communication rounds for convergence: The number of communication rounds required for_
reaching convergence is a key indicator to the efficiency of FL. In the S-Task, our algorithm takes
less than half the communication rounds required by other algorithms in most cases. In the L-Task
with C=0.05, our algorithm reaches 90% accuracy within 100 rounds whilst FEDAVG and CFCFM
need more than 200. In this case, FEDPROF also achieves approximately 7% higher accuracy in
50 rounds. Partial aggregation turns out to be much more communication-efficient: FEDAVG-RP


-----

0.84 **Full aggregation, C=0.2** 0.84 **Full aggregation, C=0.3** 0.86 **Partial aggregation, C=0.2** 0.86 **Partial aggregation, C=0.3**

0.82 0.82 0.84 0.84

0.8 0.8

0.82 0.82

0.78

0.78

0.76 0.8 0.8

0.76

Accuracy 0.74 Accuracy 0.74 Accuracy 0.78 FedAvg-RP Accuracy 0.78 FedAvg-RP

0.720.7 FedAvg 0.72 FedAvg 0.76 FedProxFedAdam 0.76 FedProxFedAdam

0.68 CFCFMours 0.7 CFCFMours 0.74 AFLours 0.74 AFLours

0.66 0.68 0.72 0.72

0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Communication rounds Communication rounds Communication rounds Communication rounds


(b)


(a)


Figure 4: The traces of evaluation accuracy of the global model through 100 rounds in the S-Task
using (a) the full-aggregation method, and (b) the partial-aggregation method.


**Full aggregation, C=0.05** **Full aggregation, C=0.10** **Partial aggregation, C=0.05** **Partial aggregation, C=0.10**

1 1 1 1
0.9 0.9 0.9 0.9
0.8 0.8 0.8 0.8
0.7 0.7 0.7 0.7
0.6 0.6 0.6 0.6
0.5 0.5 0.5 FedAvg-RP 0.5 FedAvg-RP

Accuracy 0.40.30.2 FedAvgCFCFM Accuracy 0.40.30.2 FedAvgCFCFM Accuracy 0.40.3 FedProxFedAdamAFL Accuracy 0.40.3 FedProxFedAdamAFL

0.1 ours 0.1 ours 0.2 ours 0.2 ours
0 0 0.1 0.1

0 50 100 150 200 250 300 0 50 100 150 200 250 300 0 5 10 15 20 25 30 35 40 45 50 0 5 10 15 20 25 30 35 40 45 50
Communication rounds Communication rounds Communication rounds Communication rounds


(a)


(b)


Figure 5: The traces of evaluation accuracy of the global model in the L-Task using (a) the fullaggregation method (through 300 rounds), and (b) the partial aggregation method (through 50
rounds).

needs 8 rounds to reach the accuracy target for the S-Task and 12 rounds for the L-Task, whilst our
algorithm reduces the numbers to 5 and 8, respectively.

_4) Total time needed for convergence: The overall time consumption is closely related to total com-_
munication rounds needed for convergence and the time cost for each round. Algorithms requiring more rounds to converge typically take longer to reach the accuracy target except the case of
CFCFM, which priorities the clients that work faster. Using FEDAVG as the baseline, CFCFM
accelerates the training process by 2.1x whilst our algorithm provides a 4.5x speedup for achieving
the same target accuracy (S-Task, C=0.3). FEDPROF also has a clear advantage over FEDAVGRP, FEDPROX and FEDADAM in the partial aggregation group where it shows a 2.6x speedup over
FEDAVG-RP in the S-Task with C=0.2.

_5) Energy consumption of end devices: A main concern for the end devices, as the participants of_
FL, is their power usage (Watt hours). Full aggregation methods experience slower convergence and
thus endure higher energy cost on the devices. For example, with a small selection fraction C=0.05
in the L-Task, FEDAVG and CFCFM consume over 60Wh to reach the target accuracy. In this case,
our algorithm reduces the cost by more than a half (28.22Wh). With the partial aggregation mode,
the reduction by our algorithm is up to 62% (S-Task, C = 0.2). Considering all the cases, FEDPROF
achieves the target accuracy with the least energy cost, providing an reduction of 29% ∼ 53%.

_6) Differentiated participation with FEDPROF: Fig. 6 reflects the preference of our selection strat-_
egy. In the S-Task we can observe that the clients with useless samples or noisy data get significantly
less involved (<10 on average) in training. In the L-Task our algorithm also effectively limits (basically excludes) the clients who owns the image data of poor quality (i.e., irrelevant or severely
blurred), whereas the clients with moderately noisy images are selected with reduced frequency as
compared to those with normal data. A potential issue of having the preference towards some of
the devices is about fairness. Nonetheless, one can apply our algorithm together with an incentive
mechanism (e.g., Yu et al. (2020)) to address it.


-----

|clients with irrelevant clients wi or noisy data|th normal data|
|---|---|

|clients with irrelevant data|clients with blurred data|clients with noisy data|clients with normal data|
|---|---|---|---|

|clients with irrelevant clients with or noisy data|normal data|
|---|---|

|clients with irrelevant data|clients with blurred data|clients with noisy data|clients with normal data|
|---|---|---|---|


6040 clients with irrelevantor noisy data **Full participation, FedProfclients with normal data** 6040 clients    with     irrelevant  clients withblurred dataFull participation, FedProfclients with noisy data clients with normal data

data

20 20

participation count participation count

0 0

0 5 10 15 20 25 30 35 40 45 50 0 100 200 300 400 500 600 700 800 900 1000
client id client id

**Partial participation, FedProf** **Partial participation, FedProf**

60 20

clients with irrelevant clients with normal data clients clients with clients with clients with normal data

40 or noisy data 15 with blurred data noisy data

irrelevant

10 data

20

5

participation count participation count

0 0 5 10 15 20 25 30 35 40 45 50 00 100 200 300 400 500 600 700 800 900 1000

client id client id


(a)


(b)


Figure 6: Total counts by client of participation (i.e., being selected) in (a) the S-Task and (b) the
L-Task. For clarity, clients are indexed according to their local data quality.

6 CONCLUSION

Federated learning provides a privacy-preserving approach to decentralized training but is vulnerable
to the heterogeneity and uncertain quality of on-device data. In this paper, we use a novel approach
to address the issue without violating the data locality restriction. We first provide key insights for
the distribution of data representations and then develop a dynamic data representation profiling and
matching scheme. Based on the scheme we propose a selective FL training algorithm FEDPROF
that adaptively adjusts clients’ participation chance based on their profile dissimilarity. We have
conducted extensive experiments on public datasets under various environment settings. Evaluation
results show that our algorithm significantly improves the efficiency of FL and reduces the time and
energy costs for the global model to converge.

REFERENCES

Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and
Venkatesh Saligrama. Federated learning based on dynamic regularization. In International Con_ference on Learning Representations, 2020._

Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd:
Communication-efficient sgd via gradient quantization and encoding. Advances in Neural In_formation Processing Systems, 30:1709–1720, 2017._

Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to
backdoor federated learning. In International Conference on Artificial Intelligence and Statistics,
pp. 2938–2948. PMLR, 2020.

Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828,
2013.

Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated
learning through an adversarial lens. In International Conference on Machine Learning, pp. 634–
643. PMLR, 2019.

Patrick Billingsley. Probability and measure. John Wiley & Sons, 2008.

Aaron Carroll, Gernot Heiser, et al. An analysis of power consumption in a smartphone. In USENIX
_annual technical conference, volume 14, pp. 21–21. Boston, MA, 2010._


-----

Zheng Chai, Hannan Fayyaz, Zeshan Fayyaz, Ali Anwar, Yi Zhou, Nathalie Baracaldo, Heiko Ludwig, and Yue Cheng. Towards taming the resource and data heterogeneity in federated learning.
In 2019 {USENIX} Conference on Operational Machine Learning (OpML 19), pp. 19–21, 2019.

Zheng Chai, Ahsan Ali, Syed Zawad, Stacey Truex, Ali Anwar, Nathalie Baracaldo, Yi Zhou, Heiko
Ludwig, Feng Yan, and Yue Cheng. Tifl: A tier-based federated learning system. In Proceedings
_of the 29th International Symposium on High-Performance Parallel and Distributed Computing,_
pp. 125–136, 2020.

Mingzhe Chen, H Vincent Poor, Walid Saad, and Shuguang Cui. Convergence time optimization for
federated learning over wireless networks. IEEE Transactions on Wireless Communications, 20
(4):2457–2471, 2020a.

Mingzhe Chen, Nir Shlezinger, H Vincent Poor, Yonina C Eldar, and Shuguang Cui.
Communication-efficient federated learning. Proceedings of the National Academy of Sciences,
118(17), 2021.

Wenlin Chen, Samuel Horvath, and Peter Richtarik. Optimal client sampling for federated learning.
_arXiv preprint arXiv:2010.13723, 2020b._

Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Client selection in federated learning: Convergence
analysis and power-of-choice selection strategies. arXiv preprint arXiv:2010.01243, 2020.

Laizhong Cui, Xiaoxin Su, Yipeng Zhou, and Yi Pan. Slashing communication traffic in federated
learning by transmitting clustered model updates. IEEE Journal on Selected Areas in Communi_cations, 2021._

Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based
on effective number of samples. In Proceedings of the IEEE/CVF conference on computer vision
_and pattern recognition, pp. 9268–9277, 2019._

Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated
learning. arXiv preprint arXiv:2003.13461, 2020.

Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. Advances in Neural Information
_Processing Systems, 33:3557–3568, 2020._

Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. Local model poisoning attacks to
byzantine-robust federated learning. In 29th {USENIX} Security Symposium ({USENIX} Se_curity 20), pp. 1605–1622, 2020._

AJ Feelders. Learning from biased data using mixture models. In KDD, pp. 102–107, 1996.

Siwei Feng and Han Yu. Multi-participant multi-class vertical federated learning. arXiv preprint
_arXiv:2001.11154, 2020._

Craig Gentry. Fully homomorphic encryption using ideal lattices. In Proceedings of the forty-first
_annual ACM symposium on Theory of computing, pp. 169–178, 2009._

Jack Goetz, Kshitiz Malik, Duc Bui, Seungwhan Moon, Honglei Liu, and Anuj Kumar. Active
federated learning. arXiv preprint arXiv:1909.12641, 2019.

Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Franc¸oise Beaufays, Sean
Augenstein, Hubert Eichner, Chlo´e Kiddon, and Daniel Ramage. Federated learning for mobile
keyboard prediction. arXiv preprint arXiv:1811.03604, 2018.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016.

Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.


-----

Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richt´arik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. _arXiv_
_preprint arXiv:1610.05492, 2016._

Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
network representations revisited. In International Conference on Machine Learning, pp. 3519–
3529. PMLR, 2019.

Fan Lai, Xiangfeng Zhu, Harsha V Madhyastha, and Mosharaf Chowdhury. Oort: Efficient federated
learning via guided participant selection. In 15th {USENIX} Symposium on Operating Systems
_Design and Implementation ({OSDI} 21), pp. 19–35, 2021._

Don S Lemons. An Introduction to Stochastic Processes in Physics. Johns Hopkins University Press,
2003.

David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht, and Joseph Dureau. Federated
learning for keyword spotting. In ICASSP 2019-2019 IEEE International Conference on Acous_tics, Speech and Signal Processing (ICASSP), pp. 6341–6345. IEEE, 2019._

Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings of
_the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10713–10722, 2021._

Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. In The 3rd MLSys Conference, 2020.

Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. In International Conference on Learning Representations, 2019.

Wei-Chao Lin, Chih-Fong Tsai, Ya-Han Hu, and Jing-Shang Jhang. Clustering-based undersampling
in class-imbalanced data. Information Sciences, 409:17–26, 2017.

WANG Luping, WANG Wei, and LI Bo. Cmfl: Mitigating communication overhead for federated learning. In 2019 IEEE 39th International Conference on Distributed Computing Systems
_(ICDCS), pp. 954–964. IEEE, 2019._

Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli_gence and Statistics (AISTATS), pp. 1273–1282. PMLR, 2017._

Robert van der Meulen. What edge computing means for infrastructure and operations leaders. https://www.gartner.com/smarterwithgartner/what-edge-computing-means-forinfrastructure-and-operations-leaders/, 2018. Accessed: 2021-07-10.

Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Interna_tional Conference on Machine Learning, pp. 4615–4625. PMLR, 2019._

Ari S Morcos, Maithra Raghu, and Samy Bengio. Insights on representational similarity in neural
networks with canonical correlation. In Proceedings of the 32nd International Conference on
_Neural Information Processing Systems, pp. 5732–5741, 2018._

Solmaz Niknam, Harpreet S Dhillon, and Jeffrey H Reed. Federated learning for wireless communications: Motivation, opportunities, and challenges. IEEE Communications Magazine, 58(6):
46–51, 2020.

Takayuki Nishio and Ryo Yonetani. Client selection for federated learning with heterogeneous
resources in mobile edge. In ICC 2019-2019 IEEE International Conference on Communications
_(ICC), pp. 1–7. IEEE, 2019._

Stephen J Roberts and Will D Penny. Variational bayes for generalized autoregressive models. IEEE
_Transactions on Signal Processing, 50(9):2245–2257, 2002._

Dipankar Sarkar, Ankur Narang, and Sumit Rai. Fed-focal loss for imbalanced data classification in
federated learning. arXiv preprint arXiv:2011.06283, 2020.


-----

Wenqi Shi, Sheng Zhou, and Zhisheng Niu. Device scheduling with fast convergence for wireless
federated learning. In ICC 2020-2020 IEEE International Conference on Communications (ICC),
pp. 1–6. IEEE, 2020.

Jie Song, Tiantian Li, Zhi Wang, and Zhiliang Zhu. Study on energy-consumption regularities of
cloud computing systems by a novel evaluation model. Computing, 95(4):269–287, 2013.

Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified sgd with memory. Ad_vances in Neural Information Processing Systems, 31:4447–4458, 2018._

Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. Towards personalized federated learning.
_arXiv preprint arXiv:2103.00710, 2021._

Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu. Data poisoning attacks against
federated learning systems. In European Symposium on Research in Computer Security, pp. 480–
501. Springer, 2020.

Nguyen H Tran, Wei Bao, Albert Zomaya, Minh NH Nguyen, and Choong Seon Hong. Federated
learning over wireless networks: Optimization model design and analysis. In IEEE INFOCOM
_2019-IEEE Conference on Computer Communications, pp. 1387–1395. IEEE, 2019._

Tiffany Tuor, Shiqiang Wang, Bong Jun Ko, Changchang Liu, and Kin K. Leung. Overcoming noisy
and irrelevant data in federated learning. arXiv preprint arXiv:2001.08300, 2020.

Hao Wang, Zakhary Kaplan, Di Niu, and Baochun Li. Optimizing federated learning on non-iid data
with reinforcement learning. In IEEE INFOCOM 2020-IEEE Conference on Computer Commu_nications, pp. 1698–1707. IEEE, 2020a._

Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. arXiv preprint arXiv:2007.07481,
2020b.

Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian Makaya, Ting He, and
Kevin Chan. Adaptive federated learning in resource constrained edge computing systems. IEEE
_Journal on Selected Areas in Communications, 37(6):1205–1221, 2019._

Jiaxiang Wu, Weidong Huang, Junzhou Huang, and Tong Zhang. Error compensated quantized
sgd and its applications to large-scale distributed optimization. In International Conference on
_Machine Learning, pp. 5325–5333. PMLR, 2018._

Wentai Wu, Ligang He, Weiwei Lin, and Rui Mao. Accelerating federated learning over reliabilityagnostic clients in mobile edge computing systems. IEEE Transactions on Parallel and Dis_tributed Systems, 32(7):1539–1551, 2021a._

Wentai Wu, Ligang He, Weiwei Lin, Rui Mao, Carsten Maple, and Stephen A Jarvis. Safa: a
semi-asynchronous protocol for fast federated learning with low overhead. IEEE Transactions on
_Computers, 70(5):655–668, 2021b._

Haibo Yang, Minghong Fang, and Jia Liu. Achieving linear speedup with partial worker participation in non-iid federated learning. In International Conference on Learning Representations,
2020.

Jaejun Yoo, Namhyuk Ahn, and Kyung-Ah Sohn. Rethinking data augmentation for image superresolution: A comprehensive analysis and a new strategy. In Proceedings of the IEEE/CVF Con_ference on Computer Vision and Pattern Recognition, pp. 8375–8384, 2020._

Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. How does data augmentation affect
privacy in machine learning? In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 35, pp. 10746–10753, 2021.

Han Yu, Zelei Liu, Yang Liu, Tianjian Chen, Mingshu Cong, Xi Weng, Dusit Niyato, and Qiang
Yang. A fairness-aware incentive scheme for federated learning. In Proceedings of the AAAI/ACM
_Conference on AI, Ethics, and Society, pp. 393–399, 2020._


-----

Yuchen Zhang, John C Duchi, and Martin J Wainwright. Communication-efficient algorithms for
statistical optimization. The Journal of Machine Learning Research, 14(1):3321–3363, 2013.

Shuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhi-Ming Ma, and Tie-Yan Liu.
Asynchronous stochastic gradient descent with delay compensation. In International Conference
_on Machine Learning, pp. 4120–4129. PMLR, 2017._


-----

A PROOF OF PROPOSITIONS

A.1 PROOF OF PROPOSITION 1

Without loss of generality, we provide the proof of Proposition 1 for the pre-activation representations from dense (fully-connected) layers and standard convolutional layers, respectively. The
results can be easily extended to other linear neural operators.

_Dense layers_

_Proof. Let Ω=_ _neu1, neu2, ..., neuq_ denote a dense layer (with q neurons) of any neural network
_{_ _}_
model and Hk denote the pre-activation output of neuk in Ω. We first provide the theoretical proof
to support the observation that Hk tends to follow the Gaussian distribution.

Let χ = R[v] denote the input feature space (with v features) and assume the feature Xi (which is a
random variable) follows a certain distribution ζi(µi, σi[2][)][ (not necessarily Gaussian) with finite mean]
_µi = E[Xi] and variance σi[2]_ [= E[][X][i][ −] _[µ][i][]][. For each neuron][ neu][k][, let][ w][k][ = [][w][k,][1][ w][k,][2][ . . . w][k,v][]]_
denote the neuron’s weight vector, bk denote the bias, and Zk,i = Xiwk,i denote the i-th weighted
input. Let Hk denote the output of neuk. During the forward propagation, we have:

_Hk = Xw[T]k_ [+][ b][k]

_v_

= _Xiwk,i + bk_

_i=1_

Xv

= _Zk,i + bk._ (9)

_i=1_

X

Apparently Zk,i is a random variable because Zk,i = Xiwk,i (where the weights wk,i are constants
during a forward pass), thus Hk is also a random variable according to Eq. (9).

In an ideal situation, the inputs variables X1, X2, . . ., Xv may follow a multivariate Gaussian
distribution, in which case Proposition 1 automatically holds due to the property of multivariate normal distribution that every linear combination of the components of the random vector
(X1, X2, . . ., Xv)[T] follows a Gaussian distribution (Lemons, 2003). In other words, Hk =
_X1wk,1 + X2wk,2 + . . . + Xvwk,v + bk is a normally distributed variable since wk,i and bk_
(k = 1, 2, . . ., v) are constants in the forward propagation. A special case for this condition is that
_X1, X2, . . ., Xv are independent on each other and Xi follows a Gaussian distribution N_ (µi, σi[2][)]
for all i = 1, 2, . . ., v. In this case, by the definition of Zk,i, we have:

_Zk,i = Xiwk,i ∼N_ _wk,iµi, (wk,iσi)[2][],_ (10)

where Z1, Z2, . . ., Zv are independent on each other. Combining Eqs. (9) and (10), we have: 

_v_ _v_
_Hk ∼N_ _i=1_ _wk,iµi + bk,_ _i=1(wk,iσi)[2][],_ (11)
  X X

For more general cases where X1, X2, . . ., Xv are not necessarily normally distributed, we assume
the weighted inputs Zk,i of the dense layer satisfy the Lyapunov’s condition (see definition 1). As
a result, we have the following according to the Central Limit Theorem (CLT) (Billingsley, 2008)
considering that Xi follows ζi(µi, σi[2][)][:]

_v_

1 _d_

_Zk,i_ _wk,iµi_ (0, 1) (12)

_sk_ _i=1_ _−_ _−→N_

X   

where sk = _vi=1_ _wk,iσi_ 2 and (0, 1) denotes the standard normal distribution. Equivalently,
_N_
for every neuk we have:
qP   

_v_ _v_

_d_
_Zk,i_ _−→N_ ( _wk,iµi, s[2]k[)]_ (13)
_i=1_ _i=1_

X X


-----

Combining Eqs. (9) and (13) we can derive that:

_v_

_d_
_Hk_ _−→N_ ( _wk,iµi + bk, s[2]k[)][,]_ (14)

_i=1_

X

which means that Hk(k = 1, 2, . . ., v) tend to follow Gaussian distribution and proves our Proposition 1 for fully-connected layers.

_Convolutional layers_

_Proof. Standard convolution in CNNs is also a linear transformation of the input feature space and_
its main difference from dense layers rests on the restricted size of receptive field. Without loss of
generality, we analyze the representation (output) of a single kernel. To facilitate our analysis for
convolutional layers, let C denote the number of input channels and K denote the kernel size. For
ease of presentation, we define a receptive field mapping function Θ(k, i, j) that maps the positions
(k for channel index, i and j for indices on the same channel) of elements in the feature map (i.e.,
the representations) to the input features. For the k-th kernel, let Wk denote its weight tensor (with
_Wk,c being the weight matrix for channel c) and bk its bias._

Given the corresponding input patch XΘ(k,i,j), The element Hk,i,j of the representations from a
convolutional layer can be formulated as:


(15)
_i[′],j[′][ +][ b][k][,]_


_XΘ(k,i,j)_ _Wk,c_
_◦_


_Hk,i,j =_


_c=1_


_i[′]=1_


_j[′]=1_


where ◦ denotes Hadamard product. The three summations reduce the results of element-wise
product between the input patch and the k-th kernel to the correspond representation element
_Hk,i,j in the feature map. For ease of presentation, here we use the notation Zc,i[(][k][)][′],j[′][ to replace]_
_XΘ(k,i,j) ◦_ _Wk,c_ _i[′],j[′][ and let][ ζ][(][µ][c,i][′][,j][′]_ _[, σ]c,i[2]_ _[′],j[′]_ [)][ be the distribution that][ Z]c,i[(][k][)][′],j[′][ follows. Note that][ ζ]
 can be any distribution since we do not make any distributional assumption on _Zc,i[(][k][)][′],j[′]_ [.]

With the notations, Eq. (15) can be rewritten in a similar form to Eq. (9):


_Zc,i[(][k][)][′],j[′][ +][ b][k][.]_ (16)
_j[′]=1_

X


_Hk,i,j =_


_c=1_ _i[′]=1_


We use the condition that the random variables Zc,i[(][k][)][′],j[′][ satisfy the Lyapunov’s condition, i.e., there]
exists a δ such that

_C_ _K_ _K_

1
lim E _Zc,i[(][k][)][′],j[′][ −]_ _[µ][c,i][′][,j][′]_ _[|][2+][δ][i]_ = 0, (17)
_C_ _K[2]_ _s[2+][δ]_ _|_
_×_ _→∞_ _c=1_ _i[′]=1_ _j[′]=1_

X X X h

where s = _Cc=1_ _Ki[′]=1_ _Kj[′]=1_ _[σ]c,i[2]_ _[′],j[′]_ [.]

Then according to the Lyapunov CLT, the following holds:qP P P

_d_
_Hk,i,j_ _−→N_ ( _µc,i′,j′ + bk,_ _σc,i[2]_ _[′],j[′]_ [)][,] (18)

_c,i[′],j[′]X∈Θ(k,i,j)_ _c,i[′],j[′]X∈Θ(k,i,j)_

which proves our Proposition 1 for standard convolution layers.

A.2 PROOF OF PROPOSITION 2

Without loss of generality, we prove Proposition 2 for the fused representations from the LSTM
layer and the residual block of ResNet models, respectively. The results can be easily extended to
other non-linear neural operators.

_LSTM_


-----

_Proof. Long Short-Term Memory (LSTM) models are popular for extracting useful representations_
from sequence data for tasks such as speech recognition and language modeling. Each LSTM layer
contains multiple neural units. For the k-th unit, it takes as input the current input feature vector
_Xt = (Xt,1, Xt,2, . . .), hidden state vector Ht_ 1 and its cell state ct 1,k. The outputs of the unit are
_−_ _−_
its new hidden state ht,k and cell state ct,k. In this paper, we study the distribution of ht,k. Multiple
gates are adopted in an LSTM unit: by it,k, ft,k, gt,k and ot,k we denote the input gate, forget gate,
cell gate and output gate of the LSTM unit k at time step t. The update rules of these gates and the
cell state are:

_it,k = sigmoid(W(i)k[Ht_ 1, Xt] + b(i)k),
_−_

_ft,k = sigmoid(W(f_ )k[Ht 1, Xt] + b(f )k),
_−_

_gt,k = tanh(W(g)k[Ht_ 1, Xt] + b(g)k),
_−_

_ot,k = sigmoid(W(o)k[Ht_ 1, Xt] + b(o)k),
_−_

_ct,k = ft,k_ _ct_ 1,k + it,k _gt,k,_ (19)
_·_ _−_ _·_

where the W(i)k, W(f )k, W(g)k and W(o)k are the weight parameters and b(i)k, b(f )k, b(g)k and b(o)k
are the bias parameters for the gates.

The output of the LSTM unit ht,k is calculated as the following:

_ht,k = ot,k · tanh(ct,k)._ (20)

Using the final hidden states hT,k (with T being the length of the sequence) as the elements of the
layer-wise representation, we apply the following layer-wise fusion to further produce H over all
the hT,k in a single LSTM layer:


_hT,k,_ (21)

_k=1_

X


_H =_


where d is the dimension of the LSTM layer. Again, by ζ(µk, σk[2][)][ we denote the distribution of]
_hT,k (where the notation T is dropped here since it is typically a fixed parameter). With {hT,k|k =_
1, 2, . . ., d} satisfying the Lyapunov’s condition and by Central Limit Theorem H, tends to follow
the Gaussian distribution:


_σk[2][)][,]_ (22)
_i=1_

X


_d_
( _µk,_
_−→N_

_i=1_

X


which proves the Proposition 2 for layer-wise fused representations from LSTM.

_Residual blocks_

_Proof. Residual blocks are the basic units in the Residual neural network (ResNet) architecture (He_
et al., 2016). A typical residual block contains two convolutional layers with batch normalization
(BN) and uses the ReLU activation function. The input of the whole block is added to the output
of the second convolution (after BN) through a skip connection before the final activation. Since
the convolution operators are the same as we formulate in the second part of Section A.1, here we
use the notation Ψ(X) to denote the sequential operations of convolution on X followed by BN,
i.e., Ψ(X) ≜ _BN_ (Conv(X)). Again, we reuse the receptive field mapping Θ(k, i, j) as defined in
Section A.1 to position the inputs of the residual block corresponding to the element Zk,i,j in the
output representation of the whole residual block.

Let X denote the input of the residual block and Zk,i,j denote an element in the output tensor of the
whole residual block. Then we have:

_Zk,i,j = f_ _Xk,i,j + BN_ _Conv_ _f_ _BN_ (Conv(XΘ(k,i,j)))

= f Xk,i,j + Ψ _f (Ψ(XΘ( _ _k,i,j _ ))) _,_ [] (23)
   []

where f is the activation function (ReLU).


-----

We perform channel-wise fusion on the representation from the residual block to produce Hk for
the k-th channel:

_dH_ _dW_


_Zk,i,j,_ (24)

_i=1_ _j=1_

X X


_Hk =_


where dH and dW are the dimensions of the feature map and k is the channel index.

Let ζ(µk,i,j, σk,i,j[2] [)][ denote the distribution that][ Z][k,i,j][ follows. Then we apply the Lyapunov’s con-]
dition to the representation elements layer-wise, i.e.,


_dH_

_i=1_

X


_dW_

E _Zk,i,j_ _µk,i,j_ = 0, (25)
_j=1_ _|_ _−_ _|[2+][δ][]_

X 


lim E _Zk,i,j_ _µk,i,j_ = 0, (25)
_dW ×dH_ _→∞_ _s[2+]k_ _[δ]_ _i=1_ _j=1_ _|_ _−_ _|[2+][δ][]_

X X 

where sk = _di=1H_ _dj=1W_ _[σ]k,i,j[2]_ [.]

With the above condition satisfied, by CLTqP P _Hk (the fused representation on channel k) tends to_
follow the Gaussian distribution:


_dH_

_d_
_Hk_ (
_−→N_

_i=1_

X


_dW_

_µk,i,j,_
_j=1_

X


_dH_

_i=1_

X


_dW_

_σk,i,j[2]_ [)][,] (26)
_j=1_

X


which proves the Proposition 2 for channel-wise fused representations from any residual block.

B PSEUDO-CODE OF FEDPROF (DETAILED VERSION)

See Algorithm 2 for a complete version of the proposed training algorithm.

C CONVERGENCE ANALYSIS

In this section we provide the proof of the proposed Theorem 1. The analysis is mainly based on the
results provided by Li et al. (2019). We first introduce several notations to facilitate the analysis.

C.1 NOTATIONS

Let U (|U _| = N_ ) denote the full set of clients and S(t) (|S(t)| = K) denote the set of clients
selected for participating. By wk(t) we denote the local model on client k at time step t. We define
an auxiliary sequence vk(t) for each client to represent the immediate local model after a local SGD
update. Note that vk(t) is updated from vk(t − 1) with learning rate ηt−1:

_vk(t) = wk(t −_ 1) − _ηt−1∇Fk(wk(t −_ 1), ξk,t−1), (27)

where _Fk(wk(t_ 1), ξk,t 1) is the stochastic gradient computed over a batch of data ξk,t 1 drawn
_∇_ _−_ _−_ _−_
from Dk with regard to wk(t − 1).

We also define two virtual sequences ¯v(t) = _k=1_ _[ρ][k][v][k][(][t][)][ and][ ¯]w(t) = Aggregate({vk(t)}k∈S(t))_
for every time step t (Note that the actual global model w(t) is only updated at the aggregation steps
_TA =_ _E, 2E, 3E, . . ._ ). Given an aggregation interval[P][N] _E_ 1, we provide the analysis for the
_{_ _}_ _≥_
partial aggregation rule that yields ¯w(t) as:


_w¯(t) = [1]_


_vk(t),_ (28)
_k∈XS(t)_


where S(t) ( _S(t)_ = K) is the selected set of clients for the round _E_

aggregation steps| _T|_ _A, w(t) is equal to ¯w(t), i.e., w(t) = ¯w(t) if t_ _⌈T[t]A[⌉]._ [that contains step][ t][. At the]
_∈_

To facilitate the analysis, we assume each client always performs model update (and synchronization) to produce vk(t) and ¯v(t) (but obviously it does not affect the resulting ¯w and w for k / _S(t))._
_∈_

_wk(t) =_ _wv¯k((tt)),, if if t t /∈TTAA_ (29)
 _∈_


-----

**Algorithm 2: the FEDPROF protocol (detailed version)**
**Input : maximum number of rounds Tmax, local iterations per round E, client set U**, client
fraction C, validation set D[∗];
**Output: the global model w**
// Server process: running on the server

**1 Initialize global model w using a seed**

**2 v ←** 0 // version of the latest global model

**3 Broadcast the seed to all clients for identical model initialization**

**4 Collect initial profiles** _RPk_ _k_ _U from all the clients_
_{_ _}_ _∈_

**5 vk** 0, _k_ _U_

**6 Generate initial baseline profile ←** _∀_ _∈_ _RP_ _[∗](0) on D[∗]_

**7 K ←|U** _| · C_
**for round T ←** 1 to Tmax do

**8** Calculate div(RPk(vk), RP (vk)) for each client k

_[∗]_

**9** Update client scores {λk}k∈U and compute Λ = _k∈U_ _[λ][k]_

**10** _S_ Choose K clients by probability distribution _[λ]Λ[k]_

**11** Distribute ← _w to the clients in S_ [P] { _[}][k][∈][U]_
**for client k in S in parallel do**

**12** _vk_ _v,_ _k_ _S_

**13** _RP ←k(vk) ∀_ _∈updateProfile(k, w, v)_
_←_

**14** _wk_ _localTraining(k, w, E)_
_←_
**end**

**15** Collect local profiles from the clients in S

**16** Update w via model aggregation

**17** _v ←_ _T_

**18** Evaluate w and generate RP _[∗](v)_
**end**

**19 return w**
// Client process: running on client k
**updateProfile(k, w, v):**

**20** Generate RPk on Dk with the global w received

**21** Label profile RPk with version number v

**22** Return RPk
**return**
**localTraining(k, w, E):**

**23** _wk_ _w_
**for ← step e ←** 1 to E do

**24** Update wk using gradient-based method
**end**

**25** Return wk
**return**


For ease of presentation, we also define two virtual gradient sequences: _g¯(t)_ =
_Nk=1_ _[ρ][k][∇][F][k][(][w][k][(][t][))][ and][ g][(][t][) =][ P]k[N]=1_ _[ρ][k][∇][F][k][(][w][k][(][t][)][, ξ][k,t][)][. Thus we have][ E[][g][(][t][)] = ¯]g(t) and_
_vP¯(t) = ¯w(t −_ 1) − _ηt−1g(t −_ 1).

C.2 ASSUMPTIONS

We formally make four assumptions to support our analysis of convergence. Assumptions 1 and
2 are standard in the literature (Li et al., 2019; Wang et al., 2019; Cho et al., 2020) defining the
convexity and smoothness properties of the objective functions. Assumptions 3 and 4 bound the
variance of the local stochastic gradients and their squared norms in expectation, respectively. These
two assumptions are also made in by Li et al. (2019).
**Assumption 1. F1, F2, . . ., FN are L-smooth, i.e., for any k ∈** _U_ _, x and y: Fk(y) ≤_ _Fk(x) + (y −_
_x)[T]_ _Fk(x) +_ _[L]2_ 2
_∇_ _[∥][y][ −]_ _[x][∥][2]_


-----

It is obvious that the global objective F is also L-smooth as a linear combination of F1, F2, . . ., FN
with ρ1, ρ2, . . ., ρN being the weights.

**Assumption 2. F1, F2, . . ., FN are µ-strongly convex, i.e., for all k ∈** _U and any x, y: Fk(y) ≥_
_Fk(x) + (y_ _x)[T]_ _Fk(x) +_ _[µ]2_ 2
_−_ _∇_ _[∥][y][ −]_ _[x][∥][2]_

**Assumption 3. The variance of local stochastic gradients on each device is bounded: For all k ∈** _U_ _,_
E _Fk(wk(t), ξt,k)_ _Fk(wk(t))_ _ϵ[2]_
_∥∇_ _−_ _∥[2]_ _≤_

**Assumption 4. The squared norm of local stochastic gradients on each device is bounded: For all**
_k_ _U_ _, E_ _Fk(wk(t), ξt,k)_ _G[2]_
_∈_ _∥∇_ _∥[2]_ _≤_

C.3 KEY LEMMAS

To facilitate the proof of our main theorem, we first present several key lemmas.


**Lemma 1 (Result of one SGD step). Under Assumptions 1 and 2 and with ηt <** 41L _[, for any][ t][ it]_

_holds true that_

_[N]_
E∥v¯(t+1)−w[∗]∥[2] _≤_ (1−ηtµ)E∥w¯(t)−w[∗]∥[2]+ηt[2][E][∥][g][t][−]g[¯]t∥[2]+6Lηt[2][Γ+2E] _ρk∥wk(t)−w¯(t)∥[2][i]_

_k=1_

h X

(30)
_where Γ = F_ _[∗]_ _−_ [P]k[N]=1 _[ρ][k][F][ ∗]k_ _[.]_

**Lemma 2 (Gradient variance bound). Under Assumption 3, one can derive that**


E _gt_ _g¯t_
_∥_ _−_ _∥[2]_ _≤_


_ρ[2]k[ϵ]k[2][.]_ (31)
_k=1_

X


**Lemma 3 (Bounded divergence of wk(t)). Assume Assumption 4 holds and a non-increasing step**
_size ηt s.t. ηt ≤_ 2ηt+E for all t = 1, 2, . . ., it follows that

_[N]_
E _ρk∥wk(t) −_ _w¯(t)∥[2][i]_ _≤_ 4ηt[2][(][E][ −] [1)][2][G][2][.] (32)

_k=1_

h X

Lemmas 1, 2 and 3 hold for both full and partial participation and are independent of the client
selection strategy. We refer the readers to Li et al. (2019) for their proofs and focus our analysis on
opportunistic selection.

The next two lemmas give important properties of the aggregated model ¯w as a result of partial
participation and non-uniform client selection/sampling.

**Lemma 4 (Unbiased aggregation). For any aggregation step t ∈** _TA and with qk = ρk in the_
_selection of S(t), it follows that_

ES(t)[ ¯w(t)] = ¯v(t). (33)

_Proof. First, we present a key observation given by Li et al. (2019) as an important trick to handle_
the randomness caused by client selection with probability distribution {qk}k[N]=1[. By taking the]
expectation over S(t), it follows that


_qkXk._ (34)

_k=1_

X


ES(t)


_Xk = KES(t)[Xk] = K_
_k∈XS(t)_


-----

Let qk = ρk, take the expectation of ¯w(t) over S(t) and notice that ¯v(t) = _k_ _U_ _[ρ][k][v][k][(][t][)][:]_

_∈_

1
ES(t)[ ¯w(t)] = ES(t) _vk(t)_ [P]

_K_

 _k∈XS(t)_ 

= [1] [vk(t)

_K_ [E][S][(][t][)]
 k∈[X]S(t) 

= [1]

_K [K][E][S][(][t][)][[][v][k][(][t][)]]_

= _qkvk(t)_

_kX∈U_

= ¯v(t).


**Lemma 5 (Bounded variance of ¯w(t)). For any aggregation step t ∈** _TA and with a non-increasing_
_step size ηt s.t. ηt_ 2ηt+E 1, it follows that
_≤_ _−_

ES(t) _w¯(t)_ _v¯(t)_ _t_ 1[E][2][G][2][.] (35)
_∥_ _−_ _∥[2]_ _≤_ _K [4]_ _[η][2]−_

_Proof. First, one can prove that vk(t) is an unbiased estimate of ¯v(t) for any k:_


_qkvk(t) = ¯v(t)._ (36)
_kX∈U_


ES(t)[vk(t)] =


Then by the aggregation rule ¯w(t) = _K[1]_


_k_ _S(t)_ _[v][k][(][t][)][, we have:]_
_∈_


ES(t) _w¯(t)_ _v¯(t)_ =
_∥_ _−_ _∥[2]_


1

_w(t)_ _Kv¯(t)_
_K_ [2][ E][S][(][t][)][∥][K][ ¯] _−_ _∥[2]_


_v¯(t)∥[2]_
_k=1_

X


_K_ [2][ E][S][(][t][)][∥]

1

_K_ [2][ E][S][(][t][)][∥]


_vk(t)_
_−_
_k∈XS(t)_


_vk(t)_ _v¯(t)_
_−_ _∥[2]_



_k∈S(t)_


_vk(t)_ _v¯(t)_
_∥_ _−_ _∥[2]_
_k∈XS(t)_


ES(t)


_K_ [2]


(37)


+ ES(t) _vi(t)_ _v¯(t), vj(t)_ _v¯(t)_ _,_ (37)

_⟨_ _−_ _−_ _⟩_
_i,j_ _S(t),i=j_
_∈X_ _̸_ 

=0

where the second term on the RHS of (37) equals zero because| {z _vk(t)_ _k_ _U are independent and}_
_{_ _}_ _∈_
unbiased (see Eq. 36). Further, by noticing t − _E ∈_ _TA (because t ∈_ _TA) which implies that_
_wk(t_ _E) = ¯w(t_ _E) since the last communication, we have:_
_−_ _−_


+ ES(t)


ES(t) _w¯(t)_ _v¯(t)_ =
_∥_ _−_ _∥[2]_


_vk(t)_ _v¯(t)_
_∥_ _−_ _∥[2]_
_k∈XS(t)_


_K_ [2][ E][S][(][t][)]


1
= _v(t)_

_K_ [2][ K][E][S][(][t][)][∥][v][k][(][t][)][ −] [¯] _∥[2]_

= [1] _vk(t)_ _w¯(t_ _E)_ _v¯(t)_ _w¯(t_ _E)_

_K_ [E][S][(][t][)][∥] _−_ _−_ _−_ _−_ _−_ _∥[2]_
     

_w(t_ _E)_ _,_ (38)

_≤_ _K[1]_ [E][S][(][t][)][∥][v][k][(][t][)][ −] [¯] _−_ _∥[2]_


-----

where the last inequality results from E[vk(t) _w¯(t_ _E)] = ¯v(t)_ _w¯(t_ _E) and that E_ _X_ EX
_−_ _−_ _−_ _−_ _∥_ _−_ _∥[2]_ _≤_
_E∥X∥[2]. Further, we have:_

ES(t)∥w¯(t) − _v¯(t)∥[2]_ _≤_ _K[1]_ [E][S][(][t][)][∥][v][k][(][t][)][ −] _w[¯](t −_ _E)∥[2]_


= [1]

_K_

= [1]


_qkES(t)_ _vk(t)_ _w¯(t_ _E)_
_∥_ _−_ _−_ _∥[2]_
_k=1_

X


_N_ _t−1_

= [1] _qk ES(t)_ _ηi_ _Fk(wk(i), ξk,i)_ _._ (39)

_K_ _∥_ _∇_ _∥[2]_

_kX=1_ _i=Xt−E_

_Z1_
| {z }

Assumption 4 and choosing a non-increasingLet im = arg maxi ∥∇Fk _wk(i), ξk,i_ _∥, i ∈_ _η[t−t s.t.E, t η−t_ 1]. By using the Cauchy-Schwarz inequality,2ηt+E 1, we have:
   _≤_ _−_


_t−1_

_Z1 = ES(t)∥_ _ηi∇Fk(wk(i), ξk,i)∥[2]_

_i=Xt−E_

_t−1_ _t−1_

= ES(t) _ηi_ _Fk_ _wk(i), ξk,i_ _, ηj_ _Fk_ _wk(j), ξk,j_

_⟨_ _∇_ _∇_ _⟩_

_i=Xt−E_ _j=Xt−E_      

_t−1_ _t−1_

ES(t) _ηi_ _Fk_ _wk(i), ξk,i_ _ηj_ _Fk_ _wk(j), ξk,j_

_≤_ _∥_ _∇_ _∥· ∥_ _∇_ _∥_

_i=t_ _E_ _j=t_ _E_

X− X− h       i

_t−1_ _t−1_

_ηiηj_ ES(t) _Fk_ _wk(im), ξk,im_

_≤_ _i=Xt−E_ _j=Xt−E_ _·_ _∥∇_   ∥[2]

_t−1_ _t−1_

_≤_ _i=Xt−E_ _j=Xt−E_ _ηt[2]−E_ _[·][ E]S(t)[∥∇][F][k] wk(im), ξk,im∥[2]_

4ηt[2] 1[E][2][G][2][.] (40)
_≤_ _−_


Plug Z1 back into (39) and notice that _k=1_ _[q][k][ = 1][, we have:]_

_N_

[P][N]

ES(t) _w¯(t)_ _v¯(t)_ _qk4ηt[2][E][2][G][2]_
_∥_ _−_ _∥[2]_ _≤_ _K[1]_

_k=1_

X

= [4] _t_ 1[E][2][G][2][.]

_K [η][2]−_


C.4 PROOF OF THEOREM 1

_Proof. Taking expectation of ∥w¯(t) −_ _w[∗]∥[2], we have:_

E _w¯(t)_ _w[∗]_ = ES(t) _w¯(t)_ _v¯(t) + ¯v(t)_ _w[∗]_
_∥_ _−_ _∥[2]_ _∥_ _−_ _−_ _∥[2]_

= E∥w¯(t) − _v¯(t)∥[2]_ + E∥v¯(t) − _w[∗]∥[2]_ + E⟨w¯(t) − _v¯(t), ¯v(t) −_ _w[∗]⟩_ (41)
_A1_ _A2_ _A3_
| {z } | {z } | {z }

where A3 vanishes because ¯w(t) is an unbiased estimate of ¯v(t) by first taking expectation over S(t)
(Lemma 4).


-----

To bound A2 for t ∈ _TA, we apply Lemma 1:_

_A2 = E∥v¯(t) −_ _w[∗]∥[2]_ _≤_ (1 − _ηt−1µ)E∥w¯(t −_ 1) − _w[∗]∥[2]_ + ηt[2]−1[E][∥][g][t][−][1] _[−]_ _g[¯]t−1∥[2]_
_B1_

_[N]_

| {z }

+ 6Lηt[2] 1[Γ + E] _ρk_ _wk(t_ 1) _w¯(t_ 1) _._
_−_ _∥_ _−_ _−_ _−_ _∥[2][i]_

_k=1_

h X

_B2_

Then we use Lemmas 2 and 3 to bound B1 and B|2 respectively, which yields:{z }


(42)


_A2 = E_ _v¯(t)_ _w[∗]_ (1 _ηt_ 1µ)E _w¯(t_ 1) _w[∗]_ + ηt[2] 1[B][,] (43)
_∥_ _−_ _∥[2]_ _≤_ _−_ _−_ _∥_ _−_ _−_ _∥[2]_ _−_

where B = _k=1_ _[ρ]k[2][ϵ][2]k_ [+ 6][L][Γ + 8(][E][ −] [1)][2][G][2][.]

To bound A1, one can first take expectation over S(t) and apply Lemma 5 where the upper bound
actually eliminates both sources of randomness. Thus, it follows that[P][N]

_A1 = E_ _w¯(t)_ _v¯(t)_ _t_ 1[E][2][G][2] (44)
_∥_ _−_ _∥[2]_ _≤_ _K [4]_ _[η][2]−_


Let C = _K[4]_ _[E][2][G][2][ and plug][ A][1][ and][ A][2][ back into (41):]_

E _w¯(t)_ _w[∗]_ (1 _ηt_ 1µ)E _w¯(t_ 1) _w[∗]_ + ηt[2] 1[(][B][ +][ C][)][.] (45)
_∥_ _−_ _∥[2]_ _≤_ _−_ _−_ _∥_ _−_ _−_ _∥[2]_ _−_

Equivalently, let ∆t = E∥w¯(t) − _w[∗]∥[2], then we have the following recurrence relation for any_
_t ≥_ 1:
∆t (1 _ηt_ 1µ)∆t 1 + ηt[2] 1[(][B][ +][ C][)][.] (46)
_≤_ _−_ _−_ _−_ _−_

Next we prove by induction that ∆t _γ+ν_ _t_ [where][ ν][ = max] _β[2]βµ(B+1C)_ _[,][ (][γ][ + 1)∆][1]_ using an
_≤_ _−_

_β_

thataggregation interval η1 ≤ min{ _µ[1]_ _[,]_ 41L E[}] ≥[ and]1[ η] and a diminishing step size[t][ ≤] [2][η][t][+][E][.] _ηt =_ _t+γn[for some][ β >][ 1]µ_ [and][ γ >]o [ 0][ such]

First, for t = 1 the conclusion holds that ∆1 ≤ _γ+1ν_ [given the conditions. Then by assuming it holds]

for some t, one can derive from (46) that

∆t+1 ≤ (1 − _ηtβµµ)∆t + ην_ _t[2][(][B][ +][ C][)]β_ 2

1 ( + )
_≤_ _−_ _t + γ_ _γ + t_ [+] _t + γ_ _B_ _C_
  _β2(_ + ) 

= _[t][ +][ γ][ −]_ [1] _B_ _C_

(t + γ)[2][ ν][ +] (t + γ)[2] _−_ ([βµ]t +[ −] γ)[1][2][ ν]
h i

_≥0_
| {z }

_≤_ _[t]([ +]t +[ γ] γ[ −])[2][1][ ν]_


_t + γ −_ 1
_≤_ (t + γ)[2] 1 _[ν]_

_−_

_ν_
= (47)

_t + γ + 1_ _[,]_

which proves the conclusion ∆t ≤ _γ+ν_ _t_ [for any][ t][ ≥] [1][.]

Then by the smoothness of the objective function F, it follows that


E[F ( ¯w(t))] − _F_ _[∗]_ _≤_ _[L]2 [E][∥]w[¯](t) −_ _w[∗]∥[2]_

_ν_

= _[L]2 [∆][t][ ≤]_ _[L]2_ _γ + t_ _[.]_ (48)


-----

Specifically, by choosing β = _µ[2]_ [(i.e.,][ η][t][ =] _µ(γ2+t)_ [),][ γ][ = max][{][ 8]µ[L] _[, E][} −]_ [1][, we have]

_β2(_ + )
_ν = max_ _B_ _C_ _, (γ + 1)∆1_
n _βµ −_ 1 o

+ (γ + 1)∆1

_≤_ _[β][2]βµ[(][B][ +]1[ C][)]_

_−_

= [4(][B][ +][ C][)] + (γ + 1)∆1. (49)

_µ[2]_

By definition, we have w(t) = ¯w(t) at the aggregation steps. Therefore, for t _TA:_
_∈_

_ν_

E[F (w(t))] − _F_ _[∗]_ _≤_ _[L]2_ _γ + t_

_L_ 2( + )
= _B_ _C_ + _[γ][ + 1]_ ∆1 _._

(γ + t) _µ[2]_ 2

 


D PROFILE DISSIMILARITY UNDER HOMOMORPHIC ENCRYPTION

The proposed representation profiling scheme encodes the representations of data into a list of distribution parameters, namely RP (w, D) = {(µi, σi[2][)][|][i][ = 1][,][ 2][, . . ., q][}][ where][ q][ is the length of the]
profile. Theoretically, the information leakage (in terms of the data in D) by exposing RP (w, D)
is very limited and it is basically impossible to reconstruct the samples in D given RP (w, D).
Nonetheless, Homomorphic Encryption (HE) can be applied to the profiles (both locally and on the
server) so as to guarantee zero knowledge disclosure while still allowing profile matching under the
encryption. In the following we give details on how to encrypt a representation profile and compute
profile dissimilarity under Homomorphic Encryption (HE).

To calculate (3) and (4) under encryption, a client needs to encrypt (denoted as [[·]]) every single
_µi and σi[2]_ [in its profile][ RP] [(][w][, D][)][ locally before upload whereas the server does the same for its]
_RP_ _[∗](w, D[∗]). Therefore, according to Eq. (4) we have:_

[[KL(Ni[(][k][)]||Ni[∗][)]] =1]2 [log[[(][σ]i[∗][)][2][]]][ −] [1]2 [log[[(][σ]i[(][k][)])[2]]] − [[ 2[1] []]]

+ [([[(][σ]i[(][k][)])[2]]] + ([[µ[(]i[k][)]]] − [[µ[∗]i []])][2] _,_ (50)

2[[(σi[∗][)][2][]]]

where the first two terms on the right-hand side require logarithm operation on the ciphertext. However, this may not be very practical because most HE schemes are designed for basic arithmetic
operations on the ciphertext. Thus we also consider the situation where HE scheme at hand only
provides additive and multiplicative homomorphisms (Gentry, 2009). In this case, to avoid the logarithm operation, the client needs to keep every σi[2] [in][ RP][ ∗][(][w][, D][i][)][ as plaintext and only encrypts]
_µi, likewise for the server. As a result, the KL divergence can be computed under encryption as:_

[[KL(Ni[(][k][)]||Ni[∗][)]] =] 12 [log(][ σ]σi[(]i[k][∗][)] )[2] + [1]2 [(] _[σ]σi[(]i[k][∗][)]_ )[2] _−_ [1]2
hh ii

1
+ _i_ ]] [[µ[∗]i []])][2] (51)

2(σi[∗][)][2][ ([[][µ][(][k][)] _−_


where the first term on the right-hand side is encrypted after calculation with plaintext values (σi[k][)][2]

and (σ[∗])[2] whereas the second term requires multiple operations on the ciphertext values [[µ[k]i []]][ and]

[[µ[∗]]].

Now, in either case, we can compute profile dissimilarity under encryption by summing up all the
KL divergence values in ciphertext:



[[div(RPk, RP )]] = [1]

_[∗]_ _q_



[[KL(Ni[(][k][)]||Ni[∗][)]]]
_i=1_

X


(52)


-----

E DETAILS OF EXPERIMENTAL SETUP

E.1 ENVIRONMENT SETUP

Table 4: Experimental setup.

Setting Symbol S-Task L-Task

Model _w_ FFN CNN
Dataset _D_ GasTurbine EMNIST digits
Total data size _|D|_ 36.7k 280k
Validation (benchmark) set size _|D[∗]|_ 11.0k 40k
Client population _N_ 50 1000
Data distribution -  _N_ (514, 154[2]) non-IID, dominant≈60%
Noise applied -  fake, gaussian noise fake, blur, s&p
Client specification (GHz) _sk_ (0.5, 0.1[2]) (1.0, 0.1[2])
_N_ _N_
Comm. bandwidth (MHz) _bwk_ (0.5, 0.1[2]) (1.0, 0.1[2])
_N_ _N_
Signal-noise ratio _SNR_ 1e2 1e2
Bits per sample _BPS_ 11*8*4 28*28*1*8
Cycles per bit _CPB_ 300 400
# of local epochs _M_ 2 5
Loss function _ℓ_ MSE Loss NLL Loss
Learning rate _η_ 1e-2 1e-2
learning rate decay -  0.99 0.99

Detailed experimental settings are listed in Table 4. In the S-Task, the total population is 50 and the
data collected by a proportion of the sensors (i.e., end devices of this task) are of low-quality: 10%
of the sensors have no valid data and 40% of them produce noisy data. In the L-Task, we set up
a relatively large population (1000 end devices) and spread the data (from EMNIST digits) across
the devices with strong class imbalance – roughly 60% of the samples on each device fall into the
same class. Besides, many local datasets are of low-quality: the images on 15% of the clients are
irrelevant (valueless for the training of this task), 20% are (Gaussian) blurred, and 25% are affected
by the salt-and-pepper noise (random black and white dots on the image, density=0.3). For the STask, the maximum number of rounds tmax is set to 100 for both aggregation modes, whilst it is
set to 300 and 50 for the full aggregation and partial aggregation, respectively, for the L-Task. The
preference factor α for our protocol is set to 10. Considering the population of the clients, the setting
of the selection fraction C is based on the scale of the training participants suggested by Kairouz
et al. (2019).

To simulate a realistic FL system that consists of disparate end devices, the clients are heterogeneous
in terms of both performance and communication bandwidth (see Table 4). A validation set is kept
by the server (as the benchmark data) and used for model evaluation.

E.2 DETAILS OF COST FORMULATION

In each FL round, the server selects a fraction (i.e., C) of clients, distributes the global model to
these clients and waits for them to finish the local training and upload the models. Given a selected
set of clients S, the time cost and energy cost of a communication round can be formulated as:

_Tround = max_ _k_ + Tk[train] + Tk[RP] _,_ (53)
_k_ _S_ _}_
_∈_ _[{][T][ comm]_

_Ek = Ek[comm]_ + Ek[train] + Ek[RP] _._ (54)

where Tk[comm] and Tk[train] are the communication time and local training time, respectively. The
device-side energy consumption Ek mainly comes from model transmission (through wireless channels) and local processing (training), corresponding to Ek[comm] and Ek[train], respectively. Tk[RP] and
_Ek[RP]_ estimate the time and energy costs for generating and uploading local profiles and only apply
to FEDPROF.


-----

Eq. (53) formulates the length of one communication round of FL, where Tk[comm] can be modeled
by Eq. (55) according to Tran et al. (2019), where bwk is the downlink bandwidth of device k (in
MHz); SNR is the Signal-to-Noise Ratio of the communication channel, which is set to be constant
as in general the end devices are coordinated by the base stations for balanced SNR with the fairnessbased policies; msize is the size of the model; the model upload time is twice as much as that for
model download since the uplink bandwidth is set as 50% of the downlink bandwidth.

_Tk[comm]_ = Tk[upload] + Tk[download]
= 2 _Tk[download]_ + Tk[download]
_×_

_msize_
= 3 (55)
_×_ _bwk_ log(1 + SNR) _[,]_

_·_


_Tk[train]_ in Eq. (53) can be modeled by Eq. (56), where sk is the device performance (in GHz) and
the numerator computes the total number of processor cycles required for processing M epochs of
local training on Dk.

_Tk[train]_ = _[M][ · |][D][k][| ·][ BPS]sk_ _[ ·][ CPB]_ _,_ (56)

_Tk[RP]_ consists of two parts: Tk[RP gen] for local model evaluation (to generate the profiles of Dk) and
_Tk[RP up]_ for uploading the profile. Tk[RP] can be modeled as:

_Tk[RP]_ = Tk[RP gen] + Tk[RP up]

_RPsize_

= [1] _k_ + 1 (57)

_M [T][ train]_

2 _[bw][k][ ·][ log(1 +][ SNR][)]_ _[,]_

where Tk[RP gen] is estimated as the time cost of one epoch of local training; Tk[RP up] is computed in a
similar way to the calculation of Tk[comm] in Eq. (55) (where the uplink bandwidth is set as one half
of the total bwk); RPsize is the size of a profile, which is equal to 4 2 _q = 8_ _q (four bytes for_
_×_ _×_ _×_
each floating point number) according to our definition of profile in (2).

Using Eq. (54) we model the energy cost of each end device by mainly considering the energy
consumption of the transmitters for communication (Eq. 58) and on-device computation for local
training (Eq. 59). For FEDPROF, there is an extra energy cost for generating and uploading profiles
(Eq. 60).

_Ek[comm]_ = Ptrans _Tk[comm]_ (58)
_·_

_Ek[train]_ = Pf _s[3]k_ _k_ (59)

_[·][ T][ train]_

_Ek[RP]_ = Ptrans _Tk[RP up]_ + Pf _s[3]k_ _k_ _,_ (60)
_·_ _[·][ T][ RP gen]_

where Pf _s[3]k_ [is a simplified computation power consumption model (Song et al., 2013) and][ P][f][ is the]
power of a baseline processor. Ptrans is the transmitter’s power. We set Ptrans and Pf to 0.5 and
0.7 Watts respectively based on the benchmarking data provided by Carroll et al. (2010).


-----

