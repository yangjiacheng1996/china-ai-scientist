# EFFICIENT AND DIFFERENTIABLE CONFORMAL PRE## DICTION WITH GENERAL FUNCTION CLASSES


**Yu Bai**
Salesforce Research
yu.bai@salesforce.com


**Song Mei**
UC Berkeley
songmei@berkeley.edu


**Huan Wang & Yingbo Zhou & Caiming Xiong**
Salesforce Research
_{huan.wang, yingbo.zhou, cxiong}@salesforce.com_

ABSTRACT

Quantifying the data uncertainty in learning tasks is often done by learning a prediction interval or prediction set of the label given the input. Two commonly
desired properties for learned prediction sets are valid coverage and good effi_ciency (such as low length or low cardinality). Conformal prediction is a power-_
ful technique for learning prediction sets with valid coverage, yet by default its
conformalization step only learns a single parameter, and does not optimize the
efficiency over more expressive function classes.
In this paper, we propose a generalization of conformal prediction to multiple
learnable parameters, by considering the constrained empirical risk minimization
(ERM) problem of finding the most efficient prediction set subject to valid empirical coverage. This meta-algorithm generalizes existing conformal prediction
algorithms, and we show that it achieves approximate valid population coverage
and near-optimal efficiency within class, whenever the function class in the conformalization step is low-capacity in a certain sense. Next, this ERM problem
is challenging to optimize as it involves a non-differentiable coverage constraint.
We develop a gradient-based algorithm for it by approximating the original constrained ERM using differentiable surrogate losses and Lagrangians. Experiments
show that our algorithm is able to learn valid prediction sets and improve the
efficiency significantly over existing approaches in several applications such as
prediction intervals with improved length, minimum-volume prediction sets for
multi-output regression, and label prediction sets for image classification.

1 INTRODUCTION

Modern machine learning models can yield highly accurate predictions in many applications. As
these predictions are often used in critical decision making, it is increasingly important to accompany
them with an uncertainty quantification of how much the true label may deviate from the prediction.
A common approach to quantifying the uncertainty in the data is to learn a prediction set—a setvalued analogue of usual (point) predictions—which outputs a subset of candidate labels instead of
a single predicted label. For example, this could be a prediction interval for regression, or a discrete
label set for multi-class classification. A common requirement for learned prediction sets is that it
should achieve valid coverage, i.e. the set should cover the true label with high probability (such as
90%) on a new test example (Lawless & Fredette, 2005). In addition to coverage, the prediction set
is often desired to have a good efficiency, such as a low length or small cardinality (Lei et al., 2018;
Sadinle et al., 2019), in order for it to be informative. Note that coverage and efficiency typically
come as a trade-off, as it is in general more likely to achieve a better coverage using a larger set.

This paper is concerned with the problem of finding the most efficient prediction set with valid
coverage. Our approach builds on conformal prediction (Vovk et al., 2005), a powerful framework
for generating prediction sets from (trained) base predictors with finite-sample coverage guarantees.

[Code available at https://github.com/allenbai01/cp-gen.](https://github.com/allenbai01/cp-gen)


-----

|Col1|Col2|Vanilla conformal|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||Class {Ct}||||
|||||||
|||||||
|||||||
|||||||
|||||||

|Col1|CP-Gen|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||Larger class||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||


CP-Gen
Larger class {C, t}

0.5 Len(C, t( )) Len(C 0[,][ t][(] 0[)][)] 1.5

Length


0.5 Len(Ct) 1.5

Vanilla conformal
Class {Ct}

Length


Figure 1: Comparison of vanilla conformal prediction and our CP-Gen for learning a prediction interval with
90% nominal coverage on a real-world regression task. Left: The function class {Ct} is the prediction intervals
used by Conformalized Quantile Regression. Right: _Cθ,t_ is a larger class of intervals used by our conformal
_{_ _}_
quantile finetuning procedure with the same base predictor; here the additional trainable parameter θ is the last
linear layer within the quantile neural network (cf. Section 5.1 and Appendix E.3 for more details).

Conformal prediction has been used for learning prediction sets in a variety of tasks in regression (Lei & Wasserman, 2014; Lei et al., 2018; Romano et al., 2019), classification (Cauchois et al.,
2020b; Romano et al., 2020; Angelopoulos et al., 2020), structured prediction (Bates et al., 2021),
and so on. However, the conformalization step in conformal prediction by default does not offer the
flexibility for optimizing additional efficiency metrics, as the efficiency is already determined by the
associated score function and the target coverage level. As a concrete example, the Conformalized
Quantile Regression algorithm learns a single width adjustment parameter that turns a two-sided
quantile predictor into a prediction interval of valid coverage (Romano et al., 2019); however, it
does not offer a way of further optimizing its length (cf. Figure 1 Left).

For certain efficiency metrics and prediction tasks, several approaches have been proposed, for example by designing a better score function (Angelopoulos et al., 2020), using base predictors of a
specific form (Izbicki et al., 2019; 2020; Sadinle et al., 2019), or selecting a best training hyperparameter (Yang & Kuchibhotla, 2021). However, optimizing the efficiency for more general tasks
or efficiency metrics still largely requires “manual” efforts by the researcher, as it (1) often relies
on specific domain knowledge about the task at hand; (2) is often done in conjunction with conformal prediction in multiple rounds of trial-and-error; (3) is often done by reasoning about high-level
properties of the efficiency loss and coverage constraints (e.g. what makes the length short), but not
by directly optimizing the efficiency-coverage trade-off in a data-dependent way. To the best of our
knowledge, there is a lack of a more principled and unified approach for optimizing any efficiency
metric subject to valid coverage over any class of prediction sets.

In this paper, we cast the above task as a constrained empirical risk minimization (ERM) problem
of optimizing the efficiency subject to the coverage constraint, over any general function class of
prediction sets with potentially multiple learnable parameters. This is motivated by a simple observation that vanilla conformal prediction is already equivalent to solving such a constrained ERM
with one learnable parameter (Section 2.1). Overall, our algorithm can be viewed as an automatic
and data-dependent approach for optimizing the efficiency simulatneously with conformal prediction. Our contributions are summarized as follows.

_• We propose CP-Gen (Conformal Prediction with General Function Class), a generalization of_
conformal prediction to learning multiple parameters. CP-Gen selects within an arbitrary class
of prediction sets by solving the constrained ERM problem of best efficiency subject to valid
empirical coverage (Section 3.1), and is a systematic extension of existing algorithms.

_• We show theoretically that CP-Gen achieves approximately valid coverage and near-optimal ef-_
ficiency within class, whenever the class is low-capacity with respect to both the coverage and
the efficiency loss (Section 3.2, with concrete examples in Appendix C). We also provide a practical variant CP-Gen-Recal using data splitting and reconformalization, which achieves exact
coverage, as well as good efficiency under additional assumptions (Section 3.3).

_• To address the issue that CP-Gen and CP-Gen-Recal involve a non-differentiable coverage_
constraint, we develop a differentiable approximation using surrogate losses and Lagrangians
(Section 4). This allows us to solve the constrained ERM problem over higher-dimensional continuous parameter spaces via gradient-based optimization, and is more flexible than existing algorithms that require discretization and brute-force search.


-----

_• We empirically demonstrate that CP-Gen-Recal with our gradient-based implementation can_
learn prediction sets with valid coverage and significantly improved efficiency on three real-data
tasks: prediction intervals for regression with improved length, minimum-volume prediction sets
for multi-output regression, and label prediction sets for ImageNet (Section 5 & Appendix F).

We illustrate our main insight via the coverage-vs-efficiency trade-off plots in Figure 1: While
vanilla conformal prediction only learns a single parameter (within its conformalization step) by a
simple thresholding rule over a coverage-efficiency curve, our CP-Gen is able to further improve
the efficiency by thresholding a region formed by a larger function class.

1.1 RELATED WORK

**Learning prediction sets via conformal prediction** The framework of conformal prediction for
learning prediction sets is originated in the early works of (Vovk et al., 1999; 2005; Shafer & Vovk,
2008). The main advantage of conformal prediction is that it yields (marginal) coverage guarantees
regardless of the data distribution (i.e. distribution-free). More recently, conformal prediction has
been applied to a variety of uncertainty quantification tasks, such as prediction intervals for regression (Papadopoulos, 2008; Vovk, 2012; 2015; Lei & Wasserman, 2014; Vovk et al., 2018; Lei et al.,
2018; Romano et al., 2019; Izbicki et al., 2019; Guan, 2019; Gupta et al., 2019; Kivaranovic et al.,
2020; Barber et al., 2021; Foygel Barber et al., 2021), label prediction sets for classification problems
(Lei et al., 2013; Sadinle et al., 2019; Romano et al., 2020; Cauchois et al., 2020b;a; Angelopoulos
et al., 2020), and prediction sets for structured output (Bates et al., 2021).

**Optimizing efficiency in addition to valid coverage** The problem of finding a prediction set with
(approximate) valid coverage and small size has been considered, e.g. in Pearce et al. (2018); Chen
et al. (2021) for regression and Park et al. (2019) for classification; however, these approaches do not
use conformal prediction. Yang & Kuchibhotla (2021) propose to minimize the length of the conformal interval over either a finite class or a linear aggregation of base predictors, and provides coverage
and efficiency guarantees. All above works formulate this task as a risk minimization problem, yet
are restricted to considering either finite classes or specific efficiency loss functions. Our work is
inspired by (Yang & Kuchibhotla, 2021) and generalizes the above works by allowing any function
class and efficiency loss, along with providing a differentiable approximate implementation.

The problem of optimizing the efficiency can also be done by utilizing structures of the particular
efficiency loss to choose a specific base predictor and an associated prediction set (Lei & Wasserman,
2014; Sadinle et al., 2019; Izbicki et al., 2019; 2020). By contrast, our approach does not require
either the efficiency loss or the base predictor to possess any structure, and is thus complementary.

**Other algorithms and theory** An alternative line of work constructs prediction intervals / prediction sets by aggregating the prediction over multiple base predictors through Bayesian neural network (Mackay, 1992; Gal & Ghahramani, 2016; Kendall & Gal, 2017; Malinin & Gales, 2018; Maddox et al., 2019) or ensemble methods (Lakshminarayanan et al., 2016; Ovadia et al., 2019; Huang
et al., 2017; Malinin et al., 2019). However, these methods do not typically come with (frequentist)
coverage guarantees. The recent work of Hoff (2021) studies ways of enhancing Bayes-optimal prediction with frequentist coverage. Prediction intervals can also be obtained by parameter estimation
using a parametric model for the data (Cox, 1975; Bjornstad, 1990; Beran, 1990; Barndorff-Nielsen
& Cox, 1996; Hall et al., 1999; Lawless & Fredette, 2005); see (Tian et al., 2020) for a review. However, the coverage of such prediction intervals relies heavily on the parametric model being correct
(well-specified), and can even fail in certain high-dimensional regimes where the model is indeed
correct (Bai et al., 2021).

2 PRELIMINARIES

**Uncertainty quantification via prediction sets** We consider standard learning problems in which
we observe a dataset D of examples (xi, yi) from some data distribution, and wish to
_∈X × Y_
predict the label y from the input x. A prediction set is a set-valued function C : X → 2[Y] where
_C(x) is a subset of Y. Two prevalent examples are regression (Y = R) in which we can choose_
_C(x) ⊂_ R as a prediction interval, and (multi-class) classification (Y = [L] := {1, . . ., L}) in
which we can choose C(x) ⊂ [L] as a (discrete) label prediction set.


-----

**Coverage and efficiency** The (marginal) coverage probability (henceforth coverage) of a prediction set C is defined as
Coverage(C) := P(Y ∈ _C(X))_
where (X, Y ) is a test example from the same data distribution. We also define the (mis)-coverage
loss Lcoverage(C) := 1 − Coverage(C) = P(Y /∈ _C(X)). A learned prediction set is often desired_
to achieve valid coverage in the sense that Coverage(C) ≥ 1 − _α for some α ∈_ (0, 1). Here 1 − _α_
is a pre-determined target coverage level; typical choices are e.g. 1 − _α ∈{90%, 95%}, which_
corresponds to picking α ∈{0.1, 0.05}.

In addition to valid coverage, it is often desired that the prediction set has a good efficiency (such as
small size). This is motivated by the fact that valid coverage can be achieved trivially if we do not
care about the size, e.g. by always outputting C = Y, which is not informative. Throughout this
paper we will use ℓeff to denote the particular efficiency loss we care about, where ℓeff (C; (x, y))
measures the efficiency loss of C on an example (x, y), such as the length (Lebesgue measure) of
prediction intervals, or the size (cardinality) of label prediction sets.

**Nested set framework** We adopt the nested set framework of (Gupta et al., 2019) for convenience
for our presentation and analysis. A family _Ct_ _t_ R is said to be a (family of) nested sets if
_{_ _}_ _∈T ⊂_
_t ≤_ _t[′]_ implies that Ct(x) ⊂ _Ct′_ (x) for all x ∈X . Throughout this paper out notation Ct or
_Cθ,t are assumed to be nested sets with respect to t. We assume that our efficiency loss ℓeff is_
non-decreasing w.r.t. its (set-valued) argument, i.e. ℓeff (C; (x, y)) _ℓeff_ (C _[′]; (x, y)) if C_ _C_ _[′]._
_≤_ _⊆_
Therefore, for nested sets the loss t _ℓeff_ (Ct; (x, y)) is non-decreasing in t. As the coverage loss
_7→_
_L(Ct) = P(Y /∈_ _Ct(X)) (and its empirical version) is instead non-increasing in t, the efficiency_
loss and the coverage loss always comes as a trade-off.

2.1 CONFORMAL PREDICTION

Conformal prediction (Vovk et al., 2005; Lei & Wasserman, 2014) is a powerful technique for learning prediction sets with coverage guarantees. The core of conformal prediction is its conformaliza_tion step, which turns any base prediction function (or training algorithm) into a prediction set._

We here briefly review conformal prediction using the vanilla (split) conformal regression method
of (Lei et al., 2018), and refer the readers to (Angelopoulos & Bates, 2021) for more examples.
Given any base predictor f : X → R (potentially learned on a training dataset Dtrain), conformal
prediction outputs a prediction interval

_Ct[(][x][) :=]_ _f_ (x) − [b]t, f (x) + _t_ _,_ (1)

where _t ∈_ R≥0 is chosen as the (1 −b _α)-quantile_ [1] of |y − _f_ (x)| on a calibration dataset Dcal with
size ncal := |Dcal| using the following conformalization step: [b]

[b] _t = ⌈(1 −_ _α)ncal⌉_ -th largest of {|yi − _f_ (xi)|}i[n]=1[cal] _[.]_ (2)
The main guarantee for the learned interval Ct [is that it achieves a][ (1][ −] _[α][)][ coverage guarantee of]_
the form PDcal,(X,Y )(Y ∈b _Ct[(][X][))][ ≥]_ [1][ −] _[α][ (][Lei et al.]b_ [,][ 2018][, Theorem 2.2). The proof relies on the]
exchangeability between the scoresto hold in a distribution-free fashion (i.e. for any data distribution).b _{|yi −_ _f_ (xi)|}i[n]=1[cal] [and][ |][Y][ −] _[f]_ [(][X][)][|][, which allows this guarantee]

**Conformal prediction as a constrained ERM with one parameter** We start by a simple reinterpretation that the conformalization step (2) is equivalent to solving a constrained empirical risk
minimization (ERM) problem with a single learnable parameter t (cf. Appendix A for the proof).
**Proposition 1 (Conformal regression as a constrained ERM with one learnable parameter). The**
_parameter_ _t ∈_ R defined in (2) is the solution to the following constrained ERM problem

1
minimize _Leff_ (Ct) := _ℓeff_ (Ct; (xi, yi)) = 2t

[b] _t≥0_ _ncal_ _i∈XDcal_ (3)

b 1

subject to _Lcoverage(Ct) :=_ **1** _yi /_ _Ct(xi)_ _α._

_ncal_ _{_ _∈_ _} ≤_

_i∈XDcal_

1Technically (2) requires the ⌈[b](1 − _α)(nrecal + 1)⌉-th largest element to guarantee valid coverage (Vovk_
et al., 2005); here we choose the close (1 _α)nrecal_ -th largest to allow the following insight.
_⌈_ _−_ _⌉_


-----

**Algorithm 1 Conformal Prediction with General Function Class (CP-Gen)**

**Input: Class of prediction sets C = {Cθ,t}θ∈Θ,t∈T ; target miscoverage level α ∈** (0, 1); ε0 ≥ 0.
Efficiency loss ℓeff ; Calibration dataset Dcal with size ncal.

1: Solve the following constrained ERM problem on dataset Dcal (with relaxation parameter ε0):


1
(θ,[b] _t)_ arg min _Leff_ (Cθ,t) :=
_←_ _θ∈Θ,t∈T_ _ncal_

[b] b

subject to _Lcoverage(Cθ,t) :=_

**Output: Prediction set Cθ,[b]t[.]** [b]
b


_ℓeff_ (Cθ,t(xi), yi)
_i∈XDcal_ (5)

1

**1** _yi /_ _Cθ,t(xi)_ _α + ε0._

_ncal_ _{_ _∈_ _} ≤_

_i∈XDcal_


_Above, ℓeff_ (C; (x, y)) = length(C(x)) is the length of the interval C(x).

Though simple, this re-interpretation suggests a limitation to the conformalization step (2) as well as
its analogue in other existing conformal methods: It only learns a single parameter t, and thus cannot
further optimize the efficiency due to the coverage-efficiency trade-off (cf. Figure 1). However, the
form of the constrained ERM problem (3) suggests that it can be readily extended to more general
function classes with more than one learnable parameters, which is the focus of this work.

3 CONFORMAL PREDICTION WITH GENERAL FUNCTION CLASSES

3.1 ALGORITHM

Our algorithm, Conformal Prediction with General Function Classes (CP-Gen; full description in
Algorithm 1), is an extension of the constrained ERM problem (3) into the case of general function
classes with multiple learnable parameters. CP-Gen takes in a function class of prediction sets

_C := {Cθ,t(x) : θ ∈_ Θ, t ∈T ⊂ R}, (4)

where (as mentioned) we assume that {Cθ,t}t∈T is a nested set for each θ ∈ Θ. The parameter set Θ
as well as the form of Cθ,t in (4) can be arbitrary, depending on applications and the available base
predictors at hand. Given C, our algorithm then solves the constrained ERM problem (5) of finding
the smallest interval among subject to valid coverage on dataset Dcal.
_C_

Compared with vanilla conformal prediction, Algorithm 1 allows more general tasks with an arbitrary function class and efficiency loss; for example, this encompasses several recent algorithms
such as finite hyperparameter selection and linear aggregation (Yang & Kuchibhotla, 2021; Chen
et al., 2021). We remark that (5) includes an additional relaxation parameter ε0 0 for the coverage
constraint. This is for analysis (for Proposition 2(b) & 7(b)) only; our implementation uses ≥ _ε0 = 0._

3.2 THEORY

An important theoretical question about CP-Gen is whether it achieves coverage and efficiency
guarantees on the population (test data). This section showcases that, by standard generalization
arguments, CP-Gen achieves approximate validity and near-optimal efficiency whenever function
class is low-capacity in a certain sense. We remark that our experiments use the modified algorithm CP-Gen-Recal (Section 3.3) which involves a reconformalization step. Here we focus on
CP-Gen as we believe its theory could be more informative.

Let L eff,coverage (Cθ,t) := E[ℓ eff,coverage (Cθ,t; (X, Y ))] denote the population coverage and ef_{_ _}_ _{_ _}_
ficiency lossesfor any (θ, t). We define the following uniform concentration quantities:

_εeff := supθ∈Θ,t∈T_ _Leff_ (Cθ,t) − _Leff_ (Cθ,t) _,_ (6)

_εcoverage := supθ_ Θ,t _Lcoverage(Cθ,t)_ _Lcoverage(Cθ,t)_ _._ (7)
_∈_ [b]∈T [b] _−_


-----

**Algorithm 2 Conformal Prediction with General Fn. Class and Recalibration (CP-Gen-Recal)**

**Input: Class of prediction sets C = {Cθ,t}θ∈Θ,t∈T ; target miscoverage level α ∈** (0, 1); ε0 ≥ 0.
Efficiency loss ℓeff ; Calibration datasets Dcal, Drecal with size ncal, nrecal.

1: Run Algorithm 1 on dataset Dcal (with relaxation parameter ε0) to obtain (θ,[b] _t)._

2: Keep _θ, and reconformalize t_ on the recalibration dataset Drecal:
_∈T_

[b]

_trecal ←[b]_ inf _t ∈T : yi ∈_ _Cθ,t[(][x][i][)][ for at least][ ⌈][(1][ −]_ _[α][)(][n][recal][ + 1)][⌉]_ [examples][ (][x][i][, y][i][)][ ∈] _[D][recal]_
n

**Output:b** Prediction set Cθ,[b]trecal [.] b
b


The following proposition connects the generalization of CP-Gen to the above uniform concentration quantities by standard arguments (see Appendix B for the proof. We remark that the proof relies
on Dcal being i.i.d., which is slightly stronger than exchangeability assumption commonly assumed
in the conformal prediction literature.)
**Proposition 2 (Generalization of CP-Gen). The prediction set Cθ,[b]t** _[learned by Algorithm][ 1][ satisfies]_

_(a) (Approximately valid population coverage) We have_ b

_Lcoverage(Cθ,[b]t[)][ ≤]_ _[α][ +][ ε][0][ +][ ε][coverage][,]_

_i.e. the population coverage of Cθ,[b]t_ _[is at least]b_ [ 1][ −] _[α][ −]_ [(][ε][0][ +][ ε][coverage][)][.]

_(b) (Near-optimal efficiency) Supposeb_ _ε0_ _εcoverage, then we further have_
_≥_

_Leff_ (Cθ,[b]t[)][ ≤] inf _Leff_ (Cθ,t) + 2εeff _,_
(θ,t)∈Θ×T
_Lcoverage(Cθ,t)_ _α_
b _≤_

_i.e. Cθ,[b]t_ _[achieves][ 2][ε][eff]_ _[-near-optimal efficiency against any prediction set within][ C][ with at least]_
(1 _α) population coverage._
_−_ b

**Examples of good generalization** Proposition 2 shows that CP-Gen acheives approximate coverage and near-optimal efficiency if the concentration terms εeff and eff coverage are small. In
Appendix C, we bound these on two example function classes: Finite Class (Proposition 4) and
_VC/Rademacher Class (Proposition 5). Both classes admit bounds of the form_ _εeff_ _, eff_ coverage
_{_ _} ≤_

Comp(C)/ncal with high probability via standard concentration arguments, where Comp(C) is a

pcertain complexity measure of C. Combined with Proposition 2, our CP-Gen algorithm with these

classes achieve an 1 _−_ _α_ _−_ Comp(C)/ncal approximate coverage guarantee and Comp(C)/ncal

near-optimal efficiency guarantee. In particular, our Proposition 4 recovers the coverage guarantee

p p

for the finite-class selection algorithm of (Yang & Kuchibhotla, 2021, Theorem 1) though our efficiency guarantee is more general.

We remark that both examples above contain important applications. The finite class contains e.g.
optimizing over a K-dimensional hyperparameter to use via grid search, with e.g. (1/δ)[K] confidence sets and thus Comp(C) = O(log((1/δ)[K])) = O(K log(1/δ)). The VC/Rademacher class
contains the important special case of linear classes with K base predictors (we defer the formal
statement and proof to Appendix C.3). Also, these examples are not necessarily exhaustive. Our
take-away is rather that we may expect CP-Gen to generalize well (and thus achieves good coverage
and efficiency) more broadly in practice, for instance whenever it learns K ≪ _ncal parameters._

3.3 ALGORITHM WITH VALID COVERAGE VIA RECONFORMALIZATION

Although CP-Gen enjoys theoretical bounds on the coverage and efficiency, a notable drawback is
that it does not guarantee exactly valid (at least) 1 _−_ _α coverage like usual conformal prediction, and_
its approximate coverage bound depends on the uniform concentration quantity εcoverage that is not
computable from the observed data without structural assumptions on the function class C.

To remedy this, we incorporate a simple reconformalization technique on another recalibration
dataset Drecal (e.g. a further data split), which guarantees valid finite-sample coverage by exchangeability. We call this algorithm CP-Gen-Recal and provide the full description in Algorithm 2.


-----

We remark that this reconformalization technique for obtaining guaranteed 1 _−_ _α coverage is widely_
used in the conformal prediction literature, e.g. (Angelopoulos et al., 2020). However, to the best of
our knowledge, there is no known analysis for our CP-Gen-Recal algorithm for general function
classes, for which we provide a result below (formal statement and proof can be found in Proposition 7 & Appendix D). The proof of the coverage bound is standard as in the conformal prediction
literature, while the proof of the efficiency bound builds upon the result for CP-Gen (Proposition 2(b)) and handles additional concentration terms from the reconformalization step.
**Proposition 3 (Coverage and efficiency guarantee for CP-Gen-Recal; Informal version).**
_The prediction set Cθ,[b]trecal_ _[learned by Algorithm][ 2][ achieves][ (1][ −]_ _[α][)][ finite-sample coverage:]_
PDrecal,(X,Y )(Y _Cθ,[b]trecal_ [)][ ≥] [1][ −] _[α][. Further, it achieves][ O][(][ε][eff][ +][ ε][coverage][ + 1][/][√][n][recal][)][ near-]_
_∈_ b
_optimal efficiency under additional regularity assumptions._
b

4 DIFFERENTIABLE OPTIMIZATION

Our (meta) algorithms CP-Gen and CP-Gen-Recal involve solving the constrained ERM problem (5). One feasible case is when Θ is finite and small, in which we enumerate all possible θ ∈ Θ
and find the optimal _t for each θ efficiently using quantile computation. However, this optimization_
is significantly more challenging when the underlying parameter set Θ is continuous and we wish
to jointly optimize over[b] (θ, t): The coverage loss _Lcoverage(Cθ,t) is non-differentiable and its “gra-_
dient” is zero almost everywhere as it uses the zero-one loss. This makes the coverage constraint
challenging to deal with and not amenable to any gradient-based algorithm.

[b]

To address this non-differentiability, we develop a gradient-based practical implementation by approximating the coverage constraint. We first rewrite each individual coverage loss into the form

**1** _y /_ _Cθ,t(x)_ = 1 _sθ,t(x, y) < 0_ = ℓ01(sθ,t(x, y)).
_{_ _∈_ _}_ _{_ _}_

where ℓ01(z) := 1 _z < 0_ is the zero-one loss. (Such a rewriting is possible in most cases by taking
_{_ _}_
_sθ,t as a suitable “score-like” function; see Appendix E for instantiations in our experiments.) Then,_
inspired by the theory of surrogate losses (Bartlett et al., 2006), we approximate ℓ01(z) by the hinge
loss ℓhinge(z) = [1 − _z]+ which is (almost everywhere) differentiable with a non-trivial gradient._
We find the hinge loss to perform better empirically than alternatives such as the logistic loss.

To deal with the (modified) constraint, we turn it into an exact penalty term with penalty parameter
_λ ≥_ 0, and use a standard primal-dual formulation (Bertsekas, 1997) to obtain an unconstrained
min-max optimization problem on the Lagrangian:

min _Leff_ (Cθ,t) + λ _Lhinge(Cθ,t)_ _α_ (8)
_θ,t_ [max]λ 0 _−_ +[,]
_≥_

where _Lhinge(Cθ,t) :=_ _ncal1_ _ni=1cal_ _[ℓ][hinge]b_ [(][s][θ,t][(][x][i][, y]h[i][))]b[ is the empirical hinge loss on the calibration]i

dataset Dcal. Our final practical implementation of (5) solves the problem (8) by Stochastic Gradient
P
Descent-Ascent (with respect to[b] (θ, t) and λ) to yield an approximate solution (θ,[b] _t). We remark_
that in our experiments where we use the reconformalized version CP-Gen-Recal, we only keep
the _θ obtained from (8) and perform additional reconformalization to compute_ _trecal[b]_ to guarantee
coverage.

We also emphasize that the approximation in ([b] 8) makes the problem differentiable at the cost of[b]
deviating from the true constrained ERM problem (5) and thus potentially may sacrifice in terms of
the efficiency. However, our experiments in Section 5 show that such an implementation can still
improve the efficiency over existing approaches in practice, despite the approximation.

5 EXPERIMENTS

We empirically test our CP-Gen-Recal algorithm (using the practical implementation (8)) on
three representative real-data tasks. The concrete construction of _Cθ,t_ will be described within
_{_ _}_
each application. Throughout this section we choose 1 − _α = 90% as the nominal coverage level,_
and use the CP-Gen-Recal algorithm to guarantee coverage in expectation. We provide ablations
with α ∈{80%, 95%} in Appendix G.1 and G.2 and the CP-Gen algorithm in Appendix G.3.
Several additional ablations and analyses can be found in Appendix H.


-----

Table 1: Results for conformal quantile finetuning on real-data regression tasks at level 1 − _α = 90%. For_
each method we report the (test) coverage, length, and pinball loss of the corresponding base quantile predictor.
All results are averaged over 8 random seeds.

CQR QR + CP-Gen-Recal (ours)

Dataset Coverage(%) Length _L[test]pinball_ Coverage(%) Length _L[test]pinball_

MEPS 19 89.98 1.167 0.112 90.09 **0.890** 0.131
MEPS 20 89.72 1.165 0.117 89.99 **0.830** 0.141
MEPS 21 89.81 1.145 0.107 90.22 **0.962** 0.129
Facebook 1 90.12 0.555 0.052 90.34 **0.384** 0.090
Facebook 2 90.13 0.491 0.044 90.02 **0.364** 0.092
kin8nm 90.03 1.214 0.076 89.31 **1.173** 0.078
naval 89.70 3.095 0.164 89.71 **3.077** 0.166
bio 90.26 2.271 0.130 90.20 **2.164** 0.148
blog data 90.19 0.605 0.058 90.01 **0.496** 0.107

Nominal (1 − _α)_ 90.00 -  -  90.00 -  - 

5.1 IMPROVED PREDICTION INTERVALS VIA CONFORMAL QUANTILE FINETUNING

**Setup** We consider regression tasks in which we use quantile regression (pinball loss) to train a
base quantile predictor _F_ (x) = [flo(x), _fhi(x)] =_ _θ0[⊤]Φ(x) on Dtrain (with learning rate decay_
by monitoring validation loss on Dcal). Here Φ : X → R[d][h] is the learned representation function,

_θ0_ R[d][h][×][2] is the last linear layer ([b] _d[b]h denotes the last hidden dimension), and[b]_ [b] [b] _flo,_ _fhi are the learned_
lower, upper ∈ quantile functions (see Appendix[b] E.1 for more details on the training procedure).
_{_ _}_
Givenb _F_, we learn a baseline prediction interval of the form [flo(x) _t,_ _fhi([b]x) +[b] t] on Drecal via_
_−_
Conformalized Quantile Regression (CQR) (Romano et al., 2019).

We then attempt to improve the length over[b] CQR by conformal quantile finetuning[b] [b] : Fix the representation function Φ and finetune the linear layer θ using our CP-Gen-Recal algorithm, so that
_Cθ,t(x) = [θlo[⊤]Φ(x)_ _−_ _t, θhi[⊤]Φ([b]_ _x)+_ _t] (where θ = [θlo, θhi]). We learn a new_ _θ on Dcal via (8) (where_
_ℓeff is chosen as the length), and then compute[b]_ _trecal on Drecal as in Algorithm 2._

We perform the above on 9 real-world regression datasets with a 3-layer MLP with width[b] [b] _dh = 64,_
similar as (Romano et al., 2019; Feldman et al.[b], 2021). Additional details about the setup can
be found in Appendix E.1. We also test various tweaks of the CQR baseline (results provided in
Appendix H.2).

**Results** Table 1 compares the (test) coverage and length between CQR and the finetuned linear
layer via our CP-Gen-Recal. While both CQR and CP-Gen-Recal achieves valid 90% coverage, CP-Gen-Recal can systematically improve the length over CQR on all tasks. Table 1 also
reports the pinball loss for both the base _θ0Φ([b]_ _x) as well as the fine-tuned_ _θ[⊤]Φ([b]_ _x) on the test set_
_Dtest. Intriguingly, our conformal finetuning made the pinball loss worse while managing to im-_
prove the length. This suggests the unique advantage of our constrained ERM objective, as it rules

[b] [b]
out the simple explanation that the length improvement is just because of a lower test loss. We
remark that while CP-Gen-Recal improves the length over CQR, it comes at a cost in terms of
worse conditional coverage (analysis presented in Appendix H.1).

5.2 MINIMUM-VOLUME PREDICTION SETS FOR MULTI-OUTPUT REGRESSION

**Setup** This task aims to learn a box-shaped prediction set for multi-output regression with a small
_volume. Our learning task is regression with output dimension dout > 1. We first learn a based_
predictor _f : R[d]_ _→_ R[d][out] by minimizing the MSE loss on Dtrain. We then learn a box-shaped
prediction set of the form Cu(x) = _i=1_ [[][ b]fi(x) − _ui,_ _fi(x) +_ _ui] by one of the following methods:_

_• (ordinate onCoord-wise[b]_ _Dcal): EachDrecalui. To guarantee is obtained by vanilla conformalization ([Q][d][out]_ 1 _αb coverage, each coordinate is conformalized at[b]_ b 2) over the i-th output colevel 1 _α/dout ∪, motivated by the union bound. −_
_−_ b


-----

Table 2: Results for multi-output regression on next-state prediction tasks, at level 1 − _α = 90%. For each_
method we report the (test) coverage and volume of its learned box-shaped prediction set. The reported volume
is the “halfened” version _i=1_ _[u][i][. All results are averaged over 8 random seeds.]_

Coord-wise Coord-wise-Recal CP-Gen-Recal (ours)

[Q][d][out]

Dataset Coverage(%) Volume Coverage(%) Volume Coverage(%) Volume

Cartpole 94.28 1.20 × 10[−][5] 90.17 5.10 × 10[−][6] 90.12 **2.30 × 10[−][6]**

Half-Cheetah 93.90 1.10 × 10[−][5] 90.06 1.23 × 10[−][6] 90.02 **9.07 × 10[−][7]**

Ant 93.56 3.37 × 10[−][3] 89.99 1.70 × 10[−][4] 90.02 **8.25 × 10[−][5]**

Walker 94.42 2.59 × 10[−][5] 90.01 7.33 × 10[−][7] 89.94 **3.47 × 10[−][7]**

Swimmer 95.62 2.80 × 10[−][5] 89.90 2.22 × 10[−][6] 90.13 **1.46 × 10[−][7]**

Hopper 92.87 2.81 × 10[−][9] 90.02 1.01 × 10[−][9] 89.92 **8.25 × 10[−][10]**

Humanoid 94.75 4.28 × 10[−][4] 89.95 8.53 × 10[−][8] 89.94 **4.95 × 10[−][8]**

Nominal (1 − _α)_ 90.00 -  90.00 -  90.00 - 



_• (Coord-wise-Recal): Perform the above on Dcal to learn_ _ui, and reconformalize an addi-_
tional t ≥ 0 on Drecal to reshape the prediction set proportionally:

_Cu,t[(][x][) =][ Q][d]i=1[out]_ [[][ b]fi(x) − _tui,_ _fi(x) + b tui]._ (9)

(CP-Gen-Recal, ours): Optimize the volume directly over allb _u_ R[d][out] using (8) on Dcal,

_•_ b [b] b ∈
where ℓeff (Cu; (x, y)) = _i=1_ [(2][u][i][)][ is chosen as the volume loss. We then reconformalize]
an additional _t ≥_ 0 on Drecal to reshape the prediction set same as in (9). Note that this
reconformalization step is equivalent to Algorithm[Q][d][out] 2 with the re-parametrization _u_ (θ,[b] _t)_
_7→_
where _θ_ R[d]>[out][b]0 _[−][1]_ denotes the ratio between _ui, and_ _t_ R>0 denotes a common scale.
_∈_ _∈_
b [b]

Our datasets are a collection of next-state prediction tasks with multi-dimensional continuous states
in offline reinforcement learning (RL), constructed similarly as D4RL ([b] b [b] Fu et al., 2020) with some
differences. Additional details about the dataset and experimental setup are in Appendix E.2. We
also test an additional Max-score-Conformal baseline (which uses vanilla conformal prediction with score function _y_ _f_ (x), equivalent to a hypercube-shaped predictor) in Appendix H.3,
_∥_ _−_ [b] _∥∞_
which we find also performs worse than our CP-Gen-Recal.

**Results** Table 2 reports the (test) coverage and volume of the above three methods.
The Coord-wise method achieves valid coverage but is quite conservative (over-covers),
which is as expected as the union bound is worst-case in nature and the coordinate-wise
conformalization does not utilize the potential correlation between the output coordinates.
Coord-wise-Recal achieves approximately 90% coverage with a much smaller volume. Our
CP-Gen-Recal also achieves valid 90% coverage but a further lower volume across all tasks.
This suggests that optimizing the volume over all possible u ∈ R[d][out] data-dependently using our
CP-Gen-Recal is indeed more flexible than pre-determined conformalization schemes such as
Coord-wise.

**Additional experiment: label prediction sets for ImageNet** We show that CP-Gen-Recal can
learn label prediction sets for ImageNet with valid coverage and improved size over existing approaches, by finding an optimized set of ensemble weights over multiple base neural networks (Table 5). The full setup and results are presented in Appendix F.

6 CONCLUSION

This paper proposes Conformal Prediction with General Function Class, a conformal prediction
algorithm that optimizes the efficiency metric subject to valid coverage over a general function
class of prediction sets. We provide theoretical guarantees for its coverage and efficiency in certain
situations, and develop a gradient-based practical implementation which performs well empirically
on several large-scale tasks. We believe our work opens up many directions for future work, such as
stronger theoretical guarantees via more structured function classes, further improving the gradientbased approximate implementation, or experiments on other uncertainty quantification tasks.


-----

ACKNOWLEDGMENT

We thank Silvio Savarese for the suggestion on the multi-output regression task, and Yuping Luo
for the help with preparing the offline reinforcement learning dataset. We thank the anonymous
reviewers for their valuable feedback.

REFERENCES

Physicochemical properties of protein tertiary structure data set. [https://archive.](https://archive.ics.uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+Tertiary+Structure)
[ics.uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+](https://archive.ics.uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+Tertiary+Structure)
[Tertiary+Structure. Accessed: January, 2019.](https://archive.ics.uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+Tertiary+Structure)

Blogfeedback data set. [https://archive.ics.uci.edu/ml/datasets/](https://archive.ics.uci.edu/ml/datasets/BlogFeedback)
[BlogFeedback. Accessed: January, 2019.](https://archive.ics.uci.edu/ml/datasets/BlogFeedback)

[Facebook comment volume data set. https://archive.ics.uci.edu/ml/datasets/](https://archive.ics.uci.edu/ml/datasets/Facebook+Comment+Volume+Dataset)
[Facebook+Comment+Volume+Dataset. Accessed: January, 2019.](https://archive.ics.uci.edu/ml/datasets/Facebook+Comment+Volume+Dataset)

[Kinematics of an 8 link robot arm. http://ftp.cs.toronto.edu/pub/neuron/delve/](http://ftp.cs.toronto.edu/pub/neuron/delve/data/tarfiles/kin-family/)
[data/tarfiles/kin-family/. Accessed: May, 2021.](http://ftp.cs.toronto.edu/pub/neuron/delve/data/tarfiles/kin-family/)

[Medical expenditure panel survey, panel 19. https://meps.ahrq.gov/mepsweb/data_](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-181)
[stats/download_data_files_detail.jsp?cboPufNumber=HC-181,](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-181) a. Accessed: January, 2019.

[Medical expenditure panel survey, panel 20. https://meps.ahrq.gov/mepsweb/data_](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-181)
[stats/download_data_files_detail.jsp?cboPufNumber=HC-181,](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-181) b. Accessed: January, 2019.

[Medical expenditure panel survey, panel 21. https://meps.ahrq.gov/mepsweb/data_](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-192)
[stats/download_data_files_detail.jsp?cboPufNumber=HC-192,](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-192) c. Accessed: January, 2019.

[Condition based maintenance of naval propulsion plants data set. http://archive.ics.uci.](http://archive.ics.uci.edu/ml/datasets/Condition+Based+Maintenance+of+Naval+Propulsion+Plants)
[edu/ml/datasets/Condition+Based+Maintenance+of+Naval+Propulsion+](http://archive.ics.uci.edu/ml/datasets/Condition+Based+Maintenance+of+Naval+Propulsion+Plants)
[Plants. Accessed: May, 2021.](http://archive.ics.uci.edu/ml/datasets/Condition+Based+Maintenance+of+Naval+Propulsion+Plants)

Anastasios Angelopoulos, Stephen Bates, Jitendra Malik, and Michael I Jordan. Uncertainty sets
for image classifiers using conformal prediction. arXiv preprint arXiv:2009.14193, 2020.

Anastasios N Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and
distribution-free uncertainty quantification. arXiv preprint arXiv:2107.07511, 2021.

Yu Bai, Song Mei, Huan Wang, and Caiming Xiong. Understanding the under-coverage bias in
uncertainty estimation. arXiv preprint arXiv:2106.05515, 2021.

Rina Foygel Barber, Emmanuel J Candes, Aaditya Ramdas, and Ryan J Tibshirani. Predictive
inference with the jackknife+. The Annals of Statistics, 49(1):486–507, 2021.

Ole E Barndorff-Nielsen and David R Cox. Prediction and asymptotics. Bernoulli, pp. 319–340,
1996.

Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds.
_Journal of the American Statistical Association, 101(473):138–156, 2006._

Stephen Bates, Anastasios Angelopoulos, Lihua Lei, Jitendra Malik, and Michael I Jordan.
Distribution-free, risk-controlling prediction sets. arXiv preprint arXiv:2101.02703, 2021.

Rudolf Beran. Calibrating prediction regions. Journal of the American Statistical Association, 85
(411):715–723, 1990.

Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48(3):
334–334, 1997.


-----

Jan F Bjornstad. Predictive likelihood: A review. Statistical Science, pp. 242–254, 1990.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.

Maxime Cauchois, Suyash Gupta, Alnur Ali, and John C Duchi. Robust validation: Confident
predictions even when distributions shift. arXiv preprint arXiv:2008.04267, 2020a.

Maxime Cauchois, Suyash Gupta, and John Duchi. Knowing what you know: valid confidence sets
in multiclass and multilabel prediction. arXiv preprint arXiv:2004.10181, 2020b.

Haoxian Chen, Ziyi Huang, Henry Lam, Huajie Qian, and Haofeng Zhang. Learning prediction
intervals for regression: Generalization and calibration. In International Conference on Artificial
_Intelligence and Statistics, pp. 820–828. PMLR, 2021._

DR Cox. Prediction intervals and empirical bayes confidence intervals. Journal of Applied Proba_bility, 12(S1):47–55, 1975._

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009.

Shai Feldman, Stephen Bates, and Yaniv Romano. Improving conditional coverage via orthogonal
quantile regression. arXiv preprint arXiv:2106.00394, 2021.

Rina Foygel Barber, Emmanuel J Candes, Aaditya Ramdas, and Ryan J Tibshirani. The limits of
distribution-free conditional predictive inference. Information and Inference: A Journal of the
_IMA, 10(2):455–482, 2021._

Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning, 2020.

Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050–1059.
PMLR, 2016.

Leying Guan. Conformal prediction with localization. arXiv preprint arXiv:1908.08558, 2019.

Chirag Gupta, Arun K Kuchibhotla, and Aaditya K Ramdas. Nested conformal prediction and
quantile out-of-bag ensemble methods. arXiv preprint arXiv:1910.10562, 2019.

Peter Hall, Liang Peng, and Nader Tajvidi. On prediction intervals based on predictive likelihood or
bootstrap methods. Biometrika, 86(4):871–880, 1999.

Peter Hoff. Bayes-optimal prediction with frequentist coverage control. _arXiv preprint_
_arXiv:2105.14045, 2021._

Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger.
Snapshot ensembles: Train 1, get m for free. arXiv preprint arXiv:1704.00109, 2017.

Rafael Izbicki, Gilson T Shimizu, and Rafael B Stern. Flexible distribution-free conditional predictive bands using density estimators. arXiv preprint arXiv:1910.05575, 2019.

Rafael Izbicki, Gilson Shimizu, and Rafael B Stern. Cd-split and hpd-split: efficient conformal
regions in high dimensions. arXiv preprint arXiv:2007.12778, 2020.

Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer
vision? arXiv preprint arXiv:1703.04977, 2017.

Danijel Kivaranovic, Kory D Johnson, and Hannes Leeb. Adaptive, distribution-free prediction
intervals for deep networks. In International Conference on Artificial Intelligence and Statistics,
pp. 4346–4356. PMLR, 2020.

Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. arXiv preprint arXiv:1612.01474, 2016.


-----

JF Lawless and Marc Fredette. Frequentist prediction intervals and predictive distributions.
_Biometrika, 92(3):529–542, 2005._

Jing Lei and Larry Wasserman. Distribution-free prediction bands for non-parametric regression.
_Journal of the Royal Statistical Society: Series B: Statistical Methodology, pp. 71–96, 2014._

Jing Lei, James Robins, and Larry Wasserman. Distribution-free prediction sets. Journal of the
_American Statistical Association, 108(501):278–287, 2013._

Jing Lei, Max G’Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. Distributionfree predictive inference for regression. Journal of the American Statistical Association, 113
(523):1094–1111, 2018.

David John Cameron Mackay. _Bayesian methods for adaptive models._ PhD thesis, California
Institute of Technology, 1992.

Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson.
A simple baseline for bayesian uncertainty in deep learning. Advances in Neural Information
_Processing Systems, 32:13153–13164, 2019._

Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. _arXiv_
_preprint arXiv:1802.10501, 2018._

Andrey Malinin, Bruno Mlodozeniec, and Mark Gales. Ensemble distribution distillation. arXiv
_preprint arXiv:1905.00076, 2019._

Pascal Massart. The tight constant in the dvoretzky-kiefer-wolfowitz inequality. The annals of
_Probability, pp. 1269–1283, 1990._

Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua
Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty?
evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Process_ing Systems, pp. 13991–14002, 2019._

Harris Papadopoulos. Inductive conformal prediction: Theory and application to neural networks.
In Tools in artificial intelligence. Citeseer, 2008.

Sangdon Park, Osbert Bastani, Nikolai Matni, and Insup Lee. Pac confidence sets for deep neural
networks via calibrated prediction. arXiv preprint arXiv:2001.00106, 2019.

Tim Pearce, Alexandra Brintrup, Mohamed Zaki, and Andy Neely. High-quality prediction intervals for deep learning: A distribution-free, ensembled approach. In International Conference on
_Machine Learning, pp. 4075–4084. PMLR, 2018._

Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers
generalize to imagenet? In International Conference on Machine Learning, pp. 5389–5400.
PMLR, 2019.

Yaniv Romano, Evan Patterson, and Emmanuel J Cand`es. Conformalized quantile regression. arXiv
_preprint arXiv:1905.03222, 2019._

Yaniv Romano, Matteo Sesia, and Emmanuel J Cand`es. Classification with valid and adaptive
coverage. arXiv preprint arXiv:2006.02544, 2020.

Mauricio Sadinle, Jing Lei, and Larry Wasserman. Least ambiguous set-valued classifiers with
bounded error levels. Journal of the American Statistical Association, 114(525):223–234, 2019.

Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine Learning
_Research, 9(3), 2008._

Qinglong Tian, Daniel J Nordman, and William Q Meeker. Methods to compute prediction intervals:
A review and new results. arXiv preprint arXiv:2011.03065, 2020.

Aad Van Der Vaart and Jon A Wellner. A note on bounds for vc dimensions. Institute of Mathemat_ical Statistics collections, 5:103, 2009._


-----

Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.

Vladimir Vovk. Conditional validity of inductive conformal predictors. In Asian conference on
_machine learning, pp. 475–490. PMLR, 2012._

Vladimir Vovk. Cross-conformal predictors. Annals of Mathematics and Artificial Intelligence, 74
(1):9–28, 2015.

Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random world.
Springer Science & Business Media, 2005.

Vladimir Vovk, Ilia Nouretdinov, Valery Manokhin, and Alexander Gammerman. Cross-conformal
predictive distributions. In Conformal and Probabilistic Prediction and Applications, pp. 37–51.
PMLR, 2018.

Volodya Vovk, Alexander Gammerman, and Craig Saunders. Machine-learning applications of algorithmic randomness. 1999.

Yachong Yang and Arun Kumar Kuchibhotla. Finite-sample efficient conformal prediction. arXiv
_preprint arXiv:2104.13871, 2021._

A PROOF OF PROPOSITION 1

Recall that Ct(x) = [f (x) _t, f_ (x) + t] satisfies ℓeff (Ct; (x, y)) = length(Ct(x)) = 2t for any
_−_
(x, y). Also, we have

**1** _y /_ _Ct(x)_ = 1 _y /_ [f (x) _t, f_ (x) + t] = 1 _y_ _f_ (x) _> t_ _,_
_{_ _∈_ _}_ _{_ _∈_ _−_ _}_ _{|_ _−_ _|_ _}_

or equivalently 1 _y_ _Ct(x)_ = 1 _y_ _f_ (x) _t_ . Therefore, problem (3) is equivalent to
_{_ _∈_ _}_ _{|_ _−_ _| ≤_ _}_


minimize _t_
_t≥0_

1
subject to 1 _Lcoverage(Ct) :=_
_−_ [b] _ncal_

By definition of quantiles, this problem is solved at


_ncal_

**1** _yi_ _f_ (xi) _t_ 1 _α._
_i=1_ _{|_ _−_ _| ≤_ _} ≥_ _−_

X


_t = ⌈(1 −_ _α)ncal⌉_ -th largest element of {|yi − _f_ (xi)|}i[n]=1[cal] _[,]_

which is the desired result.

b

B PROOF OF PROPOSITION 2


(a) As Cθ,[b]t [solves problem (][5][), it satisfies the constraint][ b]Lcoverage(Cθ,[b]t[)][ ≤] _[α][ +][ ε][0][. Therefore,]_
b _Lcoverage(Cθ,[b]t[) =][ b]Lcoverage(Cθ,[b]t[)]_ +Lcoverage(Cθ,[b]t[)][ −]b _L[b]coverage(Cθ,[b]t[)]_

b _≤α+ε0_ b b b

_≤_ _α + ε0 +_ (θ,tsup)∈Θ×T| _Lcoverage{z_ (C}θ,t) − _L[b]coverage(Cθ,t)_

= α + ε0 + εcoverage.

(b) Suppose εcoverage _ε0. Taking any (θ, t)_ Θ such that Lcoverage(Cθ,t) _α, we have_
_≤_ _∈_ _× T_ _≤_


_Lcoverage(Cθ,t)_ _Lcoverage(Cθ,t) + εcoverage_ _α + εcoverage_ _α + ε0._
_≤_ _≤_ _≤_

This shows that (θ, t) lies within the constraint set of problem (5). Thus as (θ,[b] _t) further minimizes_

b

the loss _Leff within the constraint set, we have_ _Leff_ (Cθ,[b]t[)][ ≤] _L[b]eff_ (Cθ,t). This shows that

[b]

_Leff_ (Cθ,[b]t[)][ −] _[L][eff]_ [(][C][θ,t][)] b

[b] [b]

b


-----

_≤_ _L[b]eff_ (Cθ,[b]t[)][ −] _L[b]eff_ (Cθ,t) +2 (θ,tsup)∈Θ×T _Leff_ (Cθ,t) − _Leff_ (Cθ,t)
b _≤0_

[b]

|2εeff _._ {z }

_≤_
As the above holds simultaneously for all (θ, t) ∈ Θ × T with at most α coverage loss, taking the
sup over the left-hand side yields
_Leff_ (Cθ,[b]t[)][ −] inf _Leff_ (Cθ,t) 2εeff _._
(θ,t) Θ _≤_
_∈_ _×T_
_Lcoverage(Cθ,t)_ _α_
b _≤_

C EXAMPLES OF GOOD GENERALIZATION FOR CP-Gen

We provide two concrete examples where the concentration terms εeff and εcoverage are small with
high probability, in which case Proposition 2 guarantees that CP-Gen learns an prediction set with
approximate validity and near-optimal efficiency.
**Assumption A (Bounded and Lipschitz efficiency loss). The loss function ℓeff satisfies**
_A1 (Bounded loss)._ _ℓeff_ (Cθ,t; (x, y)) _M for all (θ, t)_ Θ _and all (x, y)_ _._
_|_ _| ≤_ _∈_ _× T_ _∈X × Y_

_A2 (t-Lipschitzness). t_ _ℓeff_ (Cθ,t; (x, y)) is L _-Lipschitz for all (θ, x, y)_ Θ _._
_7→_ _T_ _∈_ _× X × Y_
**Assumption B (Bounded** ). The parameter space R is bounded: supt _t_ _B_ _._
_T_ _T ⊂_ _∈T |_ _| ≤_ _T_

C.1 FINITE CLASS

**Proposition 4 (Finite class). Suppose Θ is a finite set (NΘ := |Θ| < ∞), and suppose Assump-**
_tions A1, A2, B hold. Then, we have with probability at least 1 −_ _δ that_

_εcoverage_ _C_ log(NΘ/δ)/ncal and εeff _C_ _M_ log(NΘ/δ) + L _B_ _/[√]ncal,_
_≤_ _≤_ _·_ _T_ _T_

_where C > 0 is an absolute constant.p_ h p i

_Proof. We first bound εcoverage. Fix any θ_ Θ, define
_∈_
_tθ(x, y) := inf_ _t_ : Cθ,t(x) _y_ (10)
_{_ _∈T_ _∋_ _}_
to be the smallest possible t such that the Cθ,t(x) contains y. Observe that, as _Cθ,t(x)_ _t_
_∈T_ _{_ _}_ _∈T_
are nested sets, the coverage event can be rewritten as 1 _y_ _Cθ,t(x)_ = 1 _t_ _tθ(x, y)_ for any
_{_ _∈_ _}_ _{_ _≥_ _}_
(x, y). Therefore, we have


_Lcoverage(Cθ,t)_ _Lcoverage(Cθ,t)_
_−_

_ncal_

[b]


sup
_t∈T_

= sup
_t∈T_

= sup
_t∈T_

= sup
_t∈T_


**1 {yi /∈** _Cθ,t(xi)} −_ P(Y /∈ _Cθ,t(X))_
_i=1_

X

_ncal_

_i=1_ **1 {yi ∈** _Cθ,t(xi)} −_ P(Y ∈ _Cθ,t(X))_

X

_ncal_

**1 {tθ(x, y) ≤** _t} −_ P(x,y)(tθ(X, Y ) ≤ _t)_
_i=1_

X


_ncal_

1

_ncal_

1

_ncal_


= sup _Fθ(t)_ _Fθ(t)_ _,_
_t∈T_ _−_

where we have defined Fθ : R [0, 1] as the CDF of the random variable tθ(X, Y ) and similarly

[b] _→_

_Fθ as the empirical CDF of the same random variable over the finite dataset Dcal. Applying the_
Dvoretzky-Kiefer-Wolfowitz (DKW) inequality (Massart, 1990, Corollary 1) yields that
b


log(2/δ)

2ncal


sup
_t∈T_


_Fθ(t)_ _Fθ(t)_

[b] _−_


-----

with probability at least 1 − _δ. Now, taking the union bound with respect to θ ∈_ Θ (where for each
_θ we plug in tail probability δ/2NΘ) we get that with probability at least 1_ _δ/2,_
_−_


_εcoverage = sup_ sup
_θ∈Θ_ _t∈T_


_Lcoverage(Cθ,t)_ _Lcoverage(Cθ,t)_

[b] _−_


(11)

log(NΘ/δ)

_._
_ncal_


log(4NΘ/δ)

_C_
2ncal _≤_


= sup sup _Fθ(t)_ _Fθ(t)_
_θ∈Θ_ _t∈T_ _−_

[b]


for some absolute constant C > 0.

We next bound εeff . Fix any θ Θ. We have by standard symmetrization argument that
_∈_


sup
_t∈T_

sup

"t∈T


_Leff_ (Cθ,t) _Leff_ (Cθ,t)
_−_

_ncal_

[b]


= E


_ℓeff_ (Cθ,t; (xi, yi)) − E[ℓeff (Cθ,t; (X, Y ))]
_i=1_

X

_ncal_


_ncal_


_≤_ 2E(xi,yi),εi

(i)
2L Eεi
_≤_ _T ·_


sup

"t∈T

sup
_t∈T_


_εiℓeff_ (Cθ,t; (xi, yi))

_ncal_ _i=1_ #

X

1 _ncal_ 1 _ncal_

_εi_ _t_ = 2L Eεi _εi_

_ncal_ _i=1_ _·_ # _T ·_ " _ncal_ _i=1_

X X

1 _ncal_ (iii)

_εi_ 2L _B_ _/[√]ncal._

" _ncal_ _i=1_ # _≤_ _T ·_ _T_

X



_[·][ sup]t∈T_ _|t|_


(ii)
2L _B_ Eεi
_≤_ _T ·_ _T ·_


Above, (i) used the Lipschitzness Assumption A2 and the Rademacher contraction inequality (Vershynin, 2018, Exercise 6.7.7); (ii) used Assumption B, and (iii) used Eεi _ncal1_ _ni=1cal_ _[ε][i]_

_≤_
Eεi _ncal1_ _ni=1cal_ _[ε][i]_ 2[][1][/][2] = 1/[√]ncal. (Above εi iid Unif( 1 ) are Rademacher variables.)h P i

_∼_ _{±_ _}_

  

P

Next, as each loss _ℓeff_ (Cθ,t; (x, y)) _M by Assumption A1, the random variable_
_|_ _| ≤_


sup
_t∈T_


_Leff_ (Cθ,t) _Leff_ (Cθ,t)

[b] _−_


satisfies the M/ncal finite-difference property. Therefore by McDiarmid’s Inequality, we have with
probability at least 1 − _δ that_


sup _Leff_ (Cθ,t) _Leff_ (Cθ,t)
_t∈T_ _−_

[b]


_M_ [2] log(1/δ)

2ncal


_≤_ Esupt∈T


_Leff_ (Cθ,t) _Leff_ (Cθ,t)

[b] _−_


log(1/δ)

_C_ _[L][T][ B][T][ +][ M]_ _._
_≤_ _·_ _√ncal_
p

Finally, by union bound over θ ∈ Θ (where we plug in δ/2NΘ as tail probability into the above),
we have with probability at least 1 − _δ/2 that_


_εeff = supθ∈Θ_ supt∈T _Leff_ (Cθ,t) − _Leff_ (Cθ,t)

(12)
log(NΘ/δ)

_C_ _[L][T][ B][T][ +][ M][b]_ _._
_≤_ _·_ _√ncal_
p


(11) together with (12) is the desired result.


-----

C.2 VC/RADEMACHER CLASS

Next, for any class, let VC( ) := VC( (x, y) **1** _y /_ _Cθ,t(x)_ : θ Θ, t ) denote its VC
_C_ _C_ _{_ _7→_ _{_ _∈_ _}_ _∈_ _∈T }_
dimension with respect to the coverage loss.
**Proposition 5 (VC/Rademacher class). We have for some absolute constant C > 0 that**

_(a) Suppose VC(C) = K + 1 < ∞, then with probability at least 1 −_ _δ/2,_


_εcoverage_ _C_
_≤_


(K + 1 + log(1/δ))/ncal.


_(b) Suppose Assumption A1 holds. Then we have with probability at least 1 −_ _δ/2 that_

_εeff ≤_ _C_ _Rn[eff]cal_ [(][C][) +] _M_ [2] log(1/δ)/ncal _,_

_where Rn[eff]cal_ [(][C][) :=][ E](xi,yi),εi sup(θ,th ) Θ _ncal1p_ _ni=1cal_ _[ε][i][ℓ][eff]_ [(][C][θ,t][; (]i[x][i][, y][i][))] _is the Rademacher_
_∈_ _×T_

iid

h i

_complexity of the class C with respect to ℓeff (aboveP εi_ _∼_ Unif({±1})).

_Proof. (a) By assumption, the class of Boolean functions_ (x, y) **1** _y /_ _Cθ,t(x)_ (θ,t) Θ
_{_ _7→_ _{_ _∈_ _}}_ _∈_ _×T_
has VC dimension K + 1 < ∞. Therefore by the standard Rademacher complexity bound for VC
classes (Vershynin, 2018, Theorem 8.3.23) and McDiarmid’s Inequality, we have with probability at
least 1 − _δ/2 that_

_ncal_


**1 {yi /∈** _Cθ,t(xi)yi} −_ P(Y /∈ _Cθ,t(X))_
_i=1_

X


_εcoverage =_ sup
(θ,t)∈Θ×T


_ncal_


_K + 1_

_ncal_


log(2/δ)

_C_
2ncal _≤_


_K + 1 + log(1/δ)_

_ncal_


_≤_ _C_


iid
(b) We have by standard symmetrization argument that (below εi Unif( 1 ) denote
_∼_ _{±_ _}_
Rademacher variables)


E[εeff ] = E


_Leff_ (Cθ,t) _Leff_ (Cθ,t)

[b] _−_


sup
_θ∈Θ,t∈T_

_ncal_


= E


_ℓeff_ (Cθ,t; (xi, yi)) − E[ℓeff (Cθ,t; (X, Y ))]
_i=1_

X

_ncal_


sup
_θ∈Θ,t∈T_


_ncal_


_≤_ 2E(xi,yi),εi


= 2Rn( ).
_C_


sup
_θ∈Θ,t∈T_


_εiℓeff_ (Cθ,t; (xi, yi))
_i=1_

X


_ncal_


Further by Assumption A1, the quantity εeff satisfies M/ncal bounded-difference, so applying McDiarmid’s Inequality gives that with probability at least 1 − _δ/2,_


2M [2] log(2/δ)

_C_
_ncal_ _≤_


_M_ [2] log(1/δ)


_εeff ≤_ E[εeff ] +


_Rn[eff]cal_ [(][C][) +]


C.3 CASE STUDY: LINEAR CLASS

In this section, we study prediction intervals with a specific linear structure and show that it satisfies
the conditions of the VC/Rademacher class of Proposition 5.

Concretely, suppose we have a regression task (Y = R), and the prediction interval Cθ,t(x) takes a
linear form

_Cθ,t(x) = [θ[⊤]Φlo(x)_ _tσ(x), θ[⊤]Φhi(x) + tσ(x)],_ (13)
_−_


-----

wherei ∈ [K θ], ∈ σ : X →Θ ⊂ RR[K]>,0 Φ. hi, Φlo : X → R[K] are feature maps such that Φlo(x)i ≤ Φhi(x)i for all

For intuitions, we can think of Φ hi,lo as pretrained representation functions and σ as an (optional)
_{_ _}_
pretrained function for modeling the variability of y|x. Note that this encompasses linear ensembling of several existing methods, such as vanilla conformal regression (Lei et al., 2018) by taking
Φhi = Φlo = Φ where each Φi : X → R is a base predictor, as well as Conformalized Quantile Regression (Romano et al., 2019) where each (Φlo,i, Φhi,i) is a pair of learned lower and upper quantile
functions.

Our goal is to find an optimal linear function of this representation that yields the shortest prediction
interval (with fixed width) subject to valid coverage.

We assume that both the features and the parameters are bounded:

**Assumption CBΦ, supx** _σ( (Bounded features and parameters)x)_ _Bσ, and supt_ _t_ _B_ _._ **. We have supθ∈Θ ∥θ∥≤** _BΘ, supx∈X ∥Φ(x)∥≤_
_∈X_ _≤_ _∈T |_ _| ≤_ _T_

The following result shows that Proposition 5 is applicable on the linear class.

**Corollary 6 (Coverage and length guarantees for linear class). For the (K + 1)-dimensional linear**
_class (13), suppose Assumption C holds, and we take the efficiency loss to be the length of the_
_interval: ℓeff_ (C; (x, y)) := length(C(x)). Then, we have with probability at least 1 _δ (over the_
_−_
_calibration dataset Dcal) that_


_K + 1 + log(1/δ)_

_,_ and εeff _C[BΘBΦ + B_ _Bσ]_
_ncal_ _≤_ _T_ _·_


log(1/δ)

_ncal_


_εcoverage_ _C_
_≤_


_where C > 0 is an absolute constant._

_Proof. We verify the conditions of Proposition 5. First, we have_

**1** _y /_ _Cθ,t(x)_ = 1 max _y_ _θ[⊤]Φhi(x), θ[⊤]Φlo(x)_ _y_ _> tσ(x)_ _._
_{_ _∈_ _}_ _−_ _−_

The set within the indicator above is the union of two sets  (x, y) : y _θ[⊤]Φhi(x)_ _tσ(x) > 0_
_−_ _−_

and (x, y) : θ[⊤]Φlo(x) _y_ _tσ(x) > 0_ . Note that each family of sets (over (θ, t) R[K] R are
_−_ _−_  _∈_ _×_
linear halfspaces with feature dimension _K + 2), and thus has VC-dimension ≤_ _K + 2. Applying_
the VC dimension bound for unions of sets (Van Der Vaart & Wellner, 2009, Theorem 1.1), we get
VC(C) ≤ _C_ _[′](K + 2 + K + 2) ≤_ _C(K + 1) for some absolute constant C > 0. Therefore the_
condition of Proposition 5(a) holds from which we obtain the desired bound for εcoverage.

To bound εeff, we first note that for any (x, y) ∈X × R,

_ℓeff_ (Cθ,t; (x, y)) = length(Cθ,t(x))
_|_ _|_ _|_ _|_

= θ[⊤](Φhi(x) Φlo(x)) + 2tσ(x) _θ_ Φhi(x) Φlo(x) + 2tσ(x)
_−_ _≤∥_ _∥∥_ _−_ _∥_
2BΘBΦ + 2B _Bσ =: M,_
_≤_ _T_

and thus the boundedness assumption (Assumption A1) holds with M defined above. Next, we have
the following bound on the Rademacher complexity


_ncal_

_εi_ _θ[⊤](Φhi(xi)_ Φlo(xi)) + 2tσ(xi)
_−_
_i=1_

X   _ncal_


_Rn[eff]cal_ [(][C][) =][ E]


sup
(θ,t)∈Θ×T

_ncal_


_ncal_


_≤_ E


+ E


E sup _θ,_ _εi(Φhi(xi)_ Φlo(xi)) + E sup _εiσ(xi)_
_≤_ "θ∈Θ -  _ncal_ _i=1_ _−_ +# "t∈T _ncal_ _i=1_ #

X X

1 _ncal_ 2[]1/2 [2][t][ ·] 1 _ncal_ 2[]1/2

sup _θ_ E _εi(Φhi(xi)_ Φlo(xi)) + 2 sup _t_ E _εiσ(xi)_
_≤_ _θ∈Θ_ _∥_ _∥·_  _ncal_ _i=1_ _−_ _t∈T_ _|_ _| ·_  _ncal_ _i=1_ !

X X

 1/2  1/2 

1 1
_BΘ_ E Φhi(x1) Φlo(x1) + 2B E _σ[2](x1)_
_≤_ _·_ _ncal_ _∥_ _−_ _∥[2]_ _T ·_ _ncal_
   


sup
_θ∈Θ_


_θ,_


_εi(Φhi(xi)_ Φlo(xi))
_−_
_i=1_

X

_ncal_


sup
_t∈T_


-----

_C_ _[B][Θ][B][Φ][ +][ B][T][ B][σ]_ _._
_≤_ _·_ _√ncal_

Applying Proposition 5(b), we get εeff _C_ [BΘBΦ + B _Bσ]_ log(1/δ)/ncal with probability

at least 1 _δ. This is the desired bound for ≤_ _ε ·eff_ . _T_ _·_
_−_ p

D THEORETICAL GUARANTEE FOR CP-Gen-Recal

In this section we state and prove the formal theoretical guarantee for the CP-Gen-Recal algorithm (Algorithm 2).

Define the score tθ(X, Y ) := inf {t ∈T : Y ∈ _Cθ,t(X)} and let Fθ(t) := P(Y ∈_ _Cθ,t(X)) =_
P(tθ(X, Y ) ≤ _t) denote its CDF._
**Assumption D (Lower bounded density for score function). For any θ ∈** Θ, tθ(X, Y ) has a positive
_density fθ(t) = Fθ[′][(][t][)][ >][ 0][ on][ t][ ∈T][ . Further, let][ t][θ,][1][−][α][ := inf][ {][t][ ∈T][ :][ F][θ][(][t][)][ ≥]_ [1][ −] _[α][}][ denote its]_
(1 _α) quantile, then there exists some constants c0, δ0 > 0 such that_
_−_

inf
_t_ [tθ,1 _α_ _δ0,tθ,1_ _α+δ0]_ _[f][θ][(][t][)][ ≥]_ _[c][0][.]_
_∈_ _−_ _−_ _−_

**Proposition 7 (Valid coverage and near-optimal efficiency for reconformalized algorithm). The**
_following holds for Algorithm 2:_

_(a) (Valid coverage) For any possible_ _θ_ Θ learned in Line 1 and the resulting _trecal, we have_
_∈_

EDrecal _Lcoverage(Cθ,[b]trecal_ [)] _α,_ _and thus PDrecal,(X,Y )_ _Y_ _Cθ,[b]trecal_ [(][X][)] 1 _α._
_≤_ [b] _∈_ [b] _≥_ _−_
h i  
b b

_(b) (Efficiency) Suppose Assumptions A2 and D hold, max_ _εcoverage + 1/ncal, 2_ log(1/δ)/nrecal

_probability at leastc0δ0 (recall the definition of 1 −_ _δ that εcoverage in (7)), and εcoveragen ≤_ _ε0. Then for δ ≤p0.5, we have with_


log(1/δ)

_nrecal_


_εcoverage +_






_/c0._






_Leff_ (Cθ,[b]trecal [)][ ≤] (θ,tmin)∈Θ×T
_Lcoverage(Cθ,t)_ _α_
b _≤_


_Leff_ (Cθ,t) + 2εeff + CL
_T ·_


_ncal_


_Proof. (a) As the learned parameter_ _θ (and thus the family of nested sets Cθ,t[) is independent of the]_
recalibration dataset Drecal, we have that the scores tθ[(][x, y][)][ on dataset][ D][recal][ and a new test point]
b
(X, Y ) are exchangeable given any _θ[b]. Therefore by (Gupta et al., 2019, Proposition 1), we have for_
b
any _θ ∈_ Θ that

[b]

PDrecal,(X,Y ) _Y_ _Cθ,[b]trecal_ [(][X][)] 1 _α,_

[b] _∈_ _≥_ _−_

 
b

or equivalently EDrecal _Lcoverage(Cθ,[b]trecal_ [)] _≤_ _α._
h i
b

(b) For any θ Θ, define the score function tθ(x, y) the same as in (10), and similarly define the
_∈_
CDF Fθ(t) := P(tθ(X, Y ) ≤ _t) and its empirical counterpart_ _Fθ[cal][(][t][)][ and][ b]Fθ[recal](t) as the finite-_
sample version on dataset Dcal and Drecal respectively.

We first analyze _t. By the same derivation as in (11), we have_ [b]

[b]supt∈T _Fθ[cal][(][t][)][ −]_ _[F]θ[b][(][t][)]_ _≤_ (θ,tsup)∈Θ×T _Fθ[cal][(][t][)][ −]_ _[F][θ][(][t][)]_ = εcoverage.

b

As (θ,[b] _t) solves the constrained ERM ([b]_ 5) and by the assumption that[b] _ℓeff_ (Cθ,t; (x, y)) is monotone
in t, we have that _t is the minimal value of t_ such that _F_ [cal]
_∈T_ _θ_ [(][t][)][ ≥] [1][ −] _[α][. Therefore, (as]_
_|Dcal| =[b]_ _ncal and {tθ(xi, yi)}i∈Dcal are almost surely distinct by Assumptionb_ D,) we have

[b] [b]

1 − _α ≤_ _F[b]θ[cal][(][b]t) ≤_ 1 − _α + 1/ncal._
b


-----

This shows that

where we recall thatFθb[(] t[b]t)θ, −1−Fαθb[is the][(][t]θ,[b] 1−[ (1]α[)][ −] =[α][)]F[ (population) quantile of]θb[(][b]t) − (1 − _α)_ _≤_ _εcoverage[ t]θ[b][(] + 1[X, Y]/n[ )][. Note that]cal,_ _[ F][ ′]θ[(][t][) =]_
_fθ(t) ≥_ _c0 on t ∈_ [tθ,1b−α _[−]_ _[δ][0][, t]θ,[b]_ 1−α [+][ δ][0][]][ by Assumption][ D][. Further,][ ε][coverage][ + 1][/n][cal][ ≤] _[c][0][δ][0][.]_
Therefore, by monotonicity ofb _Fθ, we must have_ _t ∈_ [tθ,b 1−α _[−]_ _[δ][0][, t]θ,[b]_ 1−α [+][ δ][0][]][, and thus]
_t_ _tθ,1_ _α_ (εcoverage[b] + 1/ncal)/c0. (14)
_−_ b _−_ _≤_

We next analyze _trecal. As the dataset[b]_ _Drecal is independent of_ _θ, we can apply the DKW Inequal-_
ity (Massart, 1990, Corollary 1) to obtain that

[b] [b]

log(1/δ)

supt _Fθ[recal](t) −_ _Fθ[(][t][)]_ _≤_ s 2nrecal
_∈T_

b b

with probability at least 1 _δ. Using a similar argument as above, we get (for[b]_ _δ_ 0.5)
_−_ _≤_

log(1/δ) 1 log(1/δ)

_Fθ[(][b]trecal)_ _Fθ[(][t]θ,[b]_ 1 _α[)]_ + 2 _._
_−_ _−_ _≤_ s 2nrecal _nrecal_ _≤_ s _nrecal_
b b

As 2 log(1/δ)/nrecal _c0δ0, we can apply the similar argument as above to deduce that_

_≤_

p

_trecal_ _tθ,1_ _α_ 2 log(1/δ)/nrecal/c0. (15)
_−_ _−_ _≤_

Combining (14) and (15) and using the Lipschitzness of the efficiency loss (Assumptionb p A2), we get

[b]


log(1/δ)

2nrecal


log(1/δ)

_nrecal_


_≤LLeffT ·(Cθ,tbrecal[b]trecal −[)][ −][b]t_ _≤[L][eff]L[(]T[C] ·θ,[b]_ [b]t[)] _trecal −_ _tθ,1−α_ + _tθ,1−α_ _[−]_ [b]t
 
b b
b [b] log(1/δ)

_≤_ _CLT ·_ εcoverage + n[−]cal[1] [+] s _nrecal_ /c0.

 

Finally, as we assumed εcoverage _ε0, the condition of Proposition 2(b) holds, so we have_
_≤_

_Leff_ (Cθ,[b]t[)][ ≤] inf _Leff_ (Cθ,t) + 2εeff _._
(θ,t)∈Θ×T
_Lcoverage(Cθ,t)_ _α_
b _≤_

Summing the preceding two bounds, we get


log(1/δ)

_nrecal_


_εcoverage + n[−]cal[1]_ [+]






_/c0._






_Leff_ (Cθ,[b]trecal [)][ ≤] (θ,t)inf∈Θ×T
_Lcoverage(Cθ,t)_ _α_
b _≤_

which is the desired result.


_Leff_ (Cθ,t) + 2εeff + CL
_T ·_


E ADDITIONAL EXPERIMENTAL DETAILS

E.1 CONFORMAL QUANTILE FINETUNING

**Datasets** Our choice of the datasets follows (Feldman et al., 2021). We provide information about
these datasets in Table 3.

All datasets are standardized so that inputs and labels have mean 0 and standard deviation 1, and
split into (train, cal, recal, test) with size 70%, 10%, 10%, 10% (varying with the random seed).


-----

Table 3: Information about the regression datasets. Here (n, d) denotes the (sample size, feature dim).

Dataset _n_ _d_

MEPS 19 (mep, a) 15785 139
MEPS 20 (mep, b) 17541 139
MEPS 21 (mep, c) 15656 139
Facebook 1 (fac) 40948 53
Facebook 2 (fac) 81311 53
kin8nm (kin) 8192 8
naval (nav) 11934 17
bio (bio) 45730 9
blog data (blo) 52397 280

**Base predictor and optimization** Our network architecture is a 3-layer MLP with width 64 and
output dimension 2 (for the lower and upper quantile). We use momentum SGD with initial learning
rate 10[−][3] and momentum 0.9, batch-size 1024, and run the optimization for a max of 10000 epochs.
A 10x learning rate decay is performed if the validation loss on Dcal has not decreased in 10 epochs,
and we stop the learning whenever the learning rate decay happens for 3 times. The loss function
used in training _F = [flo,_ _fhi] is the summed pinball loss of level α/2 for_ _flo and 1 −_ _α/2 for_ _fhi,_
following (Romano et al., 2019):

[b]ℓ(F ; ([b]xi, y[b] _i)) = ℓ[α/]pinball[2]_ [(][ b]flo(xi) − _yi) + ℓ[1]pinball[−][α/][2][(][ b]fhi(xi) −[b]_ _yi),_ [b]

where for any β ∈ (0[b], 1), ℓ[β]pinball [is the pinball loss at level][ β][:]

_βt_ if t < 0,
_ℓ[β]pinball[(][t][) =]_ _−_
(1 _β)t_ if t 0.
 _−_ _≥_

**Optimization details for CP-Gen-Recal** For the conformal quantile finetuning procedure with
our CP-Gen-Recal, we rewrite the miscoverage loss for the quantile-based prediction interval as

**1 {y /∈** _Cθ,t(x)} = 1_ _t −_ max _θlo[⊤]Φ(x) −_ _y, y −_ _θhi[⊤]Φ([b]_ _x)_ _< 0_ _._

(In practice our θ also includes a trainable bias same as the original top linear layer; here we abusen n o o
notation slightly to allow easier presentation.) We approximate the right-hand side above with the[b]
hinge loss to obtain the formulation (8). To solve that optimization problem, we use SGD on (θ, t)
with learning rate 0.01 and (ascent on) λ with learning rate 0.1. The batch-size here is 256 and the
number of episodes is 1000. To ensure t > 0 we use a log parametrization for t. Finally, trecal is
computed by the reconformalization step in Algorithm 2 on Drecal.

E.2 MULTI-OUTPUT REGRESSION

**Datasets** We generate offline datasets consisting of (state, action, next state) pairs within RL tasks
within the OpenAI Gym (Brockman et al., 2016). For each task, the data is generated by executing a
medium-performing behavior policy that is extracted from standard RL training runs. All tasks are
continuous state and continuous action. Table 4 summarizes the state and action dimension, along
with the reward of the policies used for generating the data. All datasets contain 200K examples.

All datasets are standardized so that inputs and labels have mean 0 and standard deviation 1, and
split into (train, cal, recal, test) with size 70%, 10%, 10%, 10% (varying with the random seed).

**Base predictor and optimization** Our network architecture is a 3-layer MLP with width 64, input
dimension din = dS + dA, and output dimension dout = dS. We use momentum SGD with initial
learning rate 10[−][3], momentum 0.9, and batch-size 512. We run the optimization for 1000 epochs
with a 10x learning rate decay at epoch 500. The loss function for training the network is the
standard MSE loss.

**Optimization details for CP-Gen-Recal** For the conformal quantile finetuning procedure with
our CP-Gen-Recal, we rewrite the miscoverage loss for the box-shaped prediction set as

_dout_

**1** _y /_ _Cu(x)_ = 1 _y /_ [fi(x) _ui,_ _fi(x) + ui]_ = 1 1 max _fi(x)_ _/ui < 0_ _._
_{_ _∈_ _}_ ( _∈_ _iY=1_ _−_ )  _−_ 1≤i≤dout _[|][y][i][ −]_ [b] _|_ 

[b] [b]


-----

Table 4: Information about the next-state prediction datasets. Here (dS, dA) denotes the (state, action) dimension of the corresponding RL task. Datasets with a (slim) note only extract a subset of the full state (so that dS
is less than the full state dimension). We also report the mean reward of the behavior policies.

RL Task _dS_ _dA_ mean reward

Cartpole 4 1 107
Half-Cheetah 17 6 8015
Ant (slim) 27 8 4645
Walker 17 6 3170
Swimmer 8 2 51
Hopper 11 3 2066
Humanoid (slim) 45 17 1357

where we recall u ∈ R[d][out] is the learnable parameter within the initial optimization stage of
CP-Gen-Recal as discussed in Section 5.2. We approximate the right-hand side above with the
hinge loss to obtain the formulation (8). To solve that optimization problem, we use SGD on (θ, t)
with learning rate 0.01 and (ascent on) λ with learning rate 0.01. The batch-size here is 1024 and
the number of episodes is 1000. To ensure u > 0 we use a log parametrization for u.

For the reconformalization step, we keep the (relative) ratios of the _u obtained above (as the_ _θ), and_
then reconformalize an additional trecal > 0 on Drecal via the proportional reshaping of (9).
b [b]

E.3 DETAILS FOR FIGURE 1

Figure 1 is obtained on one run of our conformal quantile finetuning experiment on the MEPS 19
dataset, and illustrates the coverage-efficiency tradeoff. Both figures there compute the coverage
and length on the (unseen) test set Dtest, for better illustration. Figure 1 Left plots the family

[flo(x) _t,_ _fhi(x) + t] used by Conformalized Quantile Regression. Figure 1 Right plots the family_
_−_

[b] [b] _Cθ,t(x) = [θlo[⊤]Φ(x) −_ _t, θhi[⊤]Φ([b]_ _x)]._

The specific function class of θ shown in the thinner lines is a finite set of linear interpolations
of the original _θ0 obtained in QR and the new[b]θ obtained by conformal quantile finetuning, with_
combination weights within {−0.3, −0.2, . . ., 1.0}. The shaded region is then obtained by filling in
the area.

[b] [b]

F RESULTS FOR LABEL PREDICTION SETS ON IMAGENET

Here we present the ImageNet label prediction set experiment abbreviated in Section 5.


**Dataset and model** We take K = 9 large-scale pretrained neural networks on the ImageNet training set (Deng et al., 2009). Our models are {ResNeXt101, ResNet152, ResNet101, DenseNet161,
ResNet18, ResNet50, VGG16, Inception, ShuffleNet}, similar as in (Angelopoulos et al., 2020).

We then consider task of constructing label prediction sets with valid coverage and small cardinality. We train and test out conformal procedures on the following two datasets, neither seen by the
pretrained models:

(1) ImageNet-Val: The original validation set of ImageNet with 50000 images. We randomly
split (varying with seed) this into _Dcal_ = 10000, _Drecal_ = 10000, and _Dtest_ = 30000.
_|_ _|_ _|_ _|_ _|_ _|_

(2) ImageNet-V2 (Recht et al., 2019): A new validation set following the roughly the same
collection routines of the original images in ImageNet, however believed to have a mild distribution shift and thus slightly harder for classifiers pretrained on ImageNet. This dataset
contains 10000 images, which we randomly split (varying with seed) into _Dcal_ = 4000,
_|_ _|_
_Drecal_ = 1000, and _Dtest_ = 5000.
_|_ _|_ _|_ _|_

**Methods for learning prediction sets** Our constructions of the prediction sets are based on the
Least Ambiguous Set-Valued Classifier (LAC) method of (Sadinle et al., 2019), which turns any base


-----

Table 5: Results for ImageNet Prediction Sets with Conformal Ensembling. For each method we report
the (test) coverage and set size. Each entry reports the (mean, std) over 8 random seeds.

Best conformalized single model Conformalized uniform ensemble Ensemble via CP-Gen-Recal (ours)

Dataset Coverage(%) Size Coverage(%) Size Coverage(%) Size

ImageNet-Val 90.10 ± 0.29 1.70 ± 0.03 90.13 ± 0.21 1.62 ± 0.02 90.11 ± 0.33 **1.51 ± 0.03**
ImageNetV2 90.01 ± 0.71 5.00 ± 0.24 89.93 ± 0.71 4.66 ± 0.22 90.18 ± 0.85 **4.39 ± 0.44**

predictor p where p(·|x) denotes the predicted distribution of the L = 1000 labels into a prediction
set Ct(x) via

_Ct(x) =_ _y_ [L] : p(y _x) > t_ _,_
_{_ _∈_ _|_ _}_

where t is found by conformal prediction.

We consider learning a valid prediction set with smaller set size by finding an optimized ensem_ble weight of the K base predictors using our CP-Gen-Recal algorithm. This means we learn_
prediction sets of the form


_Cθ,t(x) =_


_y_ [L] : pθ(y _x) :=_
_∈_ _|_


_θkpk(y_ _x) > t_
_|_
_k=1_

X


where {pk}k∈[K] are the base predictors.

Our CP-Gen-Recal algorithm (and its practical implementation (8)) would solve a primal-dual
optimization problem with the efficiency loss and hinge approximate coverage constraint to optimize
(θ, t). However, here the efficiency loss we care about (the cardinality) is non-differentiable. We
make a further approximation by considering the L[q]q [norm with][ q][ = 0][.][5][ as the surrogate efficiency]
loss:



[pθ(y[′]|xi) − _t][q]+[,]_
_y[′]=1_

X


_ℓeff_ (θ, t; (xi, yi)) :=


with the intuition that the q 0 limit is exactly the cardinality of Cθ,t(xi). Our final optimization
_→_
problem is then


_ncal_

_i=1_

X


_ncal_

_ℓhinge(pθ(yi_ _xi)_ _t)._
_|_ _−_
_i=1_

X


_L_

[pθ(y[′] _xi)_ _t][q]+_ [+][ λ][ 1]
_|_ _−_ _ncal_
_y[′]=1_

X


min
_θ_ ∆K _,t>0_ [max]λ>0
_∈_


_ncal_


We solve this by SGD on (θ, t) and (ascent on) λ, with the softmax parameterization for θ ( θ ∆K
_∈_
as an ensemble weight is a probability distribution) and log parametrization for t > 0. The learning
rate is 10[−][2] for (θ, t) and 10[−][4] for λ. We perform this optimization for 500 epochs over Dcal with
batch-size 256 for ImageNet-Val and 64 for ImageNet-V2.

After we obtain the iterates _θj_ (where j denotes the epoch count), we perform a further iterate

selection of first re-computing then o _t(θ[b]j) by conformalizing on Dcal, and then choosing the iterate j_

b

with the best average set size also on Dcal, before feeding it into the reconformalization step with
_Drecal. As the Drecal is only used in the reconformalization step, such as method still guarantees_

[b]
valid coverage like the original Algorithm 2.

We compare our above algorithm against two baselines: conformalizing each individual model and
reporting the best one, or conformalizing the uniform ensemble (which uses weights θunif = _K[1]_ **[1][K][).]**

For these two baselines, for fairness of comparison, we allow them to use the whole Dcal _Drecal_
as the calibration set, as their construction (apart from pre-training) is not data-dependent. ∪

**Results** Table 5 shows that our algorithm is able to learn label prediction sets with valid coverage
and improved set sizes over the baselines. This demonstrates the advantage of our method even in
applications where the efficiency loss (here set size) is non-differentiable and needs to be further
approximated to allow gradient-based algorithms.


-----

G ABLATION STUDIES

G.1 CONFORMAL QUANTILE FINETUNING

We report ablation results for the conformal quantile finetuning problem with nominal coverage
level 1 − _α ∈{80%, 95%}, and otherwise exactly the same setup as Section 5.1. The conclusions_
are qualitatively the same as the 90% version presented in Table 1.

Table 6: Results for conformal quantile finetuning on real-data regression tasks at level 1 − _α = 80%. For_
each method we report the (test) coverage, length, and pinball loss of the corresponding base quantile predictor.
All results are averaged over 8 random seeds.

CQR QR + CP-Gen-Recal (ours)

Dataset Coverage(%) Length _L[test]pinball_ Coverage(%) Length _L[test]pinball_

MEPS 19 80.42 0.702 0.154 80.45 **0.514** 0.190
MEPS 20 80.44 0.707 0.161 80.48 **0.466** 0.200
MEPS 21 79.91 0.696 0.151 79.85 **0.618** 0.192
Facebook 1 80.38 0.348 0.072 80.01 **0.198** 0.137
Facebook 2 79.96 0.329 0.063 79.80 **0.189** 0.138
kin8nm 79.59 0.865 0.119 78.69 **0.832** 0.125
naval 79.91 2.777 0.311 79.76 **2.721** 0.311
bio 80.07 1.791 0.222 80.54 **1.674** 0.248
blog data 80.64 0.399 0.082 80.10 **0.272** 0.158

Nominal (1 − _α)_ 80.00 -  -  80.00 -  - 

Table 7: Results for conformal quantile finetuning on real-data regression tasks at level 1 − _α = 95%. For_
each method we report the (test) coverage, length, and pinball loss of the corresponding base quantile predictor.
All results are averaged over 8 random seeds.

CQR QR + CP-Gen-Recal (ours)

Dataset Coverage(%) Length _L[test]pinball_ Coverage(%) Length _L[test]pinball_

MEPS 19 94.60 1.674 0.078 95.10 **1.292** 0.091
MEPS 20 94.72 1.650 0.081 94.78 **1.261** 0.097
MEPS 21 94.64 1.633 0.071 94.99 **1.351** 0.086
Facebook 1 94.96 0.797 0.036 95.04 **0.601** 0.061
Facebook 2 95.17 0.700 0.031 94.98 **0.560** 0.060
kin8nm 95.15 1.602 0.047 94.95 **1.557** 0.048
naval 94.87 3.308 0.084 94.83 **3.265** 0.088
bio 95.17 2.698 0.073 95.22 **2.587** 0.084
blog data 95.07 0.862 0.040 95.09 **0.744** 0.068

Nominal (1 − _α)_ 95.00 -  -  95.00 -  - 

G.2 MULTI-OUTPUT REGRESSION

We report ablation results for the multi-output regression problem with nominal coverage level
1 − _α ∈{80%, 95%}, and otherwise exactly the same setup as Section 5.2. The conclusions are_
qualitatively the same as the 90% version presented in Table 2, except for one dataset at level 95%.

G.3 COMPARISON OF CP-Gen AND CP-Gen-Recal

We compare the performance of CP-Gen and CP-Gen-Recal on the multi-output regression tasks
using the same setup as Section 5.2. Recall that the vanilla CP-Gen optimizes both (θ,[b] _t) on Dcal_
(we additionally reconformalize _t on the same Dcal to address the potential bias in_ _t brought by the_
approximate optimization (8)), whereas our main CP-Gen-Recal algorithm optimizes[b]θ on Dcal
and reconformalizes _trecal on Drecal[b]_ . [b]

Table 10 reports the results. Observe that, except for the volume on one dataset (Humanoid), there is[b]
no significant difference in both the coverage and the volume for the two methods. For practice we[b]
recommend CP-Gen-Recal whenever the exact coverage guarantee is important, yet this result


-----

Table 8: Results for multi-output regression on next-state prediction tasks, at level 1 − _α = 80%. For each_
method we report the (test) coverage and volume of its learned box-shaped prediction set. The reported volume
is the “halfened” version _i=1_ _[u][i][. All results are averaged over 8 random seeds.]_

Coord-wise Coord-wise-Recal CP-Gen-Recal (ours)

[Q][d][out]

Dataset Coverage(%) Volume Coverage(%) Volume Coverage(%) Volume

Cartpole 87.82 1.74 × 10[−][6] 80.08 7.83 × 10[−][7] 80.09 **7.45 × 10[−][7]**

Half-Cheetah 88.28 4.26 × 10[−][7] 79.96 2.42 × 10[−][8] 80.04 **1.37 × 10[−][8]**

Ant 87.77 1.97 × 10[−][5] 80.12 3.97 × 10[−][7] 80.06 **1.75 × 10[−][7]**

Walker 90.25 5.88 × 10[−][7] 80.28 3.13 × 10[−][9] 80.28 **1.45 × 10[−][9]**

Swimmer 91.49 1.33 × 10[−][6] 79.99 6.18 × 10[−][8] 79.97 **4.32 × 10[−][9]**

Hopper 86.47 3.25 × 10[−][10] 79.87 7.40 × 10[−][11] 79.97 **4.41 × 10[−][11]**

Humanoid 90.84 2.86 × 10[−][7] 80.05 9.47 × 10[−][13] 80.02 **2.41 × 10[−][13]**

Nominal (1 − _α)_ 80.00 -  80.00 -  80.00 - 


Table 9: Results for multi-output regression on next-state prediction tasks, at level 1 − _α = 95%. For each_
method we report the (test) coverage and volume of its learned box-shaped prediction set. The reported volume
is the “halfened” version _i=1_ _[u][i][. All results are averaged over 8 random seeds.]_

Coord-wise Coord-wise-Recal CP-Gen-Recal (ours)

[Q][d][out]

Dataset Coverage(%) Volume Coverage(%) Volume Coverage(%) Volume

Cartpole 97.21 1.07 × 10[−][4] 95.10 4.60 × 10[−][5] 95.12 **8.61 × 10[−][6]**

Half-Cheetah 96.80 2.37 × 10[−][4] 95.03 4.03 × 10[−][5] 95.01 **3.29 × 10[−][5]**

Ant 96.65 5.30 × 10[−][1] 95.02 4.87 × 10[−][2] 95.09 **2.39 × 10[−][2]**

Walker 97.01 8.08 × 10[−][4] 94.94 6.21 × 10[−][5] 94.99 **4.27 × 10[−][5]**

Swimmer 97.74 3.44 × 10[−][4] 94.95 3.77 × 10[−][5] 95.01 **5.34 × 10[−][6]**

Hopper 96.27 1.76 × 10[−][8] 94.96 **8.23 × 10[−][9]** 94.96 1.19 × 10[−][8]

Humanoid 97.22 3.58 × 10[−][1] 94.99 7.69 × 10[−][4] 94.91 **7.49 × 10[−][4]**

Nominal (1 − _α)_ 95.00 -  95.00 -  95.00 - 


shows that—perhaps originating from the fact that here ncal = 20000 is large—the coverage (generalization error) of CP-Gen is also nearly valid, which may be better than what our Proposition 2
suggests.

Table 10: Comparison of CP-Gen and CP-Gen-Recal on the multi-output regression tasks. The reported
volume is the “halfened” version _i=1_ _[u][i][.]_

CP-Gen-Recal CP-Gen

[Q][d][out]

Dataset Coverage(%) Volume Coverage(%) Volume


Cartpole 90.12 2.30 × 10[−][6] 90.09 2.30 × 10[−][6]

Half-Cheetah 90.02 9.07 × 10[−][7] 89.96 8.83 × 10[−][7]

Ant 90.02 8.25 × 10[−][5] 89.98 8.21 × 10[−][5]

Walker 89.94 3.47 × 10[−][7] 89.91 3.30 × 10[−][7]

Swimmer 90.13 1.46 × 10[−][7] 89.96 1.29 × 10[−][7]

Hopper 89.92 8.25 × 10[−][10] 89.92 8.23 × 10[−][10]

Humanoid 89.94 4.95 × 10[−][8] 90.05 7.08 × 10[−][8]

Nominal 90.00 -  90.00 - 


-----

H ADDITIONAL EXPERIMENTS AND ANALYSES

H.1 CONDITIONAL COVERAGE OF CP-Gen-Recal

We analyze the improved length prediction intervals learned by CP-Gen-Recal (Section 5.1) by
evaluating its conditional coverage metrics and comparing with the baseline CQR method.

As conditional coverage is hard to reliably estimate from finite data, we consider two proxy metrics
proposed in (Feldman et al., 2021) that measure the independence between length and indicator of
_coverage:_

_• The correlation coefficient (Corr) between the following two random variables: the interval size_
_L = length(C(X)) and the indicator of coverage V = 1_ _Y ∈_ _C[b](X)_ . A (population) correlation of 0 is a necessary (but not sufficient) condition of perfect conditional coverage (n o Feldman
et al., 2021). Here we measure the absolute correlation, which is smaller the better.

_• HSIC: A more sophisticated correlation metric between L and V that takes into account nonlin-_
ear correlation structures. A (population) HSIC of 0 is a necessary and sufficient condition of
the independence between L and V . We estimate HSIC on the finite test data using the method
in (Feldman et al., 2021).

Table 11 reports the results. Observe that while our CP-Gen-Recal improves the length, it
achieves worse (higher) Correlation/HSIC than the baseline CQR, which is expected as length and
conditional coverage often come as a trade-off.

Table 11: Conditional coverage results for conformal quantile finetuning on real-data regression tasks at
level 1 _−_ _α = 90%. For each method we report the (absolute) correlation coefficient as well as the HSIC metric_
between length and indicator of coverage. All results are averaged over 8 random seeds.

CQR QR + CP-Gen-Recal (ours)

Dataset Corr(↓) HSIC(↓) Length(↓) Corr(↓) HSIC(↓) Length(↓)

MEPS 19 **0.022** **3.03 × 10[−][5]** 1.167 0.049 1.77 × 10[−][4] **0.890**
MEPS 20 **0.032** **3.63 × 10[−][5]** 1.165 0.113 2.66 × 10[−][4] **0.830**
MEPS 21 **0.029** **4.72 × 10[−][5]** 1.145 0.068 2.20 × 10[−][4] **0.962**
Facebook 1 **0.029** **1.27 × 10[−][5]** 0.555 0.175 7.34 × 10[−][4] **0.384**
Facebook 2 **0.024** **1.16 × 10[−][5]** 0.491 0.116 2.68 × 10[−][4] **0.364**
kin8nm **0.031** **4.85 × 10[−][5]** 1.214 0.084 9.32 × 10[−][5] **1.173**
naval 0.091 **1.05 × 10[−][5]** 3.095 **0.064** 2.16 × 10[−][5] **3.077**
bio **0.026** **4.15 × 10[−][5]** 2.271 0.041 1.09 × 10[−][4] **2.164**
blog data **0.013** **4.60 × 10[−][5]** 0.605 0.141 5.75 × 10[−][4] **0.496**

H.2 ALTERNATIVE TWEAKS FOR CQR BASELINE

Here we test two additional tweaked versions of the CQR baseline in the prediction interval experiment of Section 5.1:

_• CQRconformalize on-Dtrain ∪_ _D Dcalrecal: Use dataset._ _Dtrain ∪_ _Dcal for training the base quantile regressor, then_

CQR-PinballFt: Train the base quantile regressor on Dtrain, and finetune the last linear

_•_
layer on Dcal using the pinball loss (same as training), and conformalize on Drecal.

Optimization details about these two methods are described in Section H.2.1.

**Result** Table 12 reports the results for these two tweaked baselines, in comparison with our original baseline CQR as well as our proposed QR + CP-Gen-Recal. Observe that using more training data (CQR-Dtrain _Dcal) improves the length slightly on some datasets but not all. In con-_
trast, CQR-PinballFt ∪ is unable to improve either the pinball loss or the length over the base
uses a less expressive model in the finetuning stage). Overall, on almost all datasets (except forCQR(observe that CQR-PinballFt uses the same set of training data as CQR-Dtrain ∪ _Dcal but_
kin8nm), our CP-Gen-Recal still achieves the best length.


-----

Table 12: Results for conformal quantile finetuning on real-data regression tasks at level 1−α = 90%. Here
we compare our CP-Gen-Recal method with tweaked versions of the baseline CQR method. All results
are averaged over the same 8 random seeds as in Table 1. All (average) coverages are within (90 ± 0.5)% and
omitted here.


CQR-Dtrain CQR-Dtrain _Dcal_ CQR-PinballFt QR + CP-Gen-Recal (ours)
_∪_

Dataset Length _L[test]pinball_ Length _L[test]pinball_ Length _L[test]pinball_ Length _L[test]pinball_

MEPS 19 1.167 0.112 1.171 0.111 1.192 0.112 **0.890** 0.131
MEPS 20 1.165 0.117 1.179 0.114 1.190 0.117 **0.830** 0.141
MEPS 21 1.145 0.107 1.150 0.106 1.249 0.107 **0.962** 0.129
Facebook 1 0.555 0.052 0.549 0.051 0.578 0.052 **0.384** 0.090
Facebook 2 0.491 0.044 0.472 0.042 0.523 0.044 **0.364** 0.092
kin8nm 1.214 0.076 **1.165** 0.072 1.232 0.075 1.173 0.078
naval 3.095 0.164 3.089 0.164 3.096 0.164 **3.077** 0.166
bio 2.271 0.130 2.240 0.128 2.271 0.130 **2.164** 0.148
blog data 0.605 0.058 0.551 0.056 0.660 0.058 **0.496** 0.107

H.2.1 OPTIMIZATION DETAILS

CQRtomatically determining the early stopping (cf. Section-Dtrain ∪ _Dcal: Our original CQR baseline used D E.1cal for monitoring validation loss and au-), and Drecal for conformalization. To_
optimize on Dtrain _Dcal, we do not use automatic learning rate decay and early stopping, but in-_
stead manually picked the number of epochs and corresponding learning rate decay schedule that is ∪
close to average runs of the CQR method on each dataset. This choice ensures that our new baseline
still gets to use see the exact same amount of data for conformalizing (Drecal) and testing (Dtest),
and has a optimization setup as close as possible the original CQR baseline.

More concretely, we optimize for 800 epochs for {MEPS 19, MEPS 20, MEPS 21}, 1500 epochs
for {Facebook 1, Facebook 2, blog data}, 6000 epochs for kin8nm, 350 epochs for naval, and 2500
epochs for bio. For all datasets, the learning rate decays by 10x twice, at 90% and 95% of the total
epochs.

CQR-PinballFt: We finetuned on Dcal with 1000 epochs and batch size 256. The learning rate
was chosen within {10[−][2], 10[−][3]} and the results are not too different for these two choices (length
difference is within 0.010 for these two choices, and there is no overall winner). We presented the
results with learning rate 10[−][3].

H.3 ADDITIONAL Max-score-Conformal BASELINE FOR MULTI-OUTPUT REGRESSION

We test one additional baseline method for the multi-output regression experiment in Section 5.2:

_• Max-score-Conformal: Here we consider the hypercube-shaped predictor_

_dout_

_Ct(x) =_ [fi(x) _t, fi(x) + t],_ (16)

_−_
_i=1_

Y

[b]

prediction setand use conformal prediction on Ct[(][x][)][. In other words, we perform standard conformal prediction with score] Dcal ∪ _Drecal to compute a conformalized_ _t and the final_
function _y_ _f_ (x) . [b]
_∥_ _−_ [b]b _∥∞_

We remark that both the Coord-wise-Recal and the Max-score-Conformal baseline
methods are special instances of CP-Gen-Recalwith some fixed θ0: In our parametrization,
_u = (θ, t), θ determines the shape (i.e. relative ratios between the u[′]i[s][) whereas][ t][ determines the]_
_size. Therefore, Max-score-Conformal can be thought of as choosing θ0 to be the all-ones ra-_
tio (i.e. hypercube-shaped), whereas Coord-wise-Recalcan be thought of as choosing θ0 from
a coordinate-wise one dimensional conformal prediction.

**Result** Table 13 reports the result for Max-score-Conformal. Compared with the existing baseline Coord-wise-Recal, Max-score-Conformal achieves better volume on the
Cartpole dataset but worse volume on almost all other datasets (except for Swimmer where their


-----

volumes are similar). Further, note that Max-score-Conformal achieves significantly higher
volumes for certain datsets (Ant, Humanoid). Our inspection shows that this due to the fact that
there are a certain number of hard-to-predict state dimensions (and many other easy-to-predict
state dimensions) for these two datasets. Therefore, Coord-wise-Recal which builds on
Coord-wise adapts to this structure and uses only a high length on these dimensions only, whereas
the Max-score-Conformal method pays this max conformal score on all dimensions to yield
an unnecessarily high volume.

We remark that our CP-Gen-Recal still performs significantly better than both baselines.

Table 13: Results for multi-output regression on next-state prediction tasks, at level 1 − _α = 90%. The_
setting is the same as in Table 2 (with the same 8 random seeds), and here we compare additionally with the
Max-score-Conformal baseline method described in (16).

Coord-wise-Recal Max-score-Conformal CP-Gen-Recal (ours)

Dataset Coverage(%) Volume Coverage(%) Volume Coverage(%) Volume


Cartpole 90.17 5.10 × 10[−][6] 90.10 3.07 × 10[−][6] 90.12 **2.30 × 10[−][6]**

Half-Cheetah 90.06 1.23 × 10[−][6] 89.96 1.72 × 10[−][4] 90.02 **9.07 × 10[−][7]**

Ant 89.99 1.70 × 10[−][4] 90.06 3.46 × 10[2] 90.02 **8.25 × 10[−][5]**

Walker 90.01 7.33 × 10[−][7] 90.02 1.03 × 10[−][2] 89.94 **3.47 × 10[−][7]**

Swimmer 89.90 2.22 × 10[−][6] 90.08 2.21 × 10[−][6] 90.13 **1.46 × 10[−][7]**

Hopper 90.02 1.01 × 10[−][9] 89.96 1.29 × 10[−][8] 89.92 **8.25 × 10[−][10]**

Humanoid 89.95 8.53 × 10[−][8] 89.98 2.48 × 10[7] 89.94 **4.95 × 10[−][8]**

Nominal (1 − _α)_ 90.00 -  90.00 -  90.00 - 

H.4 100 RANDOM SEEDS AND STANDARD DEVIATION

Here we repeat the experiments in Section 5.1 & 5.2 with 100 random seeds and the exact same setups. We report the mean and standard deviations in Table 14 for the prediction intervals experiment
(Section 5.1), Table 15 for the mean and Table 16 for the standard deviation for the multi-output
regression experiment (Section 5.2).

Table 14: Results for conformal quantile finetuning on real-data regression tasks at level 1 _−_ _α = 90%. For_
each method we report the (test) coverage, length, and pinball loss of the corresponding base quantile predictor.
All results are averaged over 100 random seeds.

CQR QR + CP-Gen-Recal (ours)

Dataset Coverage(%) Length _L[test]pinball_ Coverage(%) Length _L[test]pinball_


MEPS 19 89.92 ±1.16 1.147 ±0.057 0.107 ±0.013 89.95 ±0.012 **0.895 ± 0.126** 0.130 ±0.015
MEPS 20 89.90 ±0.98 1.164 ±0.054 0.109 ±0.012 89.97 ±0.011 **0.872 ± 0.113** 0.131 ±0.015
MEPS 21 90.00 ±0.94 1.162 ±0.056 0.104 ±0.011 90.08 ±0.011 **0.910 ± 0.133** 0.126 ±0.013
Facebook 1 90.12 ±0.71 0.540 ±0.040 0.050 ±0.007 90.07 ±0.007 **0.382 ± 0.051** 0.089 ±0.009
Facebook 2 90.04 ±0.50 0.497 ±0.028 0.044 ±0.005 90.06 ±0.005 **0.389 ± 0.075** 0.091 ±0.007
kin8nm 90.34 ±1.37 1.238 ±0.067 0.076 ±0.004 90.31 ±0.013 **1.216 ± 0.068** 0.080 ±0.004
naval 89.99 ±1.13 3.101 ±0.015 0.164 ±0.001 89.95 ±0.011 **3.095 ± 0.028** 0.167 ±0.001
bio 90.00 ±0.69 2.261 ±0.033 0.130 ±0.002 89.97 ±0.005 **2.154 ± 0.031** 0.148 ±0.003
blog data 89.99 ±0.60 0.593 ±0.033 0.058 ±0.005 89.95 ±0.007 **0.460 ± 0.075** 0.104 ±0.006

Nominal (1 − _α)_ 90.00 -  -  90.00 -  - 


-----

Table 15: Results for multi-output regression (mean) on next-state prediction tasks, at level 1 − _α = 90%._
For each method we report the (test) coverage and volume of its learned box-shaped prediction set. The reported
volume is the “halfened” version _i=1_ _[u][i][. All results are averaged over 100 random seeds.]_

Coord-wise Coord-wise-Recal CP-Gen-Recal (ours)

[Q][d][out]

Dataset Coverage(%) Volume Coverage(%) Volume Coverage(%) Volume

Cartpole 94.30 1.14 × 10[−][5] 90.02 4.80 × 10[−][6] 90.03 **2.05 × 10[−][6]**

Half-Cheetah 93.84 1.05 × 10[−][5] 90.00 1.22 × 10[−][6] 90.02 **9.01 × 10[−][7]**

Ant 93.53 3.26 × 10[−][3] 89.94 1.75 × 10[−][4] 89.98 **9.22 × 10[−][5]**

Walker 94.52 2.82 × 10[−][5] 89.99 7.48 × 10[−][7] 90.00 **3.74 × 10[−][7]**

Swimmer 95.65 2.82 × 10[−][5] 90.02 2.18 × 10[−][6] 90.01 **1.24 × 10[−][7]**

Hopper 92.95 2.53 × 10[−][9] 89.98 8.86 × 10[−][10] 89.99 **7.04 × 10[−][10]**

Humanoid 94.87 7.43 × 10[−][4] 90.06 1.69 × 10[−][7] 90.03 **8.95 × 10[−][8]**

Nominal (1 − _α)_ 90.00 -  90.00 -  90.00 - 


Table 16: Results for multi-output regression (standard deviation) on next-state prediction tasks, at level
1 − _α = 90%. For each method we report the (test) coverage and volume of its learned box-shaped prediction_
set. The reported volume is the “halfened” version _i=1_ _[u][i][. All standard deviations are computed over 100]_
random seeds.

Coord-wise Coord-wise-Recal[Q][d][out] CP-Gen-Recal (ours)

Dataset Coverage(%) Volume Coverage(%) Volume Coverage(%) Volume


Cartpole 0.35 3.57 × 10[−][6] 0.31 1.33 × 10[−][6] 0.27 4.46 × 10[−][7]

Half-Cheetah 0.25 2.46 × 10[−][6] 0.32 2.75 × 10[−][7] 0.31 2.10 × 10[−][7]

Ant 0.27 1.50 × 10[−][3] 0.29 8.06 × 10[−][5] 0.29 4.15 × 10[−][5]

Walker 0.24 8.81 × 10[−][6] 0.30 2.18 × 10[−][7] 0.29 1.12 × 10[−][7]

Swimmer 0.24 6.47 × 10[−][6] 0.29 5.83 × 10[−][7] 0.33 3.36 × 10[−][8]

Hopper 0.29 6.83 × 10[−][10] 0.29 2.26 × 10[−][10] 0.33 1.97 × 10[−][10]

Humanoid 0.23 1.22 × 10[−][3] 0.30 2.71 × 10[−][7] 0.29 1.47 × 10[−][7]


-----

