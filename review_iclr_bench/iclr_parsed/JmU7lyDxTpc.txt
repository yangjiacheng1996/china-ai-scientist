# MULTI-SCALE FEATURE LEARNING DYNAMICS: INSIGHTS FOR DOUBLE DESCENT

**Anonymous authors**
Paper under double-blind review

ABSTRACT

A key challenge in building theoretical foundations for deep learning is the
complex optimization dynamics of neural networks, resulting from the highdimensional interactions between the large number of network parameters. Such
non-trivial dynamics lead to intriguing model behaviors such as the phenomenon
of “double descent” of the generalization error. The more commonly studied aspect of this phenomenon corresponds to model-wise double descent where the
test error exhibits a second descent with increasing model complexity, beyond the
classical U-shaped error curve. In this work, we investigate the origins of the
less studied epoch-wise double descent in which the test error undergoes two nonmonotonous transitions, or descents as the training time increases. By leveraging
tools from statistical physics, we study a linear teacher-student setup exhibiting
epoch-wise double descent similar to that in deep neural networks. In this setting,
we derive closed-form analytical expressions for the evolution of generalization
error over training. We find that double descent can be attributed to distinct features being learned at different scales: as fast-learning features overfit, slowerlearning features start to fit, resulting in a second descent in test error. We validate
our findings through numerical experiments where our theory accurately predicts
empirical findings and remains consistent with observations in deep neural networks.

1 INTRODUCTION

Classical wisdom in statistical learning theory predicts a trade-off between the generalization ability
of a machine learning model and its complexity, with highly complex models less likely to generalize well (Friedman et al., 2001). If the number of parameters measures complexity, deep learning
models sometimes go against this prediction (Zhang et al., 2016): deep neural networks trained by
stochastic gradient descent exhibit a so-called double descent behavior (Belkin et al., 2019b) with
increasing model parameters. Specifically, with increasing complexity, the generalization error first
obeys the classical U-shaped curve consistent with statistical learning theory. However, a second
regime emerges as the number of parameters is further increased past a transition threshold where
generalization error drops again, hence the “double descent” or more accurately model-wise double
_descent (Nakkiran et al., 2019)._

Nakkiran et al. (2019) showed that the phenomenon of double descent is not limited to varying
model size but is also observed as a function of training time or epochs. In this case as well, the
so-called epoch-wise double descent is in apparent contradiction with the classical understanding
of over-fitting (Vapnik, 1998), where one expects that longer training of a sufficiently large model
beyond a certain threshold should result in over-fitting. This has important implications for practitioners and raises questions about one of the most widely used regularization method in deep
learning (Goodfellow et al., 2016): early stopping. Indeed, while one might expect early stopping
to prevent over-fitting, it might in fact prevent models from being trained at their fullest potential.

Since the 1990s, there has been much interest in understanding the origins of non-trivial generalization behaviors of neural networks (Opper, 1995; Opper & Kinzel, 1996). The authors of Krogh
& Hertz (1992b) were among the first to provide theoretical explanations for (model-wise) double
descent in linear models. Summarily, at intermediate levels of complexity, where the model size is
equal to the number of training examples, the model is very sensitive to noise in training data and
hence, generalizes poorly. This sensitivity to noise reduces if the model complexity is either de

-----

Isotropic

latent features

_z 2 R[d]_

_Wˆ_ _2 R[d]_

_Wˆ_


_y := z[T]_ _W + ✏_


Anisotropic

input features

_x 2 R[d]_

_Wˆ_ _2 R[d]_

_yˆ := x[T]_ _W_


_x := F_ _[T]_ _z_


Noisy teacher Student

Training time


Figure 1: Left: The teacher is the data generating process that operates on isotropic Gaussian inputs
**_z. The student is trained on a dataset generated by the teacher,_** = _xi, yi_ _i=1_ [where][ x][ :=][ F][ T][ z]
_D_ _{_ _}[n]_
follow an anisotropic Gaussian distribution such that the directions with larger/smaller variance are
learned faster/slower. The condition number of F determines how much faster some features are
learned than the others. One can think of z as the latent factors of variation on which the teacher
operates, while x can be thought as the pixels that the student learns from. Right: The generalization
error as the training time proceeds. (top): The case where only the fast-learning feature or slowlearning feature are trained. (bottom): The case where both features. Features that are learned on
a faster time-scale are responsible for the classical U-shaped generalization curve, while the second
descent can be attributed to the features that are learned at a slower time-scale.

creased or increased. More recently, the double descent phenomena has been also studied for more
complex models such as two-layer neural networks and random feature models (Ba et al., 2019; Mei
& Montanari, 2019; D’Ascoli et al., 2020; Gerace et al., 2020).

The majority of previous work in this direction focuses on understanding the asymptotic behavior
of model performance, i.e., where training time t →∞. In recent years, there has been an interest
in studying the non-asymptotic (finite training time) performance, suggesting that several intriguing
properties of neural networks can be attributed to different features being learned at different scales.
Among the limited work studying the particular epoch-wise double descent, Nakkiran et al. (2019)
introduces the notion of effective model complexity and hypothesizes that it increases with training
time and hence unifies both model-wise and epoch-wise double descent. Through a combination of
theory and empirical results, Heckel & Yilmaz (2020) find that the dynamics of evolution of single
and two layer networks under gradient descent, can be perceived to be the superposition of two
bias/variance curves with different minima times, thus leading to non-monotonic test error curves.

In this work, we build on B¨os et al. (1993); B¨os (1998); Advani & Saxe (2017); Mei & Montanari

(2019) which analyze model-wise double descent through the lens of linear models, to probe the
origins of epoch-wise double descent. Particularly,

_• We introduce a linear teacher-student model which, despite its simplicity, exhibits some of_
intriguing properties of generalization dynamics in deep neural networks. (Section 2.1)

_• In the limit of high dimensions, we leverage the replica method developed in statistical_
physics to derive closed-form expressions for the generalization dynamics of our teacherstudent setup, as a function of training time and regularization strength. (Section 2.2)

_• Consistent with recent findings, we provide an explanation for the existence of epoch-wise_
double descent through the lens of multi-scale feature learning. (Figure 1)

_• We perform simulation experiments to validate our analytical predictions. We also conduct_
experiments with deep networks, showing that our teacher-student setup exhibits generalization behavior which is qualitatively similar to that of deep networks. (Figure 2)


-----

2 ANALYTICAL RESULTS

Stochastic Gradient Descent (SGD) — the de facto optimization algorithm for neural networks —
exhibits complex dynamics arising from a large number of parameters (Kunin et al., 2020). However, it is possible to describe some aspects of the high-dimensional microscopic dynamics of neural
networks in terms of low-dimensional understandable macroscopic entities. In a series of seminal
papers by Gardner (Gardner, 1988; Gardner & Derrida, 1988; 1989), the replica method of statistical physics was adopted to derive expressions describing the generalization behavior of large linear
models trained using SGD. In this paper, we employ Gardner’s analysis to build upon an established
line of work studying linear and generalized linear models (Seung et al., 1992; Kabashima et al.,
2009; Krzakala et al., 2012). While most of previous work study the asymptotic (t →∞) generalization behavior, we adapt these methods to study transient learning dynamics of generalization
for finite training time. In the following, we first introduce a teacher-student model that exhibits
interesting characteristics of modern neural networks. We then adapt the replica method to study the
generalization performance as a function of training time and amount of regularization.

2.1 A TEACHER-STUDENT SETUP

**Teacher:** We study a supervised linear regression problem in which the training labels y, are
generated by a noisy linear model (Figure 1),

_y := y[∗]_ + ϵ, _y[∗]_ := z[T] _W,_ _zi_ (0, [1] ), (1)
_∼N_ _√d_

where z ∈ R[d] is the teacher’s input and y[∗], y ∈ R are the teacher’s noiseless and noisy outputs,
respectively. W ∈ R[d] represents the (fixed) weights of the teacher and ϵ ∈ R is the noise. Both
_W and ϵ are drawn i.i.d. from Gaussian distributions with zero means and variances of 1 and σϵ[2][,]_
respectively.
**Student:** A student model is correspondingly chosen to be a similar shallow network with trainable weights _W[ˆ]_ _∈_ R[d]. The student model is trained on n training pairs {(x[µ], y[µ])}µ[n]=1[, with the]
labels y[µ] being generated by the above teacher network, as,

_yˆ := x[T][ ˆ]W,_ _s.t._ **_x := F_** _[T]_ **_z,_** (2)

where the matrix F ∈ R[d][×][d] is a predefined and fixed modulation matrix regulating the student’s
access to the true input z. One can think of z as the latent factors of variation on which the teacher
operates, while x can be thought as the pixels that the student learns from.
**Learning paradigm:** To train our student network, we use stochastic gradient descent (SGD) on
the regularized mean squared loss, evaluated on the n training examples as,


:= [1]
_LT_ 2n


_n_

(y[µ] _−_ _yˆ[µ])[2]_ + _[λ]2_ _W_ _||2[2]_ (3)
_µ=1_

_[||][ ˆ]_

X


where λ ∈ [0, ∞) is the regularization coefficient. Optimizing Eq. 3 with stochastic gradient descent
(SGD) yields the typical update rule,
_Wˆ_ _t ←_ _W[ˆ]_ _t−1 −_ _η∇Wˆ_ _[L][T][ +][ ξ,]_ (4)

in which t denotes the training step and η is the learning rate. Additionally, ξ ∼N (0, σξ[2][)][ models]
the stochasticity noise of the optimization algorithm (Bottou et al., 1991).

**Macroscopic variables:** The quantity of interest in this work, is the expected generalization error
of the student, determined by averaging the student’s error over all possible input-target pairs and
noise realizations, as,

:= [1] (y[∗] _yˆ)[2][]._ (5)
_LG_ 2 [E][z] _−_

As shown in B¨os et al. (1993), if n, d →∞ with a constant ratio _[n]d_ _[<][ ∞][, Eq.][ 5][ can be written as a]_

function of two macroscopic scalar variables R, Q ∈ R,

= [1] (6)
_LG_ 2 [(1 +][ Q][ −] [2][R][)][,]


-----

where σϵ[2] [is the variance of the teacher’s output noise and,]

_R := [1]_ _W,_ _Q := [1]_

_d_ _[W][ T][ F][ ˆ]_ _d_


_Wˆ_ _[T]_ _F_ _[T]_ _F_ _W,[ˆ]_ (7)


See App. B.1 for the proof.

Both R and Q have clear interpretations; R is the dot-product between the teacher’s weights W
and the student’s modulated weights F _W[ˆ]_, hence can be interpreted as the alignment between the
**teacher and the student. Similarly, Q can be interpreted as the student’s modulated norm. The**
negative sign of R in Eq. 6 suggests that the larger R is, the smaller the generalization error gets.
At the same time, Q appears with a positive sign suggesting the students with smaller (modulated)
norm generalize better.

As a remark, note that both R and Q are functions of _W[ˆ]_ which itself is a function of training iteration
_t and the regularization strength λ. Therefore, hereafter, we denote the above quantities as_ (t, λ),
_LG_
_R(t, λ), and Q(t, λ)._

2.2 MAIN RESULTS

In this Section, we present our main analytical results, with Section 2.3 containing a sketch of our
derivations. For brevity of the results, here, we only present the results for σϵ[2] [=][ λ][ = 0][. See App.][ B]

for the general case and the detailed proofs.

**General matrix F .** Let Z := [z[µ]][n]µ=1 _µ=1_
matrices for the teacher and student such that[∈] [R] X[n][×] :=[d][ and] ZF[ X]. For a general modulation matrix[ := [][x][µ][]][n] _[∈]_ [R][n][×][d][ denote the input] F, the
input covariance matrix has the following singular value decomposition (SVD),

_X_ _[T]_ _X = F_ _[T]_ _Z_ _[T]_ _ZF = V ΛV_ _[T]_ _,_ (8)

in which the diagonal matrix Λ contains the eigenvalues of the student’s input covariance matrix.
Solving the dynamics of gradient descent as in Eq. 4, we arrive at the following exact analytical
expressions for R(t) and Q(t),

_R(t) = [1]_ where, _D :=_ _I_ [I _ηΛ][t][],_ (9)

_d_ **[Tr][(][D][)]** _−_ _−_
 

_Q(t) = d[1]_ **[Tr]** _A[T]_ _A_ where, _A := FV DV_ _[T]_ _F_ _[−][1],_ (10)

  


in which Tr(.) is the trace operator. See App. B.2 the proof.

_Remark: The solution in Eqs. 9 and 10 are exact, however, they require the empirical computation of_
the eigenvalues Λ. Below, we treat a special case of the dynamics that allow us to derive approximate
solutions that do not explicitly depend on Λ.

**Special case: Fast and slow features.** We now study a case where the modulation matrix F has
a specific structure described in Assumption 1.
**Assumption 1. The modulation matrix, F** _, under a SVD, F := U_ ΣV _[T]_ _has two sets of singular_
_values such that the first p singular values are equal to σ1 and the remaining d −_ _p singular values_
_are equal to σ2. We let the condition number of F to be denoted by κ :=_ _[σ]σ[1]2_ _[>][ 1][.]_

By employing the replica method of statistical physics (Gardner, 1988; Gardner & Derrida, 1988),
we now derive approximate expressions for R(t) and Q(t). To begin with, we first define the following auxiliary variables,


_α1 :=_ _[n]p [, α][2][ :=]_


_n_

_λ˜1 :=_ _[d]_
_d −_ _p_ _[,]_ _p_


1

_λ2 :=_
_ησ1[2][t,][ ˜]_


1

(11)
_ησ2[2][t,]_


_d −_ _p_


and also let,


2λ[˜]i
_ai = 1 +_ _,_ for _i ∈{1, 2}._ (12)

(1 _αi_ _λi) +_ (1 _αi_ _λi)[2]_ + 4λ[˜]i
_−_ _−_ [˜] _−_ _−_ [˜]
q


-----

The closed-from scalar expression for R(t) is then given by,


_R(t) = R1 + R2,_ where, _R1 :=_


_n_

and, _R2 :=_
_a1d_ _[,]_


_n_

(13)
_a2d_


For Q(t), we accordingly define two more auxiliary variables,

_αi_ 2 _ai_
_bi =_ _,_ _ci = 1_ 2Ri _−_ for _i_ 1, 2 _,_ (14)

_a[2]i_ _−_ _−_ _[n]d_ _ai_ _∈{_ _}_

_[−]_ _[α][i]_

with which the closed-from scalar expression for Q(t) reads,


_Q(t) = Q1 + Q2,_ where, _Q1 :=_ _[b][1][b][2][c][2][ +][ b][1][c][1]_ _,_ and, _Q2 :=_ _[b][1][b][2][c][1][ +][ b][2][c][2]_ _._ (15)

1 _b1b2_ 1 _b1b2_
_−_ _−_


By plugging Eqs. 13 and 15 into Eq. 6, one obtains a closed-form expression for (t) as a function
_LG_
of the training time. See App. B.3 for the proof.

˜( W, t[ˆ] ) _t_

|Eq. 1 y mu|1 ind ltiplie|icate L˜( Wˆ d by|s that, t) t. T|the s SG hat im|ingul D step plies|ar va s that|
|---|---|---|---|---|---|---|
|of ea|ch fe|ature|is sc|aletdth s arg|bteyp otnh min ˜|eL ( m Wˆ a) ( Wˆ, t)|
|ondi|ng sin|gula|r valu|e. A L˜(|s anL Wˆ, t)|illust|
|the ri|ght s for a|hows case|the whe|evolu re|tion|of R Wˆ)1,L(|
|+ R2 impl|ying|a con|dition|p num|= d/ ber o|2 σ 1 f κ =|


|Col1|R = R|+ R 1 2|Col4|Col5|Col6|
|---|---|---|---|---|---|
||R 1 R 2|||||
|||||||
|||||||
|||||||

_L_

2.3 SKETCH OF DERIVATIONS

In this Section, we sketch the key steps in the derivation of our main results. For the sake of simplicity, here we only treat the case whereL˜( W, t[ˆ] ) SGD steps σϵ = λ = 0. The general case with detailed proofs are

**Exact dynamics of SGD.** Recall the gradient descent update rule in Eq.L˜( W, t[ˆ] )L 4. For the linear model

|ction,|we s|ketch|the|key st|eps i|n the|
|---|---|---|---|---|---|---|
|||˜ ˆ|||||
|re we in Ap|only p B.|treL(a Wt|, tt)he|caseS G th|wDh seterpe|s σ ϵ ˆ|
|||||t s arg|tep on min ˜ L|L( W) ( Wˆ, t)|
|amic|s of|SGD.|Re|callL ˜t(|Whˆe, t )gr|adien|
|Eqs.|1-2, l|earni|ng is|gover L|n( Wˆe)d|by the|



_L˜( W, t[ˆ]Wˆ)t = W[ˆ]_ _t−1 −_ _η∇Wˆ_ _t−1_ _[L][T][,]_ (16)

plicity, here we only treat the case whereL˜( W, t[ˆ] ) SGD steps σϵ =
B. t[th] step on ( W[ˆ] )

_L_

argmin ˜( W, t[ˆ] )

**Exact dynamics of SGD.** _L˜( W, t[ˆ]_ )L

( W[ˆ] )

1-2 _L_

= W[ˆ] _t−1 −_ _η_ _−_ _X_ _[T]_ (y − _XW[ˆ]_ _t−1)_ _._ (17)
 


With the assumption that _W[ˆ]_ _t=0 = 0, the dynamics admit the following exact closed-form solution,_

_t[]_
_Wˆ_ _t =_ _I −_ _I −_ _ηX_ _[T]_ _X_ (X _[T]_ _X)[−][1]X_ _[T]_ _y := W[˜]_ (t). (18)
  

With a SVD on X _[T]_ _X, Eqs. 9-10 can then be obtained by substituting_ _W[ˆ]_ _t in Eqs. 7. As a remark,_
note that one can recover the results of Advani & Saxe (2017) by setting F = I. In that case, the
eigenvalues of X _[T]_ _X follow a Marchenko–Pastur distribution (Marchenko & Pastur, 1967)._

**Induced probability density of SGD.** It is well-known (Kuhn & Bos, 1993; Solla, 1995) that
probability distribution of weight configurations for network weights _W[ˆ]_ trained via SGD on a loss
_L( W[ˆ]_ ), tend to the Gibbs distribution such that,

_P_ ( W[ˆ] ) = [1] _e[−][β][L][( ˆ]W ),_ (19)

_Zβ_

in which Zβ is the partition function d W[ˆ] exp(−βL( W[ˆ] )) and β is called the inverse temper_ature and is inversely proportional the stochastic noise of SGD,R_  _ξ, defined in Eq. 4. Intuitively,_
for small β, the distribution of P ( W[ˆ] ) is almost uniform, while as β →∞, P ( W[ˆ] ) becomes more
concentrated around the minimum of the training loss.

It is important to highlight that Eq. 19 describes the equilibrium distribution of the student network’s
weights, i.e., at the end of training (t →∞). However, we are interested in studying the trajectory


-----

of student’s weights during the course of training, i.e., for finite t. To that end, we derive the time**dependent probability density over** _W[ˆ]_,



1 _L( W,t[ˆ]_ ), where, t[th] step on ( (20)W[ˆ] )

_Zβ, t_ _[e][−][β][ ˜]_ _L_


_P_ ( W, t[ˆ] ) =


˜T ( W, t[ˆ] ) : = [1] (ˆy[µ] _y˜[µ](t))[2]_ + _[λ]_ _W_ 2[,] _L˜( W, t[ˆ]_ )L (21)
_L_ 2n _−_ 2 _||[2]_ _L( W[ˆ]_ )

_[||][ ˆ]_

X

2n X _−_ 2 _[||][ ˆ]_ _||[2]_ _L˜( W, t[ˆ]_ )

|Col1|Col2|L˜( Wˆ|, t)|SG|D step|s|
|---|---|---|---|---|---|---|
|||||tth s|tep on|L(( W2ˆ0 )|
|||||arg L˜(|min ˜ L Wˆ, t)|( Wˆ, t)|
|||||||(21 L( Wˆ)|
|2, 2|( W˜|(t) de|fined|in Eq|. 18)|(22|


(20)W[ˆ] )

)

(21)

( W[ˆ] )

(22)

_T ( W[ˆ]_ ) + [1] _λ + [1]_ _W[ˆ]_ 2[.] (23)
_≈L_ 2 _ηt_ _||_ _||[2]_

  

|Col1|R = R R|
|---|---|
||1 R 2|
|||
|||
|||


_Remark:_ [˜]T ( W, t[ˆ] ) is a modified loss such that its min- _L˜( W, t[ˆ]_ ) SGD steps
_L_
imum (equilibrium distribution) is achieved at the t[th] iterate of gradient descent on ( W[ˆ] ). The schematic dia- argmin ˜( W, t[ˆ] )
gram on the right illustrates this equivalence, such that, L ˜( W, t[ˆ] )L
arg minW ˆ _L[˜]T ( W, t[ˆ]_ ) = W[ˆ] _t, where_ _W[ˆ]_ _t is the defined in_ _L( W[ˆ]_ )
Eq. 4.

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|||˜ ˆ|||||
|||L( W|, t)|SG th|D step|s ˆ|
|||||t s arg|tep on min ˜ L|L( W) ( Wˆ, t)|
|||||L˜(|Wˆ, t)||
|||||L|( Wˆ)||


_L˜( W, t[ˆ]_ )

_L˜( W, t[ˆ]_ ) SGD steps

t[th] step on _L( W[ˆ]_ )

argmin ˜( W, t[ˆ] )

_L˜( W, t[ˆ]_ )L

_L( W[ˆ]_ )

**The typical generalization error.** To determine the typical generalization performance at time t,
one proceeds by first computing the free-energy of the system as,


_f :=_ ln Zβ,t _._ (24)
_−_ _βd[1]_ [E][W,z] 

Free-energy is a self-averaging property where its typical/most probable value coincides with its av_erage over proper probability distributions Engel & Van den Broeck (2001). Therefore, to determine_
the typical values of R and Q, we extremize the free-energy w.r.t. those variables.

Due to the logarithm inside the expectation, analytical computation of Eq. 24 is intractable. However, the replica method (M´ezard et al., 1987) allows us to tackle this through the following identity,


EW,z[Zβ,t[r] []][ −] [1]
EW,z[ln Zβ,t] = lim
_r→0_ _r_


(25)


Computation of the free-energy via replica method and its subsequent extremization w.r.t R and Q,
we arrive at Eqs. 13 and 15. See App. B.3 for more details.

To summarize, using the replica method, we are able to cast the high-dimensional dynamics of SGD
into simple scalar equations governing R and Q and, consequently, the generalization error _G._
_L_
While our analysis is limited to the specific teacher and student setup, this simple model already
exhibits dynamics qualitatively similar to those observed in more complex networks, as we now
illustrate.

3 EXPERIMENTAL RESULTS

In this Section, we conduct numerical simulations to validate our analytical results and provide
clear insights on the macroscopic dynamics of generalization. We also conduct experiments on realworld neural networks showing a close qualitative match between the generalization behavior of
neural networks and our teacher-student setup.

**For real-world experiments, we train a ResNet18 (He et al., 2016) with large layer widths [64, 2** _×_
64, 4 × 64, 8 × 64]. We follow the training setup of Nakkiran et al. (2019); Label noise with a
probability 0.15 randomly assign an incorrect label to training examples. Noise is sampled only
once before the training starts. We train using Adam (Kingma & Ba, 2014) with learning rate of
1e − 4 for 1K epochs. Real-world experiments are averaged over 50 random seeds. To ensure
[reproducibility, we include the complete source code in a GitHub repository as well as an](https://github.com/NNdoubledescent/doubledescent)
[anonymous Collab notebook.](https://colab.research.google.com/drive/10UHRBnIa2V8uwBWXd5W_-ZhKKSh2OPy7?usp=sharing)


-----

Figure 2: A qualitative comparison between a ResNet-18 and our analytical results. (a): Heatmap of empirical generalization error (0-1 classification error) for the ResNet-18 trained on CIFAR10 with 15% label noise. X-axis denotes the inverse of weight-decay regularization strength and Yaxis represents the training time. (c): Heat-map of the analytical generalization error (mean squared
error) for the linear teacher-student setup with κ = 100, the condition number of the modulation
matrix. (b, d): Three slices of the heat-maps for large, intermediate, and small amounts of regularization. Analysis: As predicted by Eqs. 13 and 15, κ = 100 implies that a subset of features
are learned 100 times faster that the rest. Intuitively, large amounts of regularization allow for the
fast-learning features to be learned by not to overfit. Intermediate levels of regularization result in a
classical U-shaped generalization curve but prevent slow features from learning. Small amounts of
regularization allow for both fast and slow features to be learned, leading to double descent.

3.1 MATCH BETWEEN THEORY AND REAL-WORLD EXPERIMENTS

We conduct an experiment on the classification task of CIFAR-10 (Krizhevsky et al., 2009) with
varying amount of weight decay regularization strength λ. We monitor the generalization error (01 test error) during the course of training and visualize a heat-map of the generalization error for
different λ’s in Figure 2 (a).

We also conduct a similar experiment with the teacher-student setup presented in Section 2.1. We
visualize a heat-map of the generalization error which is the mean squared error (MSE) over test
distribution in Figure 2 (b). Particularly, we plot Eqs. 13 and 15 with a constant κ = 100. As a
remark, we note that a κ = 100 implies that a subset of features are learned 100 times faster than
other features.

It is observed that in both experiments, a model with intermediate levels of regularization displays
a typical overfitting behavior where the generalization error decreases first and then overfits. This is
consistent with Eq. 87 which indicates larger amounts of regularization prevent slow feature from
being learned as λ and the inverse of t are summed. In other words, learning of slow features requires
large weights, something that is penalized by the weight-decay. On the other hand, a model with
smaller amount of regularization exhibits the double descent generalization curve.

We also validate our derived analytical expressions by running numerical simulations which are
presented in Figure 4.


-----

3.2 THE PHASE DIAGRAM

To further investigate the transition between the two phases of classical single descent and double
_descent, we explore the phase diagram. Recall that with Eq. 6, one can fully characterize the evo-_
lution of the generalization dynamics in terms of two scalar variables instead of the d-dimensional
parameter space. R and Q presented in Eq. 7 are macroscopic variables where R represents the
**alignment between the teacher and the student and Q is the student’s (modulated) norm. Hence,**
a better generalization performance is achieved with larger R and smaller Q.

_R and Q are not free parameters and both depend on the training dynamics through Eqs. 13 and 15._
Nevertheless, it is instructive to visualize the generalization error for all pairs of (R, Q). In Figure
3, we visualize the RQ-plane for (R, Q) ∈ [0.0, 0.8] × Q ∈ [0.0, 1.6]. At the time of initialization,
(R, Q) = (0, 0) as the models are initialized at the origin. As training time proceeds, values of
_R and Q follow the depicted trajectories. In Figure 3, different trajectories correspond to different_
values of κ, the condition number of the modulation matrix F in Eq. 2. It is important to note that
_the closer a trajectory is to the lower-right, the better the generalization error gets._

The yellow curve which corresponds to the case with large κ = 1e5 meaning that a subset of features
are extremely slower than the others that practically do not get learned. In that case, generalization
error exhibits traditional over-fitting due to over-training. On the phase diagram, the yellow trajectory starts at (0, 0) and moves towards Point A which has the lowest generalization error of this
curve. Then as the training continues, Q increases and as t →∞ the trajectory lands at Point B
which has the worse generalization error. The curves in orange, green and blue correspond to trajectories with κ = 1e3, κ = 1e2, κ = 1e1, respectively. They follow the case of κ = 1e5 up to
the vicinity of Point B, but then the trajectories slowly incline towards another fixed point, Point C
signalling a second descent in the generalization error.

The phase diagram along with the corresponding generalization curves in Figure 2 illustrate that
features that are learned on a faster time-scale are responsible for the initial conventional U-shaped
generalization curve, while the second descent can be attributed to the features that are learned at a
slower time-scale.

Figure 3: Left: Phase diagram of the generalization error as a function of R(t) and Q(t) (Eqs. 13
and 15). The generalization error for all pairs of (R, Q) ∈ [0.0, 0.8] × [0.0, 1.6] is contour-plotted
in the background in shades of beige, with the best generalization performance being attained on
the lower right part of the plot. The trajectories describe the evolution of R(t) and Q(t) as training
proceeds. Each trajectory correspond to a different κ, the condition number of the modulation matrix
**_F in Eq. 2. κ describes the ratio of the rates at which two sets of features are learned. Right: The_**
corresponding generalization curves for different plotted over the training time axis. Analysis: The
trajectory with κ = 1e5 (bright yellow) starts at the origin and advances towards point A (a descent
in generalization error). Then by over-training, it converges to point B (an ascent in generalization
error). For the other trajectories with smaller κ, a first descent in generalization error occurs up to
the point A, then an ascent happens, but they no longer converge to point B. Instead, by further
training, these trajectories converge to point C implying a second descent.


-----

4 RELATED WORK AND DISCUSSION

Although the term double descent has been introduced rather recently (Belkin et al., 2019a), similar
behaviors had already been observed and studied in several decades-old works form a statistical
physics perspective (Krogh & Hertz, 1992a; Opper, 1995; Opper & Kinzel, 1996; B¨os, 1998; Engel
& Van den Broeck, 2001). More recently, these behaviors have been investigated in the context of
modern machine learning, both from an empirical (Nakkiran et al., 2019; Amari et al., 2020; Yang
et al., 2020) and theoretical (Belkin et al., 2019a; Geiger et al., 2019; Advani & Saxe, 2017; Mei &
Montanari, 2019; Gerace et al., 2020; d’Ascoli et al., 2020; Ba et al., 2019; d’Ascoli et al., 2021)
perspectives.

Hastie et al. (2019); Advani et al. (2020); Belkin et al. (2020) use random matrix theory (RMT)
tools to characterize the asymptotic generalization behavior of over-parameterized linear and random
feature models. In an influential work, Mei & Montanari (2019) extend the same analysis to a
random feature model and theoretically derive the model-wise double descent curve for a model
with Tikhonov regularization. Jacot et al. (2020) also study double descent in ridge estimators and
show an equivalence to kernel ridge regression. Pennington & Worah (2019) used RMT to study the
curvature of single-hidden-layer neural network in an attempt to understand the efficacy of first-order
optimization methods in training DNNs. In addition, Liang & Rakhlin (2020) take a similar approach
to investigate implicit regularization in high dimensional ridgeless regression with nonlinear kernels.

While most of the related work study the non-monotonicity of the generalization error as a function
of the model size or sample size, Nakkiran et al. (2019) introduced the epoch-wise double descent.
Epoch-wise double descent refers to the phenomenon where the generalization error undergoes two
descents as the training time increases. There has been limited work on studying of epoch-wise
double descent. Very recently, Heckel & Yilmaz (2020) and Stephenson & Lee (2021) have focused
on finding the roots of this phenomenon.

Heckel & Yilmaz (2020) provides upper bounds on the risk of single and two layer models in a
regression setting where the input data has distinct feature variances. Heckel & Yilmaz (2020)
demonstrate that a superposition of two or more bias-variance tradeoff curves leads to epoch-wise
double descent. The authors also show that different layers of the network are learned at different
epochs. For that reason, epoch-wise double descent can be eliminated by appropriate selection of
learning rates for individual network weights. Consistent with these findings, our work formalizes
this phenomenon in terms of feature learning scales and provides closed-form predictions.

Stephenson & Lee (2021) arrives at similar conclusions. Authors in Stephenson & Lee (2021) take
a random matrix theory approach on a data model that exhibits epoch-wise double descent. The
data model is constructed so that the noise is explicitly added only to the fast-learning features while
slow-learning features remain noise-free. Consequently, the fast-learning features are noisy and
hence show a U-shaped generalization curve while slow-learning features are noiseless.

Our findings and those of Heckel & Yilmaz (2020) and Stephenson & Lee (2021) reinforce one
another with a common central finding that the epoch-wise double descent results from different
features/layers being learned at different time-scale. However, we also highlight that both Heckel
& Yilmaz (2020) and Stephenson & Lee (2021) built upon tool from random matrix theory and
study distinct data models from our teacher-student setup. We study the same phenomenon from a
different perspective. By leveraging the replica method from statistical physics, we characterized
the generalization behavior using a set of informative macroscopic parameters. While supporting
the notion that the interaction of different feature learning speeds causes epoch-wise double descent,
our work provides formal predictions of the dynamics that unfold during training.

We believe our theoretical framework sets the stage for further understanding of generalization dynamics in neural networks beyond the double descent. A future direction to study is a case in which
the first descent is strong enough to bring down the training loss to very small values to the point that
learning slower features is practically impossible or happens after a very large number of epochs.
Power et al. (2021) reports an instance of such behavior called Grokking where the model abruptly
learns to perfectly generalize but long after the training loss has reached very small values.

**Limitations. It should be noted that studying finer details of the dynamics would require a more**
precise model of the neural networks. Clearly, our proposed model is not a universal and unique
way to model the dynamics of the complex, over-parameterized deep neural networks.


-----

**Social Impact. The authors do not foresee a negative social impact specifically arising from this**
rather theoretical work.

REFERENCES

Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017.

Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of generalization error in neural networks. Neural Networks, 132:428–446, 2020.

Alnur Ali, J Zico Kolter, and Ryan J Tibshirani. A continuous-time view of early stopping for least
squares regression. In The 22nd International Conference on Artificial Intelligence and Statistics,
pp. 1370–1378. PMLR, 2019.

Alnur Ali, Edgar Dobriban, and Ryan Tibshirani. The implicit regularization of stochastic gradient
flow for least squares. In International Conference on Machine Learning, pp. 233–244. PMLR,
2020.

Shun-ichi Amari, Jimmy Ba, Roger Grosse, Xuechen Li, Atsushi Nitanda, Taiji Suzuki, Denny
Wu, and Ji Xu. When does preconditioning help or hurt generalization? _arXiv preprint_
_arXiv:2006.10732, 2020._

Jimmy Ba, Murat Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of twolayer neural networks: An asymptotic viewpoint. In International conference on learning repre_sentations, 2019._

Yu Bai and Jason D. Lee. Beyond linearization: On quadratic and higher-order approximation of
wide neural networks, 2020.

Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machinelearning practice and the classical bias–variance trade-off. Proceedings of the National Academy
_of Sciences, 116(32):15849–15854, 2019a._

Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machinelearning practice and the classical bias–variance trade-off. Proceedings of the National Academy
_of Sciences, 116(32):15849–15854, 2019b._

Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM
_Journal on Mathematics of Data Science, 2(4):1167–1180, 2020._

Carl M Bender and Steven A Orszag. Advanced mathematical methods for scientists and engineers
_I: Asymptotic methods and perturbation theory. Springer Science & Business Media, 2013._

S B¨os, W Kinzel, and M Opper. Generalization ability of perceptrons with continuous outputs.
_Physical Review E, 47(2):1384, 1993._

Siegfried B¨os. Statistical mechanics approach to early stopping and weight decay. Physical Review
_E, 58(1):833, 1998._

L´eon Bottou et al. Stochastic gradient learning in neural networks. Proceedings of Neuro-Nımes, 91
(8):12, 1991.

Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi. Multiple descent: Design your own
generalization curve. arXiv preprint arXiv:2008.01036, 2020.

Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. In Conference on Learning Theory, pp. 1305–1338. PMLR, 2020.

St´ephane D’Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double trouble in double descent: Bias and variance(s) in the lazy regime. In Hal Daum´e III and Aarti Singh
(eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of
_Proceedings of Machine Learning Research, pp. 2280–2290. PMLR, 13–18 Jul 2020._ URL

[https://proceedings.mlr.press/v119/d-ascoli20a.html.](https://proceedings.mlr.press/v119/d-ascoli20a.html)


-----

St´ephane d’Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overfitting:
Where & why do they appear? arXiv preprint arXiv:2006.03509, 2020.

St´ephane d’Ascoli, Marylou Gabri´e, Levent Sagun, and Giulio Biroli. On the interplay between
data structure and loss function in classification problems. In Thirty-Fifth Conference on Neural
_Information Processing Systems, 2021._

St´ephane d’Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double trouble in double descent: Bias and variance (s) in the lazy regime. In International Conference on Machine
_Learning, pp. 2280–2290. PMLR, 2020._

Andreas Engel and Christian Van den Broeck. Statistical mechanics of learning. Cambridge University Press, 2001.

Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. _The elements of statistical learning,_
volume 1. Springer series in statistics New York, 2001.

Elizabeth Gardner. The space of interactions in neural network models. Journal of physics A:
_Mathematical and general, 21(1):257, 1988._

Elizabeth Gardner and Bernard Derrida. Optimal storage properties of neural network models.
_Journal of Physics A: Mathematical and general, 21(1):271, 1988._

Elizabeth Gardner and Bernard Derrida. Three unfinished works on the optimal storage capacity of
networks. Journal of Physics A: Mathematical and General, 22(12):1983, 1989.

Mario Geiger, Stefano Spigler, Stephane d’Ascoli, Levent Sagun, Marco Baity-Jesi, Giulio Biroli,
and Matthieu Wyart. Jamming transition as a paradigm to understand the loss landscape of deep
neural networks. Physical Review E, 100(1):012115, 2019.

Stuart Geman, Elie Bienenstock, and Ren´e Doursat. Neural networks and the bias/variance dilemma.
_Neural computation, 4(1):1–58, 1992._

Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc M´ezard, and Lenka Zdeborov´a. Generalisation error in learning with random features and the hidden manifold model. In International
_Conference on Machine Learning, pp. 3452–3462. PMLR, 2020._

Sebastian Goldt, Galen Reeves, Marc M´ezard, Florent Krzakala, and Lenka Zdeborov´a. The gaussian equivalence of generative models for learning with two-layer neural networks. arXiv e-prints,
pp. arXiv–2006, 2020.

Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning. MIT press
Cambridge, 2016.

Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in highdimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016.

Reinhard Heckel and Fatih Furkan Yilmaz. Early stopping in deep networks: Double descent and
how to eliminate it. arXiv preprint arXiv:2007.10099, 2020.

Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: convergence and generalization in neural networks. In Advances in neural information processing systems, pp. 8571–
8580, 2018.

Arthur Jacot, Berfin Simsek, Francesco Spadaro, Cl´ement Hongler, and Franck Gabriel. Implicit
regularization of random feature models. In International Conference on Machine Learning, pp.
4631–4640. PMLR, 2020.

Yoshiyuki Kabashima, Tadashi Wadayama, and Toshiyuki Tanaka. A typical reconstruction limit for
compressed sensing based on lp-norm minimization. Journal of Statistical Mechanics: Theory
_and Experiment, 2009(09):L09003, 2009._


-----

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
_arXiv:1412.6980, 2014._

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.

Anders Krogh and John A Hertz. Generalization in a linear perceptron in the presence of noise.
_Journal of Physics A: Mathematical and General, 25(5):1135, 1992a._

Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances
_in neural information processing systems, pp. 950–957, 1992b._

Florent Krzakala, Marc M´ezard, Franc¸ois Sausset, YF Sun, and Lenka Zdeborov´a. Statisticalphysics-based reconstruction in compressed sensing. Physical Review X, 2(2):021005, 2012.

R Kuhn and S Bos. Statistical mechanics for neural networks with continuous-time dynamics.
_Journal of Physics A: Mathematical and General, 26(4):831, 1993._

Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka.
Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. arXiv
_preprint arXiv:2012.04728, 2020._

Yann Le Cun, Ido Kanter, and Sara A Solla. Eigenvalues of covariance matrices: Application to
neural-network learning. Physical Review Letters, 66(18):2396, 1991.

Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “ridgeless” regression can generalize. The Annals of Statistics, 48(3), Jun 2020. ISSN 0090-5364. doi: 10.1214/19-aos1849.
[URL http://dx.doi.org/10.1214/19-AOS1849.](http://dx.doi.org/10.1214/19-AOS1849)

Vladimir Alexandrovich Marchenko and Leonid Andreevich Pastur. Distribution of eigenvalues for
some sets of random matrices. Matematicheskii Sbornik, 114(4):507–536, 1967.

Song Mei and Andrea Montanari. The generalization error of random features regression: precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.

Marc M´ezard, Giorgio Parisi, and Miguel Virasoro. Spin glass theory and beyond: an introduction
_to the Replica Method and its applications, volume 9. World Scientific Publishing Company,_
1987.

Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.
Deep double descent: where bigger models and more data hurt. _ICLR 2020, arXiv preprint_
_arXiv:1912.02292, 2019._

Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon LacosteJulien, and Ioannis Mitliagkas. A modern take on the bias-variance tradeoff in neural networks.
_arXiv preprint arXiv:1810.08591, 2018._

Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.

Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947–5956,
2017.

Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards
understanding the role of over-parametrization in generalization of neural networks, 2018.

Manfred Opper. Statistical mechanics of learning: Generalization. The handbook of brain theory
_and neural networks, pp. 922–925, 1995._

Manfred Opper and Wolfgang Kinzel. Statistical mechanics of generalization. In Models of neural
_networks III, pp. 151–209. Springer, 1996._

Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. Journal
_of Statistical Mechanics: Theory and Experiment, 2019(12):124005, 2019._


-----

Mohammad Pezeshki, S´ekou-Oumar Kaba, Yoshua Bengio, Aaron C. Courville, Doina Precup,
and Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. CoRR,
[abs/2011.09468, 2020. URL https://arxiv.org/abs/2011.09468.](https://arxiv.org/abs/2011.09468)

Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. In ICLR MATH-AI Workshop, 2021.

Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua
Bengio, and Aaron Courville. On the spectral bias of neural networks. In International Confer_ence on Machine Learning, pp. 5301–5310. PMLR, 2019._

Frederick Reif. Fundamentals of statistical and thermal physics. Waveland Press, 2009.

Hyunjune Sebastian Seung, Haim Sompolinsky, and Naftali Tishby. Statistical mechanics of learning from examples. Physical review A, 45(8):6056, 1992.

Sara A Solla. A bayesian approach to learning in neural networks. International Journal of Neural
_Systems, 6:161–170, 1995._

Cory Stephenson and Tyler Lee. When and how epochwise double descent happens. arXiv preprint
_arXiv:2108.12006, 2021._

Vladimir N. Vapnik. The nature of statistical learning theory. Wiley, New York, 1st edition, September 1998. ISBN 978-0-471-03003-4.

Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance tradeoff for generalization of neural networks. In International Conference on Machine Learning, pp.
10767–10777. PMLR, 2020.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.

Xiao Zhang and Dongrui Wu. Rethink the connections among generalization, memorization and
[the spectral bias of dnns. CoRR, abs/2004.13954, 2020. URL https://arxiv.org/abs/](https://arxiv.org/abs/2004.13954)
[2004.13954.](https://arxiv.org/abs/2004.13954)

A FURTHER RELATED WORK AND DISCUSSION

If we consider plots where the generalization error on the y-axis is plotted against other quantities
on the x-axis, we find earlier works that have identified double descent behavior for quantities such
as the number of parameters, the dimensionality of the data, the number of training samples, or the
training time on the x-axis. In this paper, we studied epoch-wise double descent, i.e. we plot the
training time t, or the number of training epochs, on the x-axis. Literature displaying double descent
phenomena in generalization behavior w.r.t. other quantities do so in the limit of t →∞.

From a random matrix theory perspective, Le Cun et al. (1991); Hastie et al. (2019); Advani et al.
(2020), and Belkin et al. (2020) are among works which have analytically studied the spectral density
of the Hessian matrix. According to their analyses, at intermediate levels of complexity, the presence
of small but non-zero eigenvalues in the Hessian matrix results in high generalization error as the
inverse of the Hessian is calculated for the pseudo-inverse solution.

Neyshabur et al. (2014) demonstrated that over-parameterized networks does not necessarily overfit
thus suggesting the need of a new form of measure of model complexity other than network size.
Subsequently, Neyshabur et al. (2018) suggest a novel complexity measure based on unit-wise capacities which correlates better with the behavior of test error with increasing network size. Chizat
& Bach (2020) study the global convergence and superior generalization behavior of infinitely wide
two-layer neural networks with logistic loss. Goldt et al. (2020) make use of the Gaussian Equivalence Theorem to study the generalization performance of two-layer neural networks and kernel
models trained on data drawn from pre-trained generative models. Bai & Lee (2020) investigated
the gap between the empirical performance of over-parameterized networks and their NTK counterparts, first proposed by Jacot et al. (2018).


-----

From the perspective of bias/variance trade-off, Geman et al. (1992), and more recently, Neal et al.
(2018) empirically observe that while bias is monotonically decreasing, variance could be decreasing
too or unimodal as the number of parameters increases, thus manifesting a double descent generalization curve. Hastie et al. (2019) analytically study the variance. More recently, Yang et al. (2020)
provides a new bias/variance decomposition of bias exhibiting double desc-nt in which the variance
follows a bell-shaped curve. However, the decrease in variance as the model size increases remains
unexplained. For high dimensional regression with random features, d’Ascoli et al. (2020) provides
an asymptotic expression for the bias/variance decomposition and identifies three sources of variance with non-monotonous behavior as the model size or dataset size varies. d’Ascoli et al. (2020)
also employs the analysis of random feature models and identifies two forms of overfitting which
leads to the so-called sample-wise triple descent. More recently, Chen et al. (2020) show that as a
result of the interaction between the data and the model, one may design generalization curves with
multiple descents.

From a statistical physics perspective, Opper (1995); B¨os et al. (1993); B¨os (1998); Opper & Kinzel

(1996) are among the first studies which theoretically observe sample-wise double-descent in a ridge
regression setup where the solution is obtained by the pseudo-inverse method. Most of these studies
employ the “Gardner analysis” (Gardner, 1988; Gardner & Derrida, 1988; 1989) for models where
the number of parameters and the dimensionality of data are coupled and hence the observed form of
double descent is different from that observed in deep neural networks. A beautiful extended review
of this line of work is provided in Engel & Van den Broeck (2001). Among recent works, Gerace
et al. (2020) also apply the Gardner analysis but to a novel generalized data generating process called
the hidden manifold model and derive the model-wise double-descent equations analytically.

Finally, recall that towards providing an explanation for the epoch-wise double descent, we argue
that the epoch-wise double descent can be attributed to different features being learned at different
_time-scales, resulting in a non-monotonous generalization curve. In relation to the aspect of different_
feature learning scales, Rahaman et al. (2019) had observed that DNNs have a tendency towards
learning simple target functions first that can allow for good generalization behavior of various
data samples. Pezeshki et al. (2020) also identify and provide explanation for a feature learning
imbalance exhibited by over-parameterized networks trained via gradient descent on cross-entropy
loss, with the networks learning only a subset of the full feature spectrum over training. More
recently though, Zhang & Wu (2020), show that certain DNNs models prioritize learning highfrequency components first followed by the learning of slow but informative features, leading to the
second descent of the test error as observed in epoch-wise double descent.

**On the difference between model-wise and epoch-wise double descent curves.** In accordance
with its name, model-wise double descent (in the test error) occurs due to an increase in modelsize (number of its parameters), i.e., as the model transitions from an under-parameterized to an
over-parameterized regime. A variety of works have tried to understand this phenomenon from
the lens of implicit regularization (Neyshabur et al., 2014) or defining novel complexity measures
(Neyshabur et al., 2017). On the other hand, epoch-wise double descent (in the test error) as treated
in our work, is observed to occur for both over-parameterized (Nakkiran et al., 2019) and underparameterized (Heckel & Yilmaz, 2020) setups. As found in our work along with the latter reference,
this phenomenon seems to be a result of different feature learning speeds rather than the extent of
model parameterization. The overlap of the test-error contributions from the different weights with
varying scales of learning henceforth leads to a non-monotonous evolution of the model test error as
exemplified by epoch-wise double descent.

We also note that the peak in model-wise double descent is associated with the model’s capacity to
perfectly interpolate the data, we do not think an analogous notion exists for the case of epoch-wise
double descent. Our understanding of the peak in the latter is that it corresponds to a training time
configuration whereby a subclass of features are already learnt (due to a larger associated signal-tonoise-ratio) and are being overfitted upon to fit the target. As training proceeds further, the remaining
set of features are eventually learnt thus allowing for a lowering of the test error.

**On the implicit regularization of SGD and ridge-regularized loss.** The results presented in Eqs.

20-23 have a core dependence on the findings of Ali et al. (2019; 2020). These works first formalize
the connection between (continuous-time) GD or SGD-based training of an ordinary least squares
(OLS) setup and that of ridge regression, providing bounds on the test error under these algorithms


-----

over training time t, in terms of a ridge setup with ridge parameter λ = 1/t. We utilize these results
in the sense that by evaluating the generalization error LG of our student-teacher setup with explicit
ridge regularization, we invoke the connection between the ridge coefficient λ and training time t as
described in these works, to obtain the behavior of (ridgeless) LG over training. This determination
of an expression of _G(t) is what allows us to study the epoch-wise DD phenomenon._
_L_

B TECHNICAL PROOFS

B.1 THE GENERALIZATION ERROR AS A FUNCTION OF R AND Q (EQ. 6)

Recall that the teacher is the data generator and is defined as,


_y := y[∗]_ + ϵ, _y[∗]_ := z[T] _W,_ _zi_ (0, [1]
_∼N_ _√_


), (26)


where z ∈ R[d] is the teacher’s input and y[∗], y ∈ R are the teacher’s noiseless and noisy outputs,
respectively. W ∈ R[d] represents the (fixed) weights of the teacher and ϵ ∈ R is the noise.

And student is defined as,

_yˆ := x[T][ ˆ]W,_ _s.t._ **_x := F_** _[T]_ **_z,_** (27)

where the matrix F ∈ R[d][×][d] is a predefined and fixed modulation matrix regulating the student’s
access to the true input z.

The average generalization error of the student, determined by averaging the student’s error over all
possible input-target pairs and noise realizations is given by,

:= [1] (y[∗] _yˆ)[2][],_ (28)
_LG_ 2 [E][x][,W] _−_

in which the variables (y[∗], ˆy) form a bi-variate Gaussian distribution with zero mean and a covari-
ance of,


Σ = _< y[∗], y[∗]_ _>z_ _< y[∗], ˆy >z_
_< y[∗], ˆy >z_ _< ˆy, ˆy >z_



(29)


in which,


_R : = Ez[y[∗][T]_ _yˆ] = Ez[W_ _[T]_ _zz[T]_ _F_ _W[ˆ]_ ] = [1] _W,_ and, (30)

_d_ _[W][ T][ F][ ˆ]_

_Q : = Ez[ˆy[T]_ _yˆ] = Ez[ W[ˆ]_ _[T]_ _F_ _[T]_ _zz[T]_ _F_ _W[ˆ]_ ] = [1] _Wˆ_ _[T]_ _F_ _[T]_ _F_ _W.[ˆ]_ (31)

_d_

Eq. 29 implies a correlation between y[∗] and ˆy obstructing the calculation of the average in Eq. 28.
Following (B¨os, 1998; Krogh & Hertz, 1992a), we define decoupled variables ˜y[∗] and _y[˜]ˆ as follows,_


_Q −_ _R[2]y.[˜]ˆ_ (32)


_y[∗]_ =: ˜y[∗], and _yˆ =: Ry˜[∗]_ +


The variables ˜y[∗] and _y[˜]ˆ are independent Gaussian variables such that < ˜y[∗],_ _y >[˜]ˆ_ _z= 0. Therefore,_
two expectations can be applied independently,

: = [1] (y[∗] _yˆ)[2][],_ (33)
_LG_ 2 [E][x][,W] _−_



= [1]2 [E]y[˜][∗],y[˜]ˆ (˜y[∗] _−_ (Ry˜[∗] + _Q −_ _R[2]y[˜]ˆ))[2][],_ (34)

 p

= [1] (35)

2 [(1 +][ Q][ −] [2][R][)][.]


Finally, we note that expectation w.r.t. a Gaussian variable x is defined as,


+∞
Ex[f (x)] :=
Z−∞


_dx_
_√2π_ [exp] _−_ _[x]2[2]_

 


_f_ (x). (36)


-----

B.2 THE GENERAL CASE EXACT DYNAMICS (EQS. 9-10)

Recall that to train our student network, we use stochastic gradient descent (SGD) on the regularized
mean squared loss, evaluated on the n training examples as,


:= [1]
_LT_ 2n


_n_

(y[µ] _yˆ[µ])[2]_ + _[λ]_ _W_ 2[,] (37)
_−_ 2 _||[2]_
_µ=1_

_[||][ ˆ]_

X


where λ ∈ [0, ∞) is the regularization coefficient.

The minimum of the loss function, denoted by W, is achieved at,

1
_Wˆ_ _W_ _W_ 2 [+][ λ] _W_ 2 = 0 (38)
_∇_ _[L][T][ = 0][ ⇒∇]_ [ˆ] 2 _[||][y][ −]_ _[X][ ˆ]_ _||[2]_ 2 _||[2]_

_[||][ ˆ]_

_⇒−X_ _[T]_ (y − _XW[ˆ]_ ) + λW[ˆ] = 0  (39)

_⇒_ _W := (X_ _[T]_ _X + λI)[−][1]X_ _[T]_ _y._ (40)


An exact gradient descent has the following dynamics,

_Wˆ_ _t = W[ˆ]_ _t−1 −_ _η∇Wˆ_ _t−1_ _[L][T][,]_ (41)

= W[ˆ] _t−1 −_ _η_ _−_ _X_ _[T]_ (y − _XW[ˆ]_ _t−1) + λW[ˆ]_ _t−1_ (42)

= (1 _ηλ) W[ˆ]t_ 1 _ηX_ _[T]_ _XW[ˆ]_ _t_ 1 + ηX _[T]_ _y,_  (43)
_−_ _−_ _−_ _−_

= [(1 _ηλ)I_ _ηX_ _[T]_ _X] W[ˆ]_ _t_ 1 + ηX _[T]_ _y,_ (44)
_−_ _−_ _−_

= [(1 _ηλ)I_ _ηX_ _[T]_ _X] W[ˆ]_ _t_ 1 + η(X _[T]_ _X + λI)(X_ _[T]_ _X + λI)[−][1]X_ _[T]_ _y,_ (45)
_−_ _−_ _−_

= [(1 _ηλ)I_ _ηX_ _[T]_ _X] W[ˆ]_ _t_ 1 + η(X _[T]_ _X + λI)W,_ (46)
_−_ _−_ _−_

= [(1 _ηλ)I_ _ηX_ _[T]_ _X] W[ˆ]_ _t_ 1 + (ηX _[T]_ _X + ηλI)W,_ (47)
_−_ _−_ _−_

= [(1 _ηλ)I_ _ηX_ _[T]_ _X] W[ˆ]_ _t_ 1 + (ηX _[T]_ _X + (ηλ_ 1)I)W + W, (48)
_−_ _−_ _−_ _−_

which leads to,

_Wˆ_ _t −_ _W = [(1 −_ _ηλ)I −_ _ηX_ _[T]_ _X]( W[ˆ]_ _t−1 −_ _W_ ), (49)

= [(1 _ηλ)I_ _ηX_ _[T]_ _X][t]( W[ˆ]_ 0 _W_ ). (50)
_−_ _−_ _−_

Assuming _W[ˆ]_ 0 = 0, we arrive at the following closed-form equation,

_t[]_
_Wˆ_ _t =_ _I −_ (1 − _ηλ)I −_ _ηX_ _[T]_ _X_ _W,_ (51)
  

where W is defined in Eq 40.


Now back to definition of R in Eq. 84 and by substitution of Eq. 51, we have,

_R(t) : = [1]_ _Wt,_ (52)

_d_ _[W][ T][ F][ ˆ]_

_t[]_

= [1] _I_ (1 _ηλ)I_ _ηX_ _[T]_ _X_ _W,_ (53)

_d_ _[W][ T][ F]_ _−_ _−_ _−_
  t[]

= [1] _I_ (1 _ηλ)I_ _ηX_ _[T]_ _X_ (X _[T]_ _X + λI)[−][1]X_ _[T]_ _y,_ (54)

_d_ _[W][ T][ F]_ _−_ _−_ _−_
  _t[]_

= [1] _I_ (1 _ηλ)I_ _ηΛ_ (Λ + λI)[−][1]V _[T]_ _X_ _[T]_ _y,_ (X _[T]_ _X = V ΛV_ _[T]_ ) (55)

_d_ _[W][ T][ FV]_ _−_ _−_ _−_

= d[1] _[W][ T][ FV]_ I − (1 − _ηλ)I −_ _ηΛt[](Λ + λI)[−][1](ΛV_ _[T]_ _F_ _[−][1]W + Λ_ 12 ϵ), (56)

= d[1] _[W][ T][ FV]_ I − (1 − _ηλ)I −_ _ηΛt[](Λ + λI)[−][1](ΛV_ _[T]_ _F_ _[−][1]W + Λ_ 12 ϵ), (57)

  Λ

= [1] _I_ [(1 _ηλ)I_ _ηΛ][t][]_ _._ (58)

_d_ **[Tr]** _−_ _−_ _−_ Λ + λI
h  i


-----

_t[]_
Similarly for Q, let D := _I −_ (1 − _ηλ)I −_ _ηΛ_, then we have,
  

_Q(t) : = [1]_ _Wˆ_ _[T]_ _F_ _[T]_ _F_ _W,[ˆ]_ (59)

_d_

_T_ _t[]_ _t[]_

= [1] _I_ (1 _ηλ)I_ _ηX_ _[T]_ _X_ _F_ _[T]_ _F_ _I_ (1 _ηλ)I_ _ηX_ _[T]_ _X_ _W,_ (60)

_d_ _[W]_ _−_ _−_ _−_ _−_ _−_ _−_

= [1] _T V DV[]_ _T_ _F T FV DV T W,_     (61)

_d_ _[W]_

= [1] _T V D ˜F_ _[T][ ˜]FDV_ _[T]_ _W,_ ( F[˜] := FV, X = U Λ[1][/][2]V _[T]_ _, ˜ϵ := U_ _[T]_ _ϵ)_ (62)

_d_ _[W]_

Λ Λ

= [1] _ϵ)_ _F_ _[T][ ˜]FD_ _ϵ),_ (63)

_d_ [(][W][ T][ F][ −][1][T][ V][ + Λ][−][1][/][2][˜] Λ + λI [D][ ˜] Λ + λI [(][V][ T][ F][ −][1][W][ + Λ][−][1][/][2][˜]


= [1] _F_ + Λ[−][1][/][2]ϵ˜)

_d_ [(][W][ T][ ˜][−][1][T]


Λ

_F_ _[T][ ˜]FD_
Λ + λI [D][ ˜]


Λ

Λ + λI [( ˜]F _[−][1]W + Λ[−][1][/][2]ϵ˜),_ (64)


= d[1] _[W][ T][ ˜]F_ _[−][1][T]_


Λ

_F_ _[T][ ˜]FD_
Λ + λI [D][ ˜]


Λ

Λ + λI _F˜[−][1]W,_ (65)

Λ

_ϵ,_ (66)
Λ + λI [Λ][−][1][/][2][˜]


Λ

+ [1] _ϵ_ _F_ _[T][ ˜]FD_

_d_ [Λ][−][1][/][2][˜]Λ + λI [D][ ˜]


_ϵ_

= [1] _A[T]_ _A_ + _[σ][2]_ _B[T]_ _B_ (67)

_d_ **[Tr]** _d_ **[Tr]**
h i h i


where,

_t[]_ Λ
_A : = F[˜]_ _I −_ (1 − _ηλ)I −_ _ηΛ_ Λ + λI _F˜[−][1]_ and, (68)
  t[] Λ

_B : = F[˜]_ _I_ (1 _ηλ)I_ _ηΛ_ 2 . (69)
_−_ _−_ _−_ Λ + λI [Λ][−] [1]
  

For simplicity and brevity of the results, in the main text, we only present the results where σϵ[2] [= 0]
and λ = 0. Substituting σϵ[2] [=][ λ][ = 0][ leads to the following expressions,]

_R(t) = [1]_ _I_ _I_ _ηΛ][t][i]._ (70)

_d_ **[Tr]** _−_ _−_
h  _t[]_

_Q(t) = d[1]_ **[Tr]** _A[T]_ _A_ where, _A := FV_ _I −_ _I −_ _ηΛ_ _V_ _[T]_ _F_ _[−][1],_ (71)

h i   

and that concludes the proof.


B.3 THE SPECIAL CASE APPROXIMATE DYNAMICS (EQS. 13 AND 15)

Recall that the teacher and student are defined as,

_y := y[∗]_ + ϵ, _y[∗]_ := z[T] _W,_ _yˆ := x[T][ ˆ]W,_ _x := F_ _[T]_ _z,_ (72)

where ϵ ∼N (0, σϵ[2][)][ is the label noise,][ F][ is the modulation matrix, and][ ||][z][||]2[2] [=][ ||][W] _[||]2[2]_ [= 1][.]

The training and generalization losses are defined as,


_T := [1]_
_L_ 2n


(ˆy _y)[2]_ + _[λ]_ _W_ 2[,] _G := [1]_ _y_ _y[∗])[2]]._ (73)
_−_ 2 _[||][ ˆ]_ _||[2]_ _L_ 2 [E][z][[(ˆ] −


According to Eq. 6, the generalization loss can be written in terms of two scalar variables R and Q,

_G = [1]_ where, (74)
_L_ 2 [(1 +][ Q][ −] [2][R][)][,]

_R : = Ez[y[∗][T]_ _yˆ] = Ez[W_ _[T]_ _zz[T]_ _F_ _W[ˆ]_ ] = [1] _W,_ and, (75)

_d_ _[W][ T][ F][ ˆ]_

_Q : = Ez[ˆy[T]_ _yˆ] = Ez[ W[ˆ]_ _[T]_ _F_ _[T]_ _zz[T]_ _F_ _W[ˆ]_ ] = [1] _Wˆ_ _[T]_ _F_ _[T]_ _F_ _W.[ˆ]_ (76)


-----

Now, applying t steps of SGD on LT results in the following distribution for the student’s weights,


1 _T ( W,t[ˆ]_ )

_e[−][β][ ˜]L_ _,_ (77)
_Zβ,t_


_P_ ( W, t[ˆ] ) =


in which _L[˜]T ( W, t[ˆ]_ ) is a modified loss where its equilibrium coincides with the t[th] iterate of SGD on
the original loss LT ( W[ˆ] ).

In Eq. 77 the scalar variable β depends on the noise of SGD and Zβ,t is the partition function which
is defined as,

_∞_ _di=1_ [d] ˆWi _δ_ _d1_ _W[ˆ]_ _i[T]_ _[F][ T][ F][ ˆ]Wi_ _Q0_ _P_ ( W[ˆ] _i, t)_

_Zβ,t =_ _−∞_ _−_ _,_ (78)

R Q ∞ _di =1_ [d] ˆWi _δ_ _d1_ _W[ˆ]_ _i[T]_ _[F][ T][ F][ ˆ]Wi_ Q0

_−∞_ _−_
R Q     

in which, Q0 can be perceived to be a target norm the student weights _W[ˆ]_ are being constrained to
and d is the dimensionality of the data. It can be interpreted that the partition function Zβ,t counts
the students.

We are now interested in finding R and Q of the typical (most probable) students. Therefore, it
suffices to find the students that dominate the partition function (or more precisely the free-energy).
The free-energy is defined as,

_f :=_ ln Zβ,t _,_ (79)
_−_ _βd[1]_ [E][W,z] 

where W and z are the teacher’s weight and input, respectively.

Due to the logarithm inside the expectation, analytical computation of Eq. 79 is intractable. However, the replica method (M´ezard et al., 1987) allows us to tackle this through the following identity,


EW,z[Zβ,t[r] []][ −] [1]
EW,z[ln Zβ,t] = lim
_r→0_ _r_


(80)


**The case where F = I.** As a first step, we first study a case where F = I. In that case, as derived
in B¨os (1998), Eq. 79 can be simplified to,

_Q_ _R[2]_ _G_ 2HR + Q

_βf = [1]_ _−_ _−_ (81)
_−_ 2 _Q0_ _Q_ [+ 1]2 [ln(][Q][0][ −] _[Q][)][ −]_ 2[n]d [ln[1 +][ β][(][Q][0][ −] _[Q][)]][ −]_ _[nβ]2d_ 1 + β(Q0 _Q)_ _[,]_

_−_ _−_

in which the scalar variables G and H are defined as,

_H : = Ey∗_ [y[∗][T] _y] = Ey∗_ [y[∗][T] (y[∗] + ϵ)] = 1, (82)

_G : = Ey∗_ [y[T] _y] = Ey∗_ [(y[∗] + ϵ)[T] (y[∗] + ϵ)] = 1 + σϵ[2][.] (83)

At this point, in order to find the most probable students, one can extremize the free-energy
_f_ (R, Q, Q0) in Eq. 81. The solution to this extermination is derived in B¨os et al. (1993) and
reads,


_Rf = 0_ _R =_ _[n]_
_∇_ _⇒_ _d_

_Qf = 0_ _Q =_ _[n]_
_∇_ _⇒_ _d_


1
(84)
_a_ _[,]_


1 2 _a_

_Qf = 0_ _Q =_ _[n]_ _G_ _−_ _,_ (85)
_∇_ _⇒_ _d_ _a[2]_ _n/d_ _−_ _[n]d_ _a_

_−_  

2λ[˜]
_Q0_ _f = 0_ _a = 1 +_ _,_ (86)
_∇_ _⇒_

1 − _n/d −_ _λ[˜] +_ (1 − _n/d −_ _λ[˜])[2]_ + 4λ[˜]
q


in which,


1

and, _λ˜ := λ + [1]_ (87)
_β(Q0_ _Q)_ _[,]_ _ηt_ _[.]_
_−_


_a := 1 +_


-----

**The case where F follows Assumption 1.**

**Assumption. The modulation matrix, F** _, under a SVD, F := U_ ΣV _[T]_ _has two sets of singular values_
_such that the first p singular values are equal to σ1 and the remaining d−p singular values are equal_
_to σ2. We let the condition number of F to be denoted by κ :=_ _σ[σ]2[1]_ _[>][ 1][.]_

Without loss of generality, we assume that U = V = I. Consequently, the (noiseless) teacher and
the student can be written as the composition of two sub-models as following,

_y[∗]_ = y1[∗] [+][ y]2[∗] [=][ z]1[T] _[W][1]_ [+][ z]2[T] _[W][2][,]_ (teacher decomposition) (88)

_yˆ = ˆy1 + ˆy2 = σ1z1[T]_ _W[ˆ]_ 1 + σ2z2[T] _W[ˆ]_ 2, (student decomposition) (89)

in which z1 ∈ R[p] and z2 ∈ R[d][−][p].

Let ˆyi denote the output of the i[th] component of the student. Also let yi[∗] [and][ y][i][ denote the noiseless]
and noisy targets, respectively. Therefore, for the student components i ∈ 1, 2, we have,


_yˆ1 = σ1z1[T]_ _W[ˆ]_ 1,

_y1[∗]_ [=][ z]1[T] _[W][1][,]_


_yˆ2 = σ2z2[T]_ _W[ˆ]_ 2,

_y2[∗]_ [=][ z]2[T] _[W][2][,]_


_y1 = y1[∗]_ [+][ z]2[T] _[W][2]_ _[−]_ _[σ][2][z]2[T]_ _W[ˆ]_ 2 +ϵ, _y2 = y2[∗]_ [+][ z]1[T] _[W][1]_ _[−]_ _[σ][1][z]1[T]_ _W[ˆ]_ 1 +ϵ,
_y2[∗][−]y[ˆ]2=ϵ2(t)_ _y1[∗][−]y[ˆ]1=ϵ1(t)_

in which ϵ is the explicit noise| {z, added to the teacher’s output while} _ϵj(|t) is an implicit variable noise{z_ }
which decreases as the component j ̸= i learns to match ˆyj and yj.

Accordingly, the variables Hi and Gi for each component i are re-defined as,


_H1 = E[y1[∗]T y1] = Ey1∗_ [[][y]1[∗]T y1∗[] =][ p]

_d_ _[,]_

_G1 = E[y1[T]_ _[y][1][]][,]_

== E E[([yy1[∗]1[∗]T y[+]1∗[ y][] +]2[∗] _[−][ E][[]y[ˆ][y]22[∗])T[T] y(y2∗1[] +][∗]_ [+][ E][ y][[ˆ]2[∗]y[−]2[T] _y[ˆ]y[ˆ]22],)] + σϵ[2][,]_

_−_ 2E[y2[∗]T ˆy2] + σϵ[2][,]

= _[p]_ + Q2 2R2 + σϵ[2][,]

_d_ [+][ d][ −]d _[p]_ _−_

= 1 + Q2 − 2R2 + σϵ[2][,]


_H2 = E[y2[∗]T y2] = Ey2∗_ [[][y]2[∗]T y2∗[] =][ d][ −] _[p]_ _,_

_d_

_G2 = E[y2[T]_ _[y][2][]][,]_

== E E[([yy2[∗]2[∗]T y[+]2∗[ y][] +]1[∗] _[−][ E][[]y[ˆ][y]11[∗])T[T] y(y1∗2[] +][∗]_ [+][ E][ y][[ˆ]1[∗]y[−]1[T] _y[ˆ]y[ˆ]11],)] + σϵ[2][,]_

_−_ 2E[y1[∗]T ˆy1] + σϵ[2][,]

= _[d][ −]_ _[p]_ + _[p]_ _ϵ_ _[,]_

_d_ _d_ [+][ Q][1][ −] [2][R][1][ +][ σ][2]

= 1 + Q1 − 2R1 + σϵ[2][,]


in which Ri and Qi are defined as,

_Ri := Ez[y[∗][T]i_ _y[ˆ]i] = [1]_ _i_ _[σ][i]W[ˆ]_ _i,_ and, _Qi := Ez[ˆyi[T]_ _y[ˆ]i] = [1]_

_d_ _[W][ T]_ _d_


_Wˆ_ _i[T]_ _[σ]i[2]W[ˆ]_ _i,_


where σi denotes the singular values of the matrix F as defined in Assumption 1.

Rewriting Eqs. 84, 85, and 86 for each of the student’s components, we arrive at,


_R1 =_ _[n]_


_R2 =_ _[n]_


_a1_


_a2_


_n_ 2 _a1_
_Q1 =_ _pa[2]1_ 1 + Q2 − 2R2 + σϵ[2] _[−]_ _[n]d_ _−a1_



_[−]_ _[n]_

2λ[˜]1
_a1 = 1 +_

1 _p_ _λ1 +_ (1 _p_ _λ1)[2]_ + 4λ[˜]1
_−_ _[n]_ _[−]_ [˜] _−_ _[n]_ _[−]_ [˜]

1 q

_λ˜1 : =_ _[d]_ (λ + [1]

_p_ _σ1[2]_ _ηt_ [)][,]


_n_ 2 _a2_
_Q2 =_ (d _p)a[2]1_ 1 + Q1 − 2R1 + σϵ[2] _[−]_ _[n]d_ _−a2_

_−_ _[−]_ _[n]_ 

2λ[˜]
_a2 = 1 +_ _,_

1 _dnp_ _λ +_ (1 _dnp_ _λ)[2]_ + 4λ[˜]
_−_ _−_ _[−]_ [˜] _−_ _−_ _[−]_ [˜]

_d_ 1 q
_λ˜2 : =_ (λ + [1]

_d_ _p_ _σ2[2]_ _ηt_ [)][,]
_−_


-----

where Q1 depends on Q2 and vice versa. However, with simple calculations, we can arrive at the
following standalone equation. Let,


_α1 =_ _[n]p [, α][2][ =]_


_n_

(90)
_d_ _p_ _[,]_
_−_


and also let,


_αi_ 2 _ai_
_bi =_ _,_ _ci = 1_ 2Ri _−_ for _i_ 1, 2 _,_ (91)

_a[2]i_ _−_ _−_ _[n]d_ _ai_ _∈{_ _}_

_[−]_ _[α][i]_

with which the closed-from scalar expression for Q(t, λ) reads,

_Q(t, λ) = Q1 + Q2,_ where, _Q1 :=_ _[b][1][b][2][c][2][ +][ b][1][c][1]_ _,_ and, _Q2 :=_ _[b][1][b][2][c][1][ +][ b][2][c][2]_ _._ (92)

1 _b1b2_ 1 _b1b2_
_−_ _−_


B.4 REPLICA TRICK

In the following, we detail the mathematical arguments leading to the replica trick expression. For
some r → 0, we can write for any scalar x:

_x[r]_ = exp(r ln x) = lim
_r_ 0 [1 +][ r][ ln][ x]
_→_

lim
_⇒_ _r_ 0 _[r][ ln][ x][ = lim]r_ 0 _[x][r][ −]_ [1]
_→_ _→_

_x[r]_ 1 (93)
ln x = lim _−_
_⇒_ _r_ 0 _r_
_→_

E[x[r]] 1
∴ E[ln x] = lim _−_ _, E : averaging_
_r→0_ _r_

B.5 COMPUTATION OF THE FREE-ENERGY

The self-averaged free energy (per unit weight) of our student network, is given by (Engel & Van den
Broeck, 2001),

_βf = [1]_ (94)
_−_ _d_ _[⟨⟨][ln][ Z][⟩⟩][z,W]_

Here, β = 1/T is the inverse temperature parameter corresponding to our statistical ensemble, d the
(teacher) student network width, and Z the partition function of the system defined as (n: number
of training examples).

As Gaussian variables (with n, d →∞), in the partition function, to obtain,


dµ (W _[a]) dya[µ][d(][y][∗][)][µ][e][−][βN]_ _[E][T][ (][y][a][,y][∗][)]_


_⟨⟨Z_ _[r]⟩⟩z,W =_


_a=1_

_×_

_r_

_a=1_

Y


_µ=1_

_δ_

_d_

_µ=1_

Y


_y[∗][µ]_ _−_ _√[1]_


_W_ _[T]_ _x[∗][µ]_ _δ_ _ya[µ]_

_[−]_ _√[1]_
 


_Wa[T]_ _[x][µ]_


_z,W_


(95)


dµ (W _[a]) [d][y]a[µ][dˆ]ya[µ]_ dy[∗][µ]dˆy[∗][µ] _e[−][βN]_ _[E][T][ (][y][a][,y][∗][)]e[iy][∗]_ _[µ][ ˆ]y[∗][µ]+iya[µ]y[ˆ]a[µ]_

2π 2π


exp _−_ _√[i]_



_yˆ[∗][µ]W_ _[T]_ _x[∗][µ]_ _−_


_yˆa[µ][W][ T]a_ _[x][µ]_


_z,W_


where in the last line above, we have expressed the inserted δ functions using their integral representations. To make further progress, we introduce the auxiliary variables,

_Wa[i][∆][ij][W][ ∗][j][ =][ dR][a][,]_ (96)
_ija_

X

_Wa[i][Γ][ij][W][ j]b_ [=][ dQ][ab] (97)
_ijX⟨a,b⟩_


-----

via the respective δ functions, to arrive at,


dµ (W[a]) [d][y]a[µ][dˆ]ya[µ] dy[∗][µ]dˆy[∗][µ] _e[−][βN]_ _[E][T][ (][y][a][,y][∗][)]e[iy][∗]_ _[µ][ ˆ]y[∗][µ]+iya[µ]y[ˆ]a[µ]_

2π 2π


_⟨⟨Z_ _[n]⟩⟩z,W =_


_µ,a,b_


_P_ dQ[ab] _P_ dR[a] _δ_ _Wa[i][∆][i,j][W][ ∗][j][ −]_ _[PR][a]_ _δ_ _Wa[i][Γ][ij][W][ j]b_
_×_ Z Z i,j,a  ij _a,b_ _[−]_ _[PQ][ab]_

_⟨_ _⟩_

[X]   [X] 

exp (ˆya[µ][)][2][ −] [1] _yˆa[µ]y[ˆ]b[µ][Q][ab][ −]_ _yˆ[∗][µ]yˆa[µ][R][a]_ (ˆy[∗][µ])[2][++]

_×_ ** _−_ _[Q]2[0]_ _µ,a_ 2 _µ,_ _a,b_ _µ,a_ _[−]_ [1]2 _µ_

 X X⟨ _⟩_ X X

(98)

Repeating the procedure of expressing the above δ functions using their integral representations, we
then get (α = n/d),

dQ0 d Q[ˆ]0a dQabQ[ˆ]ab dRaR[ˆ]a _iP_

_Z_ _[n]_ _x,x∗,W =_ _Q0Q[ˆ]0a + iP_ _Q[ab][ ˆ]Q[ab]_
_⟨⟨_ _⟩⟩_ _a,b_ _√2π_ 4π 2π/d 2π/d [exp] 2 _a_ _a<b_
Z Y  X X

+ iP _R[a][ ˆ]R[a][ Z Y]_ dWi[a] _Qˆ0aWa[i][Γ][ij][W][ j]a_

_a_ _i,a_ _√2π_ [exp] _−_ 2[i] _i,j,a_

X  X

_i_ _QˆabWa[i][Γ][ij][W][ j]b_ _Rˆa∆ijWa[j]_
_−_ _i,j,a<b_ _[−]_ _[i]_ _i,j,a_ _×_
X X 

_µ,a_ dya2[µ]π[dˆ]ya[µ] d√y2[∗]π [µ] _[e][−][βN]_ _[E][T][ (][y][a][,y][∗][)][ exp]_ _−_ [1]2 _µ_ (y[∗][µ])[2] + i _µ,a_ _yˆa[µ]y[ˆ]a[µ]_

Z Y  X X

1 _Ra[2]_ (ˆya[µ][)][2][ −] [1] _yˆa[µ]y[ˆ]b[µ]_ _Q[ab]_ _R[a]R[b][]_ _i_ _y[∗][µ]yˆa[µ][R][a][]_

_−_ 2[1] _−_ 2 _−_ _−_

Xa,µ    _µ,X⟨a,b⟩_   Xµ,a

(99)

If we now, perform a singular value decomposition of the covariance matrix Γ as, Γ = U[T] SU =
V[T] V, where S: matrix of singular values of Γ, and we have expressed, V = S[1][/][2]U, then one can
proceed to write,

1 dQ0 d Q[ˆ]0a dQabQ[ˆ]ab dRaR[ˆ]a _iP_
_⟨⟨Z_ _[n]⟩⟩x,W =_ det |V | Z Ya,b _√2π_ 4π 2π/d 2π/d [exp]  2 Xa _Q0Q[ˆ]0a_

+ iP _Q[ab][ ˆ]Q[ab]_ + iP _R[a][ ˆ]R[a][ Z Y]_ d W[˜] _i[a]_ _Qˆ0a_ _W˜_ _a[i]_ 2

_a<b_ _a_ _i,a_ _√2π_ [exp] _−_ 2[i] _i,a_

X X  X  

_−_ _i_ _i,a<b_ _QˆabW[˜]_ _a[i]W[˜]_ _b[i]_ _[−]_ _[i]_ _i,j,a_ _RˆaW[˜]_ _a[j]_ _×_ _µ,a_ dya2[µ]π[dˆ]ya[µ] d√y2[∗]π [µ] _[e][−][βN]_ _[E][T][ (][y][a][,y][∗][)]_
X X  Z Y

exp (yµ[∗][)][2][ +][ i] _yˆa[µ]y[ˆ]a[µ]_ 1 _Ra[2]_ (ˆya[µ][)][2][ −] _[i]_ _y[∗][µ]yˆa[µ][R][a]_
_−_ 2[1] _µ_ _µ,a_ _[−]_ [1]2 _a,µ_ _−_ _µ,a_
 X X X    X

_yˆa[µ]y[ˆ]b[µ]_ _Q[ab]_ _R[a]R[b][ ]_

_−_ [1]2 _−_

_µ,X⟨a,b⟩_  

(100)

having expressed, _W[˜]_ _a = VWa, and identifying ∆= S[1][/][2]U from our definitions. Now, since in_
the above, the Wi[a] [integrals factorize in][ i][, and similarly the][ y]a[µ][,][ ˆ]ya[µ] [and][ d][y][∗][µ][ factorize in][ µ][, one can]
proceed to write:

1 dQ0d Q[ˆ]0a dQabQ[ˆ]ab dRaR[ˆ]a _i_
_⟨⟨Z_ _[n]⟩⟩x,W =_ det |V | Z Ya,b _√2π4π_ 2π/d 2π/d [exp] P h 2 Xa _Q0Q[ˆ]0a_ [(101)]

+ i _Q[ab][ ˆ]Q[ab]_ + i _R[a][ ˆ]R[a]_ + GS( Q[ˆ]0a, _Q[ˆ][ab],_ _R[ˆ][a]) + αGE(Q[ab], R[a])_

_a<b_ _a_

X X i


-----

where,

_GS( Q[ˆ]0a,_ _Q[ˆ][ab],_ _R[ˆ][a]) = ln_

_a_

Z Y

_GE(Q[ab], R[a]) = ln_

_a_

Z Y

_−_ 2[1] X


d W[˜] _[a]_

_√2π_ [exp] _−_ 2[i]




_Qˆ0aW[˜]_ _a[i]W[˜]_ _a[i]_ _[−]_ _[i]_


_a<b_ _QˆabW[˜]_ _aW[˜]_ _b −_ _i_

X


_RˆaW[˜]_ _a_


_yˆaR[a][]_

(102)


dy2aπdˆya _√dy2[∗]π [e][−][βN]_ _[E][T][ (][y][a][,y][∗][)][ exp]_ _−_ [1]2 [(][y][∗][)][2][ +][ i]




_yˆayˆa_


_yˆayˆb_ _Q[ab]_ _R[a]R[b][]_ _iy[∗][µ][ X]_
_−_ _−_
_⟨Xa,b⟩_   _a_


1 − _Ra[2]_ (ˆya)[2] _−_ 2[1]


Now, in the limit d →∞, Eq. 101 can be approximated using the saddle-point approach (Bender &
Orszag, 2013),

_i_
_⟨⟨Z_ _[n]⟩⟩x,W ≈_ **extrQ0, ˆQ0a,Q[ab],Q[ˆ]** _[ab],R[a],R[ˆ][a][ exp]_ _P_ 2 _a_ _Q0Q[ˆ]0a + i_ _a<b_ _Q[ab][ ˆ]Q[ab]_
 h X X (103)

+ i _R[a][ ˆ]R[a]_ + GS( Q[ˆ]0a, _Q[ˆ][ab],_ _R[ˆ][a]) + αGE(Q[ab], R[a])_

_a_

X i

where, extr corresponds to extremization of ⟨⟨Z _[n]⟩⟩x,W over the respective order parameters. Per-_
forming this extremization over _Q[ˆ]0a,_ _Q[ˆ][ab]_ and _R[ˆ][a], then generates an expression of the form,_

1 _Q_ _R[2]_

_Z_ _[n]_ _x,W = extrQ0,Q,R exp_ _nN_ _−_
_⟨⟨_ _⟩⟩_ ( 2 _Q0_ _Q_ [+ 1]2 [ln(][Q][0][ −] _[Q][)][ −]_ _[α]2 [ln [1 +][ β][(][Q][0][ −]_ _[Q][)]]_

_−_

1 − 2R + Q

_−_ _[αβ]2_ 1 + β(Q0 _Q)_ !)

_−_

(104)


where we have invoked replica symmetry in the form, Q[ab] = Q and R[a] = R, and that =
_ET_
(y[∗] _−_ _y)[2]/2. Plugging this back into Eq. ??, then finally yields,_


_Q −_ _R[2]_

_Q0_ _Q_ [+ 1]2 [ln(][Q][0][ −] _[Q][)][ −]_ _[α]2 [ln [1 +][ β][(][Q][0][ −]_ _[Q][)]]_
_−_


_βf =_ **extrQ0,Q,R**
_−_

_−_ _[αβ]2_


(105)


1 − 2R + Q

_−_ _[αβ]2_ 1 + β(Q0 _Q)_ )

_−_

The remaining pair of order parameters generate the following set of transcendental equations on
extremization (B¨os, 1998):

_R =_ _[α]_


1
_−_ [2][ −]a _[a]_


_Q =_


(106)


_a[2]_ _−_ _α_


_Q0 = Q +_


_β (a −_ 1)


where, a = max[1, α] for T → 0.

Now, the above determined values of R, Q and Q0 can be perceived as the maximally likely values of
_R, Q and Q0 of our teacher-student setup, for an inverse temperature β parameterizing the system._


C EXTENDED EXPERIMENTS

Figure 4 presents the analytical generalization dynamics for two values of κ and provides comparison between the theory and simulation results of the same model. We observe that the theory and


-----

|Col1|Col2|Col3|Col4|γ = 0.1|
|---|---|---|---|---|
||||||


Theory _γ = 0.0_

Experiment

_γ = 0.1_



|Col1|Col2|Col3|4 i a d i|
|---|---|---|---|
||F|igure||
||si e|mulat rror b||
||d|ouble||
|||||
||si|mulat||
||n|oteb||
||B ic|efore s. Co||


|Col1|Col2|Col3|Training time|Col5|
|---|---|---|---|---|
|he|||Training time||
||r-student|set-u|p in Sec. equation 2.1.|We compare the anal|
|e ra|d on our t ndom se|each eds. T|er-student setup with d he solutions and the si|= 100, p = 50, n = 15 mulations match closel|
|the genera ely match. the theory, eal gas in|he genera|lizati|on error.||
||y match.|Furth|er experiments are prov|ided in the following an|
||e theory,|we in|vite the reader to recal|l a simple equation fro|
|||a con|tainer with its large nu|mber of molecules mo|


.

liding with each other, all while obeying Newton’s laws. While the exact dynamics of each of

such molecules is intractable, the system’s macroscopic behavior can be characterized in terms of a
handful of scalar quantities, namely, the pressure P, the volume V, and the temperature T . By averaging over suitable probability measures and applying the principle of free-energy minimization,
one arrives at a remarkably simple relationship between these three macroscopic variables, i.e., the
well-known PV = nRT (n: number of moles of gas, R: gas constant) (Reif, 2009).


-----

