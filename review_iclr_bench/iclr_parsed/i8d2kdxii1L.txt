# p-LAPLACIAN BASED GRAPH NEURAL NETWORKS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Graph neural networks (GNNs) have demonstrated superior performance for semisupervised node classification on graphs, as a result of their ability to exploit node
features and topological information simultaneously. However, most GNNs implicitly assume that the labels of nodes and their neighbors in a graph are the same
or consistent, which does not hold in heterophilic graphs, where the labels of
linked nodes are likely to differ. Moreover, when the topology is non-informative
for label prediction, ordinary GNNs may work significantly worse than simply
applying multi-layer perceptrons (MLPs) on each node. To tackle the above problem, we propose a new p-Laplacian based GNN model, termed as _[p]GNN, whose_
message passing mechanism is derived from a discrete regularization framework
and could be theoretically explained as an approximation of a polynomial graph
filter defined on the spectral domain of p-Laplacians. The spectral analysis shows
that the new message passing mechanism works simultaneously as low-pass and
high-pass filters, thus making _[p]GNNs are effective on both homophilic and het-_
erophilic graphs. Empirical studies on real-world and synthetic datasets validate
our findings and demonstrate that _[p]GNNs significantly outperform several state-_
of-the-art GNN architectures on heterophilic benchmarks while achieving competitive performance on homophilic benchmarks. Moreover, _[p]GNNs can adaptively_
learn aggregation weights and are robust to noisy edges.

1 INTRODUCTION

In this paper, we explore the usage of graph neural networks (GNNs) for semi-supervised node
classification on graphs, especially when the graphs admit strong heterophily or noisy edges. Semisupervised learning problems on graphs are ubiquitous in a lot of real-world scenarios, such as user
classification in social media (Kipf & Welling, 2017), protein classification in biology (Velickovic
et al., 2018), molecular property prediction in chemistry (Duvenaud et al., 2015), and many others (Marcheggiani & Titov, 2017; Satorras & Estrach, 2018). Recently, GNNs are becoming the de
facto choice for processing graph structured data. They can exploit the node features and the graph
topology by propagating and transforming the features over the topology in each layer and thereby
learn refined node representations. A series of GNN architectures have been proposed, including
graph convolutional networks (Bruna et al., 2014; Henaff et al., 2015; Defferrard et al., 2016; Kipf
& Welling, 2017; Wu et al., 2019), graph attention networks (Velickovic et al., 2018; Thekumparampil et al., 2018), and other representatives (Hamilton et al., 2017; Xu et al., 2018; Pei et al., 2020).

Most of the existing GNN architectures work under the homophily assumption, i.e. the labels of
nodes and their neighbors in a graph are the same or consistent, which is also commonly used in
graph clustering (Bach & Jordan, 2004; von Luxburg, 2007; Liu & Han, 2013) and semi-supervised
learning on graphs (Belkin et al., 2004; Hein, 2006; Nadler et al., 2009). However, recent studies (Zhu et al., 2020; 2021; Chien et al., 2021) show that in contrast to their success on homophilic
graphs, most GNNs fail to work well on heterophilic graphs, in which linked nodes are more likely
to have distinct labels. Moreover, GNNs could even fail on graphs where their topology is not helpful for label prediction. In these cases, propagating and transforming node features over the graph
topology could lead to worse performance than simply applying multi-layer perceptrons (MLPs) on
each of the nodes independently. Several recent works were proposed to deal with the heterophily issues of GNNs. Zhu et al. (2020) finds that heuristically combining ego-, neighbor, and higher-order
embeddings improves GNN performance on heterophilic graphs. Zhu et al. (2021) uses a compatibility matrix to model the graph homophily or heterphily level. Chien et al. (2021) incorporates


-----

the generalized PageRank algorithm with graph convolutions so as to jointly optimize node feature
and topological information extraction for both homophilic and heterophilic graphs. However, the
problem of GNNs on graphs with non-informative topologies (or noisy edges) remains open.

Unlike previous works, we tackle the above issues of GNNs by proposing the discrete p-Laplacian
based message passing scheme, termed as p-Laplacian message passing. It is derived from a discrete
regularization framework and is theoretically verified as an approximation of a polynomial graph
filter defined on the spectral domain of the p-Laplacian. Spectral analysis of p-Laplacian message
passing shows that it works simultaneously as low-pass and high-pass filters [1] and thus is applicable
to both homophilic and heterophilic graphs. Moreover, when p ̸= 2, our theoretical results indicate
that it can adaptively learn aggregation weights in terms of the variation of node embeddings on
edges (measured by the graph gradient (Amghibech, 2003; Zhou & Sch¨olkopf, 2005; Luo et al.,

2010)), and work as low-pass or both low-pass and high-pass filters on a node according to the local
variation of node embeddings around the node (measured by the norm of graph gradients).

Based on p-Laplacian message passing, we propose a new GNN architecture, called _[p]GNN, to en-_
able GNNs to work with heterophilic graphs and graphs with non-informative topologies. Several
existing GNN architectures, including SGC (Wu et al., 2019), APPNP (Klicpera et al., 2019) and
GPRGNN (Chien et al., 2021), can be shown to be analogical to _[p]GNN with p = 2. Our empiri-_
cal studies on real-world benchmark datasets (homophilic and heterophilic datasets) and synthetic
datasets (cSBM (Deshpande et al., 2018)) demonstrate that _[p]GNNs obtain the best performance_
on heterophilic graphs and competitive performance on homophilic graphs against state-of-the-art
GNNs. Moreover, experimental results on graphs with different levels of noisy edges show that
_pGNNs work much more robustly than GNN baselines and even as well as MLPs on graphs with_
completely random edges. Additional experiments (reported in Appendix F.5) illustrate that intergrating _[p]GNNs with existing GNN architectures (i.e. GCN (Kipf & Welling, 2017), JKNet (Xu_
et al., 2018)) can significantly improve their performance on heterophilic graphs. In conclusion, our
contributions can be summarized as below:

(1) New methodologies. We propose p-Laplacian message passing and _[p]GNN to adapt GNNs to_
heterophilic graphs and graphs where the topology is non-informative for label prediction. (2) Supe**rior performance. We empirically demonstrate that** _[p]GNNs is superior on heterophilic graphs and_
competitive on homophilic graphs against state-of-the-art GNNs. Moreover, _[p]GNNs work robustly_
on graphs with noisy edges or non-informative topologies. (3) Theoretical justification. We theoretically demonstrate that p-Laplacian message passing works as both low-pass and high-pass filters
and the message passing iteration is guarantee to converge with proper settings. (4) New paradigm
**of designing GNN architectures. We bridge the gap between discrete regularization framework and**
GNNs, which could further inspire researchers to develop new graph convolutions or message passing schemes using other regularization techniques with explicit assumptions on graphs. Due to space
limit, we defer the discussions on related work and future work and all proofs to the Appendix.

2 PRELIMINARIES AND BACKGROUND

**Notation. Let G = (V, E, W) be an undirected graph, where V = {1, 2, . . ., N** _} is the set of nodes,_
_E ⊆V × V is the set of edges, W ∈_ R[N] _[×][N]_ is the adjacency matrix and Wi,j = Wj,i, Wi,j > 0
for [i, j], Wi,j = 0, otherwise. _i =_ _j_ [i,j] denotes the set of neighbors of node i,
_∈E_ _N_ _{_ _}_ _∈E_
**D ∈** R[N] _[×][N]_ = diag(D1,1, . . . DN,N ) denotes the diagonal degree matrix with Di,i = _j=1_ _[W][i,j][,]_
for i = 1, . . ., N . f : V → R and g : E → R are functions defined on the vertices and edges of G,
respectively. FV denotes the Hilbert space of functions endowed with the inner product ⟨[P]f,[N]f[˜]⟩FV :=

_i∈V_ _[f]_ [(][i][) ˜]f (i). Similarly define FE . We also denote by [K] = {1, 2, . . ., K}, ∀K ∈ N and we use
_∥Px∥_ = ∥x∥2 = ([P][d]i=1 _[x]i[2][)][1][/][2][,][ ∀][x][ ∈]_ [R][d][ to denote the Frobenius norm of a vector.]

**Problem Formulation. Given a graph G = (V, E, W), each node i ∈V has a feature vector Xi,:**
which is the i-th row of X and a subset of nodes in G have labels from a label set L = {1, . . ., L}.
The goal of semi-supervised node classification on G is to learn a mapping M : V →L and predict
the labels of unlabeled nodes.

1Note that if the low frequencies and high frequencies dominate the middle frequencies (the frequencies
that are around the cutoff frequency), we say that the filter works both as low-pass and high-pass filters.


-----

**Homophily and Heterophily. The homophily or heterophily of a graph is used to describe the**
relation of labels between linked nodes in the graphs. The level of homophily of a graph can be
measured by ( ) = Ei _j_ _j_ _i,yi=yj_ / _i_ (Pei et al., 2020; Chien et al., 2021), where
_H_ _G_ _∈V_ _{_ _}_ _∈N_ _|N_ _|_
_{j}j∈Ni,yi=yj_ denotes the number of neighbors of  _i ∈_ _V that share the same label as i and_
_H(G) →_ 1 corresponds to strong homophily while H(G) → 0 indicates strong heterophily. We
say that a graph is a homophilic (heterophilic) graph if it has strong homophily (heterophily).

**Graph Gradient. The graph gradient of an edge [i, j], i, j ∈V is defined to be a measurement of**
the variation of a function f [2] : V → R on the edge [i, j].
**Definition 1 (Graph Gradient). Given a graph G = (V, E) and a function f : V →** R, the graph
_gradient is an operator_ : _defined by_
_∇_ _FV →FE_

_Wi,j_ _Wi,j_

( _f_ )([i, j]) := _f_ (j) _f_ (i), _for all [i, j]_ _._ (1)
_∇_ s _Dj,j_ _−_ s _Di,i_ _∈E_

For [i, j] /∈E, (∇f )([i, j]) := 0. The graph gradient of a function f at vertex i is defined to
be ∇f (i) := ((∇f )([i, 1]), . . ., (∇f )([i, N ])) and its Frobenius norm is given by ∥∇f (i)∥2 :=
([P][N]j=1[(][∇][f] [)][2][([][i, j][]))][1][/][2][, which measures the variation of][ f][ around node][ i][.][ We measure the variation]
of f over the whole graph by _p(f_ ) where it is defined to be
_G_ _S_ _p_


_p(f_ ) := [1]
_S_ 2


_N_

( _f_ )([i, j]) = [1]
_∥_ _∇_ _∥[p]_ 2
_j=1_

X


_Wi,j_
_f_ (j)
_Dj,j_ _−_


_Wi,j_

_f_ (i)
_Di,i_


_, for p ≥_ 1, (2)


_i=1_


_i=1_


_j=1_


Note that the definition of Sp here is different with the p-Dirichlet form in Zhou & Sch¨olkopf (2005).

**Graph Divergence. The graph divergence is defined to be the adjoint of the graph gradient:**
**Definition 2 (Graph Divergence). Given a graph G = (V, E), and functions f : V →** R, g : E → R,
_the graph divergence is an operator div :_ _which satisfies_
_FE →FV_
_⟨∇f, g⟩_ = ⟨f, −divg⟩. (3)
_The graph divergence can be computed by_


_Wi,j_

(g([i, j]) _g([j, i])) ._ (4)
_Di,i_ _−_


(divg)(i) =


_j=1_


Fig. 4 in Appendix E.1 gives a tiny example of illustration of graph gradient and graph divergence.

**Graph p-Laplacian Operator. By the definitions of graph gradient and graph divergence, we reach**
the definition of graph p-Laplacian operator as below.
**Definition 3 (Graph p-Laplacian[3]). Given a graph G = (V, E) and a function f : V →** R, the graph
_p-Laplacian is an operator ∆p :_ _defined by_
_FV →FV_

∆pf := (5)
_−_ [1]2 [div(][∥∇][f] _[∥][p][−][2][∇][f]_ [)][,][ for][ p][ ≥] [1][.]

_where ∥· ∥[p][−][2]_ _is element-wise, i.e. ∥∇f_ (i)∥[p][−][2] = (∥(∇f )([i, 1])∥[p][−][2], . . ., ∥(∇f )([i, N ])∥[p][−][2]).
Substituting Eq. (1) and Eq. (4) into Eq. (5), we obtain


_Wi,j_

( _f_ )([j, i])
_Di,i_ _∥_ _∇_ _∥[p][−][2]_


_Wi,j_

_f_ (i)
_Di,i_ _−_


_Wi,j_
_f_ (j)
_Dj,j_


(6)


(∆pf )(i) =


_j=1_


The graph p-Laplacian is semi-definite: _f, ∆pf_ = _p(f_ ) 0 and we have
_⟨_ _⟩_ _S_ _≥_
_∂_ _p(f_ )
_S_ = p(∆pf )(i). (7)

_∂f_ _i_

When p = 2, ∆2 is refered as the ordinary Laplacian operator and ∆2 = I − **D[−][1][/][2]WD[−][1][/][2]**
and when p = 1, ∆1 is refered as the Curvature operator and ∆1f := − 2[1] [div(][∥∇][f] _[∥][−][1][∇][f]_ [)][. Note]

that Laplacian ∆2 is a linear operator, while in general for p ̸= 2, p-Laplacian is nonlinear since
∆p(af ) ̸= a∆p(f ) for a ∈ R.

23fNote that the definition adopted is slightly different with can be a vector function: f : V → Rc for some c ∈ N and here we use the one used in f Zhou & Sch¨ : V → R for better illustration.olkopf (2005) where
_∥· ∥(2009[p][−]), where[2]_ is not element-wise (∆pf )(i) = and the one used in some literature such asj=1 _WDi,ii,j_ Amghibech (2003); B¨uhler & Hein

_[|][f]_ [(][i][)][ −] _[f]_ [(][j][)][|][p][−][2][ (][f] [(][i][)][ −] _[f]_ [(][j][))][ for][ p >][ 1][ and][ p][ = 1][ is not allowed.]

[P][N]


-----

3 _p-LAPLACIAN BASED GRAPH NEURAL NETWORKS_

In this section, we derive the p-Laplacian message passing scheme from a p-Laplacian regularization
framework and present _[p]GNN, a new GNN architecture developed upon the new message passing_
scheme. We theoretically characterize how p-Laplacian message passing adaptively learns aggregation weights and profits _[p]GNN for being effective on both homophilic and heterophilic graphs._

3.1 _p-LAPLACIAN REGULARIZATION FRAMEWORK_

Given an undirected graph G = (V, E) and a signal function with c (c ∈ N) channels f : V → R[c],
let X = (X[⊤]1,:[, . . .,][ X][⊤]N,:[)][⊤] _[∈]_ [R][N] _[×][c][ with][ X][i,][:][ ∈]_ [R][1][×][c][, i][ ∈] [[][N] []][ denoting the node features of][ G][ and]
**F = (F[⊤]1,:[, . . .,][ F][⊤]N,:[)][⊤]** _[∈]_ [R][N] _[×][c][ be a matrix whose][ i][th][ row vector][ F][i,][:][ ∈]_ [R][1][×][c][, i][ ∈] [[][N] []][ represents]
the function value of f at the i-th vertex in G. We present a p-Laplacian regularization problem
whose cost function is defined to be


**F[∗]** = arg min (F) := arg min _p(F) + µ_
**F** _L_ **F** _S_


**Fi,:** **Xi,:** _,_ (8)
_i=1_ _∥_ _−_ _∥[2]_

X


where µ ∈ (0, ∞). The first term of the right-hand side in Eq. (8) is a measurement of variation of
the signal over the graph based on p-Laplacian. As we will discuss later, different choices of p result
in different smoothness constraint on the signals. The second term is the constraint that the optimal
signals F[∗] should not change too much from the input signal X, and µ provides a trade-off between
these two constraints.

**Regularization with p = 2. When p = 2, the solution of Eq. (8) satisfies ∆2F[∗]** + µ(F[∗] _−_ **X) = 0**
and we can obtain the closed form (Zhou et al., 2003; Zhou & Sch¨olkopf, 2005)
**F[∗]** = µ(∆2 + µIN )[−][1]X. (9)
Then, we could use the following iteration algorithm to get an approximation of Eq. (9):

**F[(][k][+1)]** = αD[−][1][/][2]WD[−][1][/][2]F[(][k][)] + βX, (10)
where k represents the iteration index, α = 1+1µ [and][ β][ =] 1+µµ [= 1][ −] _[α][. The iteration converges to]_

a closed-form solution as k goes to infinity (Zhou et al., 2003; Zhou & Sch¨olkopf, 2005). We could
relate the the result here with the personalized PageRank (PPR) (Page et al., 1999; Klicpera et al.,
2019) algorithm (proof defered to Appendix D.1):
**Theorem 1 (Relation to personalized PageRank (Klicpera et al., 2019)). µ(∆2 + µIN** )[−][1] _in the_
_closed form solution of Eq. (9) is equivalent to the personalized PageRank matrix._

**Regularization with p > 1. For p > 1, the solution of Eq. (8) satisfies p∆pF[∗]** + 2µ(F[∗] _−_ **X) = 0.**
By Eq. (6) we have that, for all i ∈ [N ],


_Wi,j_

_Di,i_ _∥(∇f_ _[∗])([j, i])∥[p][−][2]_


+ [2][µ]


1

**F[∗]i,:**
_Di,i_ _[−]_


1

**F[∗]j,:**
_Dj,j_


**F[∗]i,:** = 0.

_[−]_ **[X][i,][:]**



_j=1_


Based on which we can construct a similar iterative algorithm to obtain a solution (Zhou &
Sch¨olkopf, 2005):


_Mi,j[(][k][)]_ **F[(]j,[k]:[)]** [+][ β]i,i[(][k][)][X][i,][:][,][ for all][ i][ ∈] [[][N] []][,] (11)

_Di,iDj,j_


**F[(]i,[k]:[+1)]** = αi,i[(][k][)]


_j=1_


with M[(][k][)] _∈_ R[N] _[×][N]_, α[(][k][)] = diag(α1[(][k],1[)][, . . ., α]N,N[(][k][)] [)][,][ β][(][k][)][ = diag(][β]1[(][k],1[)][, . . ., β]N,N[(][k][)] [)][ updated by]

_p−2_

_Mi,j[(][k][)]_ [=][ W][i,j] _Wi,j_ **F[(]i,[k]:[)]** _Wi,j_ **F[(]j,[k]:[)]** _, for all i, j_ [N ], (12)

s _Di,i_ _[−]_ s _Dj,j_ _∈_

_αi,i[(][k][)]_ [= 1] _N_ _Mi,j[(][k][)]_ + [2][µ] _,_ _βi,i[(][k][)]_ [= 2][µ] for all i [N ], (13)
 _Di,i_ _p_  _p [α][i,i][,]_ _∈_

_j=1_

 X

Note that in Eq. (12), when _WDi,ii,j_ **[F]i,[(][k]:[)]** _[−]_ _WDj,ji,j_ **[F]j,[(][k]:[)]** = 0, we set Mi,j[(][k][)] [= 0][. It is easy to see]

that Eq. (10) is the special cases of Eq. (q 14) withq _p = 2._


-----

**Remark 1 (Discussion on p = 1). For p = 1, when f is a real-valued function (c = 1), ∆1f is a**
_step function, which could make the stationary condition of the objective function Eq. (8) become_
_problematic. Additionally,∆1f is not continuous at_ ( _f_ )([i, j]) = 0. Therefore, p = 1 is not
_∥_ _∇_ _∥_
_allowed when f is a real value function. On the other hand, note that there is a Frobenius norm_
_in ∆pf_ _. When f is a vector-valued function (c > 1), the step function in ∆1f only exists on the_
_axes. The stationary condition will be fine if the node embeddings F are not a matrix of vectors that_
_has only one non-zero element, which is true for many graphs. p = 1 may work for these graphs._
_Overall, we suggest to use p > 1 in practice but p = 1 may work for graphs with multiple channel_
_signals as well. We conduct experiments for p > 1 (e.g., p = 1.5, 2, 2.5) and p = 1 in Sec. 5._

3.2 _p-LAPLACIAN MESSAGE PASSING AND_ _[p]GNN ARCHITECTURE_

_p-Laplacian Message Passing. Rewrite Eq. (11) in a matrix form we obtain_

**F[(][k][+1)]** = α[(][k][)]D[−][1][/][2]M[(][k][)]D[−][1][/][2]F[(][k][)] + β[(][k][)]X. (14)

Eq. (14) provides a new message passing mechanism, named p-Laplacian message passing.

**Remark 2. αD[−][1][/][2]MD[−][1][/][2]** _in Eq. (14) can be regarded as the learned aggregation weights at_
_each step for message passing. It suggests that p-Laplacian message passing could adaptively tune_
_the aggregation weights during the course of learning, which will be demonstrated theoretically and_
_empirically in the sequel of this paper. βX in Eq. (14) can be regarded as a residual unit, which_
_helps the model escape from the oversmoothing issue (Chien et al., 2021)._

We present the following theorem to show the shrinking property of p-Laplacian message passing.

**Theorem 2 (Shrinking Property of p-Laplacian Message Passing). Given a graph G = (V, E, W)**
_with node features X, β[(][k][)], F[(][k][)], M[(][k][)], α[(][k][)]_ _are updated accordingly to Equations (11) to (13) for_
_k = 0, 1, . . ., K and F[(0)]_ = X. Then there exist some positive real value µ > 0 which depends on
**X, G, p and p > 1 such that**
_p(F[(][k][+1)])_ _p(F[(][k][)])._
_L_ _≤L_

Proof see Appendix D.2. Thm. 2 shows that with some proper positive real value µ and p > 1,
the loss of the objective function Eq. (8) is guaranteed to decline after taking one step p-Laplacian
message passing. Thm. 2 also demonstrates that the iteration Equations (11) to (13) is guaranteed to
converge for p > 1 with some proper µ which is chosen depends on the input graph and p.

_pGNN Architecture. We design the architecture of pGNNs using p-Laplacian message passing._
Given node features X ∈ R[N] _[×][c], the number of node labels L, the number of hidden units h, the_
maximum number of iterations K, and M, α, and β updated by Equations (12) and (13) respectively, we give the _[p]GNN architecture as following:_

**F[(0)]** = ReLU(XΘ[(1)]), (15)

**F[(][k][+1)]** = α[(][k][)]D[−][1][/][2]M[(][k][)]D[−][1][/][2]F[(][k][)] + β[(][k][)]F[(0)], _k = 0, 1, . . ., K −_ 1, (16)

**Z = softmax(F[(][K][)]Θ[(2)]),** (17)

where Z ∈ R[N] _[×][L]_ is the output propbability matrix with Zi,j is the estimated probability that the
label at node i ∈ [N ] is j ∈ [L] given the features X and the graph G, Θ[(1)] _∈_ R[c][×][h] and Θ[(2)] _∈_
R[h][×][L] are the first- and the second-layer parameters of the neural network, respectively.

**Remark 3 (Connection to existing GNN variants). The message passing scheme of** _[p]GNNs is dif-_
_ferent from that of several GNN variants (say, GCN, GAT, and GraphSage), which repeatedly stack_
_message passing layers. In contrast, it is similar with SGC (Wu et al., 2019), APPNP (Klicpera_
_et al., 2019), and GPRGNN (Chien et al., 2021). SGC is an approximation to the closed-form in_
_Eq. (9) (Fu et al., 2020). By Thm. 1, it is easy to see that APPNP, which uses PPR to propagate the_
_node embeddings, is analogical to_ _[p]GNN with p = 2, termed as_ [2][.][0]GNN. APPNP and [2][.][0]GNN work
_analogically and effectively on homophilic graphs._ [2][.][0]GNN can also work effectively on heterophilic
_graphs by letting Θ[(2)]_ _be negative. However, APPNP fails on heterophilic graphs as its PPR weights_


-----

_are fixed (Chien et al., 2021). Unlike APPNP, GPRGNN, which adaptively learn the generalized_
_PageRank (GPR) weights, works similarly to_ [2][.][0]GNN on both homophilic and heterophilic graphs.
_However, GPRGNN needs more supervised information in order to learn optimal GPR weights. On_
_the contrary,_ _[p]GNNs need less supervised information to obtain similar results because Θ[(2)]_ _acts_
_like a hyperplane for classification._ _[p]GNNs could work better under weak supervised information._
_Our analysis is also verified by the experimental results in Sec. 5._

We also provide an upper-bounding risk of _[p]GNNs by Thm. 4 in Appendix C.1 to study the effect of_
the hyperparameter µ on the performance of _[p]GNNs. Thm. 4 shows that the risk of_ _[p]GNNs is upper-_
bounded by the sum of three terms: the risk of label prediction using only the original node features
**X, the norm of p-Laplacian diffusion on X, and the magnitude of the noise in X. µ controls the**
trade-off between these three terms. The smaller µ, the more weights on the p-Laplacian diffusion
term and the noise term and the less weights on the the other term and vice versa.

4 SPECTRAL VIEWS OF p-LAPLACIAN MESSAGE PASSING

In this section, we theoretically demonstrate that p-Laplacian message passing is an approximation
of a polynomial graph filter defined on the spectral domain of p-Laplacian. We show by spectral
analysis that p-Laplacian message passing works simultaneously as low-pass and high-pass filters.

_p-Eigenvalues and p-Eigenvectors of the Graph p-Laplacian._ We first introduce the definitions of p-eigenvalues and p-eigenvectors of p-Laplacian. Let ϕp : R → R defined as ϕp(u) =
_∥u∥[p][−][2]u, for u ∈_ R, u ̸= 0. Note that ϕ2(u) = u. For notational simplicity, we denote by
_ϕp(u) = (ϕp(u1), . . ., ϕp(uN_ ))[⊤] for u ∈ R[N] and Φp(U) = (ϕp(U:,1), . . ., ϕp(U:,N )) for
**U ∈** R[N] _[×][N]_ and U:,i ∈ R[N] is the i[th] column vector of U.

**Definition 4 (p-Eigenvector and p-Eigenvalue). A vector u ∈** R[N] _is a p-eigenvector of ∆p if it_
_satisfies the equation_
(∆pu)i = λϕp(ui), _for all i ∈_ [N ],
_where λ ∈_ R is a real value referred as a p-eigenvalue of ∆p associated with the p-eigenvector u.

**Definition 5 (p-Orthogonal (Luo, Huang, Ding, and Nie, 2010)). Given two vectors u, v ∈** R[N] _with_
**u, v ̸= 0, we call that u and v is p-orthogonal if ϕp(u)[⊤]ϕp(v) =** _i=1_ _[ϕ][p][(][u][i][)][ϕ][p][(][v][i][) = 0][.]_

Luo et al. (2010) demonstrated that the p-eigenvectors of ∆p are p-orthogonal to each other (see

[P][N]

Thm. 5 in Appendix C.2 for details). Therefore, the space spanned by the multiple p-eigenvectors
of ∆p is p-orthogonal. Additionally, we demonstrate that the p-eigen-decomposition of ∆p is given
by: ∆p = Φp(U)ΛΦp(U)[⊤] (see Thm. 6 in Appendix C.3 for details), where U is a matrix of
_p-eigenvectors of ∆p and Λ is a diagonal matrix in which the diagonal is the p-eigenvalues of ∆p._

**Graph Convolutions based on p-Laplacian. Based on Thm. 5, the graph Fourier Transform** _f[ˆ] of_
any function f on the vertices of G can be defined as the expansion of f in terms of Φ(U) where U is
the matrix of p-eigenvectors of ∆p: _f[ˆ] = Φp(U)[⊤]f_ . Similarly, the inverse graph Fourier transform
is then given by: f = Φp(U) f[ˆ]. Therefore, a signal X ∈ R[N] _[×][c]_ being filtered by a spectral filter
_gθ can be expressed formally as: gθ ⋆_ **X = Φp(U)ˆgθ(Λ)Φp(U)[⊤]X, where Λ denotes a diagonal**
matrix in which the diagonal corresponds to the p-eigenvalues _λl_ _l=0,...,N_ 1 of ∆p and ˆgθ(Λ)
_{_ _}_ _−_
denotes a diagonal matrix in which the diagonal corresponds to spectral filter coefficients. Let ˆgθ be
a polynomial filter defined as ˆgθ = _k=0_ _[θ][k][λ]l[k][, where the parameter][ θ][ = [][θ][0][, . . ., θ][K][−][1][]][⊤]_ _[∈]_ [R][K]
is a vector of polynomial coefficients. By the p-eigen-decomposition of p-Laplacian, we have

_K−1[P][K][−][1]_ _K−1_

_gθ ⋆_ **X ≈** _θkΦp(U)Λ[k]Φp(U)[⊤]X =_ _θk∆[k]p[X][.]_ (18)

_k=0_ _k=0_

X X

**Theorem 3. The K-step p-Laplacian message passing is a K-order polynomial approximation to**
_the graph filter given by Eq. (18)._

Proof see Appendix D.3. Thm. 3 indicates that p-Laplacian message passing mechanism is implicitly
a polynomial spectral filter defined on the spectral domain of p-Laplacian.


-----

**Spectral Analysis of p-Laplacian Message Passing. Here, we analyze the spectral propecties of**
_p-Laplacian message passing. We can approximately view p-Laplacian message pasing as a filter of_
a linear combination of K spectral filters g(Λ)[(0)], g(Λ)[(1)], . . ., g(Λ)[(][K][−][1)] with each spectral filter

defined to be g(Λ)[(][k][)] := (αD[−][1][/][2]MD[−][1][/][2])[k] where Mi,j = Wi,j∥ _WDi,ii,j_ **[F][i,][:][ −]** _WDj,ji,j_ **[F][j,][:][∥][p][−][2]**

for i, j [N ] and F is the matrix of node embeddings. We can study the properties ofq q _p-Laplacian_
_∈_
message passing by studying the spectral properties of αD[−][1][/][2]MD[−][1][/][2] as given below.

**Proposition 1. Given a connected graph G = (V, E, W) with node embeddings F and the p-**
_Laplacian ∆p with its p-eigenvectors_ **u[(][l][)]** _l=0,1,...,N_ 1 and the p-eigenvalues _λl_ _l=0,1,...,N_ 1.
_{_ _}_ _−_ _{_ _}_ _−_
_Let gp(λi−1) := αi,i_ _j_ _[D]i,i[−][1][/][2]Mi,jDj,j[−][1][/][2]_ _for i ∈_ [N ] be the filters defined on the spectral domain

_of ∆p, where Mi,j = Wi,j_ _f_ ([i, j]) _, (_ _f_ )([i, j]) is the graph gradient of the edge between

P _∥∇_ _∥[p][−][2]_ _∇_

_node i and j and ∥∇f_ (i)∥ _is the norm of graph gradient at i. Ni denotes the number of edges_
_connected to i, Nmin = min{Nj}j∈[N_ ], and k = arg maxj({|u[(]j[l][)][|][/] _Dl,l}j∈[N_ ];l=0,...,N _−1), then_
p

_1. When p = 2, gp(λi−1) works as both low-pass and high-pass filters._

_2. When p > 2, if ∥∇f_ (i)∥≤ 2[(][p][−][1)][/][(][p][−][2)], gp(λi−1) works as both low-pass and high-pass
_filters on node i and gp(λi−1) works as low-pass filters on i when ∥∇f_ (i)∥≥ 2[(][p][−][1)][/][(][p][−][2)].

_3. When 1 ≤_ _p < 2, if 0 ≤∥∇f_ (i)∥≤ 2(2[√]Nk)[1][/][(][p][−][2)], gp(λi−1) works as low-pass
_filters on node∥∇f_ (i)∥≥ 2 _i2[√] andNk g1p/((pλ−i−2)1. Specifically, when) works as both low-pass and high-pass filters on p = 1, Nk can be replaced by N imin when._

  


_̸_
adaptively works as low-pass or both low-pass and high-pass filters on node i in terms of the degree
of local node embedding variation around i, i.e. the norm of the graph gradient ∥∇f (i)∥ at node
_i. When p = 2, p-Laplacian message passing works as both low-pass and high-pass filters on node_
_i regardless of the value of ∥∇f_ (i)∥. When p > 2, p-Laplacian message passing works as lowpass filters on node i for large ∥∇f (i)∥ and works as both low-pass and high-pass filters for small
_∥∇f_ (i)∥. Therefore, _[p]GNNs with p > 2 can work very effectively on graphs with strong homophily._
When 1 ≤ _p < 2, p-Laplacian message passing works as low-pass filters for small ∥∇f_ (i)∥ and
works as both low-pass and high-pass filters for large ∥∇f (i)∥. Thus, _[p]GNNs with 1 ≤_ _p < 2 can_
work effectively on graphs with low homophily, i.e. heterophilic graphs. The results here confirms
our analysis of the aggregation weights of p-Laplacian message passing presented in Thm. 2.


5 EMPIRICAL STUDIES

In this section, we empirically study the effectiveness of _[p]GNNs for semi-supervised node classifica-_
tion using and real-world benchmark and synthetic datasets with heterophily and strong homophily.
The experimental results are also used to validate our theoretical findings presented previously.

**Datasets and Experimental Setup. We use seven homophilic benchmark datasets: citation graphs**
Cora, CiteSeer, PubMed (Sen et al., 2008), Amazon co-purchase graphs Computers, Photo, coauthor
graphs CS, Physics (Shchur et al., 2018), and six heterophilic benchmark datasets: Wikipedia graphs
Chameleon, Squirrel (Rozemberczki et al., 2021), the Actor co-occurrence graph, webpage graphs
Wisconsin, Texas, Cornell (Pei et al., 2020). The node classification tasks are conducted in the
transductive setting. Following Chien et al. (2021), we use the sparse splitting (2.5%/2.5%/95%)
and the dense splitting (60%/20%/20%) to randomly split the homophilic and heterophilic graphs
into training/validation/testing sets, respectively. Dataset statistics and their levels of homophily
are presented in Appendix E.

**Baselines. We compare** _[p]GNN with seven models, including MLP, GCN (Kipf & Welling, 2017),_
SGC (Wu et al., 2019), GAT (Velickovic et al., 2018), JKNet (Xu et al., 2018), APPNP (Klicpera
et al., 2019), GPRGNN (Chien et al., 2021). We use the Pytorch Geometric library (Fey & Lenssen,
2019) to implement all baselines except GPRGNN. For GPRGNN, we use the code released by the
authors[4]. The details of hyperparameter settings are deferred to Appendix E.3.

[4https://github.com/jianhao2016/GPRGNN](https://github.com/jianhao2016/GPRGNN)


-----

Table 1: Heterophilious results. Averaged accuracy (%) for 100 runs. Best results outlined in bold
and the results within 95% confidence interval of the best results are outlined in underlined bold.

Method Chameleon Squirrel Actor Wisconsin Texas Cornell


MLP 48.02 1.72 33.80 1.05 39.68 1.43 93.56 3.14 79.50 10.62 80.30 11.38
_±_ _±_ _±_ _±_ _±_ _±_
GCN 34.54 2.78 25.28 1.55 31.28 2.04 61.93 3.00 56.54 17.02 51.36 4.59
_±_ _±_ _±_ _±_ _±_ _±_
SGC 34.76 4.55 25.49 1.63 30.98 3.80 66.94 2.58 59.99 9.95 44.39 5.88
_±_ _±_ _±_ _±_ _±_ _±_
GAT 45.16 2.10 31.41 0.98 34.11 1.28 65.64 6.29 56.41 13.01 43.94 7.33
_±_ _±_ _±_ _±_ _±_ _±_
JKNet 33.28 3.59 25.82 1.58 29.77 2.61 61.08 3.71 59.65 12.62 55.34 4.43
_±_ _±_ _±_ _±_ _±_ _±_
APPNP 36.18 2.81 26.85 1.48 31.26 2.52 64.59 3.49 82.90 5.08 66.47 9.34
_±_ _±_ _±_ _±_ _±_ _±_
GPRGNN 43.67 2.27 31.27 1.76 36.63 1.22 88.54 4.94 80.74 6.76 78.95 8.52
_±_ _±_ _±_ _±_ _±_ _±_

1122....5005GNNGNNGNNGNN **48484848....74867780±±±±1111....95628777 33 33 33 33....33607579±±±±1111....45475045 40 40 3940....80076235±±±±1111....31172535 95 958791....08153724±±±±2222....69760601 87 84 8483....06460196±±±±7766....41798027 82 727870....47310416±±±±6888....87842262**

**Superior Performance on Real-World Heterophilic Datasets. The results on homophilic bench-**
mark datasets are deferred to Appendix F.1, which show that _[p]GNNs obtains competitive perfor-_
mance against state-of-the-art GNNs on homophilic datasets. Table 1 summarizes the results on
_heterophilic benchmark datasets. Table 1 shows that_ _[p]GNNs significantly dominate the baselines_
and [1][.][0]GNN obtains the best performance on all heterophilic graphs except the Texas dataset. For
Texas, [2][.][0]GNN is the best. We also observe that MLP works very well and significantly outperforms
most GNN baselines, which indicates that the graph topology is not informative for label prediction on these heterophilic graphs. Therefore, propagating and transforming node features over the
graph topology could lead to worse performance than MLP. Unlike ordinary GNNs, _[p]GNNs can_
adaptively learn aggregation weights and ignore edges that are not informative for label prediction
and thus could work better. It confirms our theoretical findings presented in previous sections. Note
that GAT can also learn aggregation weights, i.e. the attention weights. However, the aggregation
weights learned by GAT are significantly distinct from that of _[p]GNNs, as we will show following._

|0|un|nifor|rm|Col5|Col6|
|---|---|---|---|---|---|
|0||||||
|0||||||
|0||||||
|0||||||
|0||||||
|0||||||
|0||||||
|0||||||


Computers

8000 uniform 1.0GNN (Acc: 85.24%) 1.5GNN (Acc: 85.3%) 2.5GNN (Acc: 83.68%) GAT (Acc: 82.73%)

7000

6000

5000

4000

# of nodes30002000

1000

0 0 2 4 entropy6 8 10 12 14 0 2 4 entropy6 8 10 12 14 0 2 4 entropy6 8 10 12 14 0 2 4 entropy6 8 10 12 14 0 2 4 entropy6 8 10 12 14

Wisconsin

200 uniform 1.0GNN (Acc: 98.15%) 1.5GNN (Acc: 96.3%) 2.5GNN (Acc: 84.26%) GAT (Acc: 62.96%)

175

150

125

100

# of nodes 7550

25

0 1 0 1 entropy2 3 4 5 1 0 1 entropy2 3 4 5 1 0 1 entropy2 3 4 5 1 0 1 entropy2 3 4 5 1 0 1 entropy2 3 4 5


Figure 1: Aggregation weight entropy distribution of graphs. Low entropy means high degree of
concentration, vice versa. An entropy of zero means all aggregation weights are on one source node.

**Interpretability of the Learned Aggregation Weights of** _[p]GNNs. We showcase the interpretabil-_
ity of the learned aggregation weights αi,iDi,i[−][1][/][2]Mi,jDj,j[−][1][/][2] of _[p]GNNs by studying its entropy_
distribution, along with the attention weights of GAT on real-world datasets. Denote {Ai,j}j∈Ni
as the aggregation weights of node i and its neighbors. For GAT, {Ai,j}j∈Ni are referred as the
attention weights (in the first layer) and for _[p]GNNs are αi,iDi,i[−][1][/][2]Mi,jDj,j[−][1][/][2]. For any node i,_
_{Ai,j}j∈Ni forms a discrete probability distribution over all its neighbors with the entropy given by_
_H({Ai,j}j∈Ni_ ) = − [P]j∈Ni _[A][i,j][ log(][A][i,j][)][. Low entropy means high degree of concentration and]_

vice versa. An entropy of zero means all aggregation weights or attentions are on one source node.
The uniform distribution has the highest entropy of log(Di,i). Fig. 1 reports the results on Computers, Wisconsin and we defer more results on other datasets to Appendix F.2 due to space limit.
Fig. 1 shows that the aggregation weight entropy distributions of GAT and _[p]GNNs on Computers_
(homophily) are both similar to the uniform case. It indicates the original graph topology of Computers is very helpful for label prediction and therefore GNNs could work very well on Computers.
However, for Wisconsin (heterophily), the entropy distribution of _[p]GNNs is significantly different_
from that of GAT and the uniform case. Most entropy of _[p]GNNs is around zero, which means_
that most aggregation weights are on one source node. It states that the original graph topology of
Wisconsin is not helpful for label prediction, which explains why MLP works well on Wisconsin.


-----

**Wisconsin**

**0.25** **0.5**


**Computers**

**0.25** **0.5**


**100**

**90**

**80**

**70**

**60**

**50**

**40**


**100**

**90**

**80**

**70**

**60**

**50**

**40**

**30**

**20**

**10**


**MLP**

**GCN**

**SGC**

**GAT**


**MLP**

**GCN**

**SGC**

**GAT**

**JKNet**

**APPNP**

**GPRGNN**

**1.0-GNN**

**1.5-GNN**

**2.0-GNN**

**2.5-GNN**


**JKNet**

**APPNP**

**GPRGNN**

**1.0-GNN**

**1.5-GNN**

**2.0-GNN**

**2.5-GNN**


Figure 3: Averaged accuracy (%) on graphs with noisy edges for 20 runs. Best view in colors.

On the contrary, the entropy distribution of GAT is similar to the uniform case and therefore GAT
works similarly to GCN and is significantly worse than _[p]GNNs on Wisconsin. Similar results can_
be observed on the experiments on more datasets in Appendix F.2.

**Results on cSBM Datasets.** We exam the **cSBM sparse split**
performance of _[p]GNNs on heterophilic graphs_ **100** **MLP**
whose topology is informative for label pre- **GCN**
diction using synthetic graphs generated by **90** **SGC**
cSBM (Deshpande et al., 2018) with ϕ **80** **GAT**
1, 0.75, . . ., 1 . We use the same settings
_{−_ _−_ _}_ **70** **APPNP**
of cSBM used in Chien et al. (2021). Due to
the space limit, we refer the readers to Chien **60**
et al. (2021) for more details of cSBM dataset. **1.0-GNN**
Fig. 2 reports the results on cSBM using sparse **50** **1.5-GNN**

**2.0-GNN**

splitting (for results on cSBM with dense split- **40**
ting see Appendix F.3). Fig. 2 shows that **-1** **-0.75** **-0.5** **-0.25** **0** **0.25** **0.5** **0.75** **1** **2.5-GNN**
when ϕ ≤−0.5 (heterophilic graphs), [2][.][0]GNN **ϕ**
obtains the best performance and _[p]GNNs and_ Figure 2: Averaged accuracy (%) on cSBM
GPRGNN significantly dominate the others. It (sparse split) for 20 runs. Best view in colors.
validates the effectiveness of _[p]GNNs on het-_
erophilic graphs. Moreover, [2][.][0]GNN works better than GPRGNN and it again confirms that
2.0GNN is more superior under weak supervision (2.5% training rate), as stated in Remark 3.
Note that [1][.][0]GNN and [1][.][5]GNN are not better than [2][.][0]GNN, the reason could be the iteration algorithms Eq. (11) with p = 1, 1.5 are not as stable as the one with p = 2. When the graph topology
is almost non-informative for label prediction (ϕ = −0.25, 0), The performance of _[p]GNNs is close_
to MLP and they outperform the other baselines. Again, it validates that _[p]GNNs can erase non-_
informative edges and work as well as MLP and confirms the statements in Thm. 4. When the graph
is homophilic (ϕ 0.25), [1][.][5]GNN is the best on weak homophilic graphs (ϕ = 0.25, 0.5) and
_p_ _≥_
GNNs work competitively with all GNN baselines on strong homophilic graphs (ϕ ≥ 0.75).

**Results on Datasets with Noisy Edges. We conduct experiments to evaluate the performance of**
_pGNNs on graphs with noisy edges by randomly adding edges to the graphs and randomly remove_
the same number of original edges. We define the random edge rate as r := [#][random edges]#all edges . The exper
iments are conducted on 4 homophilic datasets (Computers, Photo, CS, Physics) and 2 heterophilic
datasets (Wisconsin, Texas) with r = 0.25, 0.5, 1. Fig. 3 reports the results on Computers, Wisconsin and we defer more results to Appendix F.4. Fig. 3 shows that _[p]GNNs significantly outperform_
all baselines. Specifically, [1][.][5]GNN obtains the best performance on Computers, and [1][.][5]GNN and
2.0GNN even work as well as MLP on Computers with completely random edges (r = 1). For
Wisconsin, [1][.][0]GNN is the best, and [1][.][0]GNN and [1][.][5]GNN significantly dominate the others. We also
observed that APPNP and GPRGNN, whose architectures are analogical to [2][.][0]GNN, also work better than other GNNs. Nevertheless, they are significantly outperformed by _[p]GNNs overall. Similar_
results can be observed in the experiments conducted on more datasets as presented in Appendix F.4.
CONCLUSION. We have addressed the problem of generalizing GNNs to heterophilic graphs
and graphs with noisy edges. To this end, we derived a novel p-Laplacian message passing scheme
from a discrete regularization framework and proposed a new _[p]GNN architecture. We theoretically_
demonstrate our method works as low-pass and high-pass filters and thereby applicable to both
homophilic and heterophilic graphs. We empirically validate our theoretical results and show the
advantages of our methods on heterophilic graphs and graphs with non-informative topologies.


-----

REPRODUCIBILITY STATEMENT

In order to ensure reproducibility, we have made the efforts in the following respects: (1) Provide a
sampled code as the supplementary material; (2) Provide self-contained proofs of the main claims
in Appendices C and D; (3) Provide more details on experimental configurations in Appendix E and
experimental results in Appendix F. All the datasets are publicly available as described in the main
text. We will fully release our training and evaluation code, as well as our train/validation/test splits.

REFERENCES

Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, and Alexander A. Alemi. Watch your step:
Learning node embeddings via graph attention. In Samy Bengio, Hanna M. Wallach, Hugo
Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neu_ral Information Processing Systems 31, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada,_
pp. 9198–9208, 2018.

Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, and Joonseok Lee. N-GCN: multi-scale graph
convolution for semi-supervised node classification. In Amir Globerson and Ricardo Silva (eds.),
_Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019,_
_Tel Aviv, Israel, July 22-25, 2019, volume 115 of Proceedings of Machine Learning Research, pp._
841–851. AUAI Press, 2019.

S. Amghibech. Eigenvalues of the discrete p-laplacian for graphs. Ars Combinatoria, 67:283–302,
2003.

James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Daniel D. Lee,
Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances
_in Neural Information Processing Systems 29, NeurIPS 2016, December 5-10, 2016, Barcelona,_
_Spain, pp. 1993–2001, 2016._

Francis Bach and Michael Jordan. Learning spectral clustering. Advances in Neural Information
_Processing Systems, NIPS 2004, 16(2):305–312, 2004._

Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vin´ıcius Flores
Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner,
C¸ aglar G¨ulc¸ehre, H. Francis Song, Andrew J. Ballard, Justin Gilmer, George E. Dahl, Ashish
Vaswani, Kelsey R. Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan
Wierstra, Pushmeet Kohli, Matthew Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu.
Relational inductive biases, deep learning, and graph networks. CoRR, abs/1806.01261, 2018.

Mikhail Belkin and Partha Niyogi. Towards a theoretical foundation for laplacian-based manifold
methods. Journal of Computer and System Sciences, 74(8):1289–1308, 2008.

Mikhail Belkin, Irina Matveeva, and Partha Niyogi. Regularization and semi-supervised learning on
large graphs. In The 17th Annual Conference on Learning Theory, COLT 2004, Banff, Canada,
_July 1-4, 2004, volume 3120, pp. 624–638, 2004._

Misha Belkin, Partha Niyogi, and Vikas Sindhwani. On manifold regularization. In Robert G. Cowell and Zoubin Ghahramani (eds.), Proceedings of the Tenth International Workshop on Artificial
_Intelligence and Statistics, AISTATS 2005, Bridgetown, Barbados, January 6-8, 2005. Society for_
Artificial Intelligence and Statistics, 2005.

Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In The 2nd International Conference on Learning Representations,
_ICLR 2014, Banff, AB, Canada, April 14-16, 2014, 2014._

Thomas B¨uhler and Matthias Hein. Spectral clustering based on the graph p-laplacian. In Andrea Pohoreckyj Danyluk, L´eon Bottou, and Michael L. Littman (eds.), Proceedings of the 26th
_Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada,_
_June 14-18, 2009, volume 382 of ACM International Conference Proceeding Series, pp. 81–88._
ACM, 2009.


-----

Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via
importance sampling. In 6th International Conference on Learning Representations, ICLR 2018,
_Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018._

Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank
graph neural network. In 9th International Conference on Learning Representations, ICLR 2021,
_Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021._

Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. In Advances in Neural Information Processing
_Systems 29, NeurIPS 2016, December 5-10, 2016, Barcelona, Spain, pp. 3837–3845, 2016._

Yash Deshpande, Subhabrata Sen, Andrea Montanari, and Elchanan Mossel. Contextual stochastic
block models. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o
Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems
_31, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada, pp. 8590–8602, 2018._

David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael G´omez-Bombarelli, Timothy Hirzel, Al´an Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for
learning molecular fingerprints. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi
Sugiyama, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 28,
_NeurIPS 2015 December 7-12, 2015, Montreal, Quebec, Canada, pp. 2224–2232, 2015._

Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric.
_CoRR, abs/1903.02428, 2019._

Guoji Fu, Yifan Hou, Jian Zhang, Kaili Ma, Barakeel Fanseu Kamhoua, and James Cheng. Understanding graph neural networks from graph signal denoising perspectives. CoRR, abs/2006.04386,
2020.

Fernando Gama, Joan Bruna, and Alejandro Ribeiro. Stability properties of graph neural networks.
_IEEE Transactions on Signal Processing, 68:5680–5695, 2020._

Vikas K. Garg, Stefanie Jegelka, and Tommi S. Jaakkola. Generalization and representational limits of graph neural networks. In Proceedings of the 37th International Conference on Machine
_Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine_
_Learning Research, pp. 3419–3430. PMLR, 2020._

Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Ad_vances in Neural Information Processing Systems 17, NIPS 2004, December 13-18, 2004, Van-_
_couver, British Columbia, Canada], pp. 529–536, 2004._

William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Advances in Neural Information Processing Systems 30, NeurIPS 2017, 4-9 December
_2017, Long Beach, CA, USA, pp. 1024–1034, 2017._

Matthias Hein. Uniform convergence of adaptive graph-based regularization. In The 19th Annual
_Conference on Learning Theory, COLT 2006, Pittsburgh, PA, USA, June 22-25, 2006, volume_
4005, pp. 50–64, 2006.

Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured
data. CoRR, abs/1506.05163, 2015.

Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin
(eds.), Advances in Neural Information Processing Systems 33, NeurIPS 2020, December 6-12,
_2020, virtual, 2020._

Qian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R. Benson. Combining label
propagation and simple models out-performs graph neural networks. In 9th International Confer_ence on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenRe-_
view.net, 2021.


-----

Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France,
_April 24-26, 2017, Conference Track Proceedings, 2017._

Johannes Klicpera, Aleksandar Bojchevski, and Stephan G¨unnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In 7th International Conference on Learning
_Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019._

Guohao Li, Matthias M¨uller, Ali K. Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep
as cnns? In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul,
_Korea (South), October 27 - November 2, 2019, pp. 9266–9275. IEEE, 2019._

Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks
for semi-supervised learning. In Thirty-Second AAAI Conference on Artificial Intelligence, AAAI
_2018, New Orleans, Louisiana, USA, February 2-7, 2018, pp. 3538–3545, 2018._

Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard S. Zemel. Lanczosnet: Multi-scale deep
graph convolutional networks. In 7th International Conference on Learning Representations,
_ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019._

Fei Liu, Sounak Chakraborty, Fan Li, Yan Liu, and Aurelie C Lozano. Bayesian regularization via
graph laplacian. Bayesian Analysis, 9(2):449–474, 2014.

Jialu Liu and Jiawei Han. Spectral clustering. In Charu C. Aggarwal and Chandan K. Reddy (eds.),
_Data Clustering: Algorithms and Applications, pp. 177–200. CRC Press, 2013._

Xiaorui Liu, Wei Jin, Yao Ma, Yaxin Li, Hua Liu, Yiqi Wang, Ming Yan, and Jiliang Tang. Elastic
graph neural networks. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th Inter_national Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume_
139 of Proceedings of Machine Learning Research, pp. 6837–6849. PMLR, 2021.

Andreas Loukas. What graph neural networks cannot learn: depth vs width. In 8th International
_Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020._
OpenReview.net, 2020.

Dijun Luo, Heng Huang, Chris H. Q. Ding, and Feiping Nie. On the eigenvectors of p-laplacian.
_Machine Learning, 81(1):37–51, 2010._

Diego Marcheggiani and Ivan Titov. Encoding sentences with graph convolutional networks for
semantic role labeling. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel (eds.), Proceedings
_of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017,_
_Copenhagen, Denmark, September 9-11, 2017, pp. 1506–1515. Association for Computational_
Linguistics, 2017.

Boaz Nadler, Nathan Srebro, and Xueyuan Zhou. Semi-supervised learning with the graph laplacian:
The limit of infinite unlabelled data. Advances in Neural Information Processing Systems, NIPS
_2009, 22:1330–1338, 2009._

Partha Niyogi. Manifold regularization and semi-supervised learning: some theoretical analyses.
_Journal of Machine Learning Research, 14(1):1229–1250, 2013._

Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters.
_CoRR, abs/1905.09550, 2019._

Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node
classification. In 8th International Conference on Learning Representations, ICLR 2020, Addis
_Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020._

Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking:
Bringing order to the web. Technical report, Stanford InfoLab, 1999.

Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric
graph convolutional networks. In 8th International Conference on Learning Representations,
_ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020._


-----

Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal
_of Complex Networks, 9(2), 2021._

Victor Garcia Satorras and Joan Bruna Estrach. Few-shot learning with graph neural networks. In
_6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada,_
_April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018._

Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad.
Collective classification in network data. AI Magazine, 29(3):93–106, 2008.

Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G¨unnemann. Pitfalls
of graph neural network evaluation. CoRR, abs/1811.05868, 2018.

Vikas Sindhwani, Partha Niyogi, Mikhail Belkin, and Sathiya Keerthi. Linear manifold regularization for large scale semi-supervised learning. In Proceedings of the 22nd ICML Workshop on
_Learning with Partially Classified Training Data, volume 28, 2005._

Dejan Slepcev and Matthew Thorpe. Analysis of p-laplacian regularization in semi-supervised learning. CoRR, abs/1707.06213, 2017.

Alexander J. Smola and Risi Kondor. Kernels and regularization on graphs. In Bernhard Sch¨olkopf
and Manfred K. Warmuth (eds.), Computational Learning Theory and Kernel Machines, 16th
_Annual Conference on Computational Learning Theory and 7th Kernel Workshop, COLT/Kernel_
_2003, Washington, DC, USA, August 24-27, 2003, Proceedings, volume 2777 of Lecture Notes in_
_Computer Science, pp. 144–158. Springer, 2003._

Kiran Koshy Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li. Attention-based graph
neural network for semi-supervised learning. CoRR, abs/1803.03735, 2018.

Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine
_Learning Research, 9(86):2579–2605, 2008._

Jesper E. van Engelen and Holger H. Hoos. A survey on semi-supervised learning. Machine Learn_ing, 109(2):373–440, 2020._

Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua
Bengio. Graph attention networks. In 6th International Conference on Learning Representations,
_ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, 2018._

Petar Velickovic, William Fedus, William L. Hamilton, Pietro Li`o, Yoshua Bengio, and R. Devon
Hjelm. Deep graph infomax. In 7th International Conference on Learning Representations, ICLR
_2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019._

Saurabh Verma and Zhi-Li Zhang. Stability and generalization of graph convolutional neural networks. In The 25th ACM SIGKDD International Conference on Knowledge Discovery and Data
_Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pp. 1539–1548, 2019._

Ulrike von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395–416,
2007.

Hongwei Wang and Jure Leskovec. Unifying graph convolutional neural networks and label propagation. CoRR, abs/2002.06755, 2020.

Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Q. Weinberger.
Simplifying graph convolutional networks. In Proceedings of the 36th International Conference
_on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 6861–_
6871, 2019.

Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A
comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and
_Learning System, 32(1):4–24, 2021._

Zhang Xinyi and Lihui Chen. Capsule graph neural network. In 7th International Conference on
_Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net,_
2019.


-----

Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In Proceedings of
_the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm¨assan, Stock-_
_holm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp._
5449–5458. PMLR, 2018.

Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In 7th International Conference on Learning Representations, ICLR 2017 New Orleans,
_LA, USA, May 6-9, 2019, Conference Track Proceedings, 2019._

Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, and Jure Leskovec.
Hierarchical graph representation learning with differentiable pooling. In Samy Bengio, Hanna M.
Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.),
_Advances in Neural Information Processing Systems 31, NeurIPS 2018, December 3-8, 2018,_
_Montr´eal, Canada, pp. 4805–4815, 2018._

Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer:
Generating explanations for graph neural networks. In Advances in Neural Information Process_ing Systems 32, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pp. 9240–9251,_
2019.

Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna.
Graphsaint: Graph sampling based inductive learning method. In 8th International Conference
_on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-_
view.net, 2020.

Dengyong Zhou and Bernhard Sch¨olkopf. Regularization on discrete spaces. In The 27th DAGM
_Symposium, Vienna, Austria, August 31 - September 2, 2005, volume 3663, pp. 361–368, 2005._

Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Sch¨olkopf.
Learning with local and global consistency. In Sebastian Thrun, Lawrence K. Saul, and Bernhard
Sch¨olkopf (eds.), Advances in Neural Information Processing Systems 16, NIPS 2003, December
_8-13, 2003, Vancouver and Whistler, British Columbia, Canada], pp. 321–328. MIT Press, 2003._

Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,
Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI Open, 1:57–81, 2020.

Xueyuan Zhou and Mikhail Belkin. Semi-supervised learning by higher order regularization. In
Geoffrey J. Gordon, David B. Dunson, and Miroslav Dud´ık (eds.), Proceedings of the Fourteenth
_International Conference on Artificial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale,_
_USA, April 11-13, 2011, volume 15 of JMLR Proceedings, pp. 892–900. JMLR.org, 2011._

Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond
homophily in graph neural networks: Current limitations and effective designs. In Advances in
_Neural Information Processing Systems 33, NeurIPS 2020, December 6-12, 2020, virtual, 2020._

Jiong Zhu, Ryan A. Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K. Ahmed, and Danai
Koutra. Graph neural networks with heterophily. In 35th AAAI Conference on Artificial Intelli_gence, AAAI 2021, Virtual Event, February 2-9, 2021, pp. 11168–11176. AAAI Press, 2021._

Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In Tom Fawcett and Nina Mishra (eds.), Machine Learning,
_Proceedings of the Twentieth International Conference, ICML 2003, August 21-24, 2003, Wash-_
_ington, DC, USA, pp. 912–919. AAAI Press, 2003._

Marinka Zitnik and Jure Leskovec. Predicting multicellular function through multi-layer tissue
networks. Bioinform., 33(14):i190–i198, 2017.


-----

# Appendix

CONTENTS

**A Related Work** **16**

**B** **Discussions and Future Work** **17**

**C Additional Theorems** **18**

C.1 Theorem 4 (Upper-Bounding Risk of _[p]GNN)_ . . . . . . . . . . . . . . . . . . . . 18

C.2 Theorem 5 (p-Orthogonal Theorem (Luo et al., 2010)) . . . . . . . . . . . . . . . 18

C.3 Theorem 6 (p-Eigen-Decomposition of ∆p) . . . . . . . . . . . . . . . . . . . . . 18

C.4 Theorem 7 (Bounds of p-Eigenvalues) . . . . . . . . . . . . . . . . . . . . . . . . 19

**D Proof of Theorems** **19**

D.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

D.2 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

D.3 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

D.4 Proof of Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

D.5 Proof of Theorem 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

D.6 Proof of Theorem 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

D.7 Proof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

**E** **Dataset Statistics and Hyperparameters** **29**

E.1 Illustration of Graph Gradient and Graph Divergence . . . . . . . . . . . . . . . . 29

E.2 Dataset Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

E.3 Hyperparameter Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

**F** **Additional Experiments** **31**

F.1 Experimental Results on Homophilic Benchmark Datasets . . . . . . . . . . . . . 31

F.2 Experimental Results of Aggregation Weight Entropy Distribution . . . . . . . . . 31

F.3 Experimental Results on cSBM . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

F.4 Experimental Results on Graphs with Noisy Edges . . . . . . . . . . . . . . . . . 34

F.5 Experimental Results of Intergrating _[p]GNNs with GCN and JKNet . . . . . . . . ._ 36

F.6 Experimental Results of _[p]GNNs on PPI Dataset for Inductive Learning . . . . . . ._ 37

F.7 Experimental Results of _[p]GNNs on OGBN arXiv Dataset . . . . . . . . . . . . . ._ 37

F.8 Running Time of _[p]GNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ._ 38

F.9 Experimental Results on Benchmark Datasets for 64 Hidden Units . . . . . . . . . 38

F.10 Training Curves for p = 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

F.11 Visualization Results of Node Embeddings . . . . . . . . . . . . . . . . . . . . . 40


-----

A RELATED WORK

**Graph Neural Networks. Graph neural networks (GNNs) are a variant of neural networks for**
graph-structured data, which can propagate and transform the node features over the graph topology and exploit the information in the graphs. Graph convolutional networks (GCNs) are one type
of GNNs whose graph convolution mechanisms or the message passing schemes were mainly inspired by the field of graph signal processing. Bruna et al. (2014) defined a nonparametric graph
filter using the Fourier coefficients. Defferrard et al. (2016) introduced Chebyshev polynomial to
avoid computational expensive eigen-decomposition of Laplacian and obtain localized spectral filters. GCN (Kipf & Welling, 2017) used the first-order approximation and reparameterized trick to
simplify the spectral filters and obtain the layer-wise graph convolution. SGC (Wu et al., 2019)
further simplify GCN by removing non-linear transition functions between each layer. Chen et al.
(2018) propose importance sampling to design an efficient variant of GCN. Xu et al. (2018) explored
a jumping knowledge architecture that flexibly leverages different neighborhood ranges for each
node to enable better structure-aware representation. Atwood & Towsley (2016); Liao et al. (2019);
Abu-El-Haija et al. (2019) exploited multi-scale information by diffusing multi-hop neighbor information over the graph topology. Wang & Leskovec (2020) used label propagation to improve
GCNs. Klicpera et al. (2019) incorporated personalized PageRank with GCNs. Liu et al. (2021)
introduced a l1 norm-based graph smoothing term to enhance the local smoothnesss adaptivity of
GNNs. Hamilton et al. (2017); Zeng et al. (2020) proposed sampling and aggregation frameworks
to extent GCNs to inductive learning settings. Another variant of GNNs is graph attention networks (Velickovic et al., 2018; Thekumparampil et al., 2018; Abu-El-Haija et al., 2018), which use
attention mechanisms to adaptively learn aggregation weights based on the nodes features. There
are many other works on GNNs (Pei et al., 2020) (Ying et al., 2018; Xinyi & Chen, 2019; Velickovic et al., 2019; Zeng et al., 2020), we refer to Zhou et al. (2020); Battaglia et al. (2018); Wu et al.
(2021) for a comprehensive review. Most GNN models implicitly assume that the labels of nodes
and their neighbors should be the same or consistent, while it does not hold for heterophilic graphs.
Zhu et al. (2020) investigated the issues of GNNs on heterophilic graphs and proposed to separately
learn the embeddings of ego-node and its neighborhood. Zhu et al. (2021) proposed a framework to
model the heterophily or homophily levels of graphs. Chien et al. (2021) incorporated generalized
PageRank with graph convolution to adapt GNNs to heterophilic graphs.

There are also some works on the interpretability of GNNs proposed recently. Li et al. (2018); Ying
et al. (2019); Fu et al. (2020) showed that spectral graph convolutions work as conducting Laplacian
smoothing on the graph signals and Wu et al. (2019); NT & Maehara (2019) demonstrated that
GCN, SGC work as low-pass filters. Gama et al. (2020) studied the stability properties of GNNs.
Xu et al. (2019); Oono & Suzuki (2020); Loukas (2020) studied the expressiveness of GNNs. Verma
& Zhang (2019); Garg et al. (2020) work on the generalization and representation power of GNNs.

**Graph based Semi-supervised Learning. Graph-based semi-supervised learning works under the**
assumption that the labels of a node and its neighbors shall be the same or consistent. Many methods have been proposed in the last decade, such as Smola & Kondor (2003); Zhou et al. (2003);
Belkin et al. (2004) use Laplacian regularization techniques to force the labels of linked nodes to
be the same or consistent. Zhou & Sch¨olkopf (2005) introduce discrete regularization techniques to
impose different regularizations on the node features based on p-Laplacian. Lable propagation (Zhu
et al., 2003) recursively propagates the labels of labeled nodes over the graph topology and use
the convergence results to make predictions. To mention but a few, we refer to Zhou & Sch¨olkopf

(2005); van Engelen & Hoos (2020) for a more comprehensive review.


-----

B DISCUSSIONS AND FUTURE WORK

In this section, we discuss the future work of _[p]GNNs. Our theoretical results and experimental_
results could lead to several potential extensions of _[p]GNNs._

**New Paradigm of Designing GNN Architectures. We bridge the gap between discrete regulariza-**
tion framework, graph-based semi-supervised learning, and GNNs, which provides a new paradigm
of designing new GNN architectures. Following the new paradigm, researchers could introduce
more regularization techniques, e.g., Laplacian regularization (Smola & Kondor, 2003; Belkin et al.,
2004), manifold regularization (Sindhwani et al., 2005; Belkin et al., 2005; Niyogi, 2013), highorder regularization (Zhou & Belkin, 2011), Bayesian regularization (Liu et al., 2014), entropy regularization (Grandvalet & Bengio, 2004), and consider more explicit assumptions on graphs, e.g.
the homophily assumption, the low-density region assumption (i.e. the decision boundary is likely
to lie in a low data density region), manifold assumption (i.e. the high dimensional data lies on
a low-dimensional manifold), to develop new graph convolutions or message passing schemes for
graphs with specific properties and generalize GNNs to a much broader range of graphs. Moreover,
the paradigm also enables us to explicitly study the behaviors of the designed graph convolutions or
message passing schemes from the theory of regularization (Belkin & Niyogi, 2008; Niyogi, 2013;
Slepcev & Thorpe, 2017).

**Applications of** _[p]GNNs to learn on graphs with noisy topologies. The empirical results (as shown_
in Fig. 3 and Tables 6 and 7) on graphs with noisy edges show that _[p]GNNs are very robust to noisy_
edges, which suggests the applications of p-Laplacian message passing and _[p]GNNs on the graph_
learning scenarios where the graph topology could potentially be seriously intervened.

**Integrating with existing GNN architectures. As shown in Table 9, the experimental results on**
heterophilic benchmark datasets illustrate that integrating GCN, JKNet with _[p]GNNs can signifi-_
cantly improve their performance on heterophilic graphs. It shows that _[p]GNN could be used as a_
plug-and-play component to be integrated into existing GNN architectures and improve their performance on real-world applications.

**Inductive learning for** _[p]GNNs._ _[p]GNNs are shown to be very effective for inductive learning on_
PPI datasets as reported in Table 10. _pGNNs even outperforms GAT on PPI, while using much_
fewer parameters than GAT. It suggests the promising extensions of _[p]GNNs to inductive learning on_
graphs.


-----

C ADDITIONAL THEOREMS

C.1 THEOREM 4 (UPPER-BOUNDING RISK OF _[p]GNN)_

**Theorem 4 (Upper-bounding risks of** _[p]GNNs). Given a graph G = (V, E, W) with N nodes, let_
**X ∈** R[N] _[×][c]_ _be the node features and y ∈_ R[N] _be the node labels and M[(][k][)], α[(][k][)], β[k], F[k]_ _are_
_updated accordingly by Equations (12) to (14) for k = 0, 1, . . ., K −_ 1 and F[(0)] = X, K ∈ N.
_Assume that G is d-regular and the ground-truth node features X[∗]_ = X + ϵ, where ϵ ∈ R[N] _[×][c]_
_represents the noise in the node features and there exists a L-Lipschitz function σ : R[N]_ _[×][c]_ _→_ R[N]

_such that σ(X[∗]) = y. let ˜y[(][k][+1)]_ = α[(][k][)]D[−][1][/][2]M[(][k][)]D[−][1][/][2]σ(F[(][k][)]) + β[(][k][)]σ **F[(0)][], we have**

_N_ _N_

1  

_N_ _i=1_ _|yi −_ _y˜i| ≤_ _N[1]_ _i=1_ _βi,i[(][K][−][1)]_ _|yi −_ _σ(Xi,:)|_

X X


_K−2_

_k=0_

X


_K−2_

_l=k_

Y


_Mi,j[(][l][)]_


+ _[L]_

_N_

+ _[L]_


_αi,i[(][K][−][1)]_
_i=1_

X


∆[(]p[K][−][1)]F[(]i,[K]: _[−][1)]_


∆[(]p[k][)][X][i,][:]






_j=1_


(1 _βi,i[(][K][−][1)])_ **_ϵi,:_** _._
_−_ _∥_ _∥_
_i=1_

X


Proof see Appendix D.4. Thm. 4 shows that the risk of _[p]GNNs is upper-bounded by the sum of three_
terms: The first term of the r.h.s in the above inequation represents the risk of label prediction using
only the original node features X, the second term is the norm of p-Laplacian diffusion on the node
features X, and the third term is the magnitude of the noise in the node features. αi,i and βi,i control
the trade-off between these three terms and they are related to the hyperparameter µ in Eq. (10). The
smaller µ, the smaller βi,i and larger αi,i, thus the more important of the p-Laplacian diffusion term
but also the more effect from the noise. Therefore, for graphs whose topological information is not
helpful for label prediction, we could impose more weights on the first term by using a large µ so
that _[p]GNNs work more like MLPs which simply learn on node features. While for graphs whose_
topological information is helpful for label prediction, we could impose more weights on the second
term by using a small µ so that _[p]GNNs can benefit from p-Laplacian smoothing on node features._

In practice, to choose a proper value of µ one may first simply apply MLPs on the node features to
have a glance at the helpfulness of the node features. If MLPs work very well, there is not much
space for the graph’s topological information to further improve the prediction performance and
we may choose a large µ. Otherwise, there could be a large chance for the graph’s topological
information to further improve the performance and we should choose a small µ.

C.2 THEOREM 5 (p-ORTHOGONAL THEOREM (LUO ET AL., 2010))

**Theorem 5 (p-Orthogonal Theorem (Luo et al., 2010)). If u[(][l][)]** _and u[(][r][)]_ _are two eigenvectors of_
_p-Laplacian ∆p associated with two different non-zero eigenvalues λl and λr, W is symmetric and_
_p ≥_ 1, then u[(][l][)] _and u[(][r][)]_ _are p-orthogonal up to the second order Taylor expansion._

Thm. 5 implies that ϕp(u)[(][l][)][⊤]ϕp(u[(][r][)]) 0, for all l, r = 0, . . ., N 1 and λl = λr. Therefore, the
space spanned by the multiple eigenvectors of the graph ≈ _p-Laplacian is −_ _p-orthogonal. ̸_

C.3 THEOREM 6 (p-EIGEN-DECOMPOSITION OF ∆p)

**Theorem 6 (p-Eigen-Decomposition of ∆p). Given the p-eigenvalues** _λl_ R _l=0,1,...,N_ 1, and
_{_ _∈_ _}_ _−_
_the p-eigenvectors {u[(][l][)]_ _∈_ R[N] _}l=0,1,...,N_ _−1 of p-Laplacian ∆p and ∥u[(][l][)]∥p = ([P][N]i=1[(][u]i[(][l][)][)][p][)][1][/p][ =]_
1, let U be a matrix of p-eigenvectors with U = (u[(0)], u[(1)], . . ., u[(][N] _[−][1)]) and Λ be a diagonal_
_matrix with Λ = diag(λ0, λ1, . . ., λN_ 1), then the p-eigen-decomposition of p-Laplacian ∆p is
_−_
_given by_
∆p = Φp(U)ΛΦp(U)[⊤].
_When p = 2, it reduces to the standard eigen-decomposition of the Laplacian matrix._

Proof see Appendix D.5.


-----

C.4 THEOREM 7 (BOUNDS OF p-EIGENVALUES)

**Theorem 7 (Bounds of p-Eigenvalues). Given a graph G = (V, E, W), if G is connected and λ**
_is a p-eigenvalue associated with the p-eigenvector u of ∆p, let Ni denotes the number of edges_
_connected to node i, Nmin = min{Ni}i=1,2,...,N_ _, and k = arg max({|ui|/_ _Di,i}i=1,2,...,N_ ), then
p


_1. for p ≥_ 2, 0 ≤ _λ ≤_ 2[p][−][1];

_2. for 1 < p < 2, 0_ _λ_ 2[p][−][1][√]Nk;
_≤_ _≤_

_3. for p = 1, 0 ≤_ _λ ≤_ _[√]Nmin._

Proof see Appendix D.6.

D PROOF OF THEOREMS

D.1 PROOF OF THEOREM 1

_Proof. Let i be the one-hot indicator vector whose i-th element is one and the other elements are_
zero. Then, we can obtain the personalized PageRank on node i, denoted as πPPR(i), by using the
recurrent equation (Klicpera et al., 2019):

**_πPPR[(][k][+1)](i) = αD[−][1][/][2]WD[−][1][/][2]πPPR[(][k][)]_** [(][i][) +][ β][i][,]

where k is the iteration step, 0 < α < 1 and β = (1 − _α) represents the restart probability. Without_
loss of generality, suppose πPPR[(0)] [(][i][) =][ i][. Then we have,]


**_πPPR[(][k][)]_** [(][i][) =][ α][D][−][1][/][2][WD][−][1][/][2][π]PPR[(][k][−][1)](i) + βi

= αD[−][1][/][2]WD[−][1][/][2][ ]αD[−][1][/][2]W D[−][1][/][2]πPPR[(][k][−][2)](i) + βi + βi


= _αD[−][1][/][2]WD[−][1][/][2][][2]_ **_πPPR[(][t][−][2)][(][i][) +][ βα][D][−][1][/][2][WD][−][1][/][2][i][ +][ β][i]_**
 _k_ 1

_−_

= _αD[−][1][/][2]WD[−][1][/][2][][k]_ **_πPPR[(0)]_** [(][i][) +][ β] _αD[−][1][/][2]WD[−][1][/][2][][t]_ **_i_**

_t=0_

 X 

_k−1_

= _αD[−][1][/][2]WD[−][1][/][2][][k]_ **_i + β_** _αD[−][1][/][2]WD[−][1][/][2][][t]_ **_i_**

_t=0_

 X 

Since 0 < α < 1 and the eigenvalues of D[−][1][/][2]WD[−][1][/][2] in [−1, 1], we have

lim _αD[−][1][/][2]WD[−][1][/][2][][k]_ = 0,
_k→∞_



and we also have

_k−1_

lim _αD[−][1][/][2]WD[−][1][/][2][][t]_ = **IN** _αD[−][1][/][2]WD[−][1][/][2][][−][1]_ _._
_k→∞_ _t=0_ _−_

X  

Therefore,

**_πPPR(i) = lim_** PPR[(][i][) =][ β] **IN** _αD[−][1][/][2]WD[−][1][/][2][][−][1]_ **_i_**
_k→∞_ **_[π][(][k][)]_** _−_


= β (α∆2 + (1 − _α)IN_ )[−][1] **_i_**

= µ(∆2 + µIN )[−][1]i,

where we let α = 1+1µ [and][ β][ =] 1+µµ [,][ µ >][ 0][. Then the fully personalized PageRank matrix can be]

obtained by substituting i with IN :


**ΠPPR = µ(∆2 + µIN** )[−][1].


-----

D.2 PROOF OF THEOREM 2

_Proof. By the definition of_ _p(f_ ) in Eq. (8), we have for some positive real value µ, µ > 0
_L_


_p(F) = [1]_
_L_ 2

and by Eq. (12),

Then, we have


_Wi,j_

**Fi,:**
_Di,i_ _−_


_Wi,j_
**Fj,:**
_Dj,j_


+ µ **Fi,:** **Xi,:** _._

_i=1_ _∥_ _−_ _∥[2]_

X


_i=1_


_j=1_


_p−2_


_Wi,j_

_Di,i_ **F[(]i,[k]:[)]** _[−]_


_Wi,j_
_Dj,j_ **F[(]j,[k]:[)]**


_Mi,j[(][k][)]_ [:=][ W][i,j]


_p−2_

_Wi,j_

[ s] _Di,i_ **F[(]i,[k]:[)]** _[−]_


_∂_ _p(F[(][k][)])_
_L_ = p

_∂F[(]i,[k]:[)]_

= p

= p


_Wi,j_

_Di,i_


_Wi,j_

_Di,i_ **F[(]i,[k]:[)]** _[−]_


_Wi,j_
_Dj,j_ **F[(]j,[k]:[)]**


_Wi,j_
_Dj,j_ **F[(]j,[k]:[)]**


+ 2µ(Fi,[(][k]:[)]

_[−]_ **[X][i,][:][)]**


_j=1_

_N_



_j=1_

X



_N=1_ _MDi,j[(]i,i[k][)]_ **F[(]i,[k]:[)]** _[−]_ _jN=1_ _DMi,ii,j[(][k]D[)]_ _j,j_ **F[(]j,[k]:[)]** + 2µ(F[(]i,[k]:[)] _[−]_ **[X][i,][:][)]**
X X

_N_ p _N_ 

_Mi,j[(][k][)]_ _Mi,j[(][k][)]_

+ [2][µ] **F[(]i,[k]:[)]** **F[(]j,[k]:[)]** [+ 2][µ]

_j=1_ _Di,i_ _p_  _[−]_ j=1 _Di,iDj,j_ _p_ **[X][i,][:]**

X X

N  p

**F[(]i,[k]:[)]** _αi,i[(][k][)]_ _Mi,j[(][k][)]_ **F[(]j,[k]:[)]** [+][ β]i,i[(][k][)][X][i,][:]

 _[−]_  _j=1_ _Di,iDj,j_ 

X

  p 

**F[(]i,[k]:[)]** _i,:_ _,_

_[−]_ **[F][(][k][+1)]**
 


_MDi,j[(]i,i[k][)]_ **F[(]i,[k]:[)]** _[−]_


_αi,i[(][k][)]_

_p_
=

_αi,i[(][k][)]_

which indicates that


_αi,i[(][k][)]_
**F[(]i,[k]:[)]** _[−]_ **[F]i,[(][k]:[+1)]** = _p_ _·_ _[∂][L]∂[p]F[(][F][(]i,[k]:[(][)][k][)][)]_

For all i, j ∈ [N ], v ∈ R[1][×][c], denote by


_∂_ _p(F[(]i,[k]:[)][) :=][ ∂][L][p][(][F][(][k][)][)]_
_L_ _∂F[(]i,[k]:[)]_


_p−2_


_Wi,j_

(F[(]i,[k]:[)] [+][ v][)][ −]
_Di,i_


_Wi,j_
_Dj,j_ **F[(]j,[k]:[)]**


_Mi,j[′][(][k][)]_ := Wi,j


,[]




_Mi,j[′][(][k][)]_

+ [2][µ] _,_
_Di,i_ _p_ 




_αi,i[′][(][k][)]_ := 1


_j=1_


_βi,i[′][(][k][)]_ := [2][µ] _i,i_

_p [α][′][(][k][)]_


_Mi,j[′][(][k][)]_

**F[(]j,[k]:[)]** [+][ β]i,i[′] **[X][i,][:][.]**
_Di,iDj,j_


**F[′]i,[(]:[k][+1)]** := αi,i[′][(][k][)]

_j=1_

X


-----

Then

_∂Lp(F[(]i,[k]:[)]_ [+][ v][)][ −] _[∂][L][p][(][F]i,[(][k]:[)][)]_

_p_

= **F[(]i,[k]:[)]** [+][ v][ −] **[F]i,[′][(]:[k][+1)]**

_αi,i[′][(][k][)]_

 


**F[(]i,[k]:[)]** _i,:_

_[−]_ **[F][(][k][+1)]**


_αi,i[(][k][)]_


_p_ _p_ _p_

**v** + **F[(]i,[k]:[)]** _i,:_ **F[(]i,[k]:[)]** _i,:_

_≤_ _αi,i[′][(][k][)]_ _∥_ _∥_ _αi,i[′][(][k][)]_ _[−]_ **[F][′][(][k][)]** _−_ _αi,i[(][k][)]_ _[−]_ **[F][(][k][+1)]**

   

_p_ _p_ _p_ _p_ _p_
= **v** + **Fi,[(][k]:[)]** **F[′]i,[(]:[k][+1)]** + **F[(]i,[k]:[+1)]**

_αi,i[′][(][k][)]_ _∥_ _∥_ _αi,i[′][(][k][)]_ _−_ _αi,i[(][k][)]_ ! _[−]_ _αi,i[′][(][k][)]_ _αi,i[(][k][)]_

_p_ _N_ _Mi,j[′][(][k][)]_ _N_ _Mi,j[(][k][)]_ _N_ _Mi,j[′][(][k][)]_ _N_ _Mi,j[(][k][)]_
= **v** + p **F[(]i,[k]:[)]** **F[(]j,[k]:[)]** **Fj,[(][k]:[)]** [+ 2][µ]

_αi,i[′][(][k][)]_ _∥_ _∥_ j=1 _Di,i_ _−_ _j=1_ _Di,i_  _[−]_ _j=1_ _Di,iDj,j_ _[−]_ [2]p[µ] **[X][i,][:][ +]** _j=1_ _Di,iDj,j_ _p_ **[X][i,][:]**

X X X X
 _N_ _N_  _N_ p _N_ p

_p_ _Mi,j[′][(][k][)]_ _Mi,j[(][k][)]_ _Mi,j[′][(][k][)]_ _Mi,j[(][k][)]_
= **v** + p **F[(]i,[k]:[)]** **F[(]j,[k]:[)]** [+] **F[(]j,[k]:[)]**

_αi,i[′][(][k][)]_ _∥_ _∥_ j=1 _Di,i_ _−_ _j=1_ _Di,i_  _[−]_ _j=1_ _Di,iDj,j_ _j=1_ _Di,iDj,j_

X X X X

_N_  _N_  p _N_ p _N_

= p _j=1_ _MDi,j[(]i,i[k][)]_ + 2µ _∥v∥_ + p _j=1_ _Mi,j[′][(][k][)]D−i,iMi,j[(][k][)]_ _∥v∥_ + p _j=1_ _Mi,j[′][(][k][)]D−i,iMi,j[(][k][)]_ **F[(]i,[k]:[)]** _[−]_ _j=1_ _Mi,j[′][(][k]D[)]_ _i,i−DMj,ji,j[(][k][)]_ **F[(]j,[k]:[)]**

X X X X

 _N_  p

= p _Mi,j[(][k][)]_ + [2][µ] **v** _._

 _Di,i_ _p_ [+][ o][ (][p,][ v][,][ X][,][ G][)] _∥_ _∥_

_j=1_

X
 

Therefore, there exists some real positive value µ ∈ _o (p, v, X, G) > 0 such that_


_Mi,j[(][k][)]_

+ [2][µ]
_Di,i_ _p_



_[N]_
_∂Lp(F[(]i,[k]:[)]_ [+][ v][)][ −] _[∂][L][p][(][F][(]i,[k]:[)][)]_ _≤_ _p_

_j=1_

  X


_p_

_αi,i[(][k][)]_ _∥v∥._ (19)


_∥v∥_ =


Let γ = (γ1, . . ., γN )[⊤] _∈_ R[N] and η ∈ R[N] _[×][c]. By Taylor’s theorem, we have:_

_Lp(F[(]i,[k]:[)]_ [+][ γ][i][η][i,][:][)]

1

= Lp(F[(]i,[k]:[)][) +][ γ][i] 0 _⟨∂Lp(F[(]i,[k]:[)]_ [+][ ϵγ][i][η][i,][:][)][,][ η][i,][:][⟩][d][ϵ]

Z

1

= Lp(F[(]i,[k]:[)][) +][ γ][i][⟨][η][i,][:][, ∂][L][p][(][F][(]i,[k]:[)][)][⟩] [+][ γ][i] 0 _⟨∂Lp(F[(]i,[k]:[)]_ [+][ ϵγ][i][η][i,][:][)][ −] _[∂][L][p][(][F]i,[(][k]:[)][)][,][ η][i,][:][⟩][d][ϵ]_

Z

1

_≤Lp(F[(]i,[k]:[)][) +][ γ][i][⟨][η][i,][:][, ∂][L][p][(][F][(]i,[k]:[)][)][⟩]_ [+][ γ][i] 0 _∥∂Lp(F[(]i,[k]:[)]_ [+][ ϵγ][i][η][i,][:][)][ −] _[∂][L][p][(][F]i,[(][k]:[)][)][∥∥][η][i,][:][∥][d][ϵ]_

Z

_p_
_≤Lp(F[(]i,[k]:[)][) +][ γ][i][⟨][η][i,][:][, ∂][L][p][(][F][(]i,[k]:[)][)][⟩]_ [+] 2αi,i[(][k][)] _γi[2][∥][η][i,][:][∥][2]_


-----

Let η = _p(F[(][k][)]) and choose some positive real value µ which depends on X,_ _, p and p > 1,_
_−∇L_ _G_
i.e. µ ∈ _o (p, X, G). By Eq. (19), we have for all i ∈_ [N ],


_Lp(F[(]i,[k]:[)]_ _[−]_ _[γ∂][L][p][(][F][(]i,[k]:[)][))][ ≤L][p][(][F]i,[(][k]:[)][)][ −⟨][γ][i][∂][L][p][(][F]i,[(][k]:[)][)][, ∂][L][p][(][F]i,[(][k]:[)][)][⟩]_ [+]


_p_

_γi[2]_ _i,:_ [)][∥][2]
2αi,i[(][k][)] _[∥][∂][L][p][(][F][(][k][)]_


2αi,i[(][k][)][γ][i]

_p_ _−_ _γi[2]_


= Lp(F[(]i,[k]:[)][)][ −]

= Lp(F[(]i,[k]:[)][)][ −]


_∂_ _p(Fi,:)_
_∥_ _L_ _∥[2]_


2αi,i[(][k][)]

_p_

2αi,i[(][k][)]


_i,i_

2

2[]

= Lp(F[(]i,[k]:[)][)][ −] 2αpi,i[(][k][)]  αpi,i[(][k][2][)] _−_ _γi −_ _αpi,i[(][k][)]_ ! _∥∂Lp(F[(]i,[k]:[)][)][∥][2][.]_

 
 

Then for allγi = _α[(]i,ip[k][)]_ minimizes i ∈ [N ] S, whenp(F[(]i,[k] 0:[)] _≤[−]_ _[γ][i]γ[∂]i ≤[S][p][(][F]2αi,[(]p[k][(]i,i:[k][)][))][)]_, we have[. Therefore,] Sp(F[(]i,[k]:[)] _[−]_ _[γ][i][∂][S][p][(][F]i,[(][k]:[)][))][ ≤S][p][(][F]i,[(][k]:[)][)][ and]_

_p(F[(][k][+1)]) =_ _p(F[(][k][)]_
_L_ _L_ _−_ _p[1]_

_[·][ α][(][k][)][∇L][p][(][F][(][k][)][))][ ≤L][p][(][F][(][k][)][)][.]_


2
_αi,i[(][k][)]_

_p[2]_ 


_γi_ _αi,i[(][k][)]_
_−_ _p_


D.3 PROOF OF THEOREM 3

_Proof. Without loss of generality, suppose F[(0)]_ = X. Denote **M[˜]** [(][k][)] = D[−][1][/][2]M[(][k][)]D[−][1][/][2], by
Eq. (14), we have for K ≥ 2,

**F[(][K][)]** = α[(][K][−][1)]D[−][1][/][2]M[(][K][−][1)]D[−][1][/][2]F[(][K][−][1)] + β[(][K][−][1)]X


= α[(][K][−][1)][ ˜]M[K][−][1]F[(][K][−][1)] + β[(][K][−][1)]X

= α[(][K][−][1)][ ˜]M[K][−][1][ ]α[(][K][−][2)][ ˜]M[K][−][2]F[(][K][−][2)] + β[(][K][−][2)]X + β[(][K][−][1)]X


= α[(][K][−][1)]α[(][K][−][2)][ ˜]M[K][−][1][ ˜]M[K][−][2]F[(][K][−][2)] + α[(][K][−][1)][ ˜]M[K][−][1]β[(][K][−][2)]X + β[(][K][−][1)]X

_K−1_ _K−1_ _K−1_ _K−1_

= **_α[(][k][)]_** **M˜** [(][k][)] **F[(0)]** + **_α[(][l][)][ ˜]M[(][l][)]_** **_β[(][K][−][1][−][k][)]X + β[(][K][−][1)]X_**

_kY=0_ ! _kY=0_ ! _kX=1_ _l=YK−k_ !

_K−1_ _K−1_ _K−1_ _K−1_

= **_α[(][k][)]_** **M˜** [(][k][)] **X +** **_α[(][l][)][ ˜]M[(][l][)]_** **_β[(][K][−][1][−][k][)]X + β[(][K][−][1)]X._**

_kY=0_ ! _kY=0_ ! _kX=1_ _l=YK−k_ !

(20)

Recall Equations (12) and (13), we have


_p−2_


_M˜_ _i,j[(][k][)]_ [=] _Wi,j_

_Di,iDj,j_

and p
_αi,i[(][k][)]_ [=]


_Wi,j_

_Di,i_ **F[(]i,[k]:[)]** _[−]_


_Wi,j_
_Dj,j_ **F[(]j,[k]:[)]**


_, for all i, j = 1, 2, . . ., N,_


_, for all i = 1, 2, . . ., N._


_αi,i_ [=] _N_ _Mi,j[(][k][)]_ _,_ _i = 1, 2, . . ., N._

_j=1_ _Di,i_ [+][ 2]p[µ]

Note that the eigenvalues of **M[˜]** are not infinity andP 0 < αi,i < 1 for all i = 1, . . ., N . Then we have


_K−1_

lim **_α[(][k][)]_** = 0,
_K→∞_ _k=0_

Y


and


_K−1_ _K−1_

**_α[(][k][)]_** **M˜** [(][k][)]
_k=0_ ! _k=0_

Y Y


lim
_K→∞_


= 0.


-----

Therefore,


_K−1_

_k=1_

X


_K−1_

**_α[(][l][)][ ˜]M[(][l][)]_**
_l=YK−k_


lim
_K→∞_ **[F][(][K][)][ = lim]K→∞**


**_β[(][K][−][1][−][k][)]X + β[(][K][−][1)]X_** _._

!

_Wi,j_

_Di,iDj,j_ _∥(∇f_ )([j, i])∥[p][−][2]f (j)


(21)


By Equations (6) and (12), we have


_Wi,j_

( _f_ )([j, i]) _f_ (i)
_Di,i_ _∥_ _∇_ _∥[p][−][2]_ _−_


∆pf (i) =

_j=1_

X

_N_

=

_j=1_

X

By Eq. (13), we have


_j=1_


_N_

_Mi,j_

_f_ (i)
_Di,i_ _−_

_j=1_

X

_N_

_j=1_

X


_Mi,j_

_f_ (j). (22)
_Di,iDj,j_


_Mi,j[(][k][)]_

_Di,i_


1

_αi,i[(][k][)]_ _−_ [2]p [µ] _[.]_ (23)


Equations (22) and (23) show that

∆[(]p[k][)] = **_α[(][k][)][][−][1]_** _−_ [2]p[µ] **[I][N]** _−_ **M[˜]** [(][k][)], (24)
 

which indicates
**_α[(][k][)][ ˜]M[(][k][)]_** = IN _p_ _[.]_ (25)
_−_ [2]p[µ] **_[α][(][k][)][ −]_** **_[α][(][k][)][∆][(][k][)]_**

Eq. (25) shows that α[(][k][)][ ˜]M[(][k][)] is linear w.r.t ∆p and therefore can be expressed by a linear combination in terms of ∆p:
**_α[(][k][)][ ˜]M[(][k][)]_** = θ[′][(][k][)]∆p, (26)
where θ[′] = diag(θ0[′] _[, θ]1[′]_ _[, . . ., θ]N[′]_ 1[)][ are the parameters. Therefore, we have]
_−_

_K−1_ _K−1_

lim lim **_α[(][l][)][ ˜]M[(][l][)]_** **_β[(][K][−][1][−][k][)]X + β[(][K][−][1)]X_**
_K_ _K_
_→∞_ **[F][(][K][)][ =]** _→∞_ _kX=1_ _l=YK−k_ ! !

_K−1_ _K−1_

= lim **_θ[′][(][l][)]∆p_** **_β[(][K][−][1][−][k][)]X + β[(][K][−][1)]X_**
_K_
_→∞_ _kX=1_ _l=YK−k_ ! !

_K−1_ _K−1_

= lim **_β[(][K][−][1][−][k][)]_** **_θ[′][(][l][)]_** ∆[k]p[X][ +][ β][(][K][−][1)][X]
_K_
_→∞_ _kX=1_ _l=YK−k_ ! !

_K−1_

= lim **_θ[′′][(][k][)]∆[k]p[X][,]_**
_K→∞_ _k=0_

X

where θ[′′][(][k][)] = diag(θ1[′′][(][k][)], θ2[′′][(][k][)], . . ., θN[′′][(][k][)]) defined as θ[′′][(0)] = β[(][K][−][1)] and


_K−1_

_θi[′′][(][k][)]_ = βi,i[(][K][−][1][−][k][)] _θi[′][(][l][)], for k = 1, 2, . . ., K_ 1.

_−_
_l=YK−k_

Let θ = (θ0, θ1, . . ., θK−1) defined as θk = _i=1_ _[θ]i[′′][(][k][)]_ for all k = 0, 1, . . ., K − 1, then

_K−1_

lim lim[P][N] _θk∆[k]p[X][ +][ θ][0][X]_
_K_ _K_
_→∞_ **[F][(][K][)][ =]** _→∞_ _k=1_ !

X


_K−1_

_θk∆[k]p[X][.]_
_k=0_

X


lim
_K→∞_


Therefore complete the proof.


-----

D.4 PROOF OF THEOREM 4

_Proof. The first-order Taylor expansion with Peano’s form of remainder for σ at X[∗]i,:_ [is given by:]


_⊤_
**F[(]j,[K]:** _[−][1)]_ **X[∗]i,:** + o( **F[(]j,[K]:** _[−][1)]_ **X[∗]i,:[∥][)][.]**
_−_ _∥_ _−_



_i,:[)]_
_σ(F[(]j,[K]:_ _[−][1)]) = σ(X[∗]i,:[) +][ ∂σ]∂[(][X]X[∗]_


Note that in general the output non-linear layer σ(·) is simple. Here we assume that it can be well
approximated by the first-order Taylor expansion and we can ignore the Peano’s form of remainder.
For all i = 1, . . ., N, Di,i = Dj,j = d, we have αi,i _Nj=1_ _[D]i,i[−][1][/][2]Mi,j[(][K][−][1)]Dj,j[−][1][/][2]_ + βi,i[(][K][−][1)] = 1.
Then
P


_yi_ _y˜i[(][K][)]_
_−_

_yi_ _αi,i[(][K][−][1)]_
_−_


_Mi,j[(][K][−][1)]_ _σ_ **F[K]j,:[−][1]** _βi,i[(][K][−][1)]σ (Xi,:)_

_Di,iDj,j_ _−_
  

p


_j=1_


_N_ _Mi,j[(][K][−][1)]_ **X[∗]i,:** _⊤_

= _yi −_ _βi,i[(][K][−][1)]σ (Xi,:) −_ _αi,i[(][K][−][1)]_ _j=1_ _Di,iDj,j_ _σ_ **X[∗]i,:** + _[∂σ]∂ X_  **F[(]j,[K]:** _[−][1)]_ _−_ **X[∗]i,:** !

X     

_N_ p _N_

_Mi,j[(][K][−][1)]_ _Mi,j[(][K][−][1)]_ _∂σ(X∗i,:[)]_ _⊤[]_

= _yi −_ _αi,i[(][K][−][1)]_ Xj=1 _Di,iDj,j_ _yi −_ _βi,i[(][K][−][1)]σ(Xi,:) −_ _αi,i[(][K][−][1)]_ Xj=1 _Di,iDj,j_  _∂X_ F[(]j,[K]: _[−][1)]_ _−_ **X[∗]i,:**

p _N_ p

_Mi,j[(][K][−][1)]_ _∂σ(X∗i,:[)]_ _⊤[]_

= _βi,i[(][K][−][1)](yi −_ _σ(Xi,:)) −_ _αi,i[(][K][−][1)]_ Xj=1 _Di,iDj,j_  _∂X_ F[(]j,[K]: _[−][1)]_ _−_ **Xi,: −** **_ϵi,:_**

_N_ p

_Mi,j[(][K][−][1)]_ _∂σ(X[∗]i,:[)]_ _⊤_ _∂σ(X[∗]i,:[)]_

_≤_ _βi,i[(][K][−][1)]_ _|yi −_ _σ(Xi,:)| + αi,i[(][K][−][1)]_ _j=1_ _Di,iDj,j_ _∂X_ **F[(]j,[K]:** _[−][1)]_ _−_ **Xi,:** + (1 − _βi,i[(][K][−][1)])_ _∂X_ **_ϵ[⊤]i,:_**

X  

p _N_

_∂σ(X[∗]i,:[)]_ _Mi,j[(][K][−][1)]_ _∂σ(X[∗]i,:[)]_

_≤_ _βi,i[(][K][−][1)]_ _|yi −_ _σ(Xi,:)| + αi,i[(][K][−][1)]_ _∂X_ _j=1_ _Di,iDj,j_ **F[(]j,[K]:** _[−][1)]_ _−_ **Xi,:** + (1 − _βi,i[(][K][−][1)])_ _∂X_ _[∥][ϵ][i,][:][∥]_

X  

_N_ p

_Mi,j[(][K][−][1)]_

_≤_ _βi,i[(][K][−][1)]_ _|yi −_ _σ(Xi,:)| + αi,i[(][K][−][1)]L_ _j=1_ _d_ **F[(]j,[K]:** _[−][1)]_ _−_ **Xi,:** + (1 − _βi,i[(][K][−][1)])L ∥ϵi,:∥_

X  

= βi,i[(][K][−][1)] _|yi −_ _σ(Xi,:)| + αi,i[(][K][−][1)]L_ _jN=1_ _Mi,j[(][K]d_ _[−][1)]_ **F[(]j,[K]:** _[−][1)]_ _−_ **F[(]i,[K]:** _[−][1)]_ + F[(]i,[K]: _[−][1)]_ _−_ **Xi,:**

X  


_Mi,j[(][K][−][1)]_

_Di,iDj,j_


**X[∗]i,:** + _[∂σ]∂XX[∗]i,:_
 



_yi_ _βi,i[(][K][−][1)]σ (Xi,:)_ _αi,i[(][K][−][1)]_
_−_ _−_


_j=1_


+ (1 _βi,i[(][K][−][1)])L_ **_ϵi,:_**
_−_ _∥_ _∥_


_Mi,j[(][K][−][1)]_


_Mi,j[(][K][−][2)]_


= βi,i[(][K][−][1)] _yi_ _σ(Xi,:)_ + αi,i[(][K][−][1)]
_|_ _−_ _|_


∆pF[(]i,[K]: _[−][1)]_


**F[(]j,[K]:** _[−][2)]_ **Xi,:**
_−_


_j=1_


_j=1_

_N_



_j=1_

X
 _N_



_j=1_

X



+ (1 _βi,i[(][K][−][1)])L_ **_ϵi,:_**
_−_ _∥_ _∥_


_K−2_

_k=0_

X

_K−2_

_k=0_

X


_K−2_

_l=k_

Y

_K−2_

_l=k_

Y


_Mi,j[(][l][)]_

_d_

_Mi,j[(][l][)]_


= βi,i[(][K][−][1)] _yi_ _σ(Xi,:)_ + αi,i[(][K][−][1)]
_|_ _−_ _|_

= βi,i[(][K][−][1)] _yi_ _σ(Xi,:)_ + αi,i[(][K][−][1)]
_|_ _−_ _|_


∆[(]p[K][−][1)]F[(]i,[K]: _[−][1)]_

∆[(]p[K][−][1)]F[(]i,[K]: _[−][1)]_


+ (1 _βi,i[(][K][−][1)])L_ **_ϵi,:_**
_−_ _∥_ _∥_

+ (1 _βi,i[(][K][−][1)])L_ **_ϵi,:_**
_−_ _∥_ _∥_


∆[(]p[k][)][X][i,][:]





∆[(]p[k][)][X][i,][:]






-----

Therefore,

_N_

1

_yi_ _y˜i_

_N_ _i=1_ _|_ _−_ _| ≤_ _N[1]_

X


_βi,i[(][K][−][1)]_ _yi_ _σ(Xi,:)_
_i=1_ _|_ _−_ _|_

X


_Mi,j[(][l][)]_

∆[(]p[k][)][X][i,][:]

_d_ 




_K−2_

_k=0_

X


_K−2_

_l=k_

Y


+ _[L]_

_N_

+ _[L]_

_N_

D.5 PROOF OF THEOREM 6

_Proof. Note that_


_αi,i[(][K][−][1)]_
_i=1_

X


∆[(]p[K][−][1)]F[(]i,[K]: _[−][1)]_


_j=1_


(1 _βi,i[(][K][−][1)])_ **_ϵi,:_**
_−_ _∥_ _∥_
_i=1_

X


_ui_ =
_∥_ _∥[p]_
_i=1_

X


_|ui|[p]_ = ∥u∥p[p] [= 1][,]
_i=1_

X


_ϕp(u)[⊤]u =_


_∥ui∥[p][−][2]u[2]i_ [=]
_i=1_

X


_ϕp(ui)ui =_
_i=1_

X


then we have
∆pU = Φp(U)Λ = Φp(U)ΛΦ(U)[⊤]U.

Therefore, ∆p = Φp(U)ΛΦp(U)[⊤].

When p = 2, by Φ2(U) = U, we get ∆2 = Φ2(U)ΛΦ2(U)[⊤] = UΛU[⊤].

D.6 PROOF OF THEOREM 7

_Proof. By the definition of graph p-Laplacian, we have for all i = 1, 2, . . ., N_,


_p−2_

[ s]


_Wi,j_

_Di,i_


_Wi,j_

_ui_
_Di,i_ _−_


_Wi,j_
_uj_
_Dj,j_


_Wi,j_

_ui_
_Di,i_ _−_


_Wi,j_
_uj_
_Dj,j_


(∆pu)i =


= λϕp(ui).


_j=1_


Then, for all i = 1, 2, . . ., N,


_p−2_

_Wi,j_ _Wi,j_ _Wi,j_ _Wi,j_

_ui_ _uj_ _ui_ _uj_
_Di,i_ _−_ s _Dj,j_ [ s] _Di,i_ _−_ s _Dj,j_ !

_p−2_

_Wi,j_ _Wi,j_ _Wi,j_ _Wi,j_

_ui_ _uj_ _ui_ _uj_

s _Di,i_ _−_ s _Dj,j_ [ s] _Di,i_ _−_ s _Dj,j_


_Wi,j_

_Di,i_ s

_Wi,j_

s


_λ =_


_ϕp(ui)_


_j=1_


_ui_ _ui_
_∥_ _∥[p][−][2]_ _j=1_

X

_N_

_Wi,j_

s


_Di,i_


_p_ 2

_Wi,j_ _Wi,j_ _−_

_Wi,j_ _Di,i_ _[u][i][ −]_ _Dj,j_ _[u][j]_ _Wi,j_ _Wi,j_ _uj_

_Di,i_ q _ui_ q s _Di,i_ _−_ s _Dj,j_ _ui_

_∥_ _∥[p][−][2]_

_p_ 2

_Wi,j_ _Wi,j_ _−_

_Wi,j_ _Wi,j_ _uj_ _Di,i_ _[u][i][ −]_ _Dj,j_ _[u][j]_

_Di,i_ _−_ _Di,iDj,j_ _ui_ ! [] q _ui_ q 

_∥_ _∥_

p  _Wi,j_ _Wi,j_ _p−2_

_Wi,j_ _Wi,j_ _uj_ _Di,i_ _[u][i][ −]_ _Dj,j_ _[u][j]_

_Di,i_ _−_ _Di,iDj,j_ _ui_ ! q _ui_ q

_p_ 2

p _−_

_Wi,j_ _Wi,j_ _uj_ _Wi,j_ _Wi,j_ _uj_

_Di,i_ _−_ _Di,iDj,j_ _ui_ ! s _Di,i_ _−_ s _Dj,j_ _ui_


_j=1_

_N_

_j=1_

X

_N_

_j=1_

X

_N_

_j=1_

X


-----

Let l = arg max _ui_ _i=1,2,...,N_, the above equation holds for all i = 1, 2, . . ., N, then
_{∥_ _∥}_


_p−2_

_Wl,j_ _uj_
_Dj,j_ _ul_

_p−2_

_Wl,j_ _uj_

s _Dj,j_ _ul_


_Wl,j_ _Wl,j_

_Dl,l_ _−_ _Dl,lDj,j_
p

_Wl,j_ _Wl,j_

_Dl,l_ _−_ _Dl,lDj,j_


_uj_

_ul_

_uj_

_ul_


_Wl,j_

s _Dl,l_ _−_

_Wl,j_

! s _Dl,l_

_Wl,j_

_Dl,l_ _−_ s

_Wi,j_

! s _Di,i_

= _Ni,_
p


_λ =_


_j=1_

_N_

_≥_

_j=1_

X

_N_

_≥_

_j=1_

X

_≥_ 0.

_N_

_λ =_

_j=1_

X

_N_

_≤_

_j=1_

X


_p−2_

_Wl,j_ _uj_
_Dj,j_ _ul_

_Wi,j_ _uj_

_−_ s _Dj,j_ _ui_


_Wl,j_ _Wl,j_

_Dl,l_ _−_ _Dl,lDj,j_
p

_Wi,j_ _Wi,j_

_Di,i_ _−_ _Di,iDj,j_


When p = 1,


_−1_


_uj_
_ui_


_Wi,j_

_Di,i_


_Wi,j_

_Di,i_


_Wi,j_ _Wi,j_

_Ni_ = _Ni,_

_≤_ _j=1_ s _Di,i_ _≤_ vu _j=1_ _Di,i_

X u X p

t

where the last inequality holds by using the Cauchy-Schwarz inequality. The above inequality holds
for all i = 1, 2, . . ., N, therefore,


_Wi,j_

_Di,i_


_λ ≤_


_Nmin._

_Wi,j_

_Di,i_

_p−1_


_j=1_


When p > 1, we have for i = 1, 2, . . ., N,


_p_ 2

_N_ _−_

_Wi,j_ _Wi,j_ _uj_ _Wi,j_ _Wi,j_ _uj_

_λ =_

_j=1_ _Di,i_ _−_ _Di,iDj,j_ _ui_ ! s _Di,i_ _−_ s _Dj,j_ _ui_

X

_p_ 1

_N_ p _−_

_Wi,j_ _Wi,j_ _Wi,j_ _uj_

_≤_ _j=1_ s _Di,i_ s _Di,i_ _−_ s _Dj,j_ _ui_

X

_p_ 1

_N_ _−_

_Wi,j_ _Wi,j_ _Wi,j_ _uj_

+

_≤_ _j=1_ s _Di,i_ s _Di,i_ s _Dj,j_ _ui_

X

_p_ _p_ 1

_N_ _−_

_Wi,j_ _Di,i_ _uj_

= _._

_j=1_ s _Di,i_ ! s _Dj,j_ _ui_

X

Without loss of generality, let k = arg max([1 +] _ui_ _/_ _Di,i_ _i=1,2,...,N_ ). Because the above inequality
_{|_ _|_ _}_

holds for all i = 1, 2, . . ., N, then we have

p


_p_

1 +

!

_p_

_Wk,j_

_._

_Dk,k_ !


_p−1_
!


_Wk,j_
_Dk,k_


_Dk,k_

s _Dj,j_

_N_

_j=1_ s

X


_uj_
_uk_


_λ ≤_


_j=1_


_≤_ 2[p][−][1]


_j=1_

_Wk,j_
_Dk,k_


For p ≥ 2,


_p_
!


2
!


_Wk,j_
_Dk,k_


_λ ≤_ 2[p][−][1]


_≤_ 2[p][−][1]


= 2[p][−][1].


_j=1_


-----

For 1 < p < 2,

_λ ≤_ 2[p][−][1]


_p_
!


_Wk,j_
= 2[p][−][1][p]Nk.
_Dk,k_


_Wk,j_
_Dk,k_


_Wk,j_
2[p][−][1] _Nk_
_Dk,k_ _≤_ v

u
u
t


_≤_ 2[p][−][1]

_j=1_

X


_j=1_


_j=1_


D.7 PROOF OF PROPOSITION 1

_Proof. We proof Proposition 1 based on the bounds of p-eigenvalues as demonstrated in Thm. 7._

By Eq. (6) and Eq. (12), we have


_Wi,j_

( _f_ )([j, i]) _f_ (i)
_Di,i_ _∥_ _∇_ _∥[p][−][2]_ _−_


_Wi,j_

_Di,iDj,j_ _∥(∇f_ )([j, i])∥[p][−][2]f (j)


∆pf (i) =

_j=1_

X

_N_

=

_j=1_

X

By Eq. (13), we have


_j=1_


_Mi,j_

_f_ (i)
_Di,i_ _−_


_Mi,j_

_f_ (j). (27)
_Di,iDj,j_


_j=1_

_N_

_j=1_

X


_Mi,j[(][k][)]_

_Di,i_


1

_αi,i[(][k][)]_ _−_ [2]p [µ] _[.]_ (28)


Equations (27) and (28) show that


∆[(]p[k][)] = **_α[(][k][)][][−][1]_** _−_ [2]p[µ] **[I][N]**



_−_ **D[−][1][/][2]M[(][k][)]D[−][1][/][2],** (29)


which indicates


**_α[(][k][)]D[−][1][/][2]M[(][k][)]D[−][1][/][2]_** = IN _p_ _[.]_ (30)
_−_ [2]p[µ] **_[α][(][k][)][ −]_** **_[α][(][k][)][∆][(][k][)]_**

For i = 1, 2, . . ., N, let ˜α := (˜α1, . . ., ˜αN ), ˜αi := 1/ _j=1_ _MDi,ii,j_ [, then]

_N_

_Mi,j_ [P][N]

_αi,i_ = (1

_j=1_ _Di,iDj,j_ _−_ [2]p [µ] _[α][i,i][)][ −]_ _[α][i,i][λ][i]_

X

p _N_ _Mi,j_

= _N_ _j=1Mi,jDi,i_ 1 _N_ 1 _Mi,j_ _λi_

 _−_ 

_jP=1_ _Di,i_ [+][ 2]p[µ] _j=1_ _Di,i_
P 1  P 

= _αi_ (1 _α˜iλi),_ (31)

1 + [2][µ]p[˜] _−_

Recall the Eq. (12) that

_p−2_

_Wi,j_ _Wi,j_

_Mi,j = Wi,j_ **Fi,:** **Fj,:** = Wi,j ( _f_ )([i, j]) _,_

s _Di,i_ _−_ s _Dj,j_ _∥_ _∇_ _∥[p][−][2]_


1. When p = 2, for all i = 1, . . ., N, ˜αi = 1 and 0 _λi_ 1 2, g2(λi 1) works as both
low-pass and high-pass filters. _≤_ _−_ _≤_ _−_

2. Whenthen 0 p > ≤ 21, by Thm. − _α˜iλi ≤ 7 we have for all1, which indicates that i = 1, . . ., N gp,( 0λ ≤i−1λ) works as a low-pass filter; Ifi−1 ≤_ 2[p][−][1]. If 0 ≤ _α˜i ≤_ 2[1][−][p],


-----

_α˜i > 2[1][−][p], then gp(λi_ 1) works as both low-pass and high-pass filters. Since
_−_


_Wi,j_ ( _f_ )([i, j])
_∥_ _∇_ _∥[p][−][2]_

_Di,i_


_Mi,j_

_Di,i_


_j=1_


_j=1_


_Wi,j_

_Di,i_




2



_∥(∇f_ )([i, j])∥[2(][p][−][2)]
_j=1_

X


_j=1_


_≤_ _∥(∇f_ )([i, j])∥[2(][p][−][2)]

uj=1
uX
t

_≤∥∇f_ (i)∥[p][−][2],

which indicates that ˜αi _f_ (i) . 0 _α˜i_ 2[1][−][p] directly implies that 0
_≥∥∇_ _∥[2][−][p]_ _≤_ _≤_ _≤_
_∥∇f_ (i)∥[2][−][p] _≤_ 2[1][−][p], i.e. ∥∇f (i)∥≥ 2[(][p][−][1)][/][(][p][−][2)] and when ∥∇f (i)∥[2][−][p] _≥_ 2[1][−][p], i.e.
_f_ (i) 2[(][p][−][1)][/][(][p][−][2)], ˜αi 2[1][−][p] always holds. Therefore, if _f_ (i) 2[(][p][−][1)][/][(][p][−][2)],
_∥∇gp(λi−1∥≤) works as both low-pass and high-pass filters on node ≥_ _i ∥∇; If gp(∥≤λi−1) works as a_
low-pass filter, ∥∇f (i)∥≥ 2[(][p][−][1)][/][(][p][−][2)].

3. When0 ≤ _α˜ 1i ≤ ≤2[1]p <[−][p]/ 2[√], by Thm.Nk, 0 ≤_ 1 7 − we have for allα˜iλi ≤ 1, which indicates that i = 1, . . ., N, 0 g ≤p(λλi−i−1)1 work as low-pass ≤ 2[p][−][1][√]Nk. If
filters; If ˜αi ≥ 2[1][−][p]/[√]Nk, gp(λi−1) work as both low-pass and high-pass filters. By

_N_

_Wi,j_ 1 1

_j=1_ _Di,i_ ( _f_ )([i, j]), _Nj=1_ _WDi,ii,j_

XN _∥_ _∇_ _∥[p][−][2]_ _N_ _[∥][(][∇][f]_ [)([][i, j][])][∥][p][−][2]

_Wi,j_ 1 PWi,j

= ( _f_ )([i, j])

_j=1_ _Di,i_ ( _f_ )([i, j]) _j=1_ _Di,i_ _∥_ _∇_ _∥[p][−][2]_

X _∥_ _∇_ _∥[p][−][2][ ·]_ X

_N_

_Wi,j_ 1

_≥_ _j=1_ _Di,i_ ( _f_ )([i, j]) !

X _∥_ _∇_ _∥[p][−][2][ · ∥][(][∇][f]_ [)([][i, j][])][∥][p][−][2]

= 1,
we have

1 1
_α˜i =_ _N_ _Mi,j_ = _N_ _Wi,j_

_i=1_ _Di,i_ _j=1_ _Di,i_

_N_ _[∥][(][∇][f]_ [)([][i, j][])][∥][p][−][2]

P _Wi,j_ P 1


_∥(∇f_ )([i, j])∥[p][−][2]


_Di,i_


_j=1_

_N_

_j=1_

X


_Wi,j_

( _f_ )([i, j])
_Di,i_ _∥_ _∇_ _∥[2][−][p]_


_≤∥∇f_ (i)∥[2][−][p].

_α˜i ≥_ 2[1][−][p]/[√]Nk directly implies that ∥∇f (i)∥[2][−][p] _≥_ 2[1][−][p]/[√]Nk, i.e. _∥∇f_ (i)∥≥
2(2[√]Nk)[1][/][(][p][−][2)] and when 0 _f_ (i) 2[1][−][p]/[√]Nk, i.e. 0 _f_ (i)
_≤∥∇_ _∥[2][−][p]_ _≤_ _≤∥∇_ _∥≤_
2(2[√]Nk)[1][/][(][p][−][2)], 0 ≤ _α˜i ≤_ 2[1][−][p]/[√]Nk always holds. Therefore, if 0 ≤∥∇f (i)∥≤
2(2and high-pass filters,[√]Nk)[1][/][(][p][−][2)], gp(λi−f1()i work as low-pass filters; If) 2 2[√]Nk 1/(p−2). _gp(λi−1) work as both low-pass_

_∥∇_ _∥≥_

Specifically, when p = 1, by Thm.  7 we have for all _i = 1, . . ., N_, 0 _λi_ 1
2[p][−][1][√]Nmin. Following the same derivation above we attain if 0 _≤_ _≤∥∇f_ (i−)∥ _≤≤_
2(2and high-pass filters,[√]Nmin)[1][/][(][p][−][2)], g ∥∇p(λfi−(1i))∥≥ work as low-pass filters; If2 2[√]Nmin 1/(p−2). _gp(λi−1) work as both low-pass_

  


-----

E DATASET STATISTICS AND HYPERPARAMETERS

datasets. Note that the homophily scores here is different with the scores reported by Chien et al.

E.1 ILLUSTRATION OF GRAPH GRADIENT AND GRAPH DIVERGENCE

𝑓2

**2** **2**

𝑔([1, 2])

𝛻𝑓 [1,2] = 𝑊𝐷2,21,2 𝑓2 [−] 𝑊𝐷1,11,2 𝑓1

𝛻𝑓 [2,1] = 𝑊𝐷1,11,2 𝑓1 [−] 𝑊𝐷2,21,2 𝑓2 𝑔([2, 1])

𝑔([1, 3])

𝑓1 𝛻𝑓 [1,3] = 𝑊𝐷3,31,3 𝑓3 [−] 𝑊𝐷1,11,3 𝑓1 𝑓3 𝟏

𝟏 𝟑

𝛻𝑓 [3,1] = 𝑊𝐷1,11,3 𝑓1 [−] 𝑊𝐷3,31,3 𝑓3 𝟑 div𝑔 1 = 𝑊𝐷1,11,2 𝑔1, 2 −𝑔2, 1 + 𝑊𝐷1,11,3 𝑔1, 3 −𝑔3, 1 𝑔([3, 1])

(b) Graph divergence.

(a) Graph gradient.

Figure 4: A tiny example of illustration of graph gradient and graph divergence. Best view in colors.

E.2 DATASET STATISTICS

Table 2 summarizes the dataset statistics and the levels of homophily H(G) of all benchmark

(2021). There is a bug in their code when computing the homophily scores (doing division with
torch integers) which caused their homophily scores to be smaller.

Table 2: Statistics of datasets.

Dataset #Class #Feature #Node #Edge Training Validation Testing H(G)

Cora 7 1433 2708 5278 2.5% 2.5% 95% 0.825
CiteSeer 6 3703 3327 4552 2.5% 2.5% 95% 0.717
PubMed 3 500 19717 44324 2.5% 2.5% 95% 0.792
Computers 10 767 13381 245778 2.5% 2.5% 95% 0.802
Photo 8 745 7487 119043 2.5% 2.5% 95% 0.849
CS 15 6805 18333 81894 2.5% 2.5% 95% 0.832
Physics 5 8415 34493 247962 2.5% 2.5% 95% 0.915

Chameleon 5 2325 2277 31371 60% 20% 20% 0.247
Squirrel 5 2089 5201 198353 60% 20% 20% 0.216
Actor 5 932 7600 26659 60% 20% 20% 0.221
Wisconsin 5 251 499 1703 60% 20% 20% 0.150
Texas 5 1703 183 279 60% 20% 20% 0.097
Cornell 5 1703 183 277 60% 20% 20% 0.386

E.3 HYPERPARAMETER SETTINGS

We set the number of layers as 2, the maximum number of epochs as 1000, the number for early
stopping as 200, the weight decay as 0 or 0.0005 for all models. The other hyperparameters for each
model are listed as below:

-  [1][.][0]GNN, [1][.][5]GNN, [2][.][0]GNN, [2][.][5]GNN:

**– Number of hidden units: 16**
**– Learning rate: {0.001, 0.01, 0.05}**
**– Dropout rate: {0, 0.5}**
**– µ: {0.01, 0.1, 0.2, 1, 10}**
**– K: 4, 6, 8**


-----

-  MLP:

**– Number of hidden units: 16**
**– Learning rate: {0.001, 0.01}**
**– Dropout rate: {0, 0.5}**

-  GCN:

**– Number of hidden units: 16**
**– Learning rate: {0.001, 0.01}**
**– Dropout rate: {0, 0.5}**

-  SGC:

**– Number of hidden units: 16**
**– Learning rate: {0.2, 0.01}**
**– Dropout rate: {0, 0.5}**
**– K: 2**

-  GAT:


**– Number of hidden units: 8**
**– Number of attention heads: 8**
**– Learning rate: {0.001, 0.005}**
**– Dropout rate: {0, 0.6}**

-  JKNet:

**– Number of hidden units: 16**
**– Learning rate: {0.001, 0.01}**
**– Dropout rate: {0, 0.5}**
**– K: 10**
**– α: {0.1, 0.5, 0.7, 1}**
**– The number of GCN based layers: 2**
**– The layer aggregation: LSTM with 16 channels and 4 layers**

-  APPNP:

**– Number of hidden units: 16**
**– Learning rate: {0.001, 0.01}**
**– Dropout rate: {0, 0.5}**
**– K: 10**
**– α: {0.1, 0.5, 0.7, 1}**

-  GPRGNN:

**– Number of hidden units: 16**
**– Learning rate: {0.001, 0.01, 0.05}**
**– Dropout rate: {0, 0.5}**
**– K: 10**
**– α: {0, 0.1, 0.2, 0.5, 0.7, 0.9, 1}**
**– dprate: {0, 0.5, 0.7}**


-----

F ADDITIONAL EXPERIMENTS

F.1 EXPERIMENTAL RESULTS ON HOMOPHILIC BENCHMARK DATASETS

**Competitive Performance on Real-World Homophilic Datasets. Table 3 summarizes the aver-**
aged accuracy (the micro-F1 score) and standard deviation of semi-supervised node classification
on homophilic benchmark datasets. Table 3 shows that the performance of _[p]GNN is very close to_
APPNP, JKNet, GCN on Cora, CiteSeer, PubMed datasets and slightly outperforms all baselines on
Computers, Photo, CS, Physics datasets. Moreover, we observe that _[p]GNNs outperform GPRGNN_
on all homophilic datasets, which confirms that _[p]GNNs work better under weak supervised infor-_
mation (2.5% training rate) as discussed in Remark 3. We also see that all GNN models work
significantly better than MLP on all homophilic datasets. It illustrates that the graph topological information is helpful for the label prediction tasks. Notably, [1][.][0]GNN is slightly worse than the other
_pGNNs with larger p, which suggests to use p ≈_ 2 for homophilic graphs. Overall, the results of
Table 3 indicates that _[p]GNNs obtain competitive performance against all baselines on homophilic_
datasets.

Table 3: Results on homophilic benchmark datasets. Averaged accuracy (%) for 100 runs. Best
results are outlined in bold and the results within 95% confidence interval of the best results are
outlined in underlined bold. OOM denotes out of memory.

Method Cora CiteSeer PubMed Computers Photo CS Physics

MLP 43.47 3.82 46.95 2.15 78.95 0.49 66.11 2.70 76.44 2.83 86.24 1.43 92.58 0.83
_±_ _±_ _±_ _±_ _±_ _±_ _±_
GCN 76.23 0.79 62.43 0.81 83.72 0.27 84.17 0.59 90.46 0.48 90.33 0.36 94.46 0.08
_±_ _±_ _±_ _±_ _±_ _±_ _±_
SGC 77.19 1.47 64.10 1.36 79.26 0.69 84.32 0.59 89.81 0.57 91.06 0.05 OOM
_±_ _±_ _±_ _±_ _±_ _±_
GAT 75.62 1.01 61.28 1.09 83.60 0.22 82.72 1.29 90.48 0.57 89.96 0.27 93.96 0.21
_±_ _±_ _±_ _±_ _±_ _±_ _±_
JKNet 77.19 0.98 63.32 0.95 82.54 0.43 79.94 2.47 88.29 1.64 89.69 0.66 93.92 0.32
_±_ _±_ _±_ _±_ _±_ _±_ _±_
APPNP **79.58** 0.59 63.02 1.10 84.80 0.22 83.32 1.11 90.42 0.53 91.54 0.24 94.93 0.06
_±_ _±_ _±_ _±_ _±_ _±_ _±_
GPRGNN 76.10±1.30 61.60±1.69 83.16±0.84 82.78±1.87 89.81±0.66 90.59±0.38 94.72±0.16

1122....0505GNNGNNGNNGNN 77787878....59869387±±±±0000....69756057 63 636363....19288065±±±±0001....98977908 84 83 8483....21651945±±±±0000....30172218 85 838484....46398503±±±±0000....89858790 90 909089....40829169±±±±0000....63645066 92 92 9191....46941228±±±±0000....50404047 94 94 94 94....72909387±±±±0000....37161114

F.2 EXPERIMENTAL RESULTS OF AGGREGATION WEIGHT ENTROPY DISTRIBUTION

Here we present the visualization results of the learned aggregation weight entropy distribution
of _[p]GNNs and GAT on all benchmark datasets. Fig. 5 and Fig. 6 show the results obtained on_
homophilic and heterophilic benchmark datasets, respectively.

We observe from Fig. 5 that the aggregation weight entropy distributions learned by _[p]GNNs and_
GAT on homophilic benchmark datasets are similar to the uniform cases, which indicates that aggregating and transforming node features over the original graph topology is very helpful for label
prediction. It explains why _[p]GNNs and GNN baselines obtained similar performance on homophilic_
benchmark datasets and all GNN models significantly outperform MLP.

Contradict to the results on homophilic graphs shown in Fig. 5, Fig. 6 shows that the aggregation
weight entropy distributions of _[p]GNNs on heterophilic benchmark datasets are very different from_
that of GAT and the uniform cases. We observe from Fig. 6 that the entropy of most of the aggregation weights learned by _[p]GNNs are around zero, which means that most aggregation weights are on_
one source node. It indicates that the graph topological information in these heterophilic benchmark
graphs is not helpful for label prediction. Therefore, propagating and transforming node features
over the graph topology could lead to worse performance than MLPs, which validates the results in
Table 3 that the performance of MLP is significantly better most GNN baselines on all heterophilic
graphs and closed to _[p]GNNs._


-----

Citeseer


Cora

uniform 1.0GNN (Acc: 78.43%) 1.5GNN (Acc: 78.58%) 2.5GNN (Acc: 78.39%) GAT (Acc: 73.86%)

1400

1200

1000

800

600

# of nodes 400

200

0 0 2 entropy4 6 8 0 2 entropy4 6 8 0 2 entropy4 6 8 0 2 entropy4 6 8 0 2 entropy4 6 8


uniform

entropy2


1.0GNN (Acc: 62.95%)

0 1 entropy2 3 4


1.5GNN (Acc: 64.87%)

0 1 entropy2 3 4


2.5GNN (Acc: 63.32%)

0 1 entropy2 3 4


GAT (Acc: 61.59%)

0 1 entropy2 3


2000

1750

1500

1250

1000

750

500

250

Pubmed

uniform 1.0GNN (Acc: 83.2%) 1.5GNN (Acc: 83.73%) 2.5GNN (Acc: 84.35%) GAT (Acc: 83.85%)

14000

12000

10000

8000

6000

# of nodes 4000

2000

0 0 2 entropy4 6 8 0 2 entropy4 6 8 0 2 entropy4 6 8 0 2 entropy4 6 8 0 2 entropy4 6 8

Computers

8000 uniform 1.0GNN (Acc: 85.24%) 1.5GNN (Acc: 85.3%) 2.5GNN (Acc: 83.68%) GAT (Acc: 82.73%)

7000

6000

5000

4000

3000

# of nodes2000

1000

0 0 2 4 entropy6 8 10 12 14 0 2 4 entropy6 8 10 12 14 0 2 4 entropy6 8 10 12 14 0 2 4 entropy6 8 10 12 14 0 2 4 entropy6 8 10 12 14

Photo

2500 uniform 1.0GNN (Acc: 91.33%) 1.5GNN (Acc: 91.28%) 2.5GNN (Acc: 90.25%) GAT (Acc: 90.29%)

2000

1500

1000

# of nodes

500

0 0 2 4 entropy6 8 10 12 14 0 2 4 entropy6 8 10 12 14 0 2 4 entropy6 8 10 12 14 0 2 4 entropy6 8 10 12 14 0 2 4 entropy6 8 10 12 14


4000

3000

2000

1000

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||


Pubmed

GNN (Acc: 83.73%)

2 entropy4

Computers

GNN (Acc: 85.3%)

4 entropy6 8

Photo

GNN (Acc: 91.28%)

4 entropy6 8

CS


uniform

entropy2 3


1.0GNN (Acc: 91.17%)

0 1 entropy2 3 4 5


1.5GNN (Acc: 92.14%)

0 1 entropy2 3 4 5


2.5GNN (Acc: 92.11%)

0 1 entropy2 3 4 5


GAT (Acc: 89.8%)

1 entropy2 3 4

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|||||||||||||||||||||
|||||||||||||||||||||
|||||||||||||||||||||
|||||||||||||||||||||
|||||||||||||||||||||


Physics

10000 uniform 1.0GNN (Acc: 94.73%) 1.5GNN (Acc: 94.94%) 2.5GNN (Acc: 94.77%) GAT (Acc: 94.46%)

8000

6000

4000

# of nodes

2000

0 0 2 entropy4 6 8 10 0 2 entropy4 6 8 10 0 2 entropy4 6 8 10 0 2 entropy4 6 8 10 0 2 entropy4 6 8 10

Figure 5: Aggregation weight entropy distribution of homophilic benchmark graphs. Low entropy
means high degree of concentration, vice versa. An entropy of zero means all aggregation weights
are on one source node.


-----

Chameleon

1.5GNN (Acc: 48.8%)


uniform

1 entropy2


1.0GNN (Acc: 48.8%)

0 1 entropy2 3 4


2.5GNN (Acc: 49.23%)

0 1 entropy2 3 4


GAT (Acc: 44.64%)

0 1 entropy2 3


2000

1750

1500

1250

1000

750

500

250

Squirrel

5000 uniform 1.0GNN (Acc: 32.08%) 1.5GNN (Acc: 28.15%) 2.5GNN (Acc: 33.33%) GAT (Acc: 28.72%)

4000

3000

2000

# of nodes

1000

0 1 0 1 entropy2 3 4 5 6 1 0 1 entropy2 3 4 5 6 1 0 1 entropy2 3 4 5 6 1 0 1 entropy2 3 4 5 6 1 0 1 entropy2 3 4 5 6

Actor

uniform 1.0GNN (Acc: 41.21%) 1.5GNN (Acc: 40.63%) 2.5GNN (Acc: 39.46%) GAT (Acc: 31.12%)

6000

5000

4000

3000

# of nodes2000

1000

0 1 0 1 entropy2 3 4 5 1 0 1 entropy2 3 4 5 1 0 1 entropy2 3 4 5 1 0 1 entropy2 3 4 5 1 0 1 entropy2 3 4 5


200

175

150

125

100

75

50

25


|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||


entropy

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||
||||||
||||||
||||||
||||||
||||||


1 0 1 2 3 4 5


Squirrel

GNN (Acc: 28.15%)

1 entropy2 3

Actor

GNN (Acc: 40.63%)

0 1 entropy2

Wisconsin

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||
||||||||
||||||||

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
|||||||
|||||||
|||||||


uniform

entropy2


1.0GNN (Acc: 98.15%)

0 1 entropy2 3 4


1.5GNN (Acc: 96.3%)

0 1 entropy2 3 4


2.5GNN (Acc: 84.26%)

0 1 entropy2 3 4


GAT (Acc: 62.96%)

0 1 entropy2 3

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||
||||||
||||||
||||||
||||||
||||||


1 0 1 2 3 4


1 0 1 2 3 4


1 0 1 2 3 4 5 1 0 1 2 3 4 5


exas


uniform

entropy2


1.0GNN (Acc: 86.25%)

0 1 entropy2 3 4


1.5GNN (Acc: 83.75%)

0 1 entropy2 3 4


2.5GNN (Acc: 86.25%)

0 1 entropy2 3 4


GAT (Acc: 48.75%)

0 1 entropy2 3


140

120

100

80

60

40

20

0

1 0 1 2 3 4


140

120

100

80

60

40

20

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||
||||||
||||||
||||||
||||||
||||||

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||
||||||
||||||
||||||
||||||
||||||


1 0 1 2 3 4


1 0 1 2 3 4 5


Cornell


uniform

entropy1 2


1.0GNN (Acc: 86.25%)

1 0 entropy1 2 3 4


1.5GNN (Acc: 81.25%)

1 0 entropy1 2 3 4


2.5GNN (Acc: 75.0%)

1 0 entropy1 2 3 4


GAT (Acc: 42.5%)

0 entropy1 2 3

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|Col21|Col22|Col23|Col24|Col25|Col26|Col27|Col28|Col29|Col30|Col31|Col32|Col33|Col34|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|||||||||||||||||||||||||||||||||||
|||||||||||||||||||||||||||||||||||
|||||||||||||||||||||||||||||||||||
|||||||||||||||||||||||||||||||||||
|||||||||||||||||||||||||||||||||||
|||||||||||||||||||||||||||||||||||
|||||||||||||||||||||||||||||||||||
|||||||||||||||||||||||||||||||||||


Figure 6: Aggregation weight entropy distribution of heterophilic benchmark graphs. Low entropy
means high degree of concentration and vice versa. An entropy of zero means all aggregation
weights are on one source node.


-----

F.3 EXPERIMENTAL RESULTS ON CSBM

In this section we present the experimental results on cSBM using sparse splitting and dense splitting, respectively. We used the same settings in Chien et al. (2021) in which the number of nodes
_n = 5000, the number of features f = 2000, ϵ = 3.25 for all experiments. Table 4 reports the_
results on cSBM with sparse splitting setting, which also are presented in Fig. 2 and discussed in
Sec. 5. Table 5 reports the results on cSBM with dense splitting settings.

Table 4: Results on cSBM with sparse splitting setting. Average accuracy (%) for 20 runs. Best
results are outlined in bold and the results within 95% confidence interval of the best results are
outlined in underlined bold.

Method _ϕ = −1_ _ϕ = −0.75_ _ϕ = −0.5_ _ϕ = −0.25_ _ϕ = 0_ _ϕ = 0.25_ _ϕ = 0.5_ _ϕ = 0.75_ _ϕ = 1_

GPRGNNAPPNPJKNetGCNMLPSGCGAT 49575548599749.......72249845722670±±±±±±±0110200.......36981548236639 51585849609449.......42195665208175±±±±±±±1110200.......83464046149179 53595756558249.......21309731381465±±±±±±±1100100.......01305489964752 **61515156506148.......97545815159357±±±±±±±0000200.......44585522554838 61545260536052.......45691005203670±±±±±±±1201020.......38366540760930** 59656464646262.......92701402009076±±±±±±±1212222.......88380523032254 57827982818387.......20458895046110±±±±±±±0111111.......62353828577152 **97549190959096.......48313749379643±±±±±±±0000100.......48540943334136 97985076897578.......09079485246901±±±±±±±0300100.......51309260955271**

1221....0055GNNGNNGNNGNN **98959795....75749037±±±±1030....99012178 96969394....06103278±±±±1410....13575044 84837377....39210893±±±±4220....21125939** **61616161....13383044±±±±0000....51393041 61 61 6161....80777974±±±±0000....35343429 66 656362....55880173±±±±1221....73311188 90858879....85556457±±±±3120....24051571 97979695....80715638±±±±0000....87344316 97 979797....40257694±±±±0100....58108639**

Table 5: Results on cSBM with dense splitting setting. Average accuracy (%) for 20 runs. Best
results are outlined in bold and the results within 95% confidence interval of the best results are
outlined in underlined bold.

Method _ϕ = −1_ _ϕ = −0.75_ _ϕ = −0.5_ _ϕ = −0.25_ _ϕ = 0_ _ϕ = 0.25_ _ϕ = 0.5_ _ϕ = 0.75_ _ϕ = 1_

GPRGNNAPPNPJKNetGCNMLPSGCGAT **99508349789268.......37148635999506±±±±±±±0000090.......60394936860525 97658250829079.......22594713892114±±±±±±±0000070.......92488909606731 94657577778767.......82172876029759±±±±±±±0000050.......65596812802232 83815859736856.......18581498401284±±±±±±±0000140.......55644157601069** 79617860796158.......86188131379833±±±±±±±1010011.......69300663661670 79858682818280.......97855996609215±±±±±±±0100000.......57015034735180 **92759292919291.......03082068052145±±±±±±±0000000.......89812713735039 97979767979697.......53215628496762±±±±±±±0000000.......68270925252214 98 985197979698.......96108765043246±±±±±±±0000000.......69120549462115**

1122....0505GNNGNNGNNGNN **98999998....19882121±±±±0000....28160914 96 969495....38629194±±±±0000....44211616** 86929386....40879628±±±±1100....00223137 80808080....57708393±±±±0000....43716144 **80808080....21280428±±±±0000....42493138 87848683....96298332±±±±0000....60437047 92929186....18104240±±±±0000....27396224 97 979796....28525641±±±±0000....43332514 98 98 9897....76372445±±±±0000....18263214**

Table 5 shows that _[p]GNNs obtain the best performance on weak homophilic graphs (ϕ = 0, 0.25)_
while competitive performance against GPRGNN on strong heterophilic graphs (ϕ = −0.75, −1)
and competitive performance with state-of-the-art GNNs on strong homophilic graphs (ϕ = 0.75, 1).
We also observe that GPRGNN is slightly better than _[p]GNNs on weak heterophilic graphs (ϕ =_
_−0.25, −0.5), which suggests that GPRGNN could work very well using strong supervised infor-_
mation (60% training rate and 20% validation rate). However, as shown in Table 4, _[p]GNNs work_
better than GPRGNN under weak supervised information (2.5% training rate and 2.5%) on all heterophilic graphs. The result is reasonable, as discussed in Remark 3 in Sec. 3.2, GPRGNN can
adaptively learn the generalized PageRank (GPR) weights and it works similarly to [2][.][0]GNN on both
homophilic and heterophilic graphs. However, it needs more supervised information in order to
learn optimal GPR weights. On the contrary, _[p]GNNs need less supervised information to obtain_
similar results because Θ[(2)] acts like a hyperplane for classification. Therefore, _[p]GNNs can work_
better under weak supervised information.

F.4 EXPERIMENTAL RESULTS ON GRAPHS WITH NOISY EDGES

Here we present more experimental results on graph with noisy edges. Table 6 reports the results on
homophilic graphs (Computers, Photo, CS, Physics) and Table 7 reports the results on heterophilic
graphs (Wisconsin Texas). We observe from Tables 6 and 7 that _[p]GNNs dominate all baselines._
Moreover, _[p]GNNs even slightly better than MLP when the graph topologies are completely random,_
i.e. the noisy edge rate r = 1. We also observe that the performance of GCN, SGC, JKNet on
homophilic graphs dramatically degrades as the noisy edge rate r increases while they do not change


-----

a lot for the cases on heterophilic graphs. It is reasonable since the original graph topological
information is very helpful for label prediction on these homophilic graphs. Adding noisy edges and
remove the same number of original edges could significantly degrade the performance of ordinary
GNNs. On the other hand, since we find that the original graph topological information in Wisconsin
and Texas is not helpful for label prediction. Therefore, adding noisy edges and removing original
edges on these heterophilic graphs would not affect too much their performance.

Table 6: Results on homophilic graphs with random edges. Average accuracy (%) for 20 runs. Best
results are outlined in bold and the results within 95% confidence interval of the best results are
outlined in underlined bold. OOM denotes out of memory.

Computers Photo
Method

_r = 0.25_ _r = 0.5_ _r = 1_ _r = 0.25_ _r = 0.5_ _r = 1_

MLP 66.11 2.70 66.11 2.70 66.11 2.70 76.44 2.83 76.44 2.83 76.44 2.83
_±_ _±_ _±_ _±_ _±_ _±_
GCN 74.70 1.72 62.16 2.76 8.95 6.90 81.43 0.76 75.52 3.59 12.78 5.20
_±_ _±_ _±_ _±_ _±_ _±_
SGC 75.15 1.08 66.96 1.05 15.79 7.47 82.22 0.36 77.80 0.49 13.57 3.63
_±_ _±_ _±_ _±_ _±_ _±_
GAT 76.44 1.81 68.34 2.61 11.58 7.70 82.70 1.31 77.20 2.10 13.74 5.14
_±_ _±_ _±_ _±_ _±_ _±_
JKNet 56.74 6.48 46.11 8.43 12.50 6.56 73.46 6.74 64.18 4.06 15.66 6.10
_±_ _±_ _±_ _±_ _±_ _±_
APPNP 78.23 1.84 74.57 2.25 66.67 2.68 87.63 1.05 86.22 1.73 75.55 1.72
_±_ _±_ _±_ _±_ _±_ _±_
GPRGNN 77.30 2.24 77.11 1.80 66.85 1.65 85.95 1.05 85.64 1.22 77.46 1.44
_±_ _±_ _±_ _±_ _±_ _±_

11..05GNNGNN 7581..1479±141..3395 78 63..2612±202..0867 66 41..6004±162..7317 88 87..9709±01..7018 84 86..4720±31..0561 4168..1778±188..9715
22..50GNNGNN 7980..1434±±11..5107 7576..4990±±11..2593 6764..9517±±21..2763 8787..3865±±00..8594 8786..1106±±11..1050 7776..6507±±11..4683
_±_ _±_ _±_ _±_ _±_ _±_

CS Physics
Method

_r = 0.25_ _r = 0.5_ _r = 1_ _r = 0.25_ _r = 0.5_ _r = 1_

MLP 86.24±1.43 86.24±1.43 86.24±1.43 92.58±0.83 92.58±0.83 92.58±0.83
GCN 81.05 0.59 68.37 0.85 7.72 2.39 89.02 0.16 80.45 0.34 19.78 3.94
_±_ _±_ _±_ _±_ _±_ _±_
SGC 83.41 0.01 71.98 0.12 8.00 1.43 OOM OOM OOM
_±_ _±_ _±_
GAT 80.11 0.67 68.66 1.42 8.49 2.39 88.72 0.61 82.05 1.86 22.39 5.04
_±_ _±_ _±_ _±_ _±_ _±_
JKNet 81.35 0.74 71.30 2.14 11.43 1.18 87.98 0.97 81.90 2.27 26.38 5.80
_±_ _±_ _±_ _±_ _±_ _±_
APPNP 88.63 0.68 87.56 0.51 76.90 0.96 93.46 0.12 92.81 0.24 90.49 0.33
_±_ _±_ _±_ _±_ _±_ _±_
GPRGNN 85.77 0.81 83.89 1.54 72.79 2.24 92.18 0.29 90.96 0.48 91.77 0.41
_±_ _±_ _±_ _±_ _±_ _±_

1.0GNN 90.27 0.86 89.56 0.81 86.60 1.22 94.35 0.39 94.23 0.27 92.97 0.36
221...505GNNGNNGNN **919089...979027±±±000...494540 908989...980050±±±000...505971 848076...408482±±±112...844811 94 9493...613414±±±000...302118 93 9392...773077±±±000...293126** 929191...517216±±±000...354447
_±_ _±_ _±_ _±_ _±_ _±_

Table 7: Results on heterophilic graphs with random edges. Average accuracy (%) for 20 runs. Best
results are outlined in bold and the results within 95% confidence interval of the best results are
outlined in underlined bold.

Wisconsin Texas
Method

_r = 0.25_ _r = 0.5_ _r = 1_ _r = 0.25_ _r = 0.5_ _r = 1_

MLP 93.56 3.14 93.56 3.14 93.56 3.14 79.50 10.62 79.50 10.62 79.50 10.62
_±_ _±_ _±_ _±_ _±_ _±_
GCN 62.31 8.12 59.44 5.76 64.21 4.49 41.56 8.89 44.69 23.05 40.31 18.26
_±_ _±_ _±_ _±_ _±_ _±_
SGC 64.68 7.34 62.36 2.64 51.81 2.63 42.50 5.49 40.94 18.34 23.81 14.54
_±_ _±_ _±_ _±_ _±_ _±_
GAT 65.37 9.04 60.05 9.12 60.05 7.46 39.50 9.29 34.88 21.59 29.38 11.53
_±_ _±_ _±_ _±_ _±_ _±_
JKNet 64.91 13.07 51.39 10.36 57.41 2.57 47.75 7.30 46.62 23.23 40.69 13.57
_±_ _±_ _±_ _±_ _±_ _±_
APPNP 70.19 9.04 60.32 4.70 72.64 4.73 66.69 13.46 63.25 9.87 69.81 7.76
_±_ _±_ _±_ _±_ _±_ _±_
GPRGNN 90.97 3.83 87.50 3.86 87.55 2.97 74.25 7.25 76.75 14.05 80.69 5.87
_±_ _±_ _±_ _±_ _±_ _±_

1.0GNN **94.91** 2.73 95.97 2.00 95.97 2.27 81.50 9.24 82.12 11.09 81.81 5.67
12..50GNNGNN **9490..4658±±21..7925 9590..1997±±24..1822** 9491..9544±±22..7927 8682..5006±±65..3917 6978..3812±±115..3047 7863..5050±±78..9890
2.5GNN 82.45±3.93 88.24±2.79 84.40±1.98 80.00 _±10.83 56.62±10.01 52.31_ _±10.58_
_±_ _±_ _±_ _±_ _±_ _±_


-----

F.5 EXPERIMENTAL RESULTS OF INTERGRATING _[p]GNNS WITH GCN AND JKNET_

Here we further conduct experiments to study whether _[p]GNNs can be intergrated into existing GNN_
architectures and improve their performance on heterophilic graphs. We use two popular GNN
architectures: GCN (Kipf & Welling, 2017) and JKNet (Xu et al., 2018).

To incorporate _[p]GNNs with GCN, we use the_ _[p]GNN layers as the first layer of the combined models,_
termed as _[p]GNN + GCN, and GCN layer as the second layer. Specifically, we use the aggregation_
weights αD[−][1][/][2]MD[−][1][/][2] learned by the _[p]GNN in the first layer as the input edge weights of GCN_
layer in the second layer. To combine _[p]GNN with JKNet, we use the_ _[p]GNN layer as the GNN layers_
in the JKNet framework, termed as _[p]GNN + JKNet. Tables 8 and 9 report the experimental results_
on homophilic and heterophilic benchmark datasets, respectively.

Table 8: The results of _[p]GNNs + GCN and_ _[p]GNNs + JKNet on homophilic benchmark dataset._
Averaged accuracy (%) for 20 runs. Best results are outlined in bold and the results within 95%
confidence interval of the best results are outlined in underlined bold.

Method Cora CiteSeer PubMed Computers Photo CS Physics

GCN **76.23** 0.79 62.43 0.81 83.72 0.27 84.17 0.59 90.46 0.48 90.33 0.36 94.46 0.08
1122....0550GNN + GCNGNN + GCNGNN + GCNGNN + GCN 72 72 72 72....37728539±±±±±1111....35391955 60605960....56236819±±±±±1111....59808560 82828282....14212324±±±±±0000....22343123 83 838383....75698992±±±±±1001....05927409 90 90 9090....00022417±±±±±0110....68091283 89 898989....60604853±±±±±0000....46714568 94 94 9494....51597058±±±±±0000....39333118

1.0GNN+JKNetJKNet 7577..6719±10..5498 6360..3832±10..6595 8281..6854±00..4443 83 79..9419±21..4736 8988..2971±11..6405 8990..6926±00..6672 9394..9227±00..3269
221...505GNN+JKNetGNN+JKNetGNN+JKNet 76 76 76...487540±±±111...282659 606160...970567±±±011...974893 82 8282...565042±±±100...043553 82 8281...457836±±±122...550939 908989...213125±±±111...103903 908990...663376±±±000...636875 94 9494...298270±±±000...593334
_±_ _±_ _±_ _±_ _±_ _±_ _±_

Table 9: The results of _[p]GNNs + GCN and_ _[p]GNNs + JKNet on heterophilic benchmark dataset._
Averaged accuracy (%) for 20 runs. Best results are outlined in bold and the results within 95%
confidence interval of the best results are outlined in underlined bold.

Method Chameleon Squirrel Actor Wisconsin Texas Cornell

GCN 34.54 2.78 25.28 1.55 31.28 2.04 61.93 3.00 56.54 17.02 51.36 4.59
11..50GNN + GCNGNN + GCN 4848..8552±±21..1389 3434..6178±±11..1111 3232..3737±±32..1248 6866..2552±±33..9575 6765..6294±±1112..9960 6764..8881±±97..1961
2.0GNN + GCN 48.71±2.24 35.06± 1.18 32.72± 2.02 66.34±4.51 65.94± 7.63 **68.62±** 6.55
2.5GNN + GCN 49.53± 2.19 34.40 _±1.60_ 32.40 _±3.23_ 67.18±3.50 **68.31±** 9.18 66.06 _±9.56_
_±_ _±_ _±_ _±_ _±_ _±_

JKNet 33.28 3.59 25.82 1.58 29.77 2.61 61.08 3.71 59.65 12.62 55.34 4.43
11..50GNN + JKNetGNN + JKNet 48 49..7700±±22..2209 35 35..5698±±10..3493 40 40..2274±±10..2798 9594..8623±±22..0043 8080..2538±±69..8779 **7872..2538±±98..8314**
2.0GNN + JKNet 48.88±1.63 35.77 _±1.73_ 40.16±1.31 88.84±2.78 **86.12±** 5.59 74.75±7.81
2.5GNN + JKNet 49.04± 1.95 35.78±1.87 40.00±1.12 85.42±3.86 79.06 _±7.60_ 76.81±7.66
_±_ _±_ _±_ _±_ _±_ _±_

We observe from Table 8 that intergrating _[p]GNNs with GCN and JKNet does not improve their_
performance on homophilic graphs. The performance of GCN slightly degrade after incorporating
_pGNNs. The performance of JKNet also slightly degrade on Cora, CiteSeer, and PubMed but is_
improved on Computers, Photo, CS, Physics. It is reasonable since GCN and JKNet can predict
well on these homophilic benchmark datasets based on their original graph topology.

However, for heterophilic benchmark datasets, Table 9 shows that there are significant improvements
over GCN, and JKNet after intergrating with _[p]GNNs. Moreover,_ _[p]GNNs + JKNet obtain advanced_
performance on all heterophilic benchmark datasets and even better than _[p]GNNs on Squirrel. The_
results of Table 9 demonstrate that intergrating _[p]GNNs with GCN and JKNet can sigificantly im-_
prove their performance on heterophilic graphs.


-----

F.6 EXPERIMENTAL RESULTS OF _[p]GNNS ON PPI DATASET FOR INDUCTIVE LEARNING_

Additionally, we conduct comparison experiments of _[p]GNNs against GAT on PPI dataset (Zitnik &_
Leskovec, 2017) using the inductive learning settings as in Velickovic et al. (2018) (20 graphs for
training, 2 graphs for validation, 2 graphs for testing). We use three layers of GAT architecture with
256 hidden units, use 1 attention head for GAT (1 head) and 4 attention heads for GAT (4 heads).
We use three _[p]GNN layers and a MLP layer as the first layer for_ _[p]GNNs, set µ = 0.01, K = 1, and_
use 256 hidden units for _[p]GNN-256 and 512 hidden units for_ _[p]GNN-512. The experimental results_
are reported in Table 10.

Table 10: Results on PPI datasets. Averaged micro-F1 scores for 10 runs. Best results are outlined
in bold.

Method PPI

GAT (1 head) 0.917 ± 0.041
GAT (4 heads) 0.972 ± 0.002

1.0GNN-256 0.961 0.003
1.5 _±_
GNN-256 0.967 0.008
2.0 _±_
GNN-256 0.968 0.006
2.5 _±_
GNN-256 0.973 0.002
1.0 _±_
GNN-512 0.978 0.005
1.5 _±_
GNN-512 0.977 0.008
2.0 _±_
GNN-512 **0.981** 0.006
2.5 _±_
GNN-512 0.978 ± 0.005

From Appendix F.6 we observe that the results of [2][.][5]GNN on PPI slightly better than GAT with 4
attention heads and other _[p]GNNs are very close to it. Moreover, all results of_ _[p]GNNs significantly_
outperform GAT with one attention head. The results of _[p]GNNs on PPI is impressive._ _[p]GNNs have_
much less parameters than GAT with 4 attention heads while obtain very completitive performance
on PPI. When we use more hidden units, 512 hidden units, _[p]GNNs-512 significantly outperform_
GAT, while _[p]GNNs-512 still have less parameters. It illustrates the superior potential of applying_
_pGNNs to inducting learning on graphs._

F.7 EXPERIMENTAL RESULTS OF _[p]GNNS ON OGBN ARXIV DATASET_

Table 11: Results on OGBN arXiv dataset. Average accuracy (%) for 10 runs. Best results are
outlined in bold.

Method OGBN arXiv

MLP 55.50 ± 0.23
GCN 71.74 ± 0.29
JKNet (GCN-based) 72.19 ± 0.21
DeepGCN 71.92 ± 0.16
GCN + residual (6 layers) 72.86 ± 0.16
GCN + residual (8 layers) + C&S 72.97 ± 0.22
GCN + residual (8 layers) + C&S v2 73.13 ± 0.17

1GNN 72.40 0.19
2 _±_
GNN 72.45 0.20
3 _±_
GNN 72.58 0.23
1 _±_
GNN + residual (6 layers) + C&S 72.96 0.22
2 _±_
GNN + residual (6 layers) + C&S 73.13 0.20
3 _±_
GNN + residual (6 layers) + C&S **73.23 ± 0.16**

Here we present the experimental of _[p]GNNs on OGBN arXiv dataset (Hu et al., 2020). We use the_
official data split setting of OGBN arXiv. We use three layers _[p]GNN architecture and 256 hidden_
units with µ = 0.5, K = 2. We also combine _[p]GNNs with correct and smooth model (C&S) (Huang_
et al., 2021) and introduce residual units. The results of MLP, GCN, JKNet, DeepGCN (Li et al.,


-----

2019), GCN with residual units, C&S model are extracted from the leaderboard for OGBN arXiv.
dataset[5]. Table 11 summaries the results of _[p]GNNs against the baselines._

We observe from Table 11 that _[p]GNNs outperform MLP, GCN, JKNet, and DeepGCN. The perfor-_
mance of _[p]GNNs can be further improved by combining it with C&S model and residual units and_
3GNN + residual (6 layers) + C&S obtains the best performance against the baselines.

F.8 RUNNING TIME OF _[p]GNNS_

Tables 12 and 13 report the averaged running time of _[p]GNNs and baselines on homophilic and_
heterophilic benchmark datasets, respectively.

Table 12: Efficiency on homophilic benchmark datasests. Averaged running time per epoch (ms) /
averaged total running time (s). OOM denotes out of memory.

Method Cora CiteSeer PubMed Computers Photo CS Physics

MLP 7.7 ms / 5.27s 8.1 ms / 5.37s 7.8 ms / 5.52s 8.8 ms / 5.45s 8.4 ms / 5.34s 10.5 ms / 8.18s 14.6 ms / 12.78s
GCN 82.2 ms / 6.1s 84.2 ms / 6.1s 85 ms / 6.13s 85.2 ms / 7.07s 83.6 ms / 6.08s 85 ms / 9.68s 90 ms / 13.8s
SGC 89.5 ms / 4.96s 74.7 ms / 4.86s 80.6 ms / 5.28s 109 ms / 5.21s 85.9 ms / 4.96s 213.6 ms / 8.01s OOM
GAT 534.8 ms / 13.06s 313.6 ms / 13.36s 314.6 ms / 13.97s 441.3 ms / 24.62s 309.8 ms / 15.96s 454 ms / 21.87s 436.9 ms / 40.9s
JKNet 95.4 ms / 20.07s 101.1 ms / 19.58s 105.4 ms / 20.8s 106.1 ms / 29.72s 97.9 ms / 21.18s 102.7 ms / 24.94s 119.2 ms / 40.83s
APPNP 86.7 ms / 11.6s 86.3 ms / 11.98s 85.5 ms / 11.97s 92.1 ms / 15.75s 86 ms / 12.19s 90.5 ms / 17.36s 99.6 ms / 25.89s
GPRGNN 86.5 ms / 12.42s 195.8 ms / 12.6s 88.6 ms / 12.59s 93.3 ms / 15.98s 86.7 ms / 12.65s 92 ms / 17.8s 217.1 ms / 26.33s
1.0GNN 96 ms / 20.12s 98.1 ms / 19.81s 100.2 ms / 21.74s 151.4 ms / 64.08s 121.3 ms / 34.07s 109.7 ms / 25.03s 122.9 ms / 49.59s
1.5GNN 98.2 ms / 20.19s 97 ms / 20.26s 100.2 ms / 22.6s 140.3 ms / 64.08s 120 ms / 34.22s 112.3 ms / 25.11s 127.9 ms / 49.54s
2.0GNN 98.1 ms / 20.11s 96.3 ms / 19.97s 99.3 ms / 22.17s 141 ms / 64.04s 129.3 ms / 34.14s 104.7 ms / 24.93s 124.6 ms / 49.35s
2.5GNN 96.6 ms / 20.12s 92.9 ms / 20.16s 103 ms / 22.17s 141.6 ms / 64.01s 128.1 ms / 34.22s 110.8 ms / 25.07s 124 ms / 49.39s

Table 13: Efficiency on heterophilic benchmark datasests. Averaged running time per epoch (ms) /
averaged total running time (s).

Method Chameleon Squirrel Actor Wisconsin Texas Cornell

MLP 7.7 ms / 5.29s 8 ms / 5.44s 8.6 ms / 5.4s 7.7 ms / 5.16s 7.9 ms / 5.22s 7.6 ms / 5.19s
GCN 83.4 ms / 6.1s 83.2 ms / 6.2s 90.7 ms / 6.07s 83.5 ms / 5.94s 80.7 ms / 5.96s 87.1 ms / 5.92s
SGC 78.1 ms / 4.93s 110.9 ms / 5.21s 77.1 ms / 4.71s 73.2 ms / 4.52s 74.2 ms / 4.79s 71.3 ms / 4.8s
GAT 374.9 ms / 13.49s 324.2 ms / 17.15s 420 ms / 13.82s 317.5 ms / 12.68s 357.9 ms / 12.38s 383.3 ms / 12.45s
JKNet 102.4 ms / 21.15s 101 ms / 22.84s 97.2 ms / 21.24s 98.5 ms / 21.07s 103.6 ms / 20.92s 102.2 ms / 20.79s
APPNP 87.1 ms / 12.12s 98.8 ms / 12.41s 87.2 ms / 11.81s 84.2 ms / 11.83s 86 ms / 11.9s 83.1 ms / 11.94s
GPRGNN 93 ms / 12.98s 86.1 ms / 13.01s 94.2 ms / 13.01s 84.3 ms / 12.66s 92 ms / 12.64s 89.1 ms / 12.6s
1.0GNN 107.3 ms / 22.43s 116.3 ms / 30.92s 117.8 ms / 23.6s 94.5 ms / 18.47s 92 ms / 18.83s 92.7 ms / 18.97s
1.5GNN 97.2 ms / 22.54s 115 ms / 31.04s 119.2 ms / 23.47s 93.3 ms / 18.64s 90.8 ms / 19.09s 94.9 ms / 18.88s
2.0GNN 98.7 ms / 22.37s 114.8 ms / 31.14s 100.8 ms / 23.73s 92.2 ms / 19.09s 92.5 ms / 18.72s 98 ms / 18.64s
2.5GNN 97.9 ms / 22.38s 115.9 ms / 31.09s 97.3 ms / 23.77s 92.8 ms / 19.03s 91 ms / 18.84s 90.7 ms / 18.83s

F.9 EXPERIMENTAL RESULTS ON BENCHMARK DATASETS FOR 64 HIDDEN UNITS

Table 14: Results on heterophilic benchmark datasets for 64 hidden units. Averaged accuracy (%)
for 20 runs. Best results outlined in bold and the results within 95% confidence interval of the best
results are outlined in underlined bold.

Method Chameleon Squirrel Actor Wisconsin Texas Cornell

MLP 46.55±0.90 33.83±0.59 38.40±0.76 93.91±2.47 87.51±8.53 86.75±8.22
GCN 34.74 2.62 25.68 1.17 30.86 1.51 65.93 5.47 58.56 13.28 46.81 4.28
_±_ _±_ _±_ _±_ _±_ _±_
SGC 34.57 4.71 24.39 1.54 35.50 2.09 62.87 8.92 50.62 5.60 29.44 14.83
_±_ _±_ _±_ _±_ _±_ _±_
GAT 43.33 1.53 30.07 0.99 33.44 2.45 66.57 4.69 50.69 12.89 42.62 13.37
_±_ _±_ _±_ _±_ _±_ _±_
JKNet 32.69 4.47 27.18 0.76 25.72 2.75 66.57 10.53 43.88 17.10 47.69 3.25
_±_ _±_ _±_ _±_ _±_ _±_
APPNP 35.09 3.18 28.15 0.93 32.28 1.75 66.30 1.60 69.00 4.53 54.88 3.85
_±_ _±_ _±_ _±_ _±_ _±_
GPRGNN 34.65 2.86 28.56 1.35 34.58 1.45 93.70 3.12 86.50 6.04 84.75 8.38
_±_ _±_ _±_ _±_ _±_ _±_

11..05GNNGNN **4949..5152±11..3215 32 33..6714±11..0010 4039..8270±10..5488 9594..0323±21..2660 8486..1294±76..3999 8682..5689±66..9763**
22..50GNNGNN 4849..9319±±±00..7481 3333..3178±±±10..2787 3939..7547±±±11..2620 9492..4913±±±12..8116 87 87..6225±±±65..6457 85 80..5656±±±75..2528

5https://ogb.stanford.edu/docs/leader nodeprop/#ogbn-arxiv


-----

Table 15: Results on homophilic benchmark datasets for 64 hidden units. Averaged accuracy (%)
for 20 runs. Best results are outlined in bold and the results within 95% confidence interval of the
best results are outlined in underlined bold. OOM denotes out of memory.

Method Cora CiteSeer PubMed Computers Photo CS Physics


MLP 49.05 0.82 50.67 1.25 80.32 0.40 70.58 0.82 79.44 0.79 89.48 0.50 92.84 0.62
_±_ _±_ _±_ _±_ _±_ _±_ _±_
GCN 77.65±0.42 64.72±0.52 84.13±0.12 84.56±0.79 90.16±0.88 91.14±0.10 94.75±0.04
SGC 70.32 1.87 65.77 0.99 76.27 0.94 83.24 0.81 89.43 1.03 91.11 0.10 OOM
_±_ _±_ _±_ _±_ _±_ _±_
GAT 76.97±1.18 61.28±1.62 83.57±0.23 83.84±1.93 90.54±0.56 89.68±0.42 93.91±0.20
JKNet 78.77 0.79 64.62 0.80 82.82 0.16 82.22 1.32 88.43 0.53 90.48 0.13 93.75 0.32
_±_ _±_ _±_ _±_ _±_ _±_ _±_
APPNP **79.95±0.72 65.56±0.64 84.00±0.22** 83.83±0.78 90.50±0.59 91.90±0.12 94.84±0.08
GPRGNN 78.17 1.31 61.26 2.14 84.54 0.24 83.77 1.06 89.86 0.63 91.34 0.25 94.63 0.26
_±_ _±_ _±_ _±_ _±_ _±_ _±_


1122....0505GNNGNNGNNGNN 77787979....11690615±±±0000....39434139 63636363....17149216±±±0011....89931425 84838384....14972488±±±0000....46042709 84 84 8382....64846457±±±0001....98719642 90 908989....60170567±±±0000....69888567 92 929292....53319374±±±0000....22191426 94 94 9594....92930586±±±0000....10241209
_±_ _±_ _±_ _±_ _±_ _±_ _±_

F.10 TRAINING CURVES FOR p = 1


cora

|Col1|Col2|Col3|
|---|---|---|
||||
||||
||||
||||
||||



200 400Epoch600 800 1000




100

80

60

40

20

0

100

80

60

40

20


2.00

1.75

1.50

1.25

1.00

0.75

0.50

0.25

0.00

3.0

2.5

2.0

1.5

1.0

0.5

0.0


Training Loss Testing Accuracy


computers

|Col1|Col2|Col3|
|---|---|---|
||||
||||
||||
||||


200 400Epoch600 800 1000

Training Loss Testing Accuracy

physics


100

80

60

40

20

0

100

80

60

40

20


2.00

1.75

1.50

1.25

1.00

0.75

0.50

0.25

0.00

2.00

1.75

1.50

1.25

1.00

0.75

0.50

0.25

0.00


|Training Loss|Testing|Accuracy|
|---|---|---|
||||
||||
||||
||||
||||
||||


200 400Epoch600 800 1000

Training Loss Testing Accuracy

actor


|Training Loss|Testing|Accuracy|
|---|---|---|
||||
||||
||||
||||
||||
||||


Training Loss Testing Accuracy

2.00 citeseer 100 2.00 pubmed 100

1.75 Training Loss Testing Accuracy 1.75 Training Loss Testing Accuracy

1.50 80 1.50 80

1.25 60 1.25 60

1.00 1.00

Loss Loss

0.75 40 Accuracy 0.75 40 Accuracy

0.50 20 0.50 20

0.25 0.25

0.00 0 200 400Epoch600 800 1000 0 0.00 0 200 400Epoch600 800 1000 0

3.0 photo 100 3.0 cs 100

Training Loss Testing Accuracy Training Loss Testing Accuracy

2.5 80 2.5 80

2.0 60 2.0 60

1.5 1.5

Loss Loss

1.0 40 Accuracy 1.0 40 Accuracy

0.5 20 0.5 20

0.0 0 200 400Epoch600 800 1000 0 0.0 0 200 400Epoch600 800 1000 0

2.00 chameleon 100 2.00 squirrel 100

1.75 Training Loss Testing Accuracy 1.75 Training Loss Testing Accuracy

1.50 80 1.50 80

1.25 60 1.25 60

1.00 1.00

Loss Loss

0.75 40 Accuracy 0.75 40 Accuracy

0.50 20 0.50 20

0.25 0.25

0.00 0 200 400Epoch600 800 1000 0 0.00 0 200 400Epoch600 800 1000 0

3.0 wisconsin 100 2.00 texas 100

2.5 Training Loss Testing Accuracy 80 1.751.50 Training Loss Testing Accuracy 80

2.0 60 1.25 60

1.5 1.00

Loss Loss

1.0 40 Accuracy 0.75 40 Accuracy

0.5 20 0.500.25 20

0.0 0 200 400Epoch600 800 1000 0 0.00 0 200 400Epoch600 800 1000 0

2.00 cornell 100

1.75 Training Loss Testing Accuracy

1.50 80

1.25 60

1.00

Loss

0.75 40 Accuracy

0.50 20

0.25

0.00 0 200 400Epoch600 800 1000 0

Figure 7: The curves of training loss and testing accuracy for p = 1.


-----

F.11 VISUALIZATION RESULTS OF NODE EMBEDDINGS


cora cora 80 cora

80 80

60

60 60

40

40 40

20

20 20

0

0 0

20

20 20

40

40 40

60 60

60 40 20 0 20 40 60 60 40 20 0 20 40 60 40 20 0 20 40 60


(a) [1][.][0]GNN.


(b) [1][.][5]GNN.


(c) [2][.][0]GNN.


80 cora 80 cora

60 60

40 40

20 20

0 0

20 20

40 40

60

60 40 20 0 20 40 60 60 40 20 0 20 40 60


(d) [2][.][5]GNN.


(e) GCN.


Figure 8: Visualization of node embeddings for Cora dataset using t-SNE (van der Maaten & Hinton,
2008)


computers computers computers

75 75 75

50 50 50

25 25 25

0 0 0

25 25 25

50 50 50

75 75 75

75 50 25 0 25 50 75 75 50 25 0 25 50 75 100 75 50 25 0 25 50 75


(a) [1][.][0]GNN.


(b) [1][.][5]GNN.


(c) [2][.][0]GNN.


computers computers

75 100

75

50

50

25

25

0 0

25 25

50 50

75 75

100

75 50 25 0 25 50 75 100 75 50 25 0 25 50 75 100


(d) [2][.][5]GNN.


(e) GCN.


Figure 9: Visualization of node embeddings for Computers dataset using t-SNE.


-----

chameleon chameleon chameleon

40 40

40

20 20

20

0 0

0

20 20

20

40

40

40

60 40 20 0 20 40 60 60 40 20 0 20 40 60 40 20 0 20 40 60


(a) [1][.][0]GNN.


(b) [1][.][5]GNN.


(c) [2][.][0]GNN.


chameleon chameleon

60

40

40

20 20

0 0

20

20

40

40

60

60 40 20 0 20 40 60 20 0 20 40 60


(d) [2][.][5]GNN.


(e) GCN.


Figure 10: Visualization of node embeddings for Chameleon dataset using t-SNE.


wisconsin

15 10 5 0 5 10 15

|15 10 5 0 5 10|Col2|
|---|---|



(a) [1][.][0]GNN.


wisconsin

10 5 0 5 10 15

|10 5 0 5 10|Col2|
|---|---|



(b) [1][.][5]GNN.


20 wisconsin

10 5 0 5 10 15

|20 15 10 5 0 5 10|Col2|
|---|---|



(c) [2][.][0]GNN.

wisconsin


wisconsin

|15 10 5 0 5 10 15|Col2|
|---|---|


10 5 0 5 10 15


(d) [2][.][5]GNN.


15 10 5 0 5 10 15 20 25

|10 5 0 5 10 15|Col2|
|---|---|



(e) GCN.


Figure 11: Visualization of node embeddings for Wisconsin Dataset using t-SNE.


-----

