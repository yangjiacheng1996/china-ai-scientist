# ANTI-OVERSMOOTHING IN DEEP VISION TRANS## FORMERS VIA THE FOURIER DOMAIN ANALYSIS:
# FROM THEORY TO PRACTICE

**Peihao Wang, Wenqing Zheng, Tianlong Chen & Zhangyang Wang**
Department of Electrical and Computer Engineering, The University of Texas at Austin
{peihaowang,w.zheng,tianlong.chen,atlaswang}@utexas.edu

ABSTRACT

Vision Transformer (ViT) has recently demonstrated promise in computer vision
problems. However, unlike Convolutional Neural Networks (CNN), it is known
that the performance of ViT saturates quickly with depth increasing, due to the
observed attention collapse or patch uniformity. Despite a couple of empirical
solutions, a rigorous framework studying on this scalability issue remains elusive. In this paper, we first establish a rigorous theory framework to analyze
ViT features from the Fourier spectrum domain. We show that the self-attention
mechanism inherently amounts to a low-pass filter, which indicates when ViT
scales up its depth, excessive low-pass filtering will cause feature maps to only
preserve their Direct-Current (DC) component. We then propose two straightforward yet effective techniques to mitigate the undesirable low-pass limitation.
The first technique, termed AttnScale, decomposes a self-attention block into
low-pass and high-pass components, then rescales and combines these two filters to produce an all-pass self-attention matrix. The second technique, termed
**FeatScale, re-weights feature maps on separate frequency bands to amplify the**
high-frequency signals. Both techniques are efficient and hyperparameter-free,
while effectively overcoming relevant ViT training artifacts such as attention collapse and patch uniformity. By seamlessly plugging in our techniques to multiple
ViT variants, we demonstrate that they consistently help ViTs benefit from deeper
architectures, bringing up to 1.1% performance gains “for free” (e.g., with little
parameter overhead). We publicly release our codes and pre-trained models at
[https://github.com/VITA-Group/ViT-Anti-Oversmoothing.](https://github.com/VITA-Group/ViT-Anti-Oversmoothing)

1 INTRODUCTION

Transformers have achieved phenomenal success in Natural Language Processing (NLP) (Vaswani
et al., 2017; Devlin et al., 2018; Dai et al., 2019; Brown et al., 2020), and recently in a wide range
of computer vision applications too (Dosovitskiy et al., 2020; Liu et al., 2021; Arnab et al., 2021;
Carion et al., 2020; Jiang et al., 2021a). One representative advance, the Vision Transformer (ViT)
(Dosovitskiy et al., 2020), stacks Multi-head Self-Attention (MSA) blocks, by treating each local
image patch as semantic tokens and modeling their interactions globally. Unlike Convolutional
Neural Networks (CNNs) that hierarchically enlarge the receptive from local to global, even a shallow
ViT is able to effectively capture the global contexts, leading to their very competitive performance
on image classification and other tasks (Liu et al., 2021; Jiang et al., 2021a).

Going deep has always been a trend in deep learning (LeCun et al., 2015; Krizhevsky et al., 2012),
and ViT was expected to make no exception. One might reasonably conjecture that a deeper ViT with
more MSA blocks significantly outperform its shallower baseline. Unfortunately, building deeper
ViTs face practical challenges. Empirically, Zhou et al. (2021a) shows a vanilla ViT of 32 layers
under-performs the 24-layer one. Gong et al. (2021) demonstrates a downgraded patch diversity in
deeper layers, and Dong et al. (2021) mathematically reveals the rank collapse phenomenon when
Transformer goes deeper. Despite efforts towards deep ViT through patch diversification (Gong
et al., 2021; Zhou et al., 2021b), rank collapse alleviation (Zhou et al., 2021a; Zhang et al., 2021),
and training stabilization (Touvron et al., 2021b; Zhang et al., 2019), most of them are restricted to


-----

empirical studies. Rethinking the problem with deep ViT from a more principled angle pends further
efforts.

In this paper, we present the first rigorous analysis of stacking self-attention mechanism in the Fourier
space. We mathematically show that cascading self-attention blocks is equivalent to repeatedly
applying a low-pass filter, regardless of the input key or query tokens (Section 2.2). As a consequence,
going deeper with vanilla ViT blocks only preserves Direct Component (DC) of the signal at the
output layer. This theoretical finding explains the observations of patch uniformity and rank collapse,
and is also inherently related to the over-smoothing phenomenon in Graph Convolutional Networks
(GCNs) (Kipf & Welling, 2017; NT & Maehara, 2021; Oono & Suzuki, 2019; Cai & Wang, 2020).
Moreover, we also reveal the role of other transformer modules (e.g., MLP and residual connection)
in preventing this undesirable low-pass filtering (Section 2.3).

Built on the aforementioned analysis framework in the Fourier domain, we propose two novel
techniques, to mitigate the low-pass filtering effect of self-attention and effectively scale up the
depth of ViTs. The first technique, termed Attention Scaling (AttnScale), directly manipulates on the
calculated attention map to enforce an all-pass filter (Section 3.1). It decomposes the self-attention
matrix into a low-pass filter plus a high-pass filter, then adopts a learnable weight to adaptively
amplify the effect of high-pass filter. The second technique, termed Feature Scaling (FeatScale),
hinges on feature maps to re-weight different frequency bands separately (Section 3.2). It employs
trainable coefficients to re-mix the DC and high-frequency components, hence selectively enhancing
the high-frequency portion of the MSA output. Both AttnScale and FeatScale are extremely memory
and computationally friendly. Neither runs Fourier transformation explicitly, bringing little extra
complexity to the original ViTs.

Our contributions can be summarized as follows:

-  We establish the first rigorous theoretical analysis of ViT from the spectral domain. We
characterize the low-pass filtering effect of cascading MSAs, which connects to the recent
empirical findings of ViT patch diversity loss or rank collapse.

-  We present two theoretically grounded Fourier-domain scaling techniques, named AttnScale
and FeatScale. They operating on re-adjusting the low- and high-frequency components of
the attention maps and feature maps, respectively. Both are efficient, hyperparameter-free,
easy-to-use, and able to generalize across different ViT variants.

-  We conduct extensive experiments by integrating AttnScale and FeatScale with different ViT
backbones. Both of our approaches substantially boost DeiT, CaiT, and Swin-Transformer
with up to 1.1%, 0.6% and 0.5% performance gains, without whistles and bells.

2 WHY VIT CANNOT GO DEEPER?

2.1 NOTATION AND PRELIMINARIES

We begin by introducing our notations. Let X ∈ R[n][×][d] denote the feature matrix, where n is the
number of samples, anddenote the feature vector of therepresent signals of the j d-th channel. In the context of ViT, is the feature dimension. Let i-th sample, and zj ∈ xRi[n] ∈, ∀ XRj = 1[d] denotes a set (sequence) of image, ∀i = 1, · · ·, d, · · ·, the, n j, the-th column of i-th row of X X,,
patches, xi denotes the flatten version of the i-th patch embedding (d = patch width × patch height),
and zj denotes a whole image signal of the j-th channel.

**Transformer Architecture** Vision Transformer (ViT) consists of three main components: a patch
embedding and position encoding part, a stack of transformer encoder block with Multi-Head
Self-Attention (MSA) and Feed-Forward Network (FFN), and a score readout function for image
classification. We depict a transformer block in Fig. 3a. The key ingredient here is the Self-Attention
(SA) module, which takes in the token representation of the last layer, and encodes each image
token by aggregating information from other patches with respect to the computed attention value,
formulated as below (Vaswani et al., 2017):


**_XWQ(XWK)T_**
SA(X) = softmax _√d_



**_XWV,_** (1)


-----

8

|Col1|MHA|Col3|Col4|
|---|---|---|---|
||||Actual Theorem|
|||||
|||||
|||||
|||||
|||||
|||||


Layer Index


MHA + FFN

8

|Col1|Col2|Col3|Actual|
|---|---|---|---|
|||||
||||Theorem|
|||||
|||||
|||||
|||||
|||||


Actual
Theorem

Layer Index


MHA + FFN + Res

4 8

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
||||Actual|
|||||
||||Theorem|
|||||
|||||
|||||


Actual
Theorem

Layer Index


10

15


10

15


12


12


12


Figure 1: Visualize the intensity of high-frequency component and their theoretical upper bounds
under different transformer blocks. The blue line is defined by log(∥HC [Xl]∥F /∥HC [X0]∥F ), and
the red line is estimated using the results in Section 2.2 & 2.3. See details in Appendix F.1.

where WK ∈ R[d][×][d][k] _, WQ ∈_ R[d][×][d][q] _, WV ∈_ R[d][×][d] are the key, query, and value weight matrices,
respectively, _√d here denotes a scaling factor, and softmax(·) operates on X row-wisely. Multi-Head_

Self-Attention (MSA) involves a group of SA heads and combines their outputs through a linear
projection (Vaswani et al., 2017):
MSA(X) = [SA1(X) SAH (X)] WO, (2)

_· · ·_
where the subscripts denote the SA head number, H is the total number of SA heads, and WO
R[Hd][×][d] projects multi-head outputs to the hidden dimension. Besides MSA module, each transformer ∈
block is equipped with a normalization layer, feed-forward network, and skip connections to cooperate
with MSA. Formally, a transformer block can be written as follows:
**_X_** _[′]_ = MSA(LayerNorm(X)) + X, (3)

**_Y = FFN(LayerNorm(X_** _[′])) + X_ _[′]._ (4)


**Fourier Analysis** The main analytic tool in this paper is Fourier transform. Denote F : R[n] _→_
C[n] be the Discrete Fourier Transform (DFT) with the Inverse Discrete Fourier Transform (IFT)
_F_ _[−][1]_ : C[n] _→_ R[n]. Applying F to a flatten image signal x is equivalent to left multiplying a DFT
matrix, whose rows are the Fourier basis fk = _e[2][π][j(][k][−][1)][·][0]_ _· · ·_ _e[2][π][j(][k][−][1)][·][(][n][−][1)][][T][ ][√]n ∈_ R[n],
wherespectrum ofrespectively. DefineHC [z k] = [ denotes thef2 z, and· · · _DC ˜ kzfdcn-th row of DFT matrix, and [] ∈ ˜zzhc] = ∈C ˜z, ˜Cdcz[n]hcfthe complementary high-frequency component.1 ∈ ∈_ CC[n][n]as the Direct-Current (DC) component of signal[−][1] take the first element and the rest elements of j is the imaginary unit. Let ˜z = Fz be the z, and ˜z,

In signal processing, a low-pass filter is a system that suppresses the high-frequency component
of signals and retain the low-frequency component. In this paper, we refer to low-pass filter as a
particular type of filters that only preserve the DC component, while diminishing the remaining
high-frequency component. To be more precise, we define low-pass filters in Definition 1.
**Definition 1. Given an endomorphism f : R[n]** _→_ R[n] _with f_ _[t]_ _denoting applying f for t times, f is a_
_low-pass filter if and only if for all z ∈_ R[n]:

[f _[t](z)]_ 2
lim _∥HC_ _∥_ = 0. (5)
_t→∞_ _∥DC [f_ _[t](z)]∥2_

Definition 1 reveals the nature of low-pass filters: they will produce a dominant response on DC
component, while imposing an inhibition effect on the high-frequency band. We refer interested
readers to Appendix A for more useful backgrounds.

2.2 SELF-ATTENTION IS A LOW-PASS FILTER


In this subsection, we will give theoretical justification on self-attention in terms of its spectral-domain
effect. Our main result is that self-attention is constantly a low-pass filter, which continuously erases
high-frequency information, thus causing ViT to lose features expressiveness at deep layers.

Formally, we have the following theorem that shows attention matrix produced by a softmax function
(e.g, Eqn. 1) is a low-pass filter independent of the input token features or key/query matrices.
**Theorem 1. (SA matrix is a low-pass filter) Let A = softmax(P ), where P ∈** R[n][×][n]. Then A must
_be a low-pass filter. For all z_ R[n], limt [A[t]z] 2/ [A[t]z] 2 = 0.
_∈_ _→∞∥HC_ _∥_ _∥DC_ _∥_


-----

Theorem 1 is a straightforward result of Perron-Frobenius
theorem. See Appendix B.1 for a proof. Theorem 1 also 4 Layer 4
reveals that no matter how attention is computed inside the
softmax function, including dot product (Vaswani et al., 3 Layer 12
2017), linear combination (Veliˇckovi´c et al., 2018), or L2

2

distance (Kim et al., 2021), the resulting attention matrix

Magnitude

is always a low-pass filter. One can see consecutively

1

applying self-attention matrix simulates the process of
ViT’s forward propagation. As the layer number increases 0
infinitely, the final output will only keep the DC bias, and 100 50 0 50 100

|Col1|Layer 4|
|---|---|
||Layer 8 Layer 12|
|||
|||
|||
|||


Frequency

ViT loses all the feature expressive power.

Figure 2: Visualize the spectral response

**Corollary 2. Let P1, P2, · · ·, Pn be a sequence of ma-** of an attention map. We randomly pick
_trix in R[n][×][n], and each Pk, ∀k = 1, · · ·, L has Ak =_ a sample and depict its first head of
softmax(Pk). Then _k=1_ **_[A][k][ is also a low-pass filter.]_** 4/8/12-th layer. Refer to Appendix F.2.

In fact, ViT re-computes self-attention matrices per layer, which seems to avoid the consecutive

[Q][L]

power of an identical self-attention matrix. However, we also provide Corollary 2, which suggests
even the ViT consists of distinctive self-attention matrix at each layer, their composition turns out to
act like a low-pass filter as well. We also visualize the spectrum of attention maps (Fig. 2 and more
on Appendix F.2) to support our theoretical conclusions.

Knowing that self-attention matrices amount to low-pass filters, we are also interested in to which
extent an MSA layer would suppress the high-frequency component. Thereby, we also provide a
convergence rate to illustrate this speed that the high-frequency component are being annihilated.
**Theorem 3. (smoothening rate of SA) Let A = softmax(P ) and α = maxi,j|Pij|, where P ∈**
R[n][×][n]. Define SA(X) = AXWV as the output of a self-attention module, then

_ne[2][α]_

[SA(X)] _F_ (6)
_∥HC_ _∥_ _≤_ r _e[2][α]_ + n − 1 _[∥][W][V][ ∥][2][∥HC][ [][X][]][∥][F][ .]_

_In particular, whenradius γ > 0, i.e., ∥ Px =i∥2 XW ≤_ _γ, ∀Q(i = 1XW, · · ·K)[T], n/√, thend, and assume tokens are distributed inside a ball with α ≤_ _γ[2]∥WQWK[T]_ _[∥][2][/]√d._

The proof of Theorem 3 can be found in Appendix B.3. Theorem 3 says the high-frequency

intensity ratio to the pre- and post- attention aggregation is upper bounded by ∥WV ∥2 _e[2][α]ne+[2]n[α]−1_ [.]

_ne[2][α]_ q

When ∥WV ∥2 _e[2][α]+n−1_ _[<][ 1][,][ HC][ [SA(][X][)]][ converges to zero exponentially. We note that, no]_
q _ne[2][α]_

matter how attention is computed or signals are initialized, since _e[2][α]+n_ 1 [is bounded by][ √][n][,]

_−_

_∥dot-product attention is adopted, a sufficient condition thatWV ∥2 < 1/[√]n will definitely cause a monotonically decreasing high-frequency component. When ∥HC [Xq]∥F decreases to zero within_
logarithmic time is γ[2]∥WQWK[T] _[∥][2][/]√d + log∥WV ∥2 ≤_ log _[n][−]n_ [1] _[/][2][.]_

2.3 EXISTING MECHANISMS THAT COUNTERACT LOW-PASS FILTERING


In this section, we take other ViT building blocks into consideration. We will justify whether MultiHead Self-Attention (MSA), Feed-Forward Network (FFN), and residual connections can effectively
alleviate the low-pass filtering drawbacks. All the derivations follow from Theorem 3, and some
proof ideas are borrowed from Dong et al. (2021). We further present Fig. 1 to justify our results.

**Does multi-head help?** MSA employs weights to combine the results of multiple self-attention
blocks. We can rewrite it as MSA(X) = _h=1_ [SA(][X][)][W][ h]O[. We show by Proposition 4 in]

Appendix C.1 that the convergence rate turns to σ1σ2H _e[2][α]ne+[2]n[α]_ 1 [, where][ H][ is the number of heads,]

_−_

[P][H]

_σconvergence up to a constant1 = max[H]h=1[∥][W][ h]V_ _[∥][2][ and][ σ] σ[2]2[ = max]H, which does not root out the problem.h[H]=1[∥][W][ h]O[∥][2][. One can see MSA can only slow down the]q_

**Does residual connection benefit?** In addition to MSA, a transformer block also leverages a skip
connection, which can be formulated as Res(X) = MSA(X)+X. We show that residual connection


-----

can effectively prevent high-frequency component from diminishing to zero by promoting the rate


_ne[2][α]_

_e[2][α]+n_ 1 [to][ 1 +][ σ][1][σ][2][H]
_−_


_ne[2][α]_

_e[2][α]+n_ 1 _[>][ 1][. Refer to Proposition 5 in Appendix C.2.]_
_−_


_σ1σ2H_


**Does FFN make any difference?** A feed-forward network is appended to MSA module. We
characterize its effect in Appendix C.3. Our Proposition 6 suggests that a FFN with Lipschitz constant

_σ3 contributes a σ3_ 1 + σ1σ2H _e[2][α]ne+[2]n[α]−1_ convergence rate, which does not improve the original

one. However, if skip connection is adopted over FFN, q  _σ3 > 1 can guarantee the upper bound of the_
high-frequency component is non-contractive.

Although multi-head, FFN, and skip connection all help preserve the high-frequency signals, none
would change the fact that MSA block as a whole only possesses the representational power of
low-pass filters. Our Proposition 4, 5, 6 states multi-head, FFN, skip connections can only slow down
the convergence by indistinguishably amplifying low- and high-frequency components with the same
factor. However, since they are incapable of promoting high-frequency information separately, it is
inevitable that high-frequency components are continuously diluted as ViT goes deeper. This restricts
the expressiveness of ViT, resulting in the performance saturation in deeper ViT.

2.4 CONNECTIONS TO EXISTING THEORETIC UNDERSTANDING WORKS

It is known that Graph Convolutional Networks (GCN) are not more than low-pass filters (NT
& Maehara, 2021). In the meanwhile, Oono & Suzuki (2019); Cai & Wang (2020) pointed out
GCN’s node features will be exponentially trapped into the nullspace of the graph Laplacian matrix.
Similarly, our work concludes that self-attention module is yet another low-pass filter. Combining
with our theoretical derivation, one can see the root reason is that both graph Laplacian matrices
and self-attention matrices consistently own a fixed leading eigenvector, namely the DC basis. This
makes aggregating information via such matrices inherently project the token representation onto
these invariant eigenspaces. And we note that over-smoothing, rank collapse, and patch uniformity
are all the manifestation of excessive low-pass filtering. See Appendix D.1 for more discussion.

In Dong et al. (2021), the authors proved that ViT’s feature maps will doubly exponentially collapses
to a rank-1 matrix, which reveals ViT loses feature expressiveness at deep layers. Besides, they
also gave a systematic study on other building blocks of transformer. While they share the similar
insights with us, our work further specifies which rank-1 matrix the feature activation will converge
to, namely the subspace spanned by the DC basis. That makes our theory to be better grounded
with signal-processing and geometric interpretations, via directly measuring the intensity of the highfrequency residual, instead of examining a composite norm distance to an agnostic rank-1 matrix.
Although Dong et al. (2021) presented a faster convergence speed, we respectfully suggest that the
current proof of Dong et al. (2021) might be deficient, or at least incomplete in the assumptions
(see Appendix D.2). Moreover, our theory can be generalized to other attention mechanisms such as
logistic attention (Veliˇckovi´c et al., 2018) and L2 distance (Kim et al., 2021). See Appendix D.3.

3 ATTNSCALE & FEATSCALE: SCALING FROM THE FOURIER-DOMAIN

3.1 ATTNSCALE: MAKE ATTENTION AN ALL-PASS FILTER

As we discussed in Section 2.2, self-attention matrix can only perform low-pass filtering, which
narrows the filter space ViT can express. Inspired by this, we propose a scaling techniques directly
manipulating the attention map, termed Attention Scaling (AttnScale), to balance the effects of lowand high-pass filtering and produce all-pass filters. AttnScale decomposes the self-attention matrix to
a low-pass filter plus a high-pass filter, and introduces a trainable parameter to rescale the high-pass
filter to match the magnitude with the low-pass component.

Formally, let A denote a self-attention matrix. To decompose a low-pass filter from A, we find the
_largest possible low-pass filter that can be extracted from A. We use Lemma 8 in Appendix E to_
justify our solution. By Lemma 8, we can simply extract L = F _[−][1]_ diag(1, 0, · · ·, 0)F = 11[T] _/n_
from A and take the complementary part as the high-pass filter. Afterward, we can rescale the
high-pass component of the filter, and combine low-pass and high-pass together to form a new
self-attention matrix. We illustrate this scaling trick in Fig. 3b. To be more precise, for the l-th layer
and h-th head, we recompute the self-attention map as follows:


-----

0.8

0.6

0.4

0.2

0.5

0.4

0.3


|+ × ( ×( ×|+ + × ' × '×|
|---|---|


&$% &$%&$% &#% &#%&#%


|+|Col2|+ + × %× %×|
|---|---|---|
||||


#!" #!"#!" ##" ##"##"

|e|Col2|
|---|---|
|||


12 15 18 21 24

|Col1|Col2|
|---|---|
|SoftMSaoxf|tSMofatxMax|
|||
|MatMMual|tMMautlMul|

|Col1|Col2|
|---|---|
|MultiM-HuelMta AttenAtitotne|id-uHltei-aHdead Antttieonntion|
|||
|||

|yer Index|Col2|
|---|---|
|e||
|||
|||


DeiT-S
DeiT-S + AttnScale


0 3 6 9 12 15 18 21 24

DeiT-S
DeiT-S + FeatScale

Layer Index

Figure 4: Visualize cosine similarity of attention and feature maps
with/without our proposed methods.
Refer to Appendix F.3 for details.

|Col1|Col2|+|+ +|Col5|Col6|
|---|---|---|---|---|---|
|M||LPMLMP||LP||
|No||rmNo +|rNmo + +|||
|M|ulti Atte|M-HuelMtaid-uHltei- nAtitotneAntttieo||aHdead nntion||
|||||||
|No||rmNo|rNmo|rm||
|||||||


(a)


(b)


(c)


Figure 3: Illustration of our proposed techniques. (a) recalls
the standard ViT block. (b) and (c) illustrate our proposed
AttnScale and FeatScale, which scaling high-pass filter component and high-frequency signals, respectively.


**_A[(]LP[l,h][)]_** = n[1] **[11][T][,]** (7)

**_A[(]HP[l,h][)]_** [=][ A][(][l,h][)][ −] **_[A]LP[(][l,h][)][,]_** (8)

**_Aˆ[(][l,h][)]_** = A[(]LP[l,h][)] + (ωl,h + 1)A[(]HP[l,h][)][,] (9)

where ωl,h is a trainable parameter, and different layers and heads adopt separate ωl,h. During
training time, ωl,h’s are initialized with 0, and jointly tuned with the other network parameters. By
adjusting ωl,h, **_A[ˆ][(][l,h][)]_** can simulate any type of filters: low-pass, high-pass, band-pass, or all-pass.
Note that our AttnScale is extremely lightweight, as it only brings O(HL) extra parameters, where
_H is the number of heads, and L is the number of ViT blocks._


3.2 FEATSCALE: REWEIGHT HIGH-FREQUENCY SIGNALS

According to our analysis in Section 2.2, MSA module will indiscriminately suppress high-frequency
signals, which leads to severe information loss. Even though residual connection can retrieve lost
information through the skip path, the high-frequency portion will be inevitably diluted (Theorem
1). To this end, we propose another scaling technique that operates on feature maps, named Feature
_Scaling (FeatScale). FeatScale processes the output of MSA by mixing the information from varying_
frequency bands discriminatively. FeatScale first decomposes the resultant signals into their DC
and high-frequency components. Then it introduces two groups of parameters to re-weight the two
components for each channel, respectively. The pipeline of this scaling technique is depicted in Fig.
3c. To be more precise, we re-weight the output of the l-th MSA by


**_XDC[(][l][)]_** [=][ DC][ [MSA(][X][)] (diag(][s][l][) +][ I][)][,] (10)

**_XHC[(][l][)]_** [=][ HC][ [MSA(][X][)] (diag(][t][l][) +][ I][)][,] (11)

**_X_** [(][l][)] = XDC[(][l][)] [+][ X]HC[(][l][)] _[,]_ (12)

initializewhere s sl ∈l andR[d] tandl with zeros and tune them with gradient descent. After adjusting the proportion of tl ∈ R[d] are learnable parameters to perform channel-wise re-weighting. We
different frequency signals, FeatScale can prevent the dominance of the DC component. DC [·] and
_HC [·] are cheap to compute without explicit Fourier transform. Calculating DC [X] is as simple as_
running the column average of matrix X, and HC [X] can be efficiently computed by X −DC [X].


3.3 DISCUSSION

We have proposed two methods to facilitate the deeper stacking of ViT MSA modules, and discussed
their motivations and strengths from our perspective of filtering and signal processing. In this section,
we will connect these two techniques with commonly mentioned problems with ViTs.

**How does AttnScale prevent attention collapse?** Deep ViT suffers from the attention collapse
issue (Zhou et al., 2021a). When transformer goes deepr, the attention maps gradually become similar


-----

and even much the same after certain layers. Combining with our thoery, one can see collapsed
attention maps turn out to be a pure low-pass filter, which wipes off all the high-frequency information
in one shot. Zhou et al. (2021a) proposed the re-attention trick, which blends attention map across
different heads. By doing this, modified attention maps aggregate high-pass components from other
heads and is endowed with richer filtering property. We note that our AttnScale is akin to a more
lightweight re-attention mechanism with better interpretability. We rewrite the attention map as a sum
of an already-collapsed attention (11[T] _/n) with the complementary residual map that encodes diverse_
patterns in a self-attention matrix. By re-weighting the residual map, the diversified patterns can be
amplified, which prevents it from degenerating to a rank-1 matrix. We further verify this argument
using cosine similarity metric (Zhou et al., 2021a) in the upper sub-figure of Fig. 4.

**How does FeatScale conserve patch diversity?** Self-attention blocks tend to map different patches
into similar latent representations, yielding information loss and performance degradation (Gong et al.,
2021). By our theory, this asymptotic smoothness of feature map is caused by excessive low-pass
filtering, and the remaining DC bias signifies uniform patch representations. Conventional approaches
to addressing this problem include incorporating convolutional layers (Wu et al., 2021; Jiang et al.,
2021b) and enforcing patch diversity regularizations (Gong et al., 2021). As diverse features are
often characterized by high-frequency signals, our FeatScale instead elevating the high-frequency
component via a learnable scaling factor, can be regarded as a more straightforward way to reconstruct
the patch richness. Compared with LayerScale (Touvron et al., 2021b), in which each frequency
band is equally scaled, our FeatScale treating DC and high-frequency components differently, not
only perform a per-channel normalization, but also perform a spectral-domain calibration with highfrequency details and low-frequency characteristics. The lower sub-plot of Fig. 4 shows ViT with our
FeatScale has lower feature similarity at deep layer.

4 RELATED WORK

**Transformers in Vision.** Transformer (Vaswani et al., 2017) entirely relies on self-attention mechanism to capture correlation and exchange information globally among the input. It has achieved a
remarkable performance in natural language processing (Devlin et al., 2018; Dai et al., 2019; Brown
et al., 2020) and many cross-disciplinary applications (Jumper et al., 2021; Ying et al., 2021; Zheng
et al., 2021b). Recent advances have also successfully applied Transformer to computer vision tasks.
Dosovitskiy et al. (2020) first adopts a pure transformer architecture (ViT) for image classification.
The follow-up works (Chen et al., 2021b) extend ViT to various vision tasks, such as object detection
(Carion et al., 2020; Zhu et al., 2021; Zheng et al., 2021a; Sun et al., 2020), segmentation (Chen
et al., 2021a; Wang et al., 2021), image generation (Parmar et al., 2018; Jiang et al., 2021a), video
processing (Zhou et al., 2018; Arnab et al., 2021), and 3D instance processing (Guo et al., 2021; Lin
et al., 2021). To capture multi-scale non-local contexts, Zhang et al. (2020) designs transformers
in self-level, top-down, and bottom-up interaction fashion. Liu et al. (2021) presents hierarchical
ViTs with shifted window based attention that can efficiently extract multi-scale features. To dismiss
ViT from the heavy reliance on large-scale dataset pre-training, Touvron et al. (2021a); Yuan et al.
(2021) propose knowledge distillation and progressive tokenization for data-efficient training. Despite
impressive effectiveness, most of these model are only based on relatively shallow ViT backbones
with a dozen of MSA blocks.

**Advances in deep ViTs.** Building deeper ViTs has arisen many interests. Zhou et al. (2021a) first
investigated the depth scalability of ViT. The authors found that the attention collapse hinders ViT
from scaling up, and propose two methods to conquer this problem i) increasing the embedding
dimension, and ii) a cross-head re-attention trick to regenerate attention map. A concurrent work
Touvron et al. (2021b) came up with a LayerScale layer that performs per-channel multiplication for
each residual block. More importantly, they make explicit separation of transformer layers involving
self-attention between patches, from class-attention layers that are devoted to extract the global
content into a single embedding to be decoded. Gong et al. (2021) further proposed a series of losses
that can enforce patch diversity in ViT. Such regularizations include penalty on cosine similarity,
patch-wise contrastive loss, and mixing loss. Tang et al. (2021) presented a shortcut augmentation
scheme with block-circulant projection to improve feature diversity. Although these existing solutions
manage to deepen ViTs, most of them are empirical works and bring no principled theory.

**Role of depth in NNs.** Discussing the importance of deep structures in Neural Networks (NNs) is
an overly broad topic. Here we only focus on a subset of works that scaling up a transformer could


-----

Table 1: Experimental evalutation of AttnScale & FeatScale plugged into DeiT and CaiT. The number
inside the (↑·) represents the performance gain compared with the baseline model, and accuracies
within/out of parenthesis are the reported/reproduced performance.

|Backbone|Method|Input size # Layer # Param FLOPs Throughput|Top-1 Acc (%)|
|---|---|---|---|
|DeiT|DeiT-S DeiT-S + AttnScale DeiT-S + FeatScale|224 12 22.0M 4.57G 1589.4 224 12 22.0M 4.57G 1416.7 224 12 22.0M 4.57G 1509.9|79.8 (79.9) 80.7 (↑0.9) 80.9 (↑1.1)|
||DeiT-S DeiT-S + AttnScale DeiT-S + FeatScale|224 24 43.3M 9.09G 836.4 224 24 43.3M 9.10G 722.0 224 24 43.4M 9.10G 772.5|80.5 (81.0) 81.1 (↑0.6) 81.3 (↑0.8)|
|CaiT|CaiT-S CaiT-S + AttnScale CaiT-S + FeatScale|224 24 46.9M 9.33G 371.9 224 24 46.9M 9.34G 339.0 224 24 46.9M 9.34G 358.2|82.6 (82.7) 83.2 (↑0.6) 83.2 (↑0.6)|
|Swin|Swin-S Swin-S + AttnScale Swin-S + FeatScale|224 24 49.6M 8.74G 593.2 224 24 49.6M 8.75G 553.4 224 24 49.6M 8.75G 550.3|83.0 (83.0) 83.4 (↑0.4) 83.5 (↑0.5)|



relate to. For ordinal deep learning models, such as FFNs and CNNs, deep architecture immediately
benefits from the universal approximation power and expressive capacity (Cybenko, 1989; Hornik,
1991; Telgarsky, 2016; Lu et al., 2017; Petersen & Voigtlaender, 2020; Zhou, 2020). In contrast,
several studies in graph learning domain have reported severe performance degradation due to oversmoothing when stacking many layers (Kipf & Welling, 2017; Wu et al., 2019; Li et al., 2018). The
subsequent studies (NT & Maehara, 2021; Oono & Suzuki, 2019; Cai & Wang, 2020) gave theoretical
explanations of the over-smoothing phenomena from the views of graph signal filtering and feature
dynamics. Likewise, ViT have been witnessed performance saturation when going deeper. However,
to our best knowledge, Dong et al. (2021) is the sole work in the literature that systematically and
rigorously analyzes this issue with deep ViT. The main idea of this work is showing the self-attention
block will downgrade the rank of the feature maps. Our work takes one step forward by studying ViT
on spectral domain, and manages to reveal the signals will ultimately fall into the one-dimension DC
subspace. We see our theory and techniques are also applicable to NLP transformers. However, we
only focus on ViT because empirical observations indicate NLP modeling (including Transformer)
does not require a deep structure (Vaswani et al., 2017; Brown et al., 2020), while vision tasks always
demand one (LeCun et al., 2015).

5 EXPERIMENTS

In this section, we report experiment results to validate our proposed methods. First, we validate the
effectiveness of our AttnScale and FeatScale when integrated with different deep ViT backbones
(Section 5.1). Second, we compare our best models with state-of-the-art (SOTA) results (Section 5.2).
All of our experiments are conducted on the ImageNet dataset (Russakovsky et al., 2015) with around
1.3M images in the training set and 50k images in the validation set. Our implementations are based
on Timm (Wightman, 2019) and DeiT (Touvron et al., 2021a) repositories.

5.1 HOW CAN ATTNSCALE & FEATSCALE BENEFIT DEEP VIT?

**Experiment Settings.** In this subsection, we intend to testify our models are beneficial to various
ViT backbones with different depth settings and training modes. We choose DeiT (Touvron et al.,
2021a) as our first backbone in order to train from scratch. When training 12-layer DeiT, we follow
the same training recipe, hyper-parameters, and data augmentation with Touvron et al. (2021a). When
training 24-layer DeiT, we follow the setting in Gong et al. (2021). Specially, we set dropout rate to
0.2 when training 24-layer DeiT (Touvron et al., 2021b). Our second backbone is CaiT (Touvron
et al., 2021b). We only apply our techniques to the patch embedding layers. The third backbone is
the SOTA model Swin-Transformer (Liu et al., 2021). All experimental settings share the same with
Liu et al. (2021). In addition to training from scratch, we also investigate the fine-tuning setting. We
defer this part to Appendix G.1.

**Results.** All of our experimental evaluations are summarized in Table 1. The results suggest
our proposed AttnScale and FeatScale successfully facilitate both DeiT, CaiT, Swin-Transformer
under different depth settings and training modes. Specifically, AttnScale brings less than 100/150
_extra parameters for 12/24-layer DeiT while boosting the top-1 accuracy by 0.9% for 12-layer_
DeiT and 0.6% for 24-layer DeiT. Our FeatScale substantially improves top-1 accuracy by 1% for
12-layer DeiT and 0.8% for 24-layer DeiT. Compared with existing techniques, the improvements


-----

Table 2: Compared with state-of-the-art models on ImageNet dataset. Accuracies with superscript (*)
are reported by Gong et al. (2021), with superscript (†) are reported by Yuan et al. (2021), and others
are reported by the original papers. Bold accuracies signifies best models among pure transformers.

|Category|Method|# Param Input size # Layer|Top-1 Acc (%)|
|---|---|---|---|
|CNN|ResNet-152 (He et al., 2016) DenseNet-201 (Huang et al., 2017)|230M 224 152 77M 224 201|78.1 * 77.6 *|
|CNN+ Transformer|CVT-21 (Wu et al., 2021) LV-ViT-S (Jiang et al., 2021b)|32M 224 21 26M 224 16|82.5 * 83.3 *|
|Transformer|ViT-S/16 (Dosovitskiy et al., 2020) ViT-B/16 (Dosovitskiy et al., 2020) DeiT-S (Touvron et al., 2021a) DeiT-S Distilled (Touvron et al., 2021a) Swin-S (Liu et al., 2021)|49M 224 12 86M 224 12 22M 224 12 22M 224 12 50M 224 12|78.1 † 79.8 † 79.8 81.2 83.0 82.3 80.1 82.7 82.2|
||T2T-ViT-24 (Yuan et al., 2021) DeepViT-24B (Zhou et al., 2021a) CaiT-S (Touvron et al., 2021b) DeiT-S + DiversePatch (Gong et al., 2021)|64M 224 24 36M 224 24 47M 224 24 44M 224 24||
|Ours|DeiT-S + AttnScale DeiT-S + FeatScale CaiT-S + AttnScale CaiT-S + FeatScale Swin-S + AttnScale Swin-S + FeatScale|43M 224 24 43M 224 24 47M 224 24 47M 224 24 50M 224 24 50M 224 24|81.1 81.3 83.2 83.2 83.4 83.5|



of AttnScale and FeatScale already surpass re-attention (0.6%) (Zhou et al., 2021a), LayerScale
(0.7%) (Touvron et al., 2021b), and late class token insertion (0.6%) (Touvron et al., 2021b). We also
observe a consistent 0.6% performance gain when AttnScale and FeatScale plugged into CaiT. Under
fine-tuning setting, as we will show in Appendix G.1, only tens of epoch’s fine-tuning can further
promote their performance by ≥ 0.2% (see Table 3). On Swin-Transformer, both our AttnScale
and FeatScale bring around 0.5% accuracy gain. This makes Swin-S with 50M parameters even
comparable to Swin-B (83.5% top-1 accuracy on ImageNet1k) with 88M parameters. We defer more
model interpretation and visualization to Appendix G. For a brief summary, we observe that both
shallow and deep ViT enjoy from AttnScale and FeatScale that: 1) the attention maps can simulate
richer filtering properties (compare Fig. 9 with Fig. 10), and 2) more high-frequency data can be
preserved (refer to Fig. 11).

5.2 COMPARISON WITH SOTA MODELS

In this subsection, we compare our best models with state-of-the-art models on ImageNet benchmark.
We choose SOTA models from three classes: CNN only, CNN + transformer, and pure transformer.
For transformer domain, we only conduct experiments with those lightweight models with comparable
number of parameters, such as ViT-S and DeiT-S. All the results are presented in Table 2.

Among all methods, Swin-Transformer combined with our methods achieves the state-of-the-art
performance. Our CaiT-S + AttnScale and CaiT-S + FeatScale on 24-layer CaiT-S also attain superior
results over all other pure transformers, while keeping low parameter cost. That our performance
surpasses some CNN-based models (e.g., ResNet-152 and CVT) indicates by increasing depth, ViT
will be endowed with higher potential to surpass CNNs that have been dominating computer vision
domain so far. Our DeiT-S+FeatScale result also outperforms ViT-B/16 and DeiT-S Distilled, which
suggests deepening network can bring more considerable accuracy gain than increasing model width
or employing a teacher model.

6 CONCLUSION

In this paper, we investigate the scalability issue with ViT and propose two practical solutions
via Fourier domain analysis. Our theoretical findings indicate Multi-Head Self-Attention (MSA)
inherently performs low-pass filtering on image signals, thus causes rank collapse and patch uniformity
problems in deep ViT. To this end, we proposed two techniques, AttnScale and FeatScale, that can
effectively break such low-pass filtering bottleneck by adaptively scaling high-pass filter component
and high-frequency signals, respectively. Our experiments also validate the effectiveness of our
methods. Both techniques can boost various ViT backbones by a significant performance gain.
Grounded with our theoretical framework, interesting directions for further work include designing
parameter regularizations and spectrum-specific normalization layers.


-----

ACKNOWLEDGMENTS
Z.W. is in part supported by an NSF SCALE MoDL project (#2133861).

REFERENCES

Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇci´c, and Cordelia Schmid.
Vivit: A video vision transformer. In IEEE International Conference on Computer Vision (ICCV),
2021.

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in Neural Information Processing Systems (NeurIPS), 2020.

Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. In International
_Conference on Machine Learning Workshop (ICMLW), 2020._

Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey
Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer
_Vision (ECCV), 2020._

Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing
Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In IEEE Conference on
_Computer Vision and Pattern Recognition (CVPR), 2021a._

Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, and Zhangyang Wang. Chasing sparsity in
vision transformers: An end-to-end exploration. In Advances in Neural Information Processing
_Systems (NeurIPS), 2021b._

George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
_Signals and Systems, 1989._

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.
Transformer-xl: Attentive language models beyond a fixed-length context. In Annual Meeting of
_the Association for Computational Linguistics (ACL), 2019._

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv:1810.04805, 2018.

Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure
attention loses rank doubly exponentially with depth. In International Conference on Machine
_Learning (ICML), 2021._

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image
is worth 16x16 words: Transformers for image recognition at scale. In International Conference
_on Learning Representations (ICLR), 2020._

Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and Qiang Liu. Vision transformers with
patch diversification. arXiv: 2104.12753, 2021.

Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min Hu.
Pct: Point cloud transformer. Computational Visual Media, 2021.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks,
1991.

Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2017.


-----

Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two pure transformers can make
one strong gan, and that can scale up. In Advances in Neural Information Processing Systems
_(NeurIPS), 2021a._

Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie Jin, Anran Wang, and Jiashi Feng. All
tokens matter: Token labeling for training better vision transformers. In Advances in Neural
_Information Processing Systems (NeurIPS), 2021b._

John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 2021.

Hyunjik Kim, George Papamakarios, and Andriy Mnih. The lipschitz constant of self-attention. In
_International Conference on Machine Learning (ICML), 2021._

Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In International Conference on Learning Representations (ICLR), 2017.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS),
2012.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 2015.

Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In AAAI Conference on Artificial Intelligence (AAAI), 2018.

Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end human pose and mesh reconstruction with
transformers. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.

Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In International
_Conference on Computer Vision (ICCV), 2021._

Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of
neural networks: A view from the width. In Advances in Neural Information Processing Systems
_(NeurIPS), 2017._

Carl D Meyer. Matrix analysis and applied linear algebra, volume 71. SIAM, 2000.

Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters.
In International Conference on Pattern Recognition (ICPR), 2021.

Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node
classification. In International Conference on Learning Representations (ICLR), 2019.

Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and
Dustin Tran. Image transformer. In International Conference on Machine Learning (ICML), 2018.

Philipp Petersen and Felix Voigtlaender. Equivalence of approximation by convolutional neural
networks and fully-connected networks. Proceedings of the American Mathematical Society, 2020.

Dana Randall. Rapidly mixing markov chains with applications in computer science and physics.
_Computing in Science & Engineering, 2006._

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV),
2015.

Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani. Rethinking transformer-based set
prediction for object detection. In IEEE International Conference on Computer Vision (ICCV),
2020.


-----

Yehui Tang, Kai Han, Chang Xu, An Xiao, Yiping Deng, Chao Xu, and Yunhe Wang. Augmented
shortcuts for vision transformers. In Advances in Neural Information Processing Systems (NeurIPS),
2021.

Matus Telgarsky. Benefits of depth in neural networks. In Conference on Learning Theory, 2016.

Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé
Jégou. Training data-efficient image transformers & distillation through attention. In International
_Conference on Machine Learning(ICML), 2021a._

Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going
deeper with image transformers. In International Conference on Computer Vision (ICCV), 2021b.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
_processing systems (NeurIPS), 2017._

Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations
_(ICLR), 2018._

Nitika Verma, Edmond Boyer, and Jakob Verbeek. Feastnet: Feature-steered graph convolutions for
3d shape analysis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2018.

Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia
Xia. End-to-end video instance segmentation with transformers. In IEEE Conference on Computer
_Vision and Pattern Recognition (CVPR), 2021._

Ross Wightman. Pytorch image models. [https://github.com/rwightman/](https://github.com/rwightman/pytorch-image-models)
[pytorch-image-models, 2019.](https://github.com/rwightman/pytorch-image-models)

Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In International Conference on Machine Learning (ICML),
2019.

Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:
Introducing convolutions to vision transformers. In IEEE International Conference on Computer
_Vision (ICCV), 2021._

Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and
Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural
_Information Processing Systems (NeurIPS), 2021._

Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi
Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on
imagenet. In International Conference on Computer Vision (ICCV), 2021.

Aston Zhang, Alvin Chan, Yi Tay, Jie Fu, Shuohang Wang, Shuai Zhang, Huajie Shao, Shuochao
Yao, and Roy Ka-Wei Lee. On orthogonality constraints for transformers. In Annual Meeting of
_the Association for Computational Linguistics (ACL), 2021._

Biao Zhang, Ivan Titov, and Rico Sennrich. Improving deep transformer with depth-scaled initialization and merged attention. In Conference on Empirical Methods in Natural Language Processing
_(EMNLP), 2019._

Dong Zhang, Hanwang Zhang, Jinhui Tang, Meng Wang, Xiansheng Hua, and Qianru Sun. Feature
pyramid transformer. In European Conference on Computer Vision (ECCV), 2020.

Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, and Hao Dong. End-to-end object
detection with adaptive clustering transformer. In British Machine Vision Conference (BMVC),
2021a.


-----

Wenqing Zheng, Qiangqiang Guo, Hao Yang, Peihao Wang, and Zhangyang Wang. Delayed
propagation transformer: A universal computation engine towards practical control in cyberphysical systems. In Advances in Neural Information Processing Systems (NeurIPS), 2021b.

Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and
Jiashi Feng. Deepvit: Towards deeper vision transformer. arXiv:2103.11886, 2021a.

Daquan Zhou, Yujun Shi, Bingyi Kang, Weihao Yu, Zihang Jiang, Yuan Li, Xiaojie Jin, Qibin Hou,
and Jiashi Feng. Refiner: Refining self-attention for vision transformers. arXiv:2106.03714,
2021b.

Ding-Xuan Zhou. Universality of deep convolutional neural networks. Applied and computational
_harmonic analysis, 2020._

Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong. End-to-end dense
video captioning with masked transformer. In IEEE Conference on Computer Vision and Pattern
_Recognition (CVPR), 2018._

Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:
Deformable transformers for end-to-end object detection. In International Conference on Learning
_Representations (ICLR), 2021._

A MORE PRELIMINARIES ON FOURIER ANALYSIS

In this appendix, we provide more preliminary knowledge about Fourier analysis. Here, we only
consider discrete Fourier transform on real-value domain F : R[n] _→_ C[n]. A Discrete Fourier
transform (DFT) can be written in a matrix form as below [1]:

1 1 _· · ·_ 1

1. _e[2].[π][j]_ _· · ·_ _e[2][π][j(].[n][−][1)]_ 

1 .. .. ... ..
**_DF T =_**   _,_ (13)
_√n_ 1 _e[2][π][j(][k][−][1)][·][1]_ _· · ·_ _e[2][π][j(][k][−][1)][·][(][n][−][1)]_

 ... ... ... ... 
 
1 _e[2][π][j(][n][−][1)]_ _e[2][π][j(][n][−][1)][2]_ 
 _· · ·_ 
 

and its inverse discrete Fourier transform is DF T _[−][1]_ = DF T . In this paper, we regard matrices as
multi-channel signals. For example, X ∈ R[n][×][d] means d-channel n-length signals. When DFT and
inverse DFT are applies to multi-channel signals, each channel is transformed independently, i.e.,
(X) = [ (x1) (xd)] = DF T **_X._**
_F_ _F_ _· · ·_ _F_ _·_

Hereby, we can simply operators DC [·] and HC [·] using the matrices in Eqn. 13. By definition, we
can write DC [·] as below:

_DC [x] = DF T_ _[−][1]_ diag(1, 0, · · ·, 0)DF T x (14)

= [1] (15)

_n_ **[11][T][ x][,]**

namely DC [·] = 11[T] _/n. Conversely, we can write HC [·] as:_

_HC [x] = DF T_ _[−][1]_ diag(0, 1, · · ·, 1)DF T x (16)

= DF T _[−][1](I −_ diag(1, 0, · · ·, 0))DF T x (17)

= I (18)
_−_ _n[1]_ **[11][T][ x][,]**

which indicates HC [·] = I − **11[T]** _/n. We will frequently use these derivations later in the proofs._

1Without loss of generality, we can only consider 1D Fourier transformer, since the DC components are
invariant to the dimension of signals.


-----

B DEFERRED PROOFS

B.1 PROOF OF THEOREM 1

**Theorem 1. (SA matrix is a low-pass filter) Let A = softmax(P ), where P ∈** R[n][×][n]. Then A must
_be a low-pass filter. For all z ∈_ R[n],

[A[t]z] 2
lim _∥HC_ _∥_ = 0.
_t→∞_ _∥DC [A[t]z]∥2_

_Proof.Notice that, Let λ A1 is a positive matrix, each of whose element is strictly greater than zero (, λ2, · · ·, λs ∈_ C be the eigenvalues of A with ordering |λ1| ≥|λ2| ≥· · · ≥|Aij > 0, ∀i, jλs).|.
Besides, for all i = 1, · · ·, n, _j=1_ **_[A][i,j][ = 1][. Therefore,][ A][1][ =][ 1][ implies][ A][ must have an]_**
eigenvalue 1 and its corresponding eigenvector is the all-one vector 1.

By Perron-Frobenius Theorem (Meyer, 2000), eigenvalue 1 corresponds to a all-positive eigenvector[P][n]
**1, implies λ1 = 1 should be the largest eigenvalue without multiplicity, and the absolute value of**
other eigenvalues λ2, · · ·, λs must be less than 1. Let us rewrite A in the Jordan canonical form
**_A = P JP_** _[−][1]:_

_λ1_ **_J_** (λ2) **_u.[T]1_**

**_A = [v1_** _· · ·_ **_vn]_**  ...   .. , (19)
**_P_**  **_J_** (λs) u[T]n 
| {z }  **_J_**  P _[−][1]_

where the Jordan block J (λ) can be written as| {z } | {z }

_λ_ 1
_λ_ 1

 

**_J_** (λ) = ... ... _._ (20)

 
 


Applying A to z for t times can be written as A[t]z which is equivalent to:


_λ1_
**_J_** (λ2)
...
**_J_** (λs)


**_A[t]z = P J_** _[t]P_ _[−][1]z = P_


**_P_** _[−][1]z_ (21)


_λ[t]1_
**_J_** (λ2)[t]

= P   (22)

...

 **_J_** (λs)[t]
 
  **_[P][ −][1][z]_**

Let f (x) = x[t], then A[t] = P f (J )P = P diag(f (λ1), f (J (λ2)), _, f_ (J (λs)))P . Suppose

_[−][1]_ _· · ·_ _[−][1]_
a Jordan block with shape k × k, then

_f_ (λ) _f_ _[′](λ)_ _f_ _[′′]2!(λ)_ _· · ·_ _f_ [(]([k]k[−]−[1)]1)!(λ)

.

 _f_ (λ) _f_ _[′](λ)_ ... .. 

_f_ (J (λ)) = (23)

 ... ... _f_ _[′′](λ)_ 
 2! 
 _f_ (λ) _f_ (λ) 
 _[′]_ 
 _f_ (λ) 
 
 

Therefore, on the diagonal number m ≤ min(t, k − 1) above the main diagonal stands:

_t(t_ 1)...(t _m + 1)_
_−_ _−_ _λ[t][−][m]_ (24)

_m!_


-----

For arbitrary m ≤ _k −_ 1 and |λ| < 1,

_t(t_ 1)...(t _m + 1)_
lim _−_ _−_ _λ[t][−][m]_ = 0 (25)
_t→∞_ _m!_

Recall that λ1 = 1 and according to the definition of Jordan canonical form, v1 = 1. By Eqn. 25:

lim (26)
_t→∞_ **_[A][t][z][ =][ P][ lim]t→∞_** [diag(][f] [(][λ][1][)][, f] [(][J] [(][λ][2][))][,][ · · ·][, f] [(][J] [(][λ][s][)))][P][ −][1][z]

= P diag(λ[t]1[,][ 0][,][ · · ·][,][ 0][)][P][ −][1][z] (27)

= λ[t]1[v][1][u][T]1 **_[z]_** (28)


= 1u[T]1 **_[z]_** (29)

Plug the result from Eqn. 29 into the original limit:


[A[t]z] 2
lim _∥HC_ _∥_ = lim
_t→∞_ _∥DC [A[t]z]∥2_ _t→∞_

= lim
_t→∞_

= lim
_t→∞_


_∥HC [A[t]z]∥2[2]_ (30)

**_z_** [A[t]z] 2
_∥_ _−HC_ _∥[2]_


= lim _∥HC [A[t]z]∥2[2]_ (31)
_t→∞_ s _∥z∥2[2]_ _[−∥HC][ [][A][t][z][]][∥]2[2]_

= lim _∥(I −_ _n[1]_ **[11][T][ )][A][t][z][∥]2[2]** (32)
_t→∞_ s _∥z∥2[2]_ _[−∥][(][I][ −]_ _n[1]_ **[11][T][ )][A][t][z][∥]2[2]**

= _∥(I −_ _n[1]_ **[11][T][ )][1][u]1[T]** **_[z][∥]2[2]_** (33)

s _∥z∥2[2]_ _[−∥][(][I][ −]_ _n[1]_ **[11][T][ )][1][u]1[T]** **_[z][∥]2[2]_**

(I1u[T]1 **_[z][ −]_** **[1][u]1[T]** **_[z][∥]2[2]_**

= _∥_ (34)

s _∥z∥2[2]_ _[−∥][(][I][1][u]1[T]_ **_[z][ −]_** **[1][u]1[T]** **_[z][∥]2[2]_**

= 0 (35)


where Eqn. 31 is due to the orthogonality of DC and HC terms.

B.2 PROOF OF COROLLARY 2

**Corollary 2. Let P1, P2, · · ·, Pn be a sequence of matrix in R[n][×][n], and each Pk, ∀k = 1, · · ·, L**
_has Ak = softmax(Pk). Then_ _k=1_ **_[A][k][ is also a low-pass filter.]_**

_Proof. Let A =_ _k=1_ **_[A][k][, then we show][Q][L]_** **_[ A][ satisfies the following conditions, so that][ A][ can be]_**
regarded as another self-attention matrix. Then we can conclude the proof by Theorem 1.

1) For every i = 1[Q], · · ·[L] _, n,_ _j=1_ **_[A][ij][ = 1][.]_**

Suppose _j=1_ **_[B][ij][ = 1][ for every][ i][ = 1][,][ · · ·][, n][, then for every][ k][ = 1][,][ · · ·][, L][ and][ i][ = 1][,][ · · ·][, n][,]_**

[P][n]

_n_ _n_ _n_

[P][n] (AkB)ij = **_Ak,imBmj_** (36)

_j=1_ _j=1_ _m=1_

X X X


= **_Ak,im_** **_Bmj_** (37)

  

_m=1_ _j=1_

X X

_n_   

= **_Ak,im = 1._** (38)

_m=1_

X

By induction, for every i, _j=1_ **_[A][1][,ij][ = 1][ ⇒]_** [P]j[n]=1[(][A][2][A][1][)][ij][ = 1][ ⇒· · · ⇒] [P][n]j=1 **_[A][ij][ = 1][.]_**

2) For every i, j = 1, · · ·, n[P],[n] Aij > 0.

Suppose Bij > 0, ∀i, j, then for every k = 1, · · ·, L, (AkB)ij = _k=1_ **_[A][k,im][B][mj][. Since]_**
**_Ak,im > 0, Bmj > 0, then (AkB)ij > 0. By induction, for every i, j, A1,ij > 0 ⇒_** (A2A1)ij >
0 **_Aij > 0._**
_⇒· · · ⇒_ [P][n]


-----

B.3 PROOF OF THEOREM 3

**Theorem 3. (convergence rate of SA) Let A = softmax(P ) and α = maxi,j|Pij|, where P ∈**
R[n][×][n]. Define SA(X) = AXWV as the output of a self-attention module, then

_ne[2][α]_

[SA(X)] _F_
_∥HC_ _∥_ _≤_ r _e[2][α]_ + n − 1 _[∥][W][V][ ∥][2][∥HC][ [][X][]][∥][F][ .]_

_In particular, whenradius γ > 0, i.e., ∥ Px =i∥2 XW ≤_ _γ, ∀Q(i = 1XW, · · ·K)[T], n/√, thend, and assume tokens are distributed inside a ball with α ≤_ _γ[2]∥WQWK[T]_ _[∥][2][/]√d._

_Proof. First, we write X = DC [X] + HC [X] = 1[T]_ **_z + H, where DC [X] = 1z[T]_** equals to the
orthogonal projection of X to subspace span(1), and H = HC [X] represents the remaining part of
the original signals.
[SA(X)] = (I **11[T]** )AXWV (39)
_HC_ _−_

= **_I_** **_A(1z[T]_** + H)WV (40)
_−_ _n[1]_ **[11][T]**
 

= **_I_** **_A1z[T]_** **_WV +_** **_I_** **_AHWV_** (41)
_−_ _n[1]_ **[11][T]** _−_ _n[1]_ **[11][T]**
   

= **_I_** **_AHWV_** (42)
_−_ _n[1]_ **[11][T]**
 


Therefore,


**_I_** **_AHWV_**
_−_ _n[1]_ **[11][T]**




(43)


_∥HC [SA(X)]∥F =_


softmax(P ) 2 **_WV_** 2 **_H_** _F_ (44)

_≤_ _n_ **[11][T]** 2 _∥_ _∥_ _∥_ _∥_ _∥_ _∥_

_≤_ **_[I]∥[ −]softmax([1]_** **_P )∥1∥softmax(P )∥∞∥WV ∥2∥H∥F_** (45)

= p softmax(P ) 1 **_WV_** 2 **_H_** _F_ (46)

_∥_ _∥_ _∥_ _∥_ _∥_ _∥_
The Eqn. 45 leverages a special case of Hölder’s inequality, and the Eqn. 46 can be yielded from

p

_∥softmax(P )∥∞_ = 1. Now we need to upper bound ∥softmax(P )∥1. Suppose α = maxi,j|Pij|,
then for each i = 1, · · ·, n, we have the following inequality for the element with the largest value
(say the j-th column):

_e[P][ij]_ _e[α]_ _e[2][α]_
**_Aij =_** _n_ (47)

_t=1_ _[e][P][it][ ≤]_ _e[α]_ + _t=j_ _[e][−][α][ =]_ _e[2][α]_ + (n 1)

_̸_ _−_

_ne[2][α]_

Hence, we have ∥softmax(PP )∥1 ≤ [P]i [max][j][ A][P][ij][ ≤] _e[2][α]+(n−1)_ [. Insert this result to Eqn. 46, we]

can conclude the proof. In particular, when P = XWQ(XWK)[T] _/√d,_

**_x[T]i_** **_[W][Q][W]K[ T]_** **_[x][j]_**

_α = maxi,j_ _i,j_ _√d_ _[.]_ (48)

_[|][P][ij][|][ = max]_

Since ∥xi∥2, ∥xj∥2 ≤ _γ, ∀i, j, α ≤_ maxi,j∥xi∥2∥WQWK[T] _[∥][2][∥][x][j][∥][2][/]√d ≤_ _γ[2]∥WQWK[T]_ _[∥][2][/]√d._

C EXTENSION OF THEOREM 3

C.1 MULTI-HEAD ATTENTION


**Proposition 4. (smoothening rate with MSA) Let A[h]** = softmax(P _[h]), where P_ _[h]_ _∈_ R[n][×][n] _with_
_h = 1, · · ·, H. Let α = max[H]h=1_ [max][i,j][|][P][ h]ij[|][. Define][ MSA(][X][) =][ P][H]h=1 **_[A][h][XW][ h]V_** **_[W][ h]O_** _[as the]_
_output of a multi-head self-attention module, then_

_ne[2][α]_

[MSA(X)] _F_ _σ1σ2H_
_∥HC_ _∥_ _≤_ r _e[2][α]_ + n − 1 _[∥HC][ [][X][]][∥][F][,]_

_where H is the number of heads, σ1 = max[H]h=1[∥][W][ h]V_ _[∥][2][ and][ σ][2][ = max]h[H]=1[∥][W]O[ h][∥][2][.]_


-----

_Proof. For the h-th head, according to Theorem 3:_


_ne[2][α][h]_

_e[2][α][h]_ + n − 1 _[∥][W][ h]V_ _[∥][2][∥HC][ [][X][]][∥][F]_ _[,]_ (49)


[SAh(X)] _F_
_∥HC_ _∥_ _≤_

where αh = maxi,j|Pij[h][|][. Then we have:]


_H_

SAh(X)WO[h]

"h=1
X


(50)


_∥HC [MSAh(X)]∥F =_


SAh(X)WO[h] (50)

"h=1 #F
X

_H_

_[HC]_ SAh(X)WO[h] _F_ (51)

_HC_

_h=1_

X  

_H_

_ne[2][α][h]_

_h=1_ r _e[2][α][h]_ + n − 1 _[∥][W][ h]V_ _[∥][2][∥HC][ [][X][]][∥][2]_ (52)

X

_H_

_ne[2][α][h]_

_V_ _O[∥][2][∥HC][ [][X][]][∥][F]_ (53)

_h=1_ r _e[2][α][h]_ + n − 1 _[∥][W][ h][∥][2][∥][W][ h]_

X


_ne[2][α]_

_σ1σ2H_ (54)
_≤_ _e[2][α]_ + n 1 _[∥HC][ [][X][]][∥][F][,]_

r _−_

where Eqn. 51 follows from the linearity of HC [·] and triangle inequality. Eqn. 54 can be obtained by

relaxing αh, ∥WV[h][∥][2][, and][ ∥][W][ h]O[∥][2][ to][ α][,][ σ][1][ and][ σ][2][ (Note that] _e[2][α]ne+[2]n[α]−1_ [is monotonically increasing]

with α). q

C.2 RESIDUAL CONNECTION

**Proposition 5. (smoothening rate with skip connection) Let A[h]** = softmax(P _[h]), where P_ _[h]_ _∈_
R[n][×][n] _with h = 1, · · ·, H. Let α = max[H]h=1_ [max][i,j][|][P][ h]ij[|][. Define][ X] _[′][ = MSA(][X][) +][ X][ as the]_
_output of a multi-head self-attention module with skip connection, then_


_ne[2][α]_

[X _[′]]_ _F_ 1 + σ1σ2H [X] _F_
_∥HC_ _∥_ _≤_ r _e[2][α]_ + n − 1 ! _∥HC_ _∥_

_where H is the number of heads, σ1 = max[H]h=1[∥][W][ h]V_ _[∥][2][ and][ σ][2][ = max]h[H]=1[∥][W]O[ h][∥][2][.]_

_Proof. By Proposition 4,_
_∥HC [X_ _[′]]∥F = ∥HC [MSA(X) + X]∥F_ (55)
_≤∥HC [MSA(X)]∥F + ∥HC [X]∥F_ (56)

_ne[2][α]_

(57)

_≤_ _e[2][α]_ + n 1 _[∥HC][ [][X][]][∥][F][ +][ ∥HC][ [][X][]][∥][F]_

r _−_

_ne[2][α]_

= 1 + σ1σ2H [X] _F ._ (58)

r _e[2][α]_ + n − 1 ! _∥HC_ _∥_

Again, Eqn. 56 follows from the linearity of HC [·] and triangle inequality.

C.3 FEED-FORWARD NETWORK

**Proposition 6. (smoothening rate with FFN) Let A[h]** = softmax(P _[h]), where P_ _[h]_ _∈_ R[n][×][n] _with_
_h = 1, · · ·, H. Let α = max[H]h=1_ [max][i,j][|][P][ h]ij[|][. Define][ Y][ = FFN(MSA(][X][) +][ X][)][ as the output of a]
_transformer block, then_


_ne[2][α]_


[Y ] _F_ _σ3_ 1 + σ1σ2H [X] _F_
_∥HC_ _∥_ _≤_ r _e[2][α]_ + n − 1 ! _∥HC_ _∥_

_where FFN : R[d]_ _→_ R[d] _represents a feed-forward network, H is the number of heads, σ1 =_
maxparticular,[H]h=1[∥][W] σV[ h]3 = 1 + Lips(FFN)[∥][2][,][ σ][2][ = max]h[H]=1[∥] when residual connection is considered in FFN.[W][ h]O[∥][2][, and][ σ][3][ = Lips(FFN)][ is the Lipschitz constant of FFN. In]


[Y ] _F_ _σ3_
_∥HC_ _∥_ _≤_


1 + σ1σ2H


-----

_Proof. Let X_ _[′]_ = MSA(X) + X and z[′] = 1[T] (X _[′]/n) ∈_ R[d], then we have

[FFN(X _[′])]_ _F_ FFN(X _[′])_ **1 FFN(z)[T]** _F_ (59)
_∥HC_ _∥_ _≤∥_ _−_ _∥_

= FFN(X _[′])_ FFN(1z[T] ) _F_ (60)
_∥_ _−_ _∥_

_σ3_ **_X_** _[′]_ **1z[T]** _F_ (61)
_≤_ _∥_ _−_ _∥_

= σ3 **_I_** **_X_** _[′]_ = σ3 [X _[′]]_ _F_ (62)

_−_ _n[1]_ **[11][T]** _F_ _∥HC_ _∥_
 

_ne[2][α]_

_σ3_ 1 + σ1σ2H [X] _F,_ (63)
_≤_ r _e[2][α]_ + n − 1 ! _∥HC_ _∥_

where Eqn. 59 follows from Lemma 7, Eqn. 60 holds because FFN operates row-wisely on feature
matrix, and Eqn. 61 is due to the definition of Lipschitz constant. Finally, Eqn. 63 is yielded from
Proposition 5.

**Lemma 7. Given X ∈** R[n][×][d], ∥HC [X]∥F ≤∥X − **1z[T]** _∥F for all z ∈_ R[d].

_Proof. We prove the Lemma by showing that z[∗]_ = X _[T]_ **1/n achieves the minimum of the optimiza-**
tion problem arg minz∥X − **1z[T]** _∥F[2]_ [.]

**_X_** **1z[T]** _F_ [= Tr(][X] _[T][ −]_ **_[z][1][T][ )(][X][ −]_** **[1][z][T][ )]** (64)
_∥_ _−_ _∥[2]_

= Tr(X _[T]_ **_X) −_** Tr(z1[T] **_X) −_** Tr(X _[T]_ **1z[T]** ) + Tr(z[T] **1[T]** **1z)** (65)

= nz[T] **_z −_** 2 Tr(X _[T]_ **1z[T]** ) + Tr(X _[T]_ **_X)_** (66)

It is easy to show the derivative in terms of z:

_∇z∥X −_ **1z[T]** _∥F[2]_ [= 2][n][z][ −] [2][X] _[T][ 1][.]_ (67)

Therefore, z[∗] = _n[1]_ **_[X]_** _[T][ 1][ achieves the minimum.]_

D DEFERRED REMARKS ON SECTION 2.4

D.1 CONNECTION WITH RANDOM WALK THEORY

We add that the asymptotic evolution of feature representations can be interpreted through the lens
of random walk theory. We can regard self-attention map A as probability transition matrices for a
Markov chain. Since each entry is larger than zero, the Markov chain should be irreducible. This
implies the Markov chain will converge into a unique stationary distribution π (Randall, 2006). Let
**_aλ ∈i denote the(0, 1) is the mixing rate of the transition matrix i-th row of A, then for all i = 1, · · ·, n we have A. As a consequence, ∥(ai)[T]_** **_A_** _−_ **_π[T]_** _∥≤ limλl∥→∞ai −Aπ[l]_ _∥=, where 1π[T]_
yields a pure low-pass filter. When repeatedly applying this self-attention matrix to feature maps,
liml→∞ **_A[l]X →_** **1π[T]** **_X only preserves the rank-1/DC portion of the signals, which is consistent_**
with our Theorem 1. Nevertheless, this interpretation does not bring other transformer components
into consideration. And our theory further provides a concrete convergence rate with respect to the
network parameters (Theorem 3).

D.2 REMARKS ON DONG ET AL. (2021)

Here we respectfully elaborate on the hidden assumptions in the proof of the current preprint of Dong
et al. (2021).

1) In the proof of Lemma A.3, Taylor expansion was used to approximate and upper bound an
exponentiation. However, to let the right-hand side upper bound satisfied, we conjectured that the
authors implicitly assumed Eij **_Eij[′] is bounded around zero. After directly communicating with_**
the authors, they confirmed that a missed assumption here is − maxi,j(Eij **_Eij′_** ) 1.
_−_ _≤_

2) In the proof of Lemma A.1, to let Eqn. (8)-(9) hold, the authors may have assumed R, WV 0,
where ≥ denotes entry-wise inequality. As the authors suggested, an entry-wise absolute value can ≥
be imposed to R and WV as a simple fix, without influencing their ℓ1 and ℓ norm. However,
_∞_


-----

even after those changes, we still have difficulty walking through Eqn. (6)-(8), and we are currently
communicating with the authors on this matter.

3) In the proof of Lemma A.1, we find Eqn. (12) may not be satisfied in general. We can raise
the following counterexample: Since E, r, R, WV can be any matrices, we simply let D =
whilediag(2,D 3)1, softmax(R 1 **_rW) = [0V_** 1 = 3.8, which disproves the claim. We conjecture that some additional0.2][T] _, R = WV = I. Then ∥D1softmax(r)[T]_ **_RWV ∥1 = 4_**
prerequisite constraints on ∥ _∥∞∥_ _∥_ _∥_ _∥ E, r might be needed here to proceed the derivation, and we are currently_
communicating with the authors on this matter.

D.3 GENERALIZE TO OTHER ATTENTION MECHANISM

Our theorizing can be smoothly generalized to other attention mechanisms because our Theorem 1
and 3 do not require any prior knowledge on pre-softmax pairwise correlation P .

**Logistic Attention.** We refer logistic attention to the attention mechanism used in Veliˇckovi´c et al.
(2018); Verma et al. (2018), where attention is calculated via a linear combination:

exp **_x[T]i_** **_[u][Q][ +][ x]j[T]_** **_[u][K][ +][ b]_**
**_Aij =_** (68)

_t_ [exp (]  **_[x][i][u][Q][ +][ x][t][u][K][ +][ b][)]_**

where uQ and uK are query/key parameters, b is the bias term. With the same condition in Theorem

P

3, we can upper bound α by |(∥uK∥2 + ∥uQ∥2)γ + b|.

**L2 Distance Attention.** L2 distance based attention (Kim et al., 2021) Lipschitz formulation of
self-attention. The pair-wise attention can be written as follows:

**_Aij =_** exp _−∥x[T]i_ **_[W][Q][ −]_** **_[x]j[T]_** **_[W][K][∥]2[2][/τ]_** (69)

_t_ [exp]  **_x[T]i_** **_[W][Q][ −]_** **_[x]t[T]_** **_[W][K][∥]2[2][/τ]_**
_−∥_

where WQ and Wk are query/key weights, and τ is a scaling factor. Similar to Theorem 3, we can

P   

upper bound α by (∥WK∥2 + ∥WQ∥2)[2]γ[2]/τ .

E AN AUXILIARY LEMMA FOR ATTNSCALE

**Lemma 8. Let** **_A[˜] = FAF_** _[−][1]_ _be the spectral response of attention matrix A, and parameterize a_
_low-filter by L = F_ _[−][1]_ diag(β, 0, · · ·, 0)F. Then β[∗] = 1 is the optimal solution of the following
_optimization problem: arg minβ∥A −_ **_L∥F ._**

_Proof. First we make simplification L = F_ _[−][1]_ diag(β, 0, · · ·, 0)F = β11[T] (refer to Appendix A).
Then we have:
**_A_** **_L_** _F_ [=][ ∥][A][ −] _[β][11][T][ ∥]F[2]_ [= Tr(][A][ −] _[β][11][T][ )][T][ (][A][ −]_ _[β][11][T][ )]_ (70)
_∥_ _−_ _∥[2]_

= Tr **_A[T]_** **_A_** (71)
_−_ _[β]n_ **_[A][T][ 11][T][ −]_** _[β]n_ **[11][T][ A][ +][ β]n[2]** **[11][T]**
 

= _[β][2]_ (72)

_n_ [Tr][ 11][T][ −] [2]n[β] [Tr][ 11][T][ A][ + Tr][ A][T][ A]


= β[2]
_−_ [2]n[β]


**_Aij + Tr A[T]_** **_A_** (73)
_i=1_

X


_j=1_


= β[2] _−_ 2β + Tr A[T] **_A_** (74)
From Eqn. 74, β[∗] = 1 achieves the minimum of the objective function.

F MORE ON VISUALIZATION

F.1 DETAILS ON FIGURE 1

To verify our Theorem 3, we depict the high-frequency intensity of each layer’s output and its theoretical upper bound. Our visualization is based on the official checkpoint of 12-layer DeiT-S. Since


-----

training a ViT without either FFN or residual connection will certainly cause failure, we remove these
components directly from the pre-trained model to illustrate the effects of different components. We
use logarithmic scale for the purpose of better view. Let Xl denote the output of the l-th layer, and X0
be the initial inputs. For red line, we directly calculate log(∥HC [Xl]∥F /∥X0∥F ) at each layer. For
blue line, we first obtain the coefficient γl in Section 2.2 and 2.3 with respect to network parameters

(e.g., we can compute γl = _e[2][α]ne+[2]n[α]_ 1 _[∥][W][V][ ∥][2][ for attention only architecture). Then we estimate]_

_−_
the upper bound by γl [Xql 1] _F and apply the logarithm by log(γl_ [Xl 1] _F /_ **_X0_** _F ). To_
_∥HC_ _−_ _∥_ _∥HC_ _−_ _∥_ _∥_ _∥_
summarize, one can see without residual connection, the first two sub-figures imply an exponential
convergence rate, which is consistent with our Theorem 3.

F.2 MORE VISUALIZATION ON SPECTRUM

In this appendix, we provide more visualization on the spectrum of attention map to validate our
Theorem 1. We compute the spectrum of attention map A for both Fig. 2 and Fig. 5 in the
following way. By regarding A as a linear filter, its Fourier-domain response is another linear kernel


**Λ = FAF** _[−]_ . When Λ is applied to a spectrum ˜x = Fx of signals x, the i-th frequency response
will be Λix˜, where Λi is the i-th row of Λ. Hence, we can use ∥Λi∥2 to evaluate the spectral
response intensity of the i-th frequency band. Below we provide a complete spectral visualization of
attention maps computed from a random sample in ImageNet validation set.

1.0642 321 10.07.55.02.5 642 1.00.80.60.4 1.51.00.5

0 0.0 0 0.0

32 1.00.9 1.0 2.01.51.0 1.000.750.50 1.000.750.50

1 0.8 0.8 0.5 0.25 0.25

0.0

1.00.80.5 1.51.00.5 1.00.5 2.01.51.00.5 1.51.00.5 1.21.00.80.6

0.0 0.4

1.0 1.0 1.51.0 1.51.0 1.0 1.51.0

0.5 0.5 0.5 0.5 0.5 0.5

0.0 0.0

1.5

2 1.0 1.0 1.51.0 1.51.0 1.0

0.610 0.50.0 0.50.0 0.50.0 0.5 0.5

32 1.0 1.51.0 1.0 1.51.0 1.251.000.75

1 0.5 0.5 0.5 0.5 0.500.25

0 0.0 0.0

Magnitude1.51.0 1.51.0 1.0 2.01.5 1.51.0 2

0.5 0.5 0.5 1.00.5 0.5 1

0.0 0

0.41.00.5 21 1.51.00.5 1.51.00.5 321 2.01.51.00.5

0 0.0 0 0.0

2.01.51.00.5 1.51.00.5 2.01.51.00.5 1.51.00.5 4321 21

0.0 0.0 0.0 0.0 0 0

2.01.51.00.20.5 21 4321 1.51.00.5 2.01.51.00.5 321

0.0 0 0 0.0 0.0 0

2.01.5 2.01.5 2 2.01.5 2.01.5 2

1.00.5 1.00.5 1 1.00.5 1.00.5 1

0.0 0.0 0 0.0 0.0 0

43 2 2 2.01.5 32 4

21 1 1 1.00.5 1 2

0.000.00 50 100 150 200 0 0 0.2 50 100 150 200 0 0 50 0.4 100 150 200Frequency0.0 0 50 100 0.6 150 200 0 0 50 100 1500.8 200 0 0 50 100 150 2001.0

Figure 5: Visualize the spectrum of attention maps. Each row demonstrates every head at a same
layer, and from top to bottom, the 12 rows correspond to 1 ~ 12-th layer, for left to right, the 6
columns correspond to 1 ~ 6-th head, respectively. Best view in a zoomable electronic copy.


-----

F.3 DETAILS ON SIMILARITY CURVES (FIGURE 4)

In Fig. 4, we visualize the cosine similarity of attention maps and feature maps to show the
effectiveness of our AttnScale and FeatScale on 24-layer DeiT, respectively. We follow the definition
in Zhou et al. (2021a) to compute the cosine similarity metric for attention maps. Instead of measuring
cross-layer similarity, we calculate average cross-patch similarity at the same layer. Given the layer
index l and corresponding attention maps A[(][l,h][)] _∈_ R[n][×][n], the cosine similarity can be computed by:

2 _H_ _n_ _n_ **_A[(]:,i[l,h][)][T]_** **_A[(]:,j[l,h][)]_**
_Mattn[l]_ [=] _,_ (75)

_n(n_ 1)H
_−_ _hX=1_ Xi=1 _j=Xi+1_ **_A[(]:,i[l,h][)]_** 2 **_A[(]:,j[l,h][)]_** 2

where A[(]:,i[l,h][)] denotes the i-th column of A[(][l,h][)], and H is the number of heads. The cosine similarity
between i-th and j-th column of A[(][l,h][)] measures how the contribution of one token (say the i-th
token) varies from the other (say the j-th token). We average the similarity between every pair
of tokens’ attention map (excluding the self-to-self similarity) and every attention head. We refer
interested readers to Zhou et al. (2021a) for more details.

We use the similar metric to compute similarity for feature maps. Following Gong et al. (2021), we
compute pair-wise cosine similarity between every two different tokens. Formally, given the layer
index l, and its output X [(][l][)] _∈_ R[n][×][d], the cosine similarity is estimated by:

2 _n_ _n_ **_Xi,[(][l]:[)][T]_** **_Xj,[(][l]:[)]_**
_Mfeat[l]_ [=] _,_ (76)

_n(n_ 1)
_−_ Xi=1 _j=Xi+1_ **_Xi,[(][l]:[)]_** 2 **_Xj,[(][l]:[)]_** 2

where Xi,[(][l]:[)] [denotes the][ i][-th row of][ X] [(][l][)][. The cosine similarity between between][ i][-th and][ j][-th row of]
**_X_** [(][l][)] measures how similar the feature representations of two tokens are. Likewise, we average the
similarity between every pair of tokens’ features except for the self-to-self similarity. More details
can found in Gong et al. (2021). We additionally provide a visualization of these two metrics for
12-layer DeiT in Fig. 12.

G DEFERRED EXPERIMENTS AND MODEL INTERPRETATION

G.1 FINE-TUNING EXPERIMENTS

Our deferred fine-tuning experiment with CaiT (Touvron et al., 2021b) results are presented in
Table 3. Different from trining scratch, we fine-tune CaiT with AttnScale and FeatScale parameters
from the pre-trained models for 60 epochs following Gong et al. (2021). For a fair comparison, we
simultaneously train a plain CaiT for another 60 epochs. During fine-tuning, we reduce learning rate
to 5 × 10[−][5] and weight decay to 5 × 10[−][4]. All other hyper-parameters and training recipe are kept
consistent with the original paper (Touvron et al., 2021b).

Table 3: Experimental evaluation of finetuning AttnScale & FeatScale with CaiT. The number inside
the (↑·) represents the performance gain compared with the baseline model, and accuracies within/out
of parenthesis are the reported/reproduced performance.

|Backbone|Method|Input size # Layer # Param FLOPs Throughput|Top-1 Acc (%)|
|---|---|---|---|
|CaiT|CaiT-XXS CaiT-XXS + AttnScale CaiT-XXS + FeatScale|224 24 12.0M 2.53G 589.3 224 24 12.0M 2.53G 548.1 224 24 12.0M 2.53G 573.5|77.5 (77.6) 77.8 (↑0.3) 77.8 (↑0.3)|
||CaiT-S CaiT-S + AttnScale CaiT-S + FeatScale|224 24 46.9M 8.74G 371.9 224 24 46.9M 8.75G 339.0 224 24 46.9M 8.75G 358.2|82.6 (82.7) 82.8 (↑0.2) 82.9 (↑0.3)|



G.2 VISUALIZATION AND INTERPRETATION OF ATTNSCALE

In this subsection, we provide visualization to interpret our AttnScale and further support our
experiments.  In Fig. 6 we visualize the learned weights of our AttnScale. We observe conclude
our AttnScale are successfully trained to amplify the high-pass component. We also find when layer


-----

index goes larger, the scaling weights turns larger to prevent attention collapse at deeper layer. 
We also compare the attention map produced by AttnScale with those produced by original DeiT.
We observe from Fig. 8 that our AttnScale can extract more salient and higher contrastive attention
than vanilla DeiT, which indicates our AttnScale possesses higher capability to distinguish tokens
from larger variety of attention schemes.  To be more objective, we plot the spectrum of a 24-layer
DeiT’s attention maps with/without our AttnScale in Fig. 9 and 10. The visualization procedure
has been elaborated in Sec. F.2. We find that attention maps from AttnScale enjoy richer filtering
diversity, capable of performing high-pass (row 2, column 3) and band-pass (row 12, column 5)
filtering, instead of only low-pass filtering (see Fig. 9).

1.06 6

6

0.84 4 4

2 2 2

0.6

0 0 1 2 3 4 5 6 7 8 9 10 11 0 0 1 2 3 4 5 6 7 8 9 10 11 0 0 1 2 3 4 5 6 7 8 9 10 11

Magnitude0.46 6 6

4 4 4

0.2

2 2 2

0.00.00 0 1 2 3 4 5 6 70.28 9 10 11 0 0 10.42 3 4 5 6 7 8 9 10 110.6 0 0 1 2 30.84 5 6 7 8 9 10 111.0

Layer Index


Figure 6: Visualize the learned weights of DeiT-S + AttnScale. Each sub-plot depicts the scaling
weights of the same head for different layers. For left to right, top to bottom, six sub-figures
correspond to 1 ~ 6-th head, respectively. Best view in color.

1.0

4

4 Param Param 4 Param

s s s

0.8 t t t

2 2 2

0.6

0 0 1 2 3 4 5 6 7 8 9 10 11 0 0 1 2 3 4 5 6 7 8 9 10 11 0 0 1 2 3 4 5 6 7 8 9 10 11

Magnitude0.44 Param 4 Param 4 Param

s s s
t t t

0.22 2 2

0.00.00 0 1 2 3 4 5 6 70.28 9 10 11 0 0 10.42 3 4 5 6 7 8 9 10 110.6 0 0 1 2 30.84 5 6 7 8 9 10 111.0

Layer Index


Figure 7: Visualize the learned weights of DeiT-S + FeatScale. Each sub-plot depicts two groups of
scaling weights of the same head for different layers. For left to right, top to bottom, six sub-figures
correspond to 1 ~ 6-th head, respectively. Best view in color.

G.3 VISUALIZATION AND INTERPRETATION OF FEATSCALE

In this subsection, we provide visualization to interpret our FeatScale.  We plot the scaling
weights of FeatScale in Fig. 7. We observe that the re-weighting factors learned for high-frequency
components t is consistently larger than the weights for the DC term s, which indicates our FeatScale
is successfully trained to elevate high-frequency features against the dominance of DC component.
Similarly, the gap between s and t becomes huger when going deeper.  We also demonstrate the
proportion of feature maps’ high-frequency component in Fig. 11 for both 12 (lower one) / 24(upper
one) -layer DeiT. The proportion value is calculated by ∥HC [X]∥F /∥X∥F . We find high-frequency
signals diminish quickly at deeper layer, and 24-layer DeiT suffers from a faster pace. Our FeatScale
is effective to keep the high-frequency signals stand for both 12-layer and 24-layer DeiT.


-----

Figure 8: Visualize the attention map of DeiT-S with/without AttnScale. 4 × 4 max pooling has been

Block 0 Block 2 Block 4 Block 6 Block 8 Block 10

1.0

DeiT-S

0.5

0.0

DeiT-S + AttnScale

Figure 8: Visualize the attention map of DeiT-S with/without AttnScale. 4 4 max pooling has been

applied. The first row visualizes attention maps without AttnScale, and the second row visualizes


attention maps with AttnScale. Each column corresponds to the layer noted by its sub-title. The
attention map are computed from a random sample in ImageNet validation set. We only demonstrate
the first head of each layer. Best view in a zoomable electronic copy.

1.03 1 2 5 1.0

21 0 0 2.50.0 0 0.5

2 1 2 1 1 1

1 1

0 0 0 0

1.00.5 1.00.5 10 1.00.9 1.00.8 20

1.251.000.75 1.51.00.5 1.00.5 1.251.000.75 1.00.5 1.00.5

1.51.0 1.0 1.0 1 1.0 1

0.50.8 0.5 0.5 0 0.5 0

1.51.0 1 1.0 1 1.0 1

0.5 0.5 0.5

0 0 0

10 1.51.00.5 20 1.00.5 10 21

10 10 1.00.5 10 1.00.5 1.51.00.5

10 1.00.5 21 1.00.5 20 1.00.5

0.61 1 1 1.00.5 1.00.5 1

0 0 0 0

1.00.5 1 1 1.51.00.5 1.00.5 1.51.00.5

0 0

1 1 1.0 1 1 1.51.0

0.5 0.5

0 0 0 0

Magnitude 1 1.0 1 1.0 1 1

0.5 0.5

0 0 0 0

1 1 1 1.00.5 1 1

0 0 0 0 0

0.41 1.0 1 1 1 1

0.5

0 0 0 0 0

1 1 1 1 1 1

0 0 0 0 0 0

1 2 1 1 1 1

0 0 0 0 0 0

1 1 1 1 1 1

0 0 0 0 0 0

1 1 1 1 1 1

0 0 0 0 0 0

0.21 1 1 1 2 2

0 0 0 0 0 0

2 2 2 2 1 2

0 0 0 0 0 0

2.5 2.5 2 2.5 1 2.5

0.0 0.0 0 0.0 0 0.0

2 1 1 1 1 1

0 0 0 0 0 0

1 1 1 1 1 1

0.000.00 50 100 150 200 0 0 0.2 50 100 150 200 0 0 50 0.4 100 150 200Frequency0 0 50 100 0.6 150 200 0 0 50 100 1500.8 200 0 0 50 100 150 2001.0

Figure 9: Visualize the spectrum of attention maps without AttnScale. Each row demonstrates every
head at a same layer, and from top to bottom, the 24 rows correspond to 1 ~ 24-th layer, for left to
right, the 6 columns correspond to 1 ~ 6-th head, respectively. Best view in a zoomable electronic
copy.


-----

1.0

2.5

2.0

5.0

2.5

0.50

0.25

0.6

0.4

1.5

1.0

0.50.8

2

1

2

1

0.75
0.50
0.25

1.0

0.5

3

0.621

1

0

2

1

Magnitude0.30.2

0.1


7.5

5.0

2.5

0.50

0.25

1.0

0.5

0.75

0.50

0.25

0.1

0.0

0.50

0.25

1

0

2

1

2

1

2

1

1.0

0.5


7.5

5.0

2.5

3
2
1

0.75

0.50

1.0

0.5


1.0

0.5

1.5

1.0

0.5

2

0

1.5

1.0

0.5


1.5

1.0

1.5

1.0

0.5

2

1

3

2

1

3

2

1

3

2

1

1.0

0.5


0.5

0.0

4

3

2

1

1.0

0.5


1.0

0.2


0.2

0.1

5.0

2.5

1.0

0.5

3

2

1

2

1

4

2

2

0

3

2

1

2

1

2

0

5

0

4

2

2

0

5

0

20

10

15

10

5

50


1.0

0.5

1.5

1.0

0.5

2

1.50

1.0

0.5


7.5

5.0

2.5

2

1

5.0

2.5

1.5

1.0

0.5


1.0

0.5

1.5

1.0

0.5

1.5

1.0

0.5

2

1

1.5
1.0
0.5


1.0

0.5

0.2

0.1

2

1

4

2

1.5

1.0

0.5

5.0

2.5

1.0

0.5


1.0

0.5

0.4

0.2

1

5.00

2.5

1.0

0.5


0.75
0.50
0.25

4

2

1.5

1.0

0.5

7.5

5.0

2.5

1.5

1.0

0.5


5.00.4

2.5

4

2

1

100

5

5.0

2.5

0.2

2.5

0.0


5.0

2.5

5.0

2.5

2

1

10


7.5

5.0

2.5

5.0

2.5

7.5

5.0

2.5

20

10

25

0

15
10


5.0

2.5

7.5

5.0

2.5

10

5

15

10

5

10


10

5

20


20

10

10

0

7.5

5.0

2.5

5.0

2.5


10

0

20

0

15

10


2.5

0.0


0.00.00 50 100 150 200 0 0 0.2 50 100 150 200 0 50 0.4 100 150 200Frequency0 50 100 0.6 150 200 0 0 50 100 1500.8 200 0 50 100 150 2001.0

Figure 10: Visualize the spectrum of attention maps with AttnScale. Each row demonstrates every
head at a same layer, and from top to bottom, the 24 rows correspond to 1 ~ 24-th layer, for left to
right, the 6 columns correspond to 1 ~ 6-th head, respectively. Best view in a zoomable electronic
copy.

|Col1|DeiT-S DeiT-S + FeatSca|
|---|---|
|||
|||
|||

|DeiT-S DeiT-S + AttnS|cale|Col3|
|---|---|---|
||||
||||
||||

|Col1|Col2|
|---|---|
||DeiT-S|

|Col1|DeiT-S DeiT-S + FeatScale|Col3|
|---|---|---|
||||
||||


0.8 DeiT-S

DeiT-S + AttnScale

0.6

0.4

Cosine Similarity

0.2

0 2 4 6 8 10

Layer Index

0.40 DeiT-S

DeiT-S + FeatScale

0.35

0.30

0.25

Cosine Similarity

0 2 4 6 8 10 12

Layer Index


1.0

DeiT-S
DeiT-S + FeatScale

X0.9
] /

[X0.8

0.7

0 3 6 9 12 15 18 21 24

Layer Index

1.0

X0.9
] /

[X0.8

DeiT-S
DeiT-S + FeatScale

0.7

0 3 6 9 12

Layer Index


Figure 11: Visualize the proportion of the
high-frequency component of feature maps
with/without our FeatScale on 12/24 layer
DeiT. Refer to Appendix G.3 for details.


Figure 12: Visualize cosine similarity of attention and feature maps with/without our proposed methods on 12-layer DeiT. Refer to Appendix F.3 for details.


-----

