# XI-LEARNING: SUCCESSOR FEATURE TRANSFER LEARNING FOR GENERAL REWARD FUNCTIONS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Transfer in Reinforcement Learning aims to improve learning performance on
target tasks using knowledge from experienced source tasks. Successor features
(SF) are a prominent transfer mechanism in domains where the reward function
changes between tasks. They reevaluate the expected return of previously learned
policies in a new target task and to transfer their knowledge. A limiting factor of
the SF framework is its assumption that rewards linearly decompose into successor
features and a reward weight vector. We propose a novel SF mechanism, ξlearning, based on learning the cumulative discounted probability of successor
features. Crucially, ξ-learning allows to reevaluate the expected return of policies
for general reward functions. We introduce two ξ-learning variations, prove its
convergence, and provide a guarantee on its transfer performance. Experimental
evaluations based on ξ-learning with function approximation demonstrate the
prominent advantage of ξ-learning over available mechanisms not only for general
reward functions, but also in the case of linearly decomposable reward functions.

1 INTRODUCTION

Reinforcement Learning (RL) successfully addressed many complex problems such as playing
computer games, chess, and even Go with superhuman performance (Mnih et al., 2015; Silver et al.,
2018). These impressive results are possible thanks to a vast amount of interactions of the RL agent
with its environment/task. Such strategy is unsuitable in settings where the agent has to perform and
learn at the same time. Consider, for example, a care giver robot in a hospital that has to learn a new
task, such as a new route to deliver meals. In such a setting, the agent can not collect a vast amount
of training samples but has to adapt quickly instead. Transfer learning aims to provide mechanisms
quickly to adapt agents in such settings (Taylor and Stone, 2009; Lazaric, 2012; Zhu et al., 2020).
The rationale is to use knowledge from previously encountered source tasks for a new target task
to improve the learning performance on the target task. The previous knowledge can help reducing
the amount of interactions required to learn the new optimal behavior. For example, the care giver
robot could reuse knowledge about the layout of the hospital it learned in previous source tasks (e.g.
guiding a person) to learn to deliver meals.

The Successor Feature (SF) and General Policy Improvement (GPI) framework (Barreto et al., 2020)
is a prominent transfer learning mechanism for tasks where only the reward function differs. Its
basic premise is that the rewards which the RL agent tries to maximize are defined based on a lowdimensional feature descriptor φ ∈ R[n]. For our care-giver robot this could be ID’s of beds or rooms
that it is visiting, in difference to its high-dimensional visual state input from a camera. The rewards
are then computed not based on its visual input but on the ID’s of the beds or rooms that it visits. The
expected cumulative discounted successor features (ψ) are learned for each behavior that the robot
learned in the past. It represents the dynamics in the feature space that the agent experiences for a
behavior. This corresponds to the rooms or beds the care-giver agent would visit if using the behavior.
This representation of feature dynamics is independent from the reward function. A behavior learned
in a previous task and described by this SF representation can be directly re-evaluated for a different
reward function. In a new task, i.e. for a new reward function, the GPI procedure re-evaluates the
behaviors learned in previous tasks for it. It then selects at each state the behavior of a previous task
if it improves the expected reward. This allows to reuse behaviors learned in previous source tasks

[Source code at https://tinyurl.com/3xuzxff3](https://tinyurl.com/3xuzxff3)


-----

for a new target task. A similar transfer strategy can also be observed in the behavior of humans
(Momennejad et al., 2017; Momennejad, 2020; Tomov et al., 2021) .

The classical SF&GPI framework (Barreto et al., 2017; 2018) makes the assumption that rewards r
are a linear composition of the featureson the task i: ri = φ[⊤]wi. This assumption allows to effectively separate the feature dynamics of a φ ∈ R[n] via a reward weight vector wi ∈ R[n] that depends
behavior from the rewards and thus to re-evaluate previous behaviors given a new reward function, i.e.
a new weight vector wj. Nonetheless, this assumption also restricts successful application of SF&GPI
only to problems where such a linear decomposition is possible. We investigate the application of the
SF&GPI framework to general reward functions: ri = Ri(φ) over the feature space. We propose
to learn the cumulative discounted probability over the successor features, named ξ-function, and
refer to the proposed framework as ξ-learning. Our work is related to Janner et al. (2020); Touati and
Ollivier (2021), and brings two important additional contributions. First, we provide mathematical
proof of the convergence of ξ-learning. Second, we demonstrate how ξ-learning can be used for
meta-RL, using the ξ-function to re-evaluate behaviors learned in previous tasks for a new reward
function Rj. Furthermore, ξ-learning can also be used to transfer knowledge to new tasks using GPI.

The contribution of our paper is three-fold:

-  We introduce a new RL algorithm, ξ-learning, based on a cumulative discounted probability
of successor features, and two variants of its update operator.

-  We provide theoretical proofs of the convergence of ξ-learning to the optimal policy and for
a guarantee of its transfer learning performance under the GPI procedure.

-  We experimentally compare ξ-learning in tasks with linear and general reward functions,
and for tasks with discrete and continuous features to standard Q-learning and the classical
SF framework, demonstrating the interest and advantage of ξ-learning.

2 BACKGROUND

2.1 REINFORCEMENT LEARNING

RL investigates algorithms to solve multi-step decision problems, aiming to maximize the sum
over future rewards (Sutton and Barto, 2018). RL problems are modeled as Markov Decision
_Processes (MDPs) which are defined as a tuple M ≡_ (S, A, p, R, γ), where S and A are the state
and action set. An agent transitions from a state st to another state st+1 using action at at time point
_t collecting a reward rt: st_ _at,rt_ _st+1. This process is stochastic and the transition probability_
_−−−→_
_p(st+1|st, at) describes which state st+1 is reached. The reward function R defines the scalar reward_
_rt = R(st, at, st+1) ∈_ R for the transition. The goal in an MDP is to maximize the expected return
_Gt = E_ _∞k=0_ _[γ][k][R][t][+][k]_, where Rt = R(St, At, St+1). The discount factor γ [0, 1) weights
_∈_
collected rewards by discounting future rewards stronger. RL provides algorithms to learn a policy
_π :_ P defining which action to take in which state to maximise _Gt._
_S →A_

Value-based RL methods use the concept of value functions to learn the optimal policy. The stateaction value function, called Q-function, is defined as the expected future return taking action at in st
and then following policy π:


_Q[π](st, at) = Eπ_ _rt + γrt+1 + γ[2]rt+2 + . . ._ = Eπ



_rt + γ max_ _._ (1)
_at+1_ _[Q][π][(][S][t][+1][, a][t][+1][)]_



The Q-function can be recursively defined following the Bellman equation such that the current
Q-value Q[π](st, at) depends on the maximum Q-value of the next state Q[π](st+1, at+1). The optimal
policy for an MDP can then be expressed based on the Q-function, by taking at every step the
maximum action: π[∗](s) ∈ argmaxa Q[∗](s, a).

The optimal Q-function can be learned using a temporal difference method such as Q-learning
(Watkins and Dayan, 1992). Given a transition (st, at, rt, st+1), the Q-value is updated according to:


_rt + max_ _,_ (2)
_at+1_ _[Q][k][(][s][t][+1][, a][t][+1][)][ −]_ _[Q][k][(][s][t][, a][t][)]_



_Qk+1(st, at) = Qk(st, at) + αk_


where αk (0, 1] is the learning rate at iteration k.
_∈_


-----

2.2 TRANSFER LEARNING AND THE SF&GPI FRAMEWORK

We are interested in the transfer learning setting where the agent has to solve a set of tasks M =
_M1, M2, . . ., Mm_, that in our case differ only in their reward function. The Successor Feature
_{_ _}_
(SF) framework provides a principled way to perform transfer learning (Barreto et al., 2017; 2018).
SF assumes that the reward function can be decomposed into a linear combination of features
_φ ∈_ Φ ⊂ R[n] and a reward weight vector wi ∈ R[n] that is defined for a task Mi:

_ri(st, at, st+1) ≡_ _φ(st, at, st+1)[⊤]wi ._ (3)

We refer to such reward functions as linear reward functions. Since the various tasks differ only in
their reward functions, the features are the same for all tasks in M.

Given the decomposition above, it is also possible to rewrite the Q-function into an expected
discounted sum over future features ψ[π][i] (s, a) and the reward weight vector wi:

_Q[π]i_ _[i]_ [(][s, a][)] = E _rt + γ[1]rt+1 + γ[2]rt+2 + . . ._ = E _φ[⊤]t_ **[w][i]** [+][ γ][1][φ][⊤]t+1[w][i] [+][ γ][2][φ][⊤]t+2[w][i] [+][ . . .]

= E  _∞k=0_ _[γ][k][φ][t][+][k]_ _⊤_ **wi** _ψπi_ (s, a)⊤wi .
_≡_ (4)
This decouples the dynamics of the policyP _πi in the feature space of the MDP from the expected_
rewards for such features. Thus, it is now possible to evaluate the policy πi in a different task Mj using
a simple multiplication of the weight vector wj with the ψ-function: Q[π]j _[i]_ [(][s, a][) =][ ψ][π][i] [(][s, a][)][⊤][w][j][.]
Interestingly, the ψ function also follows the Bellman equation:

_ψ[π](s, a) = E {φt+1 + γψ[π](st+1, π(st+1))|st, at},_ (5)

and can therefore be learned with conventional RL methods. Moreover, (Lehnert and Littman, 2019)
showed the equivalence of SF-learning to Q-learning.

Being in a new task Mj the Generalized Policy Improvement (GPI) can be used to select the action
over all policies learned so far that behaves best:

_π(s) ∈_ argmaxa maxi _Q[π]j_ _[i]_ [(][s, a][) = argmax]a maxi _ψ[π][i]_ (s, a)[⊤]wj . (6)

(Barreto et al., 2018) proved that under the appropriate conditions for optimal policy approximates,
the policy constructed in (6) is close to the optimal one, and their difference is upper-bounded:


2
_Q[∗]_ _Q[π]_ _r_ _ri_ + min _ri_ _rj_ + ϵ _,_ (7)
_||_ _−_ _||∞_ _≤_ 1 _γ_ _||_ _−_ _||∞_ _j_ _||_ _−_ _||∞_

_−_  

whereinterpreted in the following manner. Given the arbitrary task ∥f − _g∥∞_ = maxs,a |f (s, a) − _g(s, a)|. For an arbitrary reward function M_, we identify the theoretically closest r the result can be
possible linear reward task Mi with ri. For this theoretically closest task, we search the linear task
_Mj in our set of task M (from which we also construct the GPI optimal policy (6)) which is closest to_
it. The upper bound between Q[∗] and Q is then defined by 1) the difference between task M and the
theoretically closest possible linear task Mi: ||r − _ri||∞; and by 2) the difference between theoretical_
task Mi and the closest task Mj: minj _ri_ _rj_ . If our new task M is also linear then r = ri and
the first term in (7) would vanish. _||_ _−_ _||∞_

Very importantly, this result shows that the SF framework will only provide a good approximation of
the true Q-function if the reward function in a task can be represented using a linear decomposition.
If this is not the case then the error in the approximation increases with the distance between the true
reward function r and the best linear approximation of it ri as stated by _r_ _ri_ .
_||_ _−_ _||∞_

3 METHOD: ξ-LEARNING

3.1 DEFINITION AND FOUNDATIONS OF ξ-LEARNING

The goal of this paper is to investigate the application of SF&GPI to tasks with general reward
_functions R : Φ 7→_ R over state features φ ∈ Φ:

_r(st, at, st+1)_ _R(φ(st, at, st+1)) = R(φt),_ (8)
_≡_


-----

where we define φt _φ(st, at, st+1). Under this assumption the Q-function can not be linearly_
decomposed into a part that describes feature dynamics and one that describes the rewards as in the ≡
linear SF framework (4). To overcome this issue, we propose to define the expected cumulative discounted probability of successor features or ξ-function, which is going to be the central mathematical
object of the paper, as:


_ξ[π](s, a, φ) =_


_γ[k]p(φt+k = φ|st = s, at = a; π),_ (9)
_k=0_

X


where p(φt+k = φ|st = s, at = a; π), or in short p(φt+k = φ|st, at; π), is the probability density
function of the features at time t + k, following policy π and conditioned to s and a being the state
and action at time t respectively. Note that ξ[π] depends not only on the policy π but also on the state
transition (constant through the paper). With the definition of the ξ-function, the Q-function rewrites
(this is compatible with SFQL in the linear reward case, see Appendix A.6):


_Q[π](st, at) =_


_k=0_ _γ[k]Ep(φt+k|st,at;π) {R(φt+k)} =_

X


_γ[k]_

_k=0_

X


_p(φt+k = φ|st, at; π)R(φ)dφ_

_R(φ)ξ[π](st, at, φ)dφ ._


(10)


_γ[k]p(φt+k = φ|st, at; π)dφ =_
_k=0_

X


_R(φ)_


Depending on the reward function R, there are several ξ-functions that correspond to the same Q
function. Formally, this is an equivalence relationship, and the quotient space has a one-to-one
correspondence with the Q-function space.
**Proposition 1. (Equivalence between functions ξ and Q) Let** = _Q :_ R s.t. _Q_ _<_
_Q_ _{_ _S × A →_ _∥_ _∥∞_
_∞}there is a bijective correspondence between the quotient space. Let ∼_ _be defined as ξ1 ∼_ _ξ2 ⇔_ Φ _[Rξ][1][ =]_ Φ _[Rξ][2][. Then,][ ∼] Ξ[is an equivalence relationship, and]and_ _._
R R _∼_ _Q_

**Corollary 1. The bijection between Ξ** _and_ _allows to induce a norm_ _into Ξ_ _from the_
_∼_ _Q_ _∥· ∥∼_ _∼_
_supremum norm in_ _, with which Ξ_ _is a Banach space (since_ _is Banach with_ _):_
_Q_ _∼_ _Q_ _∥· ∥∞_

_∥ξ∥∼_ = sups,a Φ _R(φ)ξ(s, a, φ)dφ_ [= sup]s,a (11)

Z _[|][Q][(][s, a][)][|][ =][ ∥][Q][∥][∞]_ _[.]_

Similar to the Bellman equation for the Q-function, we can define a Bellman operator for the
_ξ-function, denoted by Tξ, as:_
_Tξ(ξ[π]) = p(φt = φ|st, at) + γEp(st+1,at+1|st,at;π) {ξ[π](st+1, at+1, φ)} ._ (12)

As in the case of the Q-function, we can use Tξ to construct a contractive operator:
**Proposition 2. (ξ-learning has a fixed point) The operator Tξ is well-defined w.r.t. the equivalence**
_, and therefore induces an operator T_ _defined over Ξ_ _. T_ _is contractive w.r.t._ _. Since Ξ_
_∼_ _∼_ _∼_ _∼_ _∥· ∥∼_ _∼_
_is Banach, T_ _has a unique fixed point and iterating T_ _starting anywhere converges to that point._
_∼_ _∼_

In other words, successive applications of the operator T converge towards the class of optimal ξ
_∼_
functions [ξ[∗]] or equivalently to an optimal ξ function defined up to an additive function k satisfying

Φ _[k][(][s, a, φ][)][R][(][φ][)][d][φ][ = 0][,][ ∀][(][s, a][)][ ∈S × A][ (i.e.][ k][ ∈]_ [Ker][(][ξ][ →] Φ _[Rξ][)][).]_

While these two results state (see Appendix A for the proofs) the theoretical links to standardR R
Q-learning formulations, the Tξ operator defined in (12) is not usable in practice, because of the
expectation. In the next section, we define the optimisation iterate, prove its convergence, and provide
two variants to perform the ξ updates.

3.2 _ξ-LEARNING ALGORITHMS_

In order to learn the ξ-function, we introduce the ξ-learning update operator, which is an offpolicy temporal difference method analogous to Q-learning. Given a transition (st, at, st+1, φt) the
_ξ-learning update operator is defined as:_

_ξk[π]+1[(][s][t][, a][t][, φ][)][ ←]_ _[ξ]k[π][(][s][t][, a][t][, φ][) +][ α][k]_ [[][p][(][φ][t] [=][ φ][|][s][t][, a][t][) +][ γξ]k[π][(][s][t][+1][,][ ¯]at+1, φ) − _ξk[π][(][s][t][, a][t][, φ][)]][,][ (13)]_

where ¯at+1 = argmaxa Φ _[R][(][φ][)][ξ][π][(][s][t][+1][, a, φ][)][d][φ][.]_

The following is one of the main results of the manuscript, stating the convergence ofR _ξ-learning:_


-----

**Theorem 1. (Convergence of ξ-learning) For a sequence of state-action-feature** _st, at, st+1, φt_ _t=0_
_{_ _}[∞]_
_consider the ξ-learning update given in (13). If the sequence of state-action-feature triples visits_
_each state, action infinitely often, and if the learning rate αk is an adapted sequence satisfying the_
_Robbins-Monro conditions:_


_αk[2]_ _[<][ ∞]_ (14)
_k=1_

X


_αk = ∞,_
_k=1_

X


_then the sequence of function classes corresponding to the iterates converges to the optimum, which_
_corresponds to the optimal Q-function to which standard Q-learning updates would converge to:_



[ξn] [ξ[∗]] _with_ _Q[∗](s, a) =_
_→_


_R(φ)ξ[∗](s, a, φ)dφ._ (15)


The proof is provided in Appendix A and follows the same flow as for Q-learning.

The previous theorem provides convergence guarantees under the assumption that either p(φt =
_φ_ _st, at; π) is known, or an unbiased estimate can be constructed. We propose two different ways_
_|_
to approximate p(φt = φ|st, at; π) from a given transition (st, at, st+1, φt) so as to perform the
_ξ-update (13). The first instance is a model-free version and detailed in the following section. A_
second instance uses a one-step SF model, called One-Step Model-based (MB) ξ-learning, which is
further described in Sec. B.

**Model-free (MF) ξ-Learning:** MF ξ-learning uses the same principle as standard model-free
temporal difference learning methods. The update assumes for a given transition (st, at, st+1, φt)
that the probability for the observed feature is p(φ = φt _st, at) = 1. Whereas for all other features_
_|_
( _φ[′]_ Φ, φ[′] = φt) the probability is p(φ[′] = φt _st, at) = 0, see Appendix D for continuous features._
_∀_ _∈_ _̸_ _|_
The resulting updates are:

_φ = φt :_ _ξ[π](st, at, φ)_ (1 _α)ξ[π](st, at, φ) + α (1 + γξ[π](st+1, ¯at+1, φ))_
_←_ _−_ (16)
_φ[′]_ ≠ _φt :_ _ξ[π](st, at, φ[′])_ _←_ (1 − _α)ξ[π](st, at, φ[′]) + αγξ[π](st+1, ¯at+1, φ[′]) ._

Due to the stochastic update of the ξ-function and if the learning rate α ∈ (0, 1] discounts over time,
the ξ-update will learn the true probability of p(φ = φt _st, at). A potential problem with the MF_
_|_
procedure is that it might induce a high variance when the true feature probabilities are not binary.

3.3 META ξ-LEARNING

After discussing ξ-learning on a single task and showing its theoretical convergence, we can now
investigate how it can be applied in transfer learning. Similar to the linear SF framework the ξ-function
allows to reevaluate a policy learned for task Mi, ξ[π][i], in a new environment Mj:


_Q[π]j_ _[i]_ [(][s, a][) =]


_Rj(φ)ξ[π][i]_ (s, a, φ)dφ. (17)


This allows us to apply GPI in (6) for arbitrary reward functions in a similar manner to what was
proposed for linear reward functions in (Barreto et al., 2018). We extend the GPI result to the
_ξ-learning framework as follows:_
**Theorem 2.associated to a (possibly different) weighting function (Generalised policy improvement in ξ-learning) Let Ri** _L[1](Φ) M. Let be the set of tasks, each one ξ[π]i[∗] be a representative of_
_∈_
_the optimal class of ξ-functions for task Mi, i_ 1, . . ., I _, and let_ _ξ[˜][π][i]_ _be an approximation to the_
_optimal ξ-function,_ _ξ[π]i[∗]_ _ξ[π][i]_ _Ri_ _ε,_ _i. Then, for another task ∈{_ _}_ _M with weighting function R, the_
_policy defined as:_ _∥_ _−_ [˜] _∥_ _≤_ _∀_


_R(φ)ξ[˜][π][i]_ (s, a, φ)dφ, (18)


_π(s) = arg max_ max


_satisfies:_

_where_ _f_ _g = sups,a_
_∥_ _∥_


_ξ[∗]_ _ξ[π]_ _R_
_∥_ _−_ _∥_ _≤_


2

_R_ _Ri_ _p(φ_ _s,a) + ε),_ (19)
1 _γ_ [(min]i _∥_ _−_ _∥_ _|_
_−_


_where_ _f_ _g = sups,a_ Φ
_∥_ _∥_ _[|][f][ ·][ g][|][ d][φ][.]_

The proof is provided in Appendix A.R


-----

4 EXPERIMENTS

We evaluated ξ-learning in two environments. The first has discrete features. It is a modified version
of the object collection task by Barreto et al. (2017) having more complex features to allow general
reward functions. See Appendix E.1 for experimental results in the original environment. The second
environment, the racer environment, evaluates the agents in tasks with continuous features.


4.1 DISCRETE FEATURES - OBJECT COLLECTION ENVIRONMENT

**Environment:** The environment consist of 4 rooms (Fig. 1 - a). The agent starts an episode in
position S and has to learn to reach the goal position G. During an episode, the agent can collect
objects to gain further rewards. Each object has 2 properties: 1) color: orange or blue, and 2) form:
box or triangle. The state space is a high-dimensional vector s ∈ R[112]. It encodes the agent’s position
using a 10 × 10 grid of two-dimensional Gaussian radial basis functions. Moreover, it includes
a memory about which object has been already collected. Agents can move in 4 directions. The
features φ ∈ Φ = {0, 1}[5] are binary vectors. The first 2 dimensions encode if an orange or a blue
object was picked up. The 2 following dimensions encode the form. The last dimension encodes
if the agent reached goal G. For example, φ[⊤] = [1, 0, 1, 0, 0] encodes that the agent picked up an
orange box.

**Tasks:** Each agent learns sequentially 300 tasks which differ in their reward for collecting objects.
We compared agents in two settings: either in tasks with linear or general reward functions. For each
linear task Mi, the rewards r = φ[⊤]wi are defined by a linear combination of features and a weight
with a specific property. They are randomly sampled from a uniform distribution:vector wi ∈ R[5]. The weights wi,k for the first 4 dimensions define the rewards for collecting an object wi,k ( 1, 1).
The final weight defines the reward for reaching the goal position which is wi,5 = 1 for each task. The ∼U _−_
general reward functions are sampled by assigning a different reward to each possible combination
of object properties φj Φ using uniform sampling: Ri(φj) ( 1, 1), such that picking up an
_∈_ _∼U_ _−_
orange box might result in a reward of Ri(φ[⊤] = [1, 0, 1, 0, 0]) = 0.23.


(a) Collection Environment (b) Tasks with Linear Reward Functions

QL SFQL MF Xi MB Xi


600

400

200


20 40 60 80 100 120 140 160 180 200 220 240 260 280 300


S Tasks Trained

G

S


(c) Effect of Non-Linearity (d) Tasks with General Reward Functions


QL / Xi SFQL / Xi

0.1250.3750.6250.8751.1251.375

Mean Error of Linear Model


QL SFQL MF Xi MB Xi

20 40 60 80 100 120 140 160 180 200 220 240 260 280 300


800

600

400

200


0.5


Tasks Trained


Figure 1: In the (a) object collection environment, ξ-learning reached the highest average reward
per task for (b) linear, and (d) general reward functions. The average over 10 runs per algorithm and
the standard error of the mean are depicted. (c) The performance difference between ξ-learning and
SFQL is stronger for general reward tasks that have high non-linearity, i.e. where a linear reward
model yields a high error. SFQL can only reach less than 50% of MF ξ-learning’s performance in
tasks with a mean linear reward model error of 1.625.


-----

**Agents:** We compared ξ-learning to Q-learning (QL), and classical SF Q-learning (SFQL) (Barreto
et al., 2017). All agents use function approximation for their state-action functions (Q, ψ, or ξfunction). An independent linear mapping is used to map the values from the state for each of the 4
actions. As the features are discrete, the ξ-function and ˆpφ-model are approximated by an independent
mapping for each action and possible feature φ ∈ Φ. The Q-value Q(s, a) for the ξ-agents (Eq. 10) is
computed by: Q[π](s, a) = _φ_ Φ _[R][(][φ][)][ξ][π][(][s, a, φ][)][. The reward functions of each task are given to the]_

_∈_
_ξ-agents. For SFQL, the sampled reward weights wi were given in tasks with linear reward functions._
For general reward functions, a linear model[P] _r = φ[⊤]w˜_ _i approximating the rewards was learned_
for each task and its weights ˜wi given to SFQL (see Appendix C.3 for details). Each tasks was
executed for 20, 000 steps, and the average performance over 10 runs per algorithm was measured.
We performed a grid-search over the parameters of each agent, reporting here the performance of the
parameters with the highest total reward over all tasks.

**Results:** _ξ-learning outperformed SFQL and QL for tasks with linear and general reward functions_
(Fig. 1 - b; d). MF showed a slight advantage over MB ξ-learning in both settings. We further studied
the effect non-linearity of general reward functions on the performance of classical SF compared to
_ξ-learning by evaluating them in tasks with different levels of non-linearity. We sampled general_
reward functions that resulted in different levels of mean absolute model error if they are linearly
approximated with min ˜w **w** . We trained SFQL and MF ξ-learning in each of these
conditions on 300 tasks and measured the ratio between the total return of SFQL and MF[|][r][(][φ][)][ −] _[φ][⊤]_ [˜] _|_ _ξ (Fig. 1)._
The relative performance of SFQL compared to MF ξ reduces with higher non-linearity of the reward
functions. For reward functions that are nearly linear (mean error of 0.125), both have a similar
performance. Whereas, for reward functions that are difficult to model with a linear relation (mean
error of 1.625) SFQL reaches only less than 50% of the performance of ξ-learning. This follows
SFQL’s theoretical limitation in (7) and shows the advantage of ξ learning over SFQL in non-linear
reward tasks.

4.2 CONTINUOUS FEATURES - RACER ENVIRONMENT


**Environment and Tasks:** We further evaluated the agents in an environment with continuous
features (Fig. 2 - a). The agent is randomly placed in the environment and has to drive around for 200
timesteps before the episode ends. Similar to a car, the agent has an orientation and momentum, so
that it can only drive straight, or in a right or left curve. The agent reappears on the opposite side if
it exits one side. The distance to 3 markers are provided as features φ ∈ R[3]. Rewards depend on
the distances r = _k=1_ _[r][k][φ][k][, where each component][ r][k][ has 1 or 2 preferred distances defined by]_
Gaussian functions. For each of the 37 tasks, the number of Gaussians and their properties (µ, σ) are
randomly sampled for each feature dimension. Fig. 2 (a) shows a reward function with dark areas

[P][3]
depicting higher rewards. The agent has to learn to drive around in such a way as to maximize its
trajectory over positions with high rewards. The state space is a high-dimensional vector s ∈ R[120]
encoding the agent’s position and orientation. As before, the 2D position is encoded using a 10 × 10
grid of two-dimensional Gaussian radial basis functions. Similarly, the orientation is also encoded
using 20 Gaussian radial basis functions.

(a) Racer Environment (b) Tasks with General Reward Functions


QL SFQL CMF Xi

10 15 20 25 30 35


80k

60k


40k


Tasks Trained

Figure 2: (a) Example of a reward function for the racer environment based on distances to its 3
markers. (b) ξ-learning reaches the highest average reward per task. SFQL yields a performance even
below QL as it is not able to model the reward function with its linear combination of weights and
features. The average over 10 runs per agent and the standard error of the mean are depicted.


-----

**Agents:** We introduce a MF ξ-agent for continuous features (CMF ξ) (Appendix D.2.3). CMF ξ discretizes each feature dimension φk [0, 1] in 11 bins with the bin centers: X = 0.0, 0.1, . . ., 1.0 .
It learns for each dimension k and bin ∈ _i the ξ-value ξk[π][(][s, a, X][i][)][. Q-values (Eq. 10) are computed] {_ _}_
by: Q[π](s, a) = _k=1_ 11i=1 _[r][k][(][X][i][)][ξ]k[π][(][s, a, X][i][)][. SFQL learns][ ψ][ for the continuous, non-discretized]_
feature space. It received an approximated weight vector ˜wi that was trained before the task started
on several uniformly sampled features and rewards.P

[P][3]

**Results:** _ξ-learning reached the highest performance of all agents (Fig. 2 - b) outperforming QL_
and SFQL. SFQL reaches only a low performance below QL, because it is not able to sufficiently well
approximate the general reward functions with its linear reward model. This shows the advantage of
_ξ-learning over SFQL in environments with general reward functions._

5 DISCUSSION

**Performance of ξ-learning compared to classical SF&GPI:** _ξ-learning allows to disentangle_
the dynamics of policies in the feature space of a task from the associated reward, see (10). The
experimental evaluation in tasks with general reward functions (Fig. 1 - d, and Fig. 2) shows that
_ξ-learning can therefore successfully apply GPI to transfer knowledge from learned tasks to new_
ones. Given a general reward function it can re-evaluate successfully learned policies for knowledge
transfer. Instead, classical SFQL based on a linear decomposition (3) can not be directly applied
given a general reward function. In this case a linear approximation has to be learned which shows
inferior performance to ξ-learning that directly uses the true reward function.

_ξ-learning also shows an increased performance over SFQL in environments with linear reward_
functions (Fig. 1 - a). This effect can not be attributed to differences in their computation of a policy’s
expected return as both are correct (Appendix A.6). A possible explanation is that ξ-learning reduces
the complexity for the function approximation of the ξ-function compared to the ψ-function in SFQL.

**Continuous Feature Spaces:** For tasks with continuous features (racer environment), ξ-learning
used successfully a discretization of each feature dimension, and learned the ξ-values independently
for each dimension. This strategy is viable for reward functions that are cumulative over the feature
dimensions: r(φ) = _k_ _[r][k][φ][k][. The Q-value can be computed by summing over the independent]_

dimensions and the bins X: Q[π](s, a) = _k_ _x_ _X_ _[r][k][(][x][)][ξ][π][(][s, a, x][)][. For more general reward func-]_

_∈_
tions, the space of all feature combinations would need to be discretized, which grows exponentially

[P]

with each new dimension. As a solution the ξP-function could be directly defined over the continuous

[P]

feature space, but this yields some problems. First, the computation of the expected return requires an
integral Q(s, a) = _φ_ Φ _[R][(][φ][)][ξ][(][s, a, φ][)][ over features instead of a sum, which is a priori intractable.]_

_∈_
Second, the representation and training of the ξ-function, which would be defined over a continuum

R

thus increasing the difficulty of approximating the function. Janner et al. (2020) and Touati and
Ollivier (2021) propose methods that might allow to represent a continuous ξ-function, but it is
unclear if they converge and if they can be used for transfer learning.

**Learning of Features:** In principle, classical SF&GPI can also optimize general reward functions
if features and reward weights are learned. This is possible if the learned features describe the
non-linear effects in the reward functions. Nonetheless, learning of features adds further challenges
and shows to reduce performance. Barreto et al. (2017) learns features from observations sampled
from several tasks before the SF&GPI starts. Therefore, novel non-linearities potentially introduced
at later tasks are not well represent by the learned features. If instead features are learned alongside
the SF&GPI procedure, the problem on how to coordinate both learning processes needs to be
investigated. Importantly, ψ-functions for older tasks would become unusable for the GPI procedure
on newer task, because the feature representation changed between them.

Moreover, our replication (Sec. E.1) of the object collection task from Barreto et al. (2017) shows the
performance of learned features is below the performance of given features. MF Xi reaches a final
average reward per task of 850 with given features and reward functions. The best performance of
SFQL with learned features only reaches a final performance of 575 (Fig. 2 in (Barreto et al., 2017)).

In summary, if features and reward functions are known then ξ-learning outperforms SFQL. And
using given features and reward functions is natural for many applications as these are often known,
for example in robotic tasks where they are usually manually designed (Akalin and Loutfi, 2021).


-----

**Computational Complexity:** The improved performance of SFQL and ξ-learning over QL in
the transfer learning setting comes at the cost of an increased computational complexity. The GPI
procedure (6) of both approaches requires to evaluate at each step the ψ[π][i]-function or ξ[π][i]-function
over all previous experienced tasks in M. As a consequence, the computational complexity increases
linearly with each new environment that is added. A solution is to apply GPI only over a subset of
learned policies. Nonetheless, an open question is still how to optimally select this subset.

6 RELATED WORK

**Transfer Learning:** Transfer methods in RL can be generally categorized according to the type of
tasks between which transfer is possible and the type of transferred knowledge (Taylor and Stone,
2009; Lazaric, 2012; Zhu et al., 2020). In the case of SF&GPI which ξ-learning is part of, tasks only
differ in their reward functions. The type of knowledge that is transferred are policies learned in
source tasks which are re-evaluated in the target task and recombined using the GPI procedure. A
natural use-case for ξ-learning are continual problems (Khetarpal et al., 2020) where an agent has
continually adapt to changing tasks, which are in our setting different reward functions.

**Successor Features:** SF are based on the concept of successor representations (Dayan, 1993;
Momennejad, 2020). Successor representations predict the future occurrence of all states for a
policy in the same manner as SF for features. Their application is restricted to low-dimensional
state spaces using tabular representations. SF extended them to domains with high-dimensional state
spaces (Kulkarni et al., 2016; Zhang et al., 2017; Barreto et al., 2017; 2018), by predicting the future
occurrence of low-dimensional features that are relevant to define the return. Several extensions
to the SF framework have been proposed. One direction aims to learn appropriate features from
data such as by optimally reconstruct rewards (Barreto et al., 2017), using the concept of mutual
information (Hansen et al., 2019), or the grouping of temporal similar states (Madjiheurem and
Toni, 2019). Another direction is the generalization of the ψ-function over policies (Borsa et al.,
2018) analogous to universal value function approximation (Schaul et al., 2015). Similar approaches
use successor maps (Madarasz, 2019), goal-conditioned policies (Ma et al., 2020), or successor
feature sets (Brantley et al., 2021). Other directions include their application to POMDPs (Vértes
and Sahani, 2019), combination with max-entropy principles (Vertes, 2020), or hierarchical RL
(Barreto et al., 2021). In difference to ξ-learning all these approaches build on the assumption of
linear reward functions, whereas ξ-learning allows the SF&GPI framework to be used with general
reward functions. Nonetheless, most of the extensions for linear SF can be combined with ξ-learning.

**Model-based RL:** SF represent the dynamics of a policy in the feature space that is decoupled
from the rewards allowing to reevaluate them under different reward functions. It shares therefore
similar properties with model-based RL (Lehnert and Littman, 2019). In general, model-based RL
methods learn a one-step model of the environment dynamics p(st+1 _st, at). Given a policy and an_
_|_
arbitrary reward function, rollouts can be performed using the learned model to evaluate the return.
In practice, the rollouts have a high variance for long-term predictions rendering them ineffective.
Recently, (Janner et al., 2020) proposed the γ-model framework that learns to represent ξ-values
in continuous domains. Nonetheless, the application to transfer learning is not discussed and no
convergence is proven as for ξ-learning. This is the same case for the forward-backward MPD
representation proposed in Touati and Ollivier (2021). (Tang et al., 2021) also proposes to decouple
the dynamics in the state space from the rewards, but learn an internal representation of the rewards.
This does not allow to reevaluate an policy to a new reward function without relearning the mapping.

7 CONCLUSION

The introduced ξ-learning framework learns the expected cumulative discounted probability of
successor features which disentangles the dynamics of a policy in the feature space of a task from the
expected rewards. This allows ξ-learning to reevaluate the expected return of learned policies for
general reward functions and to use it for transfer learning utilizing GPI. We proved that ξ-learning
converges to the optimal policy, and showed experimentally its improved performance over Q-learning
and the classical SF framework for tasks with linear and general reward functions.


-----

ETHICS STATEMENT

_ξ-learning and its associated optimization algorithms represent general RL procedures similar to_
Q-learning. Their potential negative societal impact depends on their application domains which
range over all possible societal areas in a similar manner as for other general RL procedures.

Beyond the topic of the paper, we did our best to cite the relevant literature and to fairly compare
with previous ideas, concepts and methods. To that aim, all agents are trained and evaluated within
the same software environment, and under the very same experimental settings.

REPRODUCIBILITY STATEMENT

In order to ensure high changes of reproducibility we provided lots of details of the method and
experiments associated to the paper. In particular, we have provided the proofs for all mathematical
results announced in the main paper (see Appendix A). These constitute the theoretical foundation of
the proposed ξ-learning methodology. Secondly, we have provided all experimental details (methods,
and environments) required for reproducing our experiments, namely: appendix C for the object
collection and D for the racer environment respectively. In addition, we provide additional results
in appendix E, to completely illustrate the interest of the proposed method. Finally, we provided an
anonymous link to the source code, so that reviewers can run it if necessary.

REFERENCES

N. Akalin and A. Loutfi. Reinforcement learning approaches in social robotics. Sensors, 21(4):1292,
2021.

A. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul, H. P. van Hasselt, and D. Silver. Successor
features for transfer in reinforcement learning. In Advances in neural information processing
_systems, pages 4055–4065, 2017._

A. Barreto, D. Borsa, J. Quan, T. Schaul, D. Silver, M. Hessel, D. Mankowitz, A. Zidek, and
R. Munos. Transfer in deep reinforcement learning using successor features and generalised policy
improvement. In International Conference on Machine Learning, pages 501–510. PMLR, 2018.

A. Barreto, S. Hou, D. Borsa, D. Silver, and D. Precup. Fast reinforcement learning with generalized
policy updates. Proceedings of the National Academy of Sciences, 117(48):30079–30087, 2020.

A. Barreto, D. Borsa, S. Hou, G. Comanici, E. Aygün, P. Hamel, D. Toyama, J. Hunt, S. Mourad,
D. Silver, et al. The option keyboard: Combining skills in reinforcement learning. arXiv preprint
_arXiv:2106.13105, 2021._

D. Borsa, A. Barreto, J. Quan, D. Mankowitz, R. Munos, H. van Hasselt, D. Silver, and T. Schaul.
Universal successor features approximators. arXiv preprint arXiv:1812.07626, 2018.

K. Brantley, S. Mehri, and G. J. Gordon. Successor feature sets: Generalizing successor representations across policies. arXiv preprint arXiv:2103.02650, 2021.

P. Dayan. Improving generalization for temporal difference learning: The successor representation.
_Neural Computation, 5(4):613–624, 1993._

S. Hansen, W. Dabney, A. Barreto, T. Van de Wiele, D. Warde-Farley, and V. Mnih. Fast task
inference with variational intrinsic successor features. arXiv preprint arXiv:1906.05030, 2019.

M. Janner, I. Mordatch, and S. Levine. γ-models: Generative temporal difference learning for
infinite-horizon prediction. In NeurIPS, 2020.

K. Khetarpal, M. Riemer, I. Rish, and D. Precup. Towards continual reinforcement learning: A review
and perspectives. arXiv preprint arXiv:2012.13490, 2020.

T. D. Kulkarni, A. Saeedi, S. Gautam, and S. J. Gershman. Deep successor reinforcement learning.
_arXiv preprint arXiv:1606.02396, 2016._


-----

A. Lazaric. Transfer in reinforcement learning: a framework and a survey. In Reinforcement Learning,
pages 143–173. Springer, 2012.

L. Lehnert and M. L. Littman. Successor features support model-based and model-free reinforcement
learning. CoRR abs/1901.11437, 2019.

C. Ma, D. R. Ashley, J. Wen, and Y. Bengio. Universal successor features for transfer reinforcement
learning. arXiv preprint arXiv:2001.04025, 2020.

T. J. Madarasz. Better transfer learning with inferred successor maps. _arXiv preprint_
_arXiv:1906.07663, 2019._

S. Madjiheurem and L. Toni. State2vec: Off-policy successor features approximators. arXiv preprint
_arXiv:1910.10277, 2019._

V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement
learning. nature, 518(7540):529–533, 2015.

I. Momennejad. Learning structures: Predictive representations, replay, and generalization. Current
_Opinion in Behavioral Sciences, 32:155–166, 2020._

I. Momennejad, E. M. Russek, J. H. Cheong, M. M. Botvinick, N. D. Daw, and S. J. Gershman.
The successor representation in human reinforcement learning. Nature Human Behaviour, 1(9):
680–692, 2017.

T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In
_International conference on machine learning, pages 1312–1320, 2015._

D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and
go through self-play. Science, 362(6419):1140–1144, 2018.

R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.

H. Tang, J. Hao, G. Chen, P. Chen, C. Chen, Y. Yang, L. Zhang, W. Liu, and Z. Meng. Foresee then evaluate: Decomposing value estimation with latent future prediction. arXiv preprint
_arXiv:2103.02225, 2021._

M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A survey. Journal
_of Machine Learning Research, 10(7), 2009._

M. S. Tomov, E. Schulz, and S. J. Gershman. Multi-task reinforcement learning in humans. Nature
_Human Behaviour, pages 1–10, 2021._

A. Touati and Y. Ollivier. Learning one representation to optimize all rewards. arXiv preprint
_arXiv:2103.07945, 2021._

J. N. Tsitsiklis. Asynchronous stochastic approximation and q-learning. Machine learning, 16(3):
185–202, 1994.

E. Vertes. Probabilistic learning and computation in brains and machines. PhD thesis, UCL
(University College London), 2020.

E. Vértes and M. Sahani. A neurally plausible model learns successor representations in partially
observable environments. arXiv preprint arXiv:1906.09480, 2019.

C. J. Watkins and P. Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.

J. Zhang, J. T. Springenberg, J. Boedecker, and W. Burgard. Deep reinforcement learning with
successor features for navigation across similar environments. In 2017 IEEE/RSJ International
_Conference on Intelligent Robots and Systems (IROS), pages 2371–2378. IEEE, 2017._

Z. Zhu, K. Lin, and J. Zhou. Transfer learning in deep reinforcement learning: A survey. arXiv
_preprint arXiv:2009.07888, 2020._


-----

A THEORETICAL PROOFS

A.1 PROOF OF PROPOSITION 1

Let us start by recalling the original statement in the main paper.

**Proposition 1. (Equivalence between functions ξ and Q) Let** = _Q :_ R s.t. _Q_ _<_
_Q_ _{_ _S × A →_ _∥_ _∥∞_
_∞}there is a bijective correspondence between the quotient space. Let ∼_ _be defined as ξ1 ∼_ _ξ2 ⇔_ Φ _[Rξ][1][ =]_ Φ _[Rξ][2][. Then,][ ∼] Ξ[is an equivalence relationship, and]and_ _._
R R _∼_ _Q_


_Proof. We will proof the statements sequentially._

_∼_ **is an equivalence relationship:** To prove this we need to demonstrate that ∼ is symmetric,
reciprocal and transitive. The three are quite straightforward since: ξ ∼ _ξ, ∀ξ, ξ ∼_ _η ⇔_ _η ∼_ _ξ, ∀ξ, η_
and ξ ∼ _η, η ∼_ _ν ⇒_ _ξ ∼_ _ν._

**Bijective correspondence:** To prove the bijectivity, we will first prove that it is injective, then
surjective. Regarding the injectivity: [ξ] = [η] _Qξ_ = Qη, we prove it by contrapositive:
_̸_ _⇒_ _̸_


_R(φ)ξ(s, a, φ)dφ =_


_R(φ)η(s, a, φ)dφ ⇒_ [ξ] = [η]. (20)


_Qξ = Qη ⇒_


In order to prove the surjectivity, we start from a function Q ∈Q and select an arbitrary ξ ∈ Ξ, then
the following function:

_Q(s, a)_
_ξQ(s, a, φ) =_ (21)

Φ _[R][(¯]φ)ξ(s, a,_ _φ[¯])dφ[¯][ξ][(][s, a, φ][)]_

R


satisfies thatthere is a bijective correspondence between the elements of ξQ ∈ Ξ and that Φ _[R][(][φ][)][ξ][Q][(][s, a, φ][)][d][φ][ =][ Q][(][s, a] Ξ[)][,][ ∀]and of[(][s, a][)][ ∈S × A]._ [. We conclude that]
R _∼_ _Q_

A.2 PROOF OF COROLLARY 1

Let us recall the result:

**Corollary 1. The bijection between Ξ** _and_ _allows to induce a norm_ _into Ξ_ _from the_
_∼_ _Q_ _∥· ∥∼_ _∼_
_supremum norm in_ _, with which Ξ_ _is a Banach space (since_ _is Banach with_ _):_
_Q_ _∼_ _Q_ _∥· ∥∞_

_∥ξ∥∼_ = sups,a Φ _R(φ)ξ(s, a, φ)dφ_ [= sup]s,a (22)

Z _[|][Q][(][s, a][)][|][ =][ ∥][Q][∥][∞]_ _[,]_

_Proof. The norm induced in the quotient space is defined from the correspondence between Ξ_ and
_∼_
_Q and is naturally defined as in the previous equation. The norm is well defined since it does not_
depend on the class representative. Therefore, all the metric properties are transferred, and Ξ is
_∼_
immediately Banach with the norm .
_∥· ∥∼_

A.3 PROOF OF PROPOSITION 2

Let’s restate the result:

**Proposition 2. (ξ-learning has a fixed point) The operator Tξ is well-defined w.r.t. the equivalence**
_, and therefore induces an operator T_ _defined over Ξ_ _. T_ _is contractive w.r.t._ _. Since Ξ_
_∼_ _∼_ _∼_ _∼_ _∥· ∥∼_ _∼_
_is Banach, T_ _has a unique fixed point and iterating T_ _starting anywhere converges to that point._
_∼_ _∼_

_Proof. We prove the statements above one by one:_


-----

**The operator T** **is well defined:** Let us first recall the definition of the operator Tξ in (12), where
_∼_
we removed the dependency on π for simplicity:

_Tξ(ξ) = p(φt = φ|st, at) + γEp(st+1,at+1|st,at) {ξ(st+1, at+1, φ)}_

Let ξ1, ξ2 [ξ] two different representatives of class [ξ], we can write:
_∈_

_R(φ)(Tξ(ξ1)(s, a, φ)_ _Tξ(ξ2)(s, a, φ))dφ_
Φ _−_

Z


_p(s[′], a[′]_ _s, a)γ(ξ1(s[′], a[′], φ)_ _ξ2(s[′], a[′], φ))ds[′]dφ_
_|_ _−_


_R(φ)_


(23)


_p(s[′], a[′]|s, a)_


_R(φ)(ξ1(s[′], a[′], φ)_ _ξ2(s[′], a[′], φ))dφds[′]_
_−_


= γ

= 0


becausesince the image of class does not depend on the function chosen to represent the class. ξ1, ξ2 ∈ [ξ]. Therefore the operator T∼([ξ]) = Tξ(ξ) is well defined in the quotient space,

**Contractive operator T** **:** The contractiveness of T can be proven directly:
_∼_ _∼_


_R(φ)_ _p(φ|s, a) + γEp(s′,a′|s,a){ξ(s[′], a[′], φ)}_
 


_T_ ([ξ]) _T_ ([η]) = sup
_∥_ _∼_ _−_ _∼_ _∥∼_ _s,a_


_−p(φ|s, a) −_ _γEp(s′,a′|s,a){η(s[′], a[′], φ)}_ dφ



_R(φ)Ep(s′,a′|s,a){ξ(s[′], a[′], φ) −_ _η(s[′], a[′], φ)}dφ_


=γ sup
_s,a_


(24)


_R(φ)(ξ(s[′], a[′], φ) −_ _η(s[′], a[′], φ))dφ_


=γ sup
_s,a_ [E][p][(][s][′][,a][′][|][s,a][)]


_R(φ)(ξ(s[′], a[′], φ) −_ _η(s[′], a[′], φ))dφ_


_≤γ sups[′],a[′]_


=γ [ξ] [η]
_∥_ _−_ _∥∼_

The contractiveness of T can also be understood as being inherited from the standard Bellmann
_∼_
operator on Q. Indeed, given a ξ function, one can easily see that applying the standard Bellman
operator to the Q function corresponding to ξ leads to the Q function corresponding to T ([ξ]).
_∼_

**Fixed point of T** **:** To conclude the proof, we use the fact that any contractive operator on a Banach
_∼_
space, in our case: T∼ : Ξ∼ _→_ Ξ∼ has a unique fixed point [ξ[∗]], and that for any starting point [ξ0],
the sequence [ξn] = T∼([ξn−1]) converges to [ξ[∗]] w.r.t. to the corresponding norm ∥[ξ]∥∼.

A.4 PROOF OF THEOREM 1

These two propositions will be useful to prove that the ξ learning iterates converge in Ξ . Let us
_∼_
restate the definition of the operator from (13):

_ξk[π]+1[(][s][t][, a][t][, φ][)][ ←]_ _[ξ]k[π][(][s][t][, a][t][, φ][) +][ α][k]_ [[][p][(][φ][t] [=][ φ][|][s][t][, a][t][;][ π][) +][ γξ]k[π][(][s][t][+1][,][ ¯]at+1, φ) − _ξk[π][(][s][t][, a][t][, φ][)]]_

and the theoretical result:

**Theorem 1. (Convergence of ξ-learning) For a sequence of state-action-feature** _st, at, st+1, φt_ _t=0_
_{_ _}[∞]_
_consider the ξ-learning update given in (13). If the sequence of state-action-feature triples visits_
_each state, action infinitely often, and if the learning rate αk is an adapted sequence satisfying the_
_Robbins-Monro conditions:_


_αk[2]_ _[<][ ∞]_ (25)
_k=1_

X


_αk = ∞,_
_k=1_

X


-----

_then the sequence of function classes corresponding to the iterates converges to the optimum, which_
_corresponds to the optimal Q-function to which standard Q-learning updates would converge to:_



[ξn] [ξ[∗]] _with_ _Q[∗](s, a) =_
_→_


_R(φ)ξ[∗](s, a, x)dφ._ (26)


_Proof. The proof re-uses the flow of the proof used for Q-learning (Tsitsiklis, 1994). Indeed, we_
rewrite the operator above as:

_ξk[π]+1[(][s][t][, a][t][, φ][)][ ←]_ _[ξ]k[π][(][s][t][, a][t][, φ][) +][ α][k]_ [[][T][ξ][(][ξ]k[π][)(][s][t][, a][t][, φ][)][ −] _[ξ]k[π][(][s][t][, a][t][, φ][) +][ ε][(][s][t][, a][t][, φ][)]]_

with ε defined as:
_ε(st, at, φ) =_ _p(φt = φ|st, at; π) + γξk[π][(][s][t][+1][,][ ¯]at+1, φ)_
_−E {p(φt = φ|st, at; π) + γξk[π][(][s][t][+1][,][ ¯]at+1, φ)} ._

Obviously ε satisfies E _ε_ = 0, which, together with the contractiveness of T, is sufficient to
_{_ _}_ _∼_
demonstrate the convergence of the iterative procedure as done for Q-learning. In our case, the
optimal function ξ[∗] is defined up to an additive kernel function κ ∈ Ker. The correspondence with
the optimal Q learning function is a direct application of the correspondence between the ξ- and
Q-learning problems.

A.5 PROOF OF THEOREM 2

Let us restate the result.
**Theorem 2.associated to a (possibly different) weighting function (Generalised policy improvement in ξ-learning) Let Ri** _L[1](Φ) M. Let be the set of tasks, each one ξ[π]i[∗] be a representative of_
_∈_
_the optimal class of ξ-functions for task Mi, i_ 1, . . ., I _, and let_ _ξ[˜][π][i]_ _be an approximation to the_
_optimal ξ-function,_ _ξ[π]i[∗]_ _ξ[π][i]_ _Ri_ _ε,_ _i. Then, for another task ∈{_ _}_ _M with weighting function R, the_
_policy defined as:_ _∥_ _−_ [˜] _∥_ _≤_ _∀_


_R(φ)ξ[˜][π][i]_ (s, a, φ)dφ, (27)


_π(s) = arg max_ max


_satisfies:_

_where_ _f_ _g = sups,a_
_∥_ _∥_


_ξ[∗]_ _ξ[π]_ _R_
_∥_ _−_ _∥_ _≤_

Φ

_[|][f][ ·][ g][|][ d][φ][.]_


2

_R_ _Ri_ _p(φ_ _s,a) + ε),_ (28)
1 _γ_ [(min]i _∥_ _−_ _∥_ _|_
_−_


_Proof. The proof is stated in two steps. First, we exploit the proof of Proposition 1 of (Barreto et al.,_
2017), and in particular (13) that states:

2
_Q[∗]_ _Q[π]_ sup _,_ _i_ 1, . . ., I _,_ (29)
_∥_ _−_ _∥∞_ _≤_ 1 _γ_ _s,a_ _∀_ _∈{_ _}_

_−_  _[|][r][(][s, a][)][ −]_ _[r][i][(][s, a][)][|][ +][ ε]_

where Q[∗] and Q[π] are the Q-functions associated to the optimal and π policies in the environment
_R. The conditions on the Q functions required in the original proposition are satisfied because the_
_ξ-functions satisfy them, and there is an isometry between Q and ξ functions._

Because the above inequality is true for all training environments i, we can rewrite as:


2
_Q[∗]_ _Q[π]_ min sup _._ (30)
_∥_ _−_ _∥∞_ _≤_ 1 _γ_ _i_ _s,a_

_−_  _[|][r][(][s, a][)][ −]_ _[r][i][(][s, a][)][|][ +][ ε]_

We now realise that, in the case of ξ-learning, the reward functions rewrite as:


_R(φ)p(φ_ _s, a)dφ_ _ri(s, a) =_
_|_


_Ri(φ)p(φ_ _s, a)dφ,_ (31)
_|_


_r(s, a) =_

and therefore we have:


(R(φ) _Ri(φ))p(φ_ _s, a)dφ_
_−_ _|_

_R(φ)_ _Ri(φ)_ _p(φ_ _s, a)dφ_
_|_ _−_ _|_ _|_


sup
_s,a_ _s,a_

_[|][r][(][s, a][)][ −]_ _[r][i][(][s, a][)][|][ = sup]_

_≤_ sups,a


(32)


= ∥R − _Ri∥p(φ|s,a)_


-----

Similarly, due to the isometry between ξ and Q-learning, i.e. Proposition 2, we can write that:

_ξ[∗]_ _ξ[π]_ _R =_ [ξ[∗]] [ξ[π]] = _Q[∗]_ _Q[π]_
_∥_ _−_ _∥_ _∥_ _−_ _∥∼_ _∥_ _−_ _∥∞_


min sup
_i_ _s,a_

_[|][r][(][s, a][)][ −]_ _[r][i][(][s, a][)][|][ +][ ε]_


(33)


1 − _γ_


_R_ _Ri_ _p(φ_ _s,a) + ε),_

_≤_ 1 _γ_ [(min]i _∥_ _−_ _∥_ _|_

_−_

which proves the generalised policy improvement for ξ-learning.

A.6 RELATION BETWEEN CLASSICAL SF AND ξ-LEARNING FOR LINEAR REWARD FUNCTIONS

In the case of linear reward functions, i.e. where assumption (3) holds, it is possible to show that ξlearning can be reduced to classical SF. Classical SF represents therefore a specific case of ξ-learning
under this assumption.
**Theorem 3. (Equality of classical SF and ξ-learning for linear reward functions) Given the assump-**
_tion that reward functions are linearily decomposable with_

_ri(st, at, st+1) ≡_ _φ(st, at, st+1)[⊤]wi,_

_wheremi_ _φ ∈, then the classical SF andR[n]_ _are features for a transition and ξ-learning framework are equivalent. wi ∈_ R[n] _are the reward weight vector of task_
_∈M_

_Proof. We start with the definition of the Q-value according to ξ-learning from (10). After replacing_
the reward function Ri with our linear assumption, the definition of the Q-function according to
classical SF with the ψ-function can be recovered:


_ξ[π](st, at, φ)Ri(φ)dφ_

_ξ[π](st, at, φ)φ[⊤]widφ_


_Qi(s, a) =_


= wi[⊤]

= wi[⊤]

= wi[⊤]


_γ[k]p(φt+k = φ|st = s, at = a; π)φ dφ_
_k=0_

X


_γ[k]_

_k=0_

X


_p(φt+k = φ|st = s, at = a; π)φ dφ_


_i[⊤]_ _γ[k]E_ _φt+k_

_{_ _}_
_k=0_

X

_∞_ _⊤_

_γ[k]φt+k_ **wi** = _ψ(s, a)[⊤]wi ._

(k=0 )
X


= E


Please note, although both methods are equal in terms of their computed values, how these are
represented and learned differs between them. Thus, it is possible to see a performance difference of
the methods in the experimental results where ξ-learning outperforms SFQL in our environments.

B ONE-STEP SF MODEL-BASED (MB) ξ-LEARNING

Besides the MF ξ-learning update operator (16), we introduce a second ξ-learning procedure called
_One-step SF Model-based (MB) ξ-Learning that attempts to reduce the variance of the update._
To do so, MB ξ-learning estimates the distribution over the successor features over time. Let
_p˜(φt = φ|st, at; π) denote the current estimate of the feature distribution. Given a transition_
(st, at, st+1, φt) the model is updated according to:

_φ = φt :_ _p˜φ(φ|st, at; π)_ _←_ _p˜φ(φ|st, at; π) + β (1 −_ _p˜φ(φ|st, at; π))_
_φ[′]_ ≠ _φt :_ _p˜φ(φ[′]|st, at; π)_ _←_ _p˜φ(φ[′]|st, at; π) −_ _βp˜φ(φ[′]|st, at; π),_


-----

where β [0, 1] is the learning rate. After updating the model ˜pφ, it can be used for the ξ-update as
_∈_
defined in (13). Since the learned model ˜pφ is independent from the reward function and from the
policy, it can be learned and used over all tasks.

C EXPERIMENTAL DETAILS: OBJECT COLLECTION ENVIRONMENT

The object collection environment (Fig. 1 - a) was briefly introduced in Section 4.1. This section
provides a formal description.

C.1 ENVIRONMENT

The environment is a continuous two-dimensional area in which the agent moves. The position of
the agent is a point in the 2D space: (x, y) ∈ [0, 1][2]. The action space of the agent consists of four
movement directions: A = {up, down, left, right}. Each action changes the position of the agent in a
certain direction and is stochastic by adding a Gaussian noise. For example, the action for going right
updates the position according to xt+1 = xt + N (µ = 0.05, σ = 0.005). If the new position ends in
a wall (black areas in Fig. 1 - a) that have a width of 0.04) or outside the environment, the agent is set
back to its current position. Each environment has 12 objects. Each object has two properties with
two possible values: color (orange, blue) and shape (box, triangle). If the agent reaches an object, it
collects the object which then disappears. The objects occupy a circular area with radius 0.04. At the
beginning of an episode the agent starts at location S with (x, y)S = (0.05, 0.05). An episode ends if
the agent reaches the goal area G which is at position (x, y)G = (0.86, 0.86) and has a circular shape
with radius 0.1. After an episode the agent is reset to the start position S and all collected objects
reappear.

The state space of the agents consist of their position in the environment and the information about
which objects they already collected during an episode. Following (Barreto et al., 2017), the position
is encoded using a radial basis function approach. This upscales the agent’s (x, y) position to a
high-dimensional vectordifferent functions such as the spos ∈ ψR[100] or ξproviding a better signal for the function approximation of the-function. The vector spos is composed of the activation of
two-dimensional Gaussian functions based on the agents position (x, y):


_spos = exp_
_−_ [(][x][ −] _[c][j,][1][)][2][ + (]σ_ _[y][ −]_ _[c][j,][2][)][2]_



(34)


whereover the area of the environment. The state also encodes the memory about the objects that the agent cj ∈ R[2] is the center of the j[th] Gaussian. The centers are laid out on a regular 10 × 10 grid
has already collected using a binary vectorobject has been taken (smem,j = 1) or not (s smemmem,j ∈{ = 00). An additional constant term was added to, 1}[12]. The j[th] dimension encodes if the j[th]
the state to aid the function approximation. As a result, the state received by the agents is a column
vector with s = [s[⊤]pos[, s][⊤]mem[,][ 1]][⊤] _[∈]_ [R][113][.]

The features φ(st, at, st+1) 0, 1 in the environment describe the type of object that was collected
_∈{_ _}[5]_
by an agent during a step or if it reached the goal position. The first four feature dimensions encode
binary the properties of a collected object and the last dimension if the goal area was reached. In total
_|Φ| = 6 possible features exists: φ1 = [0, 0, 0, 0, 0][⊤]- standard observation, φ2 = [1, 0, 1, 0, 0][⊤]-_
collected an orange box, φ3 = [1, 0, 0, 1, 0][⊤]- collected an orange triangle, φ4 = [0, 1, 1, 0, 0][⊤]collected a blue box, φ5 = [0, 1, 0, 1, 0][⊤]- collected a blue triangle, and φ6 = [0, 0, 0, 0, 1][⊤]- reached
the goal area.

Two types of tasks were evaluated in this environment that have either 1) linear or 2) general reward
functions. 300 tasks, i.e. reward functions, were sampled for each type. For linear tasks, the rewards
_r = φ[⊤]wi are defined by a linear combination of discrete features φ ∈_ N[5] and a weight vector
**wobjects having specific properties, e.g. being blue or being a box. The weights for each of the fouri ∈** R[5]. The first four dimensions in wi define the reward that the agent receives for collecting
dimensions are randomly sampled from a uniform distribution:task. The final weight defines the reward for reaching the goal state which is wk∈[1,2,3,4] ∼U w(5− = 11, 1) for each for each
task. For training agents in general reward tasks, general reward functions Ri for each task Mi were
sampled. These reward functions define for each of the four features (φ2, . . ., φ5) that represent the
collection of a specific object type an individual reward. Their rewards were sampled from a uniform


-----

distribution: Ri(φk∈{2,...,5}) ∼U(−1, 1). The reward for collecting no object is Ri(φ1) = 0 and for
reaching the goal area is Ri(φ6) = 1 for all tasks. Reward functions of this form can not be linearly
decomposed in features and a weight vector.

C.2 ALGORITHMS

This section introduces the details of each evaluated algorithm. First the common elements are
discussed before introducing their specific implementations.

All agents experience the tasks M ∈M of an environment sequentially. They are informed when
a new task starts. All algorithms receive the features φ(s, a, s[′]) of the environment. For the action
selection and exploration, all agents use a ϵ-greedy strategy. With probability ϵ ∈ [0, 1] the agent
performs a random action. Otherwise it selects the action that maximizes the expected return.

As the state space (s ∈ R[113]) of the environments is high-dimensional and continuous, all agents use
an approximation of their respective functions such as for the Q-function (Q[˜](s, a) ≈ _Q(s, a)) or the_
_ξ-function (ξ[˜](s, a, φ) ≈_ _ξ(s, a, φ)). We describe the general function approximation procedure on_
the example of ξ-functions. If not otherwise mentioned, all functions are approximated by a single
linear mapping from the states to the function values. The parameters θ[ξ] _∈_ R[113][×|A|×|][Φ][|] of the
mapping have independent components θa,φ[ξ]

_[∈]_ [R][113][ for each action][ a][ ∈A][ and feature][ φ][ ∈] [Φ][:]

_ξ˜(s, a, φ; θ[ξ]) = s[⊤]θa,φ[ξ]_ (35)

To learn _ξ[˜] we update the parameters θ[ξ]_ using stochastic gradient descent following the gradients
_θξ_ _ξ(θ[ξ]) of the loss based on the ξ-learning update (13):_
_∇_ _L_

2[]
_∀φ ∈_ Φ : Lξ(θ[ξ]) = E _p(φt = φ|st, at) + γξ[˜](st+1, ¯at+1, φ; θ[¯][ξ]) −_ _ξ[˜](st, at, φ; θ[ξ])_
  (36)

with ¯at+1 = argmax _R(φ)ξ[˜](st+1, a, φ; θ[¯][ξ]),_
_a_

_φX∈Φ_

where _θ[¯][ξ]_ = θ[ξ] but _θ[¯][ξ]_ is treated as a constant for the purpose of calculating the gradients _θξ_ _ξ(θ[ξ])._
_∇_ _L_
We used PyTorch[2] for the computation of gradients and its stochastic gradient decent procedure
(SGD) for updating the parameters.

C.2.1 QL

The Q-learning (QL) agent (Algorithm 1) represents standard Q-learning (Watkins and Dayan, 1992).
The Q-function is approximated and updated using the following loss after each observed transition:

2[)]

_Q(θ[Q]) = E_ _r(st, at, st+1) + γ max_ _Q˜(st+1, at+1; θ[¯][Q])_ _Q(st, at; θ[Q])_ (37)
_L_ _at+1_ _−_ [˜]

( 

where _θ[¯][Q]_ = θ[Q] but _θ[¯][Q]_ is treated as a constant for the purpose of optimization, i.e no gradients flow
through it. Following (Barreto et al., 2017) the parameters θ[Q] are reinitialized for each new task.

C.2.2 SFQL

The classical successor feature algorithm (SFQL) is based on a linear decomposition of rewards in
features and reward weights (Barreto et al., 2017) (Algorithm 2). If the agent is learning the reward
weights ˜wi for a task Mi then they are randomly initialized at the beginning of a task. For the case
of general reward functions and where the reward weights are given to the agents, the weights are
learned to approximate a linear reward function before the task. See Section C.3 for a description of
the training procedure. After each transition the weights are updated by minimizing the error between
the predicted rewards φ(st, at, st+1)[⊤]w˜i and the observed reward rt:

2[o]
**wi** ( ˜wi) = E _r(s, a, s[′])_ _φ(st, at, st+1)[⊤]w˜_ _i_ _._ (38)
_L_ _−_

2 n  
[PyTorch v1.4: https://pytorch.org](https://pytorch.org)


-----

**Algorithm 1: Q-learning (QL)**
**Input : exploration rate: ϵ**
learning rate for the Q-function: α

**for i ←** 1 to num_tasks do

initialize _Q[˜]: θ[Q]_ _←_ small random initial values
new_episode ← true
**for t ←** 1 to num_steps do

**if new_episode then**

new_episode ← false
_st_ initial state
_←_

With probabilityTake action at and observe reward ϵ select a random action rt and next state at, otherwise st+1 at ← argmaxa _Q[˜](st, a)_
**if st+1 is a terminal state then**

new_episode ← true
_γt_ 0
_←_

**else**
_γt_ _γ_
_←_

_y ←_ _rt + γt maxat+1_ _Q[˜](st+1, at+1)_
Update θ[Q] using SGD(α) with LQ = (y − _Q[˜](st, at))[2]_
_st_ _st+1_
_←_


SFQL learns an approximated _ψ[˜]i-function for each task Mi. The parameters of the_ _ψ[˜]-function for_
the first task θ1[ψ] [are randomly initialized. For consecutive tasks, they are initialized by copying them]
from the previous task (θi[ψ] _i_ 1[). The][ ˜]ψi-function of the current task Mi is updated after each
observed transition with the loss based on (5):[←] _[θ][ψ]−_

2[]
_Lψ(θi[ψ][) = E]_ _φ(st, at, st+1) + γψ[˜]i(st+1, ¯at+1; θ[¯]i[ψ][)][ −]_ _ψ[˜]i(st, at; θi[ψ][)]_

(39)

 

with ¯at+1 = argmax max _ψ˜k(st+1, a; θ[¯]k[ψ][)][⊤]w[˜]_ _i,_
_a_ _k∈{1,2,...,i}_

where _θ[¯]i[ψ]_ [=][ θ]i[ψ] [but][ ¯]θi[ψ] [is treated as a constant for the purpose of optimization, i.e no gradients flow]
through it. Besides the current _ψ[˜]i-function, SFQL also updates the_ _ψ[˜]c-function which provided_
the GPI optimal action for the current transition: c = argmaxk 1,2,...,i maxb _ψ[˜]k(s, b)[⊤]w˜_ _i. The_
_∈{_ _}_
update uses the same loss as for the update of the active _ψ[˜]i-function (39), but instead of using the_
GPI optimal action as next action, it uses the optimal action according to its own policy: ¯at+1 =
argmaxa _ψ[˜]c(st+1, a)[⊤]w˜_ _c_

C.2.3 _ξ-LEARNING_

The ξ-learning agents (Algorithms 3, 4) allow to reevaluate policies in tasks with general reward
functions. If the reward function is not given, an approximation _R[˜]i of the reward function for each_
task Mi is learned. The parameters for the approximation are randomly initialized at the beginning of
each task. After each observed transition the approximation is updated according to the following
loss:

2[]
_LR(θi[R][) = E]_ _r(st, at, st+1) −_ _R[˜]i(φ(st, at, st+1); θi[R][)]_ (40)
 

In the case of tasks with linear reward functions the reward approximation becomes
_R˜i(φ(st, at, st+1); θi[R][) =][ φ][(][s][t][, a][t][, s][t][+1][)][⊤][θ]i[R][. Thus with][ θ]i[R]_ = ˜wi we recover the same procedure as for SFQL (38). For non-linear, general reward functions we represented _R[˜] with a neural_
network. The input of the network is the feature φ(st, at, st+1). The network has one hidden layer
with 10 neurons having ReLu activations. The output is a linear mapping to the scalar reward rt ∈ R.

All ξ-learning agents learn an approximation of the _ξ[˜]i-function for each task Mi. Analogous to_
SFQL, the parameters of the _ξ[˜]-function for the first task θ1[ξ]_ [are randomly initialized. For con-]


-----

**Algorithm 2: Classical SF Q-learning (SFQL) (Barreto et al., 2017)**
**Input : exploration rate: ϵ**
learning rate for ψ-functions: α
learning rate for reward weights w: αw
features φ
optional: reward weights for tasks: { ˜w1, ˜w2, . . ., ˜wnum_tasks}

**for i ←** 1 to num_tasks do

**if ˜wi not provided then ˜wi ←** small random initial values
**ifnew_episode i = 1 then initialize ←** true _ψ[˜]i: θi[ψ]_ _[←]_ [small random initial values][ else][ θ]i[ψ] _[←]_ _[θ]i[ψ]−1_
**for t ←** 1 to num_steps do

**if new_episode then**

new_episode ← false
_st_ initial state
_←_

_c_ argmaxk 1,2,...,i maxa _ψ[˜]k(st, a)[⊤]w˜_ _i_ // GPI optimal policy
_←_ _∈{_ _}_
With probabilityTake action at and observe reward ϵ select a random action rt and next state at, otherwise st+1 at ← argmaxa _ψ[˜]c(st, a)[⊤]w˜_ _i_
**ifUpdate st+1 is a terminal state ˜wi using SGD(αw then) with Lw = (rt −** _φ(st, at, st+1)[⊤]w˜_ _i)[2]_

new_episode ← true
_γt_ 0
_←_

**else**
_γt_ _γ_
_←_

// GPI optimal next action for task i
_a¯t+1_ argmaxa argk 1,2,...,i _ψ[˜]k(st+1, a)[⊤]w˜_ _i_
_←_ _∈{_ _}_
_y ←_ _φ(st, at, st+1) + γtψ[˜]i(st+1, ¯at+1)_
Update θi[ψ] [using SGD(][α][) with][ L][ψ][ = (][y][ −] _ψ[˜]i(st, at))[2]_
**if c ̸= i then**

_a¯ct+1 ←_ argmaxa _ψ[˜]c(st+1, a)[⊤]w˜_ _c_ // optimal next action for task
_y ←_ _φ(st, at, st+1) + γtψ[˜]c(st+1, ¯at+1)_
Update θc[ψ] [using SGD(][α][) with][ L][ψ] [= (][y][ −] _ψ[˜]c(st, at))[2]_

_st_ _st+1_
_←_

secutive tasks, they are initialized by copying them from the previous task (θi[ξ] _i_ 1[). The]
_ξ˜i-function of the current task Mi is updated after each observed transition with the loss given[←]_ _[θ][ξ]−_
in (46). The ξ-learning agents differ in their setting for p(φt = φ|st, at) in the updates which
is described in the upcoming sections. Besides the current _ξ[˜]i-function, the ξ-learning agents_
also update the _ξ[˜]c-function which provided the GPI optimal action for the current transition:_
_c = argmaxk_ 1,2,...,i maxat _φ_ Φ _ξ[˜]k(st, at, φ) R[˜]i(φ). The update uses the same loss as for_
_∈{_ _}_ _∈_

the update of the active _ξ[˜]i-function (46), but instead of using the GPI optimal action as next action, it_

P

uses the optimal action according to its own policy: ¯at+1 = maxa _φ_ Φ _ξ[˜]c(st+1, a, φ) R[˜]c(φ)._

_∈_

P


**MF ξ-learning:** The model-free ξ-learning agent (Algorithm 3) uses a stochastic update for the _ξ[˜]-_
functions. Given a transition, we set p(φt = φ|st, at) ≡ 1 for the observed feature φ = φ(st, at, st+1)
and p(φt = φ|st, at) ≡ 0 for all other features φ ̸= φ(st, at, st+1).

**MB ξ-learning:** The one-step SF model-based ξ-learning agent (Algorithm 4) uses an approximated
model ˜p to predict p(φt = φ|st, at) to reduce the variance of the ξ-function update. The model is by


-----

a linear mapping for each action. It uses a softmax activation to produce a valid distribution over Φ:

exp(s[⊤]θa,φ[p] [)]
_p˜(s, a, φ; θ[p]) =_ (41)

_φ[′]_ Φ [exp(][s][⊤][θ]a,φ[p] _[′]_ [)]
_∈_

where θa,φ[p] _p is valid for each task inP_, its weights θ[p] are only randomly initialized at
the beginning of the first task. For each observed transition, the model is updated using the following[∈] [R][113][. As][ ˜] _M_
loss:
_φ_ Φ : _p(θi[p][) = E]_ (p(φt = φ _st, at)_ _p˜(st, at, φ; θi[p][))][2][o]_ _,_ (42)
_∀_ _∈_ _L_ _|_ _−_
n

where we set p(φt = φ|st, at) ≡ 1 for the observed feature φ = φ(st, at, st+1) and p(φt =
_φ_ _st, at)_ 0 for all other features φ = φ(st, at, st+1).
_|_ _≡_ _̸_

C.3 EXPERIMENTAL PROCEDURE

All agents were evaluated in both task types (linear or general reward function) on 300 tasks. The
agents experienced the tasks sequentially, each for 20.000 steps. The agents had knowledge when
a task change happened. Each agent was evaluated for 10 repetitions to measure their average
performance. Each repetition used a different random seed that impacted the following elements: a)
the sampling of the tasks, b) the random initialization of function approximator parameters, c) the
stochastic behavior of the environments when taking steps, and d) the ϵ-greedy action selection of
the agents. The tasks, i.e. the reward functions, were different between the repetitions of a particular
agent, but identical to the same repetition of a different agent. Thus, all algorithms were evaluated
over the same tasks.

The SF agents (SFQL, ξ-learning) were evaluated under two conditions. First, that they have to learn
the reward weights or the reward function online during the training (indicated by (O) in figures).
Second, the reward weights or the reward function is given to them. As the SFQL does not support
general reward functions, it is not possible to provide the SFQL agent with the reward function in
the second condition. As a solution, before the agent was trained on a new task _i, a linear model_
_M_
of the reward Ri(φ) = φ[⊤]w˜i was fitted. The initial approximation ˜wi was randomly initialized and
then fitted for 10.000 iterations using a gradient descent procedure based on the absolute mean error
(L1 norm):

∆˜wi = η [1] _Ri(φ)_ _φ[⊤]w˜i,_ (43)

Φ _−_
_|_ _|_ _φX∈Φ_

with a learning rate of η = 1.0 that yielded the best results tested over several learning rates.

**Hyperparameters:** The hyperparameters of the algorithms were set to the same values as in
(Barreto et al., 2017). A grid search over the learning rates of all algorithms was performed. Each
learning rate was evaluated for three different settings which are listed in Table 1. If algorithms had
several learning rates, then all possible combinations were evaluated. This resulted in a different
number of evaluations per algorithm and condition: QL - 3, SFQL (O) - 9, MF Xi (O) - 9, MB Xi
(O) - 27, SFQL - 3, MF Xi - 3, MB Xi - 9. In total, 63 parameter combinations were evaluated. The
reported performances in the figures are for the parameter combination that resulted in the highest
cumulative total reward averaged over all 10 repetitions in the respective environment. Please note,
the learning rates α and αw are set to half of the rates defined in (Barreto et al., 2017). This is
necessary due to the differences in calculating the loss and the gradients in the current paper. We use
mean squared error loss formulations, whereas (Barreto et al., 2017) uses absolute error losses. The
probability for random actions of the ϵ-Greedy action selection was set to ϵ = 0.15 and the discount
rate to γ = 0.95. The initial weights θ for the function approximators were randomly sampled from a
standard distribution with θinit ∼N (µ = 0, σ = 0.01).

**Computational Resources:** Experiments were conducted on a cluster with a variety of node types
(Xeon SKL Gold 6130 with 2.10GHz, Xeon SKL Gold 5218 with 2.30GHz, Xeon SKL Gold 6126
with 2.60GHz, Xeon SKL Gold 6244 with 3.60GHz, each with 192 GB Ram, no GPU). The time
for evaluating one repetition of a certain parameter combination over the 300 tasks depended on the
algorithm and the task type. Linear reward function tasks: QL ≈ 1h, SFQL (O) ≈ 4h, MF ξ (O)
_≈_ 42h, MB ξ (O) ≈ 43h, SFQL ≈ 4h, MF ξ ≈ 15h, and MB ξ ≈ 16h. General reward function


-----

Table 1: Evaluated Learning Rates in the Object Collection Environment

Parameter Description Values

_α_ Learning rate of the Q, ψ, and ξ-function _{0.0025, 0.005, 0.025}_
_αw, αR_ Learning rate of the reward weights or the reward model 0.025, 0.05, 0.075
_{_ _}_
_β_ Learning rate of the One-Step SF Model _{0.2, 0.4, 0.6}_

tasks: QL ≈ 1h, SFQL (O) ≈ 4h, MF ξ (O) ≈ 68h, MB ξ (O) ≈ 67h, SFQL ≈ 5h, MF ξ ≈ 14h,
and MB ξ ≈ 18h. Please note, the reported times do not represent well the computational complexity
of the algorithms, as the algorithms were not optimized for speed, and some use different software
packages (numpy or pytorch) for their individual computations.

C.4 EFFECT OF INCREASING NON-LINEARITY IN GENERAL REWARD TASK

We further studied the effect of general reward functions on the performance of classical SF compared
to ξ-learning (Fig. 1 - c). We evaluated the agents in tasks with different levels of difficulty in relation
to how well their reward functions can be approximated by a linear model. Seven difficulty levels
have been evaluated. For each level, the agents were trained sequentially on 300 tasks as for the
experiments with general reward functions. The reward functions for each level were sampled with
the following procedure. Several general reward functions were randomly sampled as previously
described. For each reward function, a linear model of a reward weight vector ˜w was fitted using
the same gradient descent procedure as in Eq. 43. The final average absolute model error after
10.000 iterations was measured. Each of the seven difficulty levels defines a range of model errors its
tasks have with the following increasing ranges: {[0.0, 0.25], [0.25, 0.5], . . ., [1.5, 1.75]}. For each
difficulty level, 300 reward functions were selected that yield a linear model are in the respective
range of the level.

Q-Learning, SFQL, and MF Xi-learning were each trained on 300 tasks, i.e. reward functions, on
each difficulty level. As hyperparameters were the best performing parameters from the previous
general reward task experiments used. We measured the ratio between the total return over 300 tasks
of QL and MF Xi-learning (QL/MF Xi), and SFQL and MF Xi-learning (SFQL/MF Xi). Fig. 1 c shows the results, using as x-axis the mean average absolute model error defined by the bracket
of each difficulty level. The results show that the relative performance of SFQL compared to MF
Xi reduces with higher non-linearity of the reward functions. For reward functions that are nearly
linear (mean error of 0.125), both have a similar performance. Whereas, for reward functions that are
difficult to model with a linear relation (mean error of 1.625) SFQL reaches only less than 50% of
the performance of MF Xi-learning.


-----

**Algorithm 3: Model-free ξ-learning (MF ξ)**
**Input : exploration rate: ϵ**
learning rate for ξ-functions: α
learning rate for reward models R: αR
features φ
optional: reward functions for tasks: {R[˜]1, _R[˜]2, . . .,_ _R[˜]num_tasks}_

**for i ←** 1 to num_tasks do

**if** _R[˜]i not provided then initialize_ _R[˜]i: θi[R]_

_[←]_ [small random initial values]
new_episodeif i = 1 then initialize ← true _ξ[˜]i: θi[ξ]_ _[←]_ [small random initial values][ else][ θ]i[ξ] _[←]_ _[θ]i[ξ]−1_
**for t ←** 1 to num_steps do

**if new_episode then**

new_episode ← false
_st_ initial state
_←_

_c_ argmaxk 1,2,...,i maxa _φ_ _ξ[˜]k(st, a, φ) R[˜]i(φ)_ // GPI optimal policy
_←_ _∈{_ _}_
With probability ϵ select a random action at, otherwise
P
_at ←_ argmaxa _φ_ _ξ[˜]c(st, a, φ) R[˜]i(φ)_

Take action at and observe reward rt and next state st+1

P

**if** _R[˜]i not provided then_

Update θi[R] [using SGD(][α][R][) with][ L][R][ = (][r][t][ −] _R[˜]i(φ(st, at, st+1))[2]_

**if st+1 is a terminal state then**

new_episode ← true
_γt_ 0
_←_

**else**
_γt_ _γ_
_←_

// GPI optimal next action for task i
_a¯t+1_ argmaxa argk 1,2,...,i _φ_ _ξ[˜]k(st+1, a, φ) R[˜]i(φ)_
_←_ _∈{_ _}_
**foreach φ** Φ do
_∈_ P

**if φ = φ(st, at, st+1) then yφ ←** 1 + γtξ[˜]i(st+1, ¯at+1, φ)
**else yφ ←** _γtξ[˜]i(st+1, ¯at+1, φ)_

Update θi[ξ] [using SGD(][α][) with][ L][ξ][ =][ P]φ[(][y][φ][ −] _ξ[˜]i(st, at, φ))[2]_

**if c ̸= i then**

// optimal next action for task c
_a¯t+1 ←_ argmaxa _φ_ _ξ[˜]c(st+1, a, φ) R[˜]c(φ)_

**foreach φ** Φ do
_∈_ P

**if φ = φ(st, at, st+1) then yφ ←** 1 + γtξ[˜]c(st+1, ¯at+1, φ)
**else yφ ←** _γtξ[˜]c(st+1, ¯at+1, φ)_

Update θc[ξ] [using SGD(][α][) with][ L][ξ] [=][ P]φ[(][y][φ][ −] _ξ[˜]c(st, at, φ))[2]_

_st_ _st+1_
_←_


-----

**Algorithm 4: One Step SF-Model ξ-learning (MB ξ)**
**Input : exploration rate: ϵ**
learning rate for ξ-functions: α
learning rate for reward models R: αR
learning rate for the one-step SF model ˜p: β
features φ
optional: reward functions for tasks: {R[˜]1, _R[˜]2, . . .,_ _R[˜]num_tasks}_

initialize ˜p: θ[p] _←_ small random initial values
**for i ←** 1 to num_tasks do

**if** _R[˜]i not provided then initialize_ _R[˜]i: θi[R]_

_[←]_ [small random initial values]
new_episodeif i = 1 then initialize ← true _ξ[˜]i: θi[ξ]_ _[←]_ [small random initial values][ else][ θ]i[ξ] _[←]_ _[θ]i[ξ]−1_
**for t ←** 1 to num_steps do

**if new_episode then**

new_episode ← false
_st_ initial state
_←_

_c_ argmaxk 1,2,...,i maxa _φ_ _ξ[˜]k(st, a, φ) R[˜]i(φ)_ // GPI optimal policy
_←_ _∈{_ _}_

With probability ϵ select a random action at, otherwise

P

_at ←_ argmaxa _φ_ _ξ[˜]c(st, a, φ) R[˜]i(φ)_

Take action at and observe reward rt and next state st+1

P

**if** _R[˜]i not provided then_

Update θi[R] [using SGD(][α][R][) with][ L][R][ = (][r][t][ −] _R[˜]i(φ(st, at, st+1))[2]_

**foreach φ ∈** Φ do

**if φ = φ(st, at, st+1) then yφ ←** 1 else yφ ← 0

Update θ[p] using SGD(β) with Lp = _φ[(][y][φ][ −]_ _p[˜](st, at, φ))[2]_

**if st+1 is a terminal state then**

new_episode true

[P]

_←_
_γt_ 0
_←_

**else**
_γt_ _γ_
_←_

// GPI optimal next action for task i
_a¯t+1_ argmaxa argk 1,2,...,i _φ_ _ξ[˜]k(st+1, a, φ) R[˜]i(φ)_
_←_ _∈{_ _}_

**foreach φ** Φ do
_∈_ P

_yφ ←_ _p˜(st, at, φ) + γtξ[˜]i(st+1, ¯at+1, φ)_

Update θi[ξ] [using SGD(][α][) with][ L][ξ][ =][ P]φ[(][y][φ][ −] _ξ[˜]i(st, at, φ))[2]_

**if c ̸= i then**

// optimal next action for task c
_a¯t+1 ←_ argmaxa _φ_ _ξ[˜]c(st+1, a, φ) R[˜]c(φ)_

**foreach φ** Φ do
_∈_ P

_yφ ←_ _p˜(st, at, φ) + γtξ[˜]c(st+1, ¯at+1, φ)_

Update θc[ξ] [using SGD(][α][) with][ L][ξ] [=][ P]φ[(][y][φ][ −] _ξ[˜]c(st, at, φ))[2]_

_st_ _st+1_
_←_


-----

D EXPERIMENTAL DETAILS: RACER ENVIRONMENT

This section extends the brief introduction to the racer environment (Fig. 2 - a) given in Section 4.2.

D.1 ENVIRONMENT

The environment is a continuous two-dimensional area in which the agent drives similar to a car. The
position of the agent is a point in the 2D space: p = (x, y) ∈ [0, 1][2]. Moreover, the agent has an
orientation which it faces: θ ∈ [−π, π1]. The action space of the agent consists of three movement
directions: A = {right, straight, left }. Each action changes the position of the agent depending on
its current position and orientation. The action straight changes the agent’s position by 0.075 towards
its orientation θ. The action right changes the orientation of the agent to θ + 7[1] _[π][ and its position]_

0.06 towards this new direction, whereas left the direction to θ − [1]7 _[π][ changes. The environment is]_

stochastic by adding Gaussian noise with σ = 0.005 to the final position x, y, and orientation θ. If
the agent drives outside the area (x, y) ∈ [0, 1][2], then it reappears on the other opposite side. The
environment resembles therefore a torus (or donut). As a consequence, distances d(px, py) are also
measure in this space, so that the positions px = (0.1, 0.5) and py = (0.9, 0.5) have not a distance
of 0.8 but d(px, py) = 0.2. The environment has 3 markers at the positions m1 = (0.25, 0.75),
_m2 = (0.75, 0.25), and m3 = (0.75, 0.6). The features measure the distance of the agent to each_
marker:the beginning of an episode the agent is randomly placed and oriented in environment. An episode φ ∈ R[3] with φk = d(p, mk). Each feature dimensions is normalized to be φk ∈ [0, 1]. At
ends after 200 time steps.

The state space of the agents is similarly constructed as for the object collection environment. The
agent’s position is encoded with adifference, that the distances are measure according to the torus shape. A similar radial basis function 10 × 10 radial basis functions spos ∈ R[100] as defined in 34. In
approach is also used to encode the orientationgaussian centers in [−π, π] and σ = [1]5 _[π][. Please note,] sori ∈_ R[ π][20][ and]of the agent using[ −][π][ are also connected in this space,] 20 equally distributed

i.e. d(π, −π) = 0. The combination of the position and orientation of the agent is the final state:
_s = [s[⊤]pos[, s][⊤]ori[]][⊤]_ _[∈]_ [R][120][.]

The reward functions define preferred positions in the environment based on the features, i.e. the
distance of the agent to the markers. A preference function rk exists for each distance. The functions
are composed of a maximization over m Gaussian components that evaluate the agents distance:


_m_

_._ (44)
_j=1_




3

_rk(φk) with ri = [1]_ exp

3 [max] _−_ [(][φ][k][ −]σj[µ][j][)][2]

_k=1_  

X


_R(φ) =_


Reward functions are randomly generated by sampling the number of Gaussian components m
to be 1 or 2. The properties of each component are sampled according to µj (0.0, 0.7), and
_σj_ (0.001, 0.01). Fig. 2 - a illustrates one such randomly sampled reward function where dark ∼U
areas represent locations with high rewards. ∼U

D.2 ALGORITHMS

We evaluated Q-learning, SFQL (O), SFQL, and MF ξ-learning (see Section C.2 for their full
description) in the racer environment. In difference to their implementation for the object collection
environment, they used a different neural network architecture to approximate their respective value
functions.

D.2.1 QL

Q-learning uses a fully connected feedforward network with bias and a ReLU activation for hidden
layers. It has 2 hidden layers with 20 neurons each.

D.2.2 SFQL

Q-learning uses a feedforward network with bias and a ReLU activation for hidden layers. It has for
each of the three feature dimensions a separate fully connected subnetwork. Each subnetwork has 2
hidden layers with 20 neurons each.


-----

D.2.3 CONTINUOUS MODEL-FREE ξ-LEARNING

The racer environment has continuous features φ ∈ R[3]. Therefore, the MF ξ-learning procedure
(Alg. 3) can not be directly applied as it is designed for discrete feature spaces. We introduce here a
MF ξ-learning procedure for continuous feature spaces (CMF ξ-learning). It is a feature dimension
independent, and discretized version of ξ-learning. As the reward functions (44) are a sum over the
individual feature dimensions, the Q-value can be computed as:


_Q[π](s, a) =_


_R(φ)ξ[π](s, a, φ)dφ =_


_rk(φk)ξk[π][(][s, a, φ][k][)][d][φ][k]_ _[,]_ (45)
Φk


where Φk is the feature space for each feature dimension which is Φk = [0, 1] in the racer environment.
_ξk[π]_ [is a][ ξ][-function for the feature dimension][ k][. (45) shows that the][ ξ][-function can be independently]
represented over each individual feature dimension φk, instead of over the full features φ. This
reduces the complexity of the approximation.

Moreover, we introduce a discretization of the ξ-function that discretizes the space of each feature
dimension k in U = 11 bins with the centers:

_Xk =_ _φ[min]k_ + j∆φk : 0 < j < U _, with ∆φk :=_ _[φ]k[max]U −−_ _φ1[min]k_ _,_


where ∆φk is the distance between the centers, and φ[min]k = 0.0 is the lowest center, and φ[max]k = 1.0
the largest center. Given this discretization and the decomposition of the Q-function according to
(45), the Q-values can be computed by:


_Q[π](s, a) =_


_R(x)ξ[π](s, a, x) ._
_xX∈Xk_


Alg. 5 lists the complete CMF ξ-learning procedure with the update steps for the ξ-functions. Similar
to the SFQL agent, the CMF Xi uses a feedforward network with bias and a ReLU activation for
hidden layers. It has for each of the three feature dimensions a separate fully connected subnetwork.
Each subnetwork has 2 hidden layers with 20 neurons each. The discretized outputs per feature
dimension share the last hidden layer per subnetwork.

The ξ-function is updated according to the following procedure. Instead of providing a discrete
learning signal to the model, we encode the observed feature using continuous activation functions
around each bin center. Given the j’th bin center of dimension k, xk,j, its value is encoded to be 1.0
if the feature value of this dimension aligns with the center (φk = xk,j). Otherwise, the encoding for
the bin decreases linearily based on the distance between the bin center and the value ( _xk,j_ _φk_ ) and
reaches 0 if the value is equal to a neighboring bin center, i.e. has a distance ∆φk| . We represent − _|_
_≥_
this encoding for each feature dimension k by uk (0, 1)[U] with:
_∈_

_k_ 1,2,3 : 0<j<U : uk,j = max 0, [(1][ −|][x][k,j][ −] _[φ][k][|][)]_ _._
_∀_ _∈{_ _}_ _∀_ ∆φk
 

To learn _ξ[˜] we update the parameters θ[ξ]_ using stochastic gradient descent following the gradients
_θξ_ _ξ(θ[ξ]) of the loss based on the ξ-learning update (13):_
_∇_ _L_

3

1 2

_φ_ Φ : _ξ(θ[ξ]) = E_ **uk + γξ[˜]k(st+1, ¯at+1; θ[¯][ξ])** _ξk(st, at; θ[ξ])_
_∀_ _∈_ _L_ ( _n_ _kX=1_  _−_ [˜]  ) (46)

with ¯at+1 = argmax _R(x)ξ[˜](st+1, a, φ; θ[¯][ξ]),_
_a_

Xk _xX∈Xk_

where n = 3 is the number of feature dimensions and _ξ[˜]k is the vector of the U discretized ξ-values_
for dimension k.

D.3 EXPERIMENTAL PROCEDURE

All agents were evaluated on 37 tasks. The agents experienced the tasks sequentially, each for
1000 episodes (200, 000 steps per task). The agents had knowledge when a task change happened.


-----

Each agent was evaluated for 10 repetitions to measure their average performance. Each repetition
used a different random seed that impacted the following elements: a) the sampling of the tasks,
b) the random initialization of function approximator parameters, c) the stochastic behavior of the
environments when taking steps, and d) the ϵ-greedy action selection of the agents. The tasks, i.e.
the reward functions, were different between the repetitions of a particular agent, but identical to the
same repetition of a different agent. Thus, all algorithms were evaluated over the same tasks.

SFQL was evaluated under two conditions. First, by learning the reward weights online during the
training (indicated by (O) in figures). Second, the reward weights were trained with the iterative
gradient decent method in (43). The weights were trained for 10, 000 iterations with an learning rate
of 1.0. At each iteration, 50 random points in the task were sampled and their features and rewards
are used for the training step.

**Hyperparameters** A grid search over the learning rates of all algorithms was performed. Each
learning rate was evaluated for three different settings which are listed in Table 2. If algorithms had
several learning rates, then all possible combinations were evaluated. This resulted in a different
number of evaluations per algorithm and condition: QL - 4, SFQL (O) - 12, SFQL - 4, CMF Xi 4. In
total, 24 parameter combinations were evaluated. The reported performances in the figures are for
the parameter combination that resulted in the highest cumulative total reward averaged over all 10
repetitions in the respective environment. The probability for random actions of the ϵ-Greedy action
selection was set to ϵ = 0.15 and the discount rate to γ = 0.9. The initial weights and biases θ for the
function approximators were initialized according to an uniform distribution with θi ( _√k,_ _√k),_

where k = in_features1 [.] _∼U_ _−_

Table 2: Evaluated Learning Rates in the Racer Environment

Parameter Description Values

_α_ Learning rate of the Q, ψ, and ξ-function _{0.0025, 0.005, 0.025, 0.5}_
_αw_ Learning rate of the reward weights 0.025, 0.05, 0.075
_{_ _}_

**Computational Resources:** Experiments were conducted on the same cluster as for the object
collection environment experiments. The time for evaluating one repetition of a certain parameter
combination over the 37 tasks depended on the algorithm: QL ≈ 9h, SFQL (O) ≈ 70h, SFQL ξ
_≈_ 73h, and CMF ξ ≈ 88h. Please note, the reported times do not represent well the computational
complexity of the algorithms, as the algorithms were not optimized for speed, and some use different
software packages (numpy or pytorch) for their individual computations.


-----

**Algorithm 5: Model-free ξ-learning for Continuous Features (CMF ξ)**
**Input : exploration rate: ϵ**
learning rate for ξ-functions: α
learning rate for reward models R: αR
features φ ∈ R[n]
components of reward functions for tasks: {R1 = {r1[1][, r]2[1][, ..., r]n[1] _[}][, R][2][, . . ., R][num_tasks][}]_
discretization parameters: X, ∆φ

**for i ←** 1 to num_tasks do

**if i = 1 then**

_∀k∈{1,...,n}: initialize_ _ξ[˜]k[i]_ [:][ θ]i,k[ξ] _[←]_ [small random values]

**else**

_∀k∈{1,...,n}: θi,k[ξ]_ _[←]_ _[θ]i[ξ]−1,k_

new_episode ← true
**for t ←** 1 to num_steps do

**if new_episode then**

new_episode ← false
_st_ initial state

_c ←_ argmax ← _j∈{1,2,...,i} maxa_ _nk=1_ _x∈Xk_ _ξ[˜]k[j]_ [(][s][t][, a, x][)][r]k[i] [(][x][)] // GPI policy

With probability ϵ select a random action at, otherwise
_n_ P P
_at_ argmaxa _k=1_ _x_ _Xk_ _ξ[˜]k[j]_ [(][s][t][, a, x][)][r]k[i] [(][x][)]
Take action ← at and observe reward∈ _rt and next state st+1_
**if st+1 is a terminal stateP** **thenP**

new_episode ← true
_γt_ 0
_←_

**else**
_γt_ _γ_
_←_

// GPI optimal next action for task i
_a¯t+1 ←_ argmaxa argj∈{1,2,...,i} _nk=1_ _x∈Xk_ _ξ[˜]k[j]_ [(][s][t][, a, x][)][r]k[i] [(][x][)]

_φt_ _φ(st, at, st+1)_
**for ← k ←** 1 to n do P P

**foreach x ∈** _Xk do_

_yk,x_ max 0, 1 ∆φ + γtξ[˜]k[i] [(][s][t][+1][,][ ¯]at+1, x)
_←_ _−_ _[|][x][−][φ][t,k][|]_
 

Update θi[ξ] [using SGD(][α][) with][ L][ξ][ =][ P]k[n]=1 _x_ _Xk_ [(][y][k,x][ −] _ξ[˜]k[i]_ [(][s][t][, a][t][, x][))][2]

_∈_

**if c** = i then
_̸_ P

// optimal next action for task c
_a¯t+1_ argmaxa _nk=1_ _x_ _Xk_ _ξ[˜]k[c][(][s][t][, a, x][)][r]k[c]_ [(][x][)]
_←_ _∈_

**for k** 1 to n do
_←_ P P

**foreach x ∈** _Xk do_

_yk,x_ max 0, 1 ∆φ + γtξ[˜]k[c][(][s][t][+1][,][ ¯]at+1, x)
_←_ _−_ _[|][x][−][φ][t,k][|]_
 

Update θc[ξ] [using SGD(][α][) with][ L][ξ] [=][ P][n]k=1 _x_ _Xk_ [(][y][k,x][ −] _ξ[˜]k[c][(][s][t][, a][t][, x][))][2]_
_∈_

_st_ _st+1_
_←_ P


-----

E ADDITIONAL EXPERIMENTAL RESULTS

This section reports additional results and experiments:

1. Report of the total return and the statistical significance of differences between agents for
all experiments

2. Evaluation of the agents in the original object collection task by Barreto et al. (2017)

E.1 OBJECT COLLECTION TASK BY BARRETO ET AL. (2017)

We additionally evaluated all agents in the original object collection task by Barreto et al. (2017).


**Environment:** The environment differs to the modified object

G

collection task (Section. C) only in terms of the objects and features.
The environment has 3 object types: orange, blue, and pink (Fig. 3).
The feature encode if the agent has collected one of these object
types or if it reached the goal area. The first three dimensions of
the features φ(st, at, st+1) 0, 1 encode which object type is
_∈{_ _}[4]_
collected. The last dimension encodes if the goal area was reached.
In total |Φ| = 5 possible features exists: φ1 = [0, 0, 0, 0][⊤]- standard S
observation, φ2 = [1, 0, 0, 0][⊤]- collected an orange object, φ3 =

[0, 1, 0, 0][⊤]- collected a blue object, φ4 = [0, 0, 1, 0][⊤]- collected a Figure 3: Object collecpink object, and φ5 = [0, 0, 0, 1][⊤]- reached the goal area. tion environment from (Bar
reto et al., 2017) with 3 object

The rewards r = φ[⊤]wi are defined by a linear combination of

types: orange, blue, pink.

discrete features φ ∈ N[4] and a weight vector w ∈ R[4]. The first
three dimensions in w define the reward that the agent receives for
collecting one of the object types. The final weight defines the reward for reaching the goal state
which is w4 = 1 for each task. All agents were trained in on 300 randomly generated linear reward
functions with the same experimental procedure as described in Section. C. For each task the reward
weights for the 3 objects are randomly sampled from a uniform distribution: wk∈{1,2,3} ∼U(−1, 1).

**Results:** The results (Fig. 4) follow closely the results from the modified object collection task
(Fig. 1 - b, and 5 - a). MF ξ reaches the highest performance outperforming SFQL in terms of
learning speed and asymptotic performance. It is followed by MB ξ and SFQL which show no
statistical significant difference between each other in their final performance. Nonetheless, MB
Ξ has a higher learning speed during the initial 40 tasks. The results for the agents that learn the
reward weights online (SFQL (O), MF ξ (O), and MB ξ (O)) follow the same trend with MF ξ (O)
outperforming SFQL (O) slightly. Nonetheless, the ξ-agents have a much stronger learning speed
during the initial 70 tasks compared to SFQL (O), due to the errors in the approximation of the
weight vectors, especially at the beginning of a new task. All agents can clearly outperform standard
Q-learning.

E.2 TOTAL RETURN IN TRANSFER LEARNING EXPERIMENTS AND STATISTICAL SIGNIFICANT
DIFFERENCES

Fig. 5 shows for each of the transfer learning experiments in the object collection and the racer
environment the total return that each agent accumulated over all tasks. Each dot besides the boxplot
shows the total return for each of the 10 repetitions. The box ranges from the upper to the lower
quartile. The whiskers represent the upper and lower fence. The mean and standard deviation are
indicated by the dashed line and the median by the solid line. The tables in Fig. 5 report the p-value
of pairwise Mann–Whitney U test. A significant different total return can be expected if p < 0.05.

For the object collection environment (Fig.5 - a; b), ξ-learning outperforms SFQL in both conditions,
in tasks with linear and general reward functions. However, the effect is stronger in tasks with general
reward functions where SFQL has more problems to correctly approximate the reward function
with its linear approach. For the condition, where the agents learn a reward model online (O), the
difference between the algorithms in the general reward case is not as strong due the effect of their
poor approximated reward models for all agents.


-----

In the racer environment (Fig.5 - c) SFQL has a poor performance below standard Q-learning as
it can not appropriately approximate the reward functions with a linear model. In difference CMF
_ξ-learning outperforms QL._

(a) Environment by Barreto et al. (2017) with Linear Reward Functions


QL SFQL (O) MF Xi (O) MB Xi (O) SFQL MF Xi MB Xi


800

600

400

200

0

250k

200k

150k

100k

50k


10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300

Tasks Trained


(b) Total Return and Statistical Significance Tests

QL SFQL (O) MF Xi (O) MB Xi (O) SFQL MF Xi MB Xi


**p-value** SFQL (O) MF Xi (O) MB Xi (O) SFQL MF Xi MB Xi

QL _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_
SFQL (O) 0.038 0.121 _< 0.001_ _< 0.001_ _< 0.001_
MF Xi (O) 0.623 _< 0.001_ _< 0.001_ _< 0.001_
MB Xi (O) _< 0.001_ _< 0.001_ _< 0.001_
SFQL _< 0.001_ 0.121
MF Xi _< 0.001_

Figure 4: MF ξ-learning outperforms SFQL in the object collection environment by Barreto et al.
(2017), both in terms of asymptotic performance and learning speed. (a) The average over 10 runs
of the average reward per task per algorithm and the standard error of the mean are depicted. (b)
Total return over the 300 tasks in each evaluated condition. The table shows the p-values of pairwise
Mann–Whitney U tests between the agents.


-----

(a) Object Collection Environment with Linear Reward Functions

QL SFQL (O) MF Xi (O) MB Xi (O) SFQL MF Xi MB Xi


200k

150k

100k

50k


**p-value** SFQL (O) MF Xi (O) MB Xi (O) SFQL MF Xi MB Xi

QL _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_
SFQL (O) _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_
MF Xi (O) 0.054 _< 0.001_ _< 0.001_ _< 0.001_
MB Xi (O) _< 0.001_ _< 0.001_ _< 0.001_
SFQL _< 0.001_ _< 0.001_
MF Xi 0.162


(b) Object Collection Environment with General Reward Functions

QL SFQL (O) MF Xi (O) MB Xi (O) SFQL MF Xi MB Xi


250k

200k

150k

100k

50k


**p-value** SFQL (O) MF Xi (O) MB Xi (O) SFQL MF Xi MB Xi

QL _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_
SFQL (O) 0.045 0.009 _< 0.001_ _< 0.001_ _< 0.001_
MF Xi (O) 0.345 _< 0.001_ _< 0.001_ _< 0.001_
MB Xi (O) _< 0.001_ _< 0.001_ _< 0.001_
SFQL _< 0.001_ _< 0.001_
MF Xi _< 0.001_

(c) Racer Environment with General Reward Functions


QL SFQL (O) SFQL CMF Xi


3M


**p-value** SFQL (O) SFQL CMF Xi

QL _< 0.001_ _< 0.001 < 0.001_
SFQL (O) 0.021 _< 0.001_
SFQL _< 0.001_


2.5M

2M


1.5M

Figure 5: Total return over all tasks in each evaluated condition. The tables show the p-values of
pairwise Mann–Whitney U tests between the agents. See the text for more information.


-----

