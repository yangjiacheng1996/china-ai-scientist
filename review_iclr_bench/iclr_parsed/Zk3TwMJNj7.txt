# DIRECTIONAL BIAS HELPS STOCHASTIC GRADI## ENT DESCENT TO GENERALIZE IN NONPARAMETRIC
# MODEL

**Anonymous authors**
Paper under double-blind review

ABSTRACT

This paper studies the Stochastic Gradient Descent (SGD) algorithm in kernel regression. The main finding is that SGD with moderate and annealing step size
converges in the direction of the eigenvector that corresponds to the largest eigenvalue of the gram matrix. On the contrary, the Gradient Descent (GD) with a
moderate or small step size converges along the direction that corresponds to the
smallest eigenvalue. For a general squared risk minimization problem, we show
that directional bias towards a larger eigenvalue of the Hessian (which is the gram
matrix in our case) results in an estimator that is closer to the ground truth. Adopting this result to kernel regression, the directional bias helps the SGD estimator
generalize better. This result gives one way to explain how noise helps in generalization when learning with a nontrivial step size, which may be useful for
promoting further understanding of stochastic algorithms in deep learning. The
correctness of our theory is supported by simulations and experiments of Neural
Network on the FashionMNIST dataset.

1 INTRODUCTION

In this paper, we study the Stochastic Gradient Descent (SGD) algorithm in a nonparametric regression model. Among nonparametric models, one popular choice in both statistics and machine
learning communities is the kernel model that is generated by a Reproducing Kernel Hilbert Space
(RKHS). When fitting kernel models to the data, SGD is computationally efficient as compared to
Gradient Descent (GD) (Ma et al., 2018). This motivates us to analyze the properties of SGD in the
kernel model, especially for SGD with a nontrivial step size for a practical reason. In particular, we
aim to provide a fundamental explanation of why SGD estimators generalize well.

Our work is inspired by Wu et al. (2021), which shows that directional bias has a significant impact
on the generalization property in the linear regression model. We adopt a similar concept of the
directional bias, but generalize them to the nonparametric model. Our result is a non-trivial extension
of their approach, due to the difference in our problem setting and our SGD algorithm design. To
the best of our knowledge, we are the first to show the directional bias phenomenon of SGD and
analyze how it helps generalization in nonparametric regression.

**Our contributions are two folded. First, we study the directional bias of (S)GD in a nonparametric**
regression model. On the one hand, nonparametric regression is well studied in both statistics and
machine learning. On the other hand, the directional bias is a relatively new concept (Wu et al.,
2021) of an algorithm that affects the statistics properties, and there is no thorough understanding
of the directional bias of (S)GD algorithms for the nonparametric regression model. Note that our
result is closely related to those in Belkin et al. (2018); Liang & Rakhlin (2020): they prove that
SGD and GD algorithms both converge to the minimum norm interpolate, where the same properties
are discovered for SGD and GD; whereas we investigate the solution paths before their convergence,
and show that SGD and GD have different solution paths that lead to different properties. Our result
helps to explain why the SGD generalizes better than GD.

Our second contribution is to unify the conditions to show the directional bias of (S)GD sequences
in nonparametric models. The main condition is the diagonal dominant gram matrix, which covers
a large class of kernel functions and allows us to study their properties. Moreover, our SGD is


-----

different from those in Wu et al. (2021): they define SGD in epochs while we define SGD in steps.
The fundamental difference in the SGD algorithm requires us to develop different techniques for
analyzing the SGD sequence and showing its directional bias, which has not yet been covered in the
current literature.

**Main Theorems of this paper can be divided into two parts, briefly summarized as follows:**

**First is the directional bias of SGD. Theorem 5 shows that for a two-stage SGD with a moderate**
step size in the first stage and a small step size in the second stage, an early-stopped estimator has
a directional bias towards the eigenvector that corresponds to the largest eigenvalue of the gram
matrix. Later we refer to this direction as the direction towards the largest eigenvector to simplify
the statement.

As a comparison, Theorem 7 shows that GD with both moderate and small step sizes has a directional
bias towards the eigenvector that corresponds to the smallest eigenvalue of the gram matrix (denote
it as the direction towards the smallest eigenvector). From which, we conclude that SGD and GD
have different direction biases in kernel regression.

**Second is the implication of directional bias. The implication is very useful since it quantifies the**
effect of the directional bias on the generalization error. Theorem 9 considers a general problem
of quadratic loss. It shows that the estimator biased towards the largest eigenvector of the Hessian (which is the gram matrix in our nonparametric regression) can have the smallest parameter
estimation error, when compared with other estimators of the same loss.

With this high-level idea of directional bias helps generalization, Theorem 11 compares the generalization error of SGD and GD in our problem setting. In particular, it upper bounds the generalization error of SGD and lower bounds the generalization error of GD. By directly comparing the error
bounds, we guarantee that the generalization error of the SGD estimator is smaller than that of the
GD estimator with high probability.

We also point out that our result might shed new light on deep learning (Belkin et al., 2018). By
the state-of-the-art mathematical theory of Neural Networks (NN), kernel and/or nonparametric
methods can approximate the functional space of neural networks, see for example the NTK theory
(Jacot et al., 2018), and the Radon bounded variance space description for ReLU NN (Parhi &
Nowak, 2021). Our technique might allow one to characterize the SGD solution path and show the
generalization property in those problem settings.

**Paper organization. The rest of the paper is as follows: In Section 2, we review some rele-**
vant literature; In Section 3, we give the formulation of the nonparametric regression and define the
optimization problem. We also formalize the algorithm that is considered in this work and make
assumptions to analyze the algorithms; In Section 4, we state our main theory on the directional
bias of SGD/GD in nonparametric regression, where we include both the directional bias result and
the implication of the directional bias for generalization. Experiments are conducted to support our
theory; In Section 5, we discuss the finding in this paper, and propose some future research topic.
All the proof, experiment details, and more experiments are deferred to the appendix due to page
limits.

2 LITERATURE REVIEW

In this section, we review some relevant works. For better understanding, we split into two subsections: Subsection 2.1 reviews the background of RKHS; Subsection 2.2 presents the state-of-the-art
technique for analyzing the directional bias of the (S)GD algorithm.

2.1 RKHS

Kernel methods are among the core algorithms in machine learning and statistics (Bartlett et al.,
2021). As proposed by Wahba (1990), the kernel method and RKHS serve as a unified framework
of a group of nonparametric models, which extends the spline method. Later, kernel models become
an important component in nonparametric models. In machine learning, the kernel-based method is
always referred to as the “kernel trick”. By lifting the x variable to a high dimensional space via
the kernel method, we can explore possibly nonlinear relationships between variables. Moreover, to


-----

play the kernel trick, one can directly calculate the kernel function using original features. This is
computationally efficient since we avoid calculating high dimension or infinite dimension features.
For applications of kernel method in machine learning, one can see kernel regression for image
processing (Takeda et al., 2007), for text mining (Greene & Cunningham, 2006), and for tasks in
bioinformatics (Saigo et al., 2004).

Regarding deep learning, the kernel method is also important because it has implications for deep
learning models. On the one hand, the kernel methods have similar benign overfitting behavior
to neural network due to the implicit regularization and/or implicit bias phenomenon that we will
review in the next subsection (Belkin et al., 2018). On the other hand, the RKHS itself is closely
related to Neural Networks via the Neural Tangent Kernel theory (Jacot et al., 2018). This all
indicates that to understand deep learning, one should first study kernel methods.

2.2 DIRECTIONAL BIAS

This paper analyzes the directional bias of SGD for the nonparametric regression. Directional bias,
also referred to as implicit bias, of an algorithm refers to that its solution path is biased towards a
certain direction. It works as that the algorithm prefers some directions over the others even though
they may have the same objective function value. Since the algorithm selects a direction by itself,
instead of explicitly required by any constraint, people use the term “implicit”. It is worth noting
that the implicit regularization is related to implicit bias. The implicit regularization refers to that the
converged point of an algorithm is like a regularized estimator, even if the objective function is not
explicitly regularized. One can also interpret implicit regularization as the “final result” of implicit
bias. In the recent work by Derezinski et al. (2020), implicit regularization is used to develop an
exact bound for double descent in linear regression. In this way, implicit regularization/bias serves
as a way to explain some deep learning phenomenons that could not be addressed by the classical
empirical risk minimization (ERM) framework. Therefore, it is important to study directional bias.

State-of-the-art study on the directional bias of first-order algorithms can be divided into two categories by the technique they use:

The first category is the (stochastic) gradient flow method, by taking an infinitesimal step size in
(S)GD, the parameter dynamic follows a (stochastic) differential equation. Studying the solution
path and the stationary point of the underlying differential equation helps to reveal the property of
the parameter estimation. We list some works that use the first method to show the directional bias
result of the (stochastic) gradient descent. Liu et al. (2018) analyze the Momentum SGD (MSGD)
with infinitesimal step size, and show that the solution path escapes from the saddle for a nonconvex
objective function. It is worth noting that in their case, the associated stochastic differential equation
defines a complicated stochastic process, thus they replace it with an appropriate diffusion process,
and the analysis is done based on such diffusion approximation. If one analyzes a stochastic gradient
flow and finds it intractable, one may consider using the technique of diffusion approximation. Ali
et al. (2020) shows the stochastic gradient flow for the linear regression problem minw _Xw_ **y** 2
has a solution path close to the solution path of Ridge regression; Blanc et al. (2020 ∥) shows the − _∥[2]_
stochastic gradient flow for a general loss function has a solution path close to the solution path of
gradient flow on the objective function plus some extra penalty terms, and they explicitly identify
the penalty terms; Smith et al. (2021) go one more step from the infinitesimal step size to small step
size, and characterize the effect of small step size as an extra penalty term in the gradient flow.

Another category is analyzing the discrete (S)GD sequence. This technique in general just requires
a moderate step size such that the algorithm converges (or nearly converges), thus it is more meaningful from a practical perspective. We also find some directional bias work that is based on this
technique. Vaskevicius et al. (2019); Zhao et al. (2019); Fan et al. (2021) analyze Hadamard reparameterized GD in sparse regression. They divide the true parameter into strong, weak, and 0 parts,
and for each part, they carefully develop the stepwise error bound for each step of GD. They finally
show that an early-stopped estimator along the solution path achieves the minimax optimal error rate
for sparse regression, which indicates that the solution path is in the direction that biased towards
a sparse solution. Recently, Wu et al. (2021) show that for overparameterized linear regression,
SGD with moderate step size converges to the minimum norm interpolant in the direction that corresponds to the largest eigenvalue of the design matrix, while GD converges in the direction that
corresponds to the smallest eigenvalue. For Neural Networks in the ‘lazy training’ regime, ? shows


-----

that GD also converges in the direction of the smallest eigenvalue of the Neural Tangent Kernel.
Their result further reveal the mechanism of the directional bias as: GD fits the direction of a larger
eigenvalue faster at the beginning of the training, left the smaller eigenvalue direction unfitted; later
the direction of smaller eigenvalue is fitted, resulting in that the estimator goes in this direction.

3 PROBLEM FORMULATION

We give our problem formulation in this section. In Subsection 3.1 we define the kernel regression
model and objective function; in Subsection 3.2, we give the SGD and GD algorithms; in Subsection
3.3, we state our assumption for later analysis. Due to the page limit, details of the nonparametric
regression, RKHS and justifications for the assumption are deferred to Appendix A and B.

3.1 KERNEL REGRESSION

Suppose we are given n data pairs {xi, yi}i[n]=1 [generated from an unknown model][ y][ =][ f] [(][x][)][, where]
**_xi_** and yi . The goal is to estimate the unknown model f from the data. To achieve
the goal, one way is to find an ∈X ⊂R[p] _∈R_ _f that minimizes the empirical risk function_


_ℓ(yi, f_ (xi)) (1)
_i=1_

X


min


where ℓ is the loss function. For the regression task, we use the squared loss ℓ(y, x) = [1]2 [(][y][−][f] [(][x][))][2][.]

One can see that problem (1) is not well-defined, as there are infinitely many solutions to
_i : f_ (xi) = yi, and some of them do not generalize for a new test data. One way to fix it is to
_∀_
restrict f and add regularization term in _f_ to problem (1) for smoothness, where is a
_∈H_ _∥_ _∥H_ _H_
RKHS with reproducing kernel K( _,_ ) and is the Hilbert norm. Adding these restrictions and

_·_ _·_ _∥· ∥H_
applying Representer Theorem, problem (1) with the squared loss becomes


_i=1(yi −_ **_Ki[T]_** **_[α][)][2]_**

X


min
**_α∈R[n]_** 2n


(2)


= 2[1]n _[∥][y][ −]_ _[K][α][∥]2[2]_

where Ki[T] [is the][ i][th row of][ K][ :=][ K][(][X, X][) = (][K][(][x][i][,][ x][j][))][i,j][. For a parameter][ α][, the correspond-]
ing estimator in H is f (·) = _i=1_ _[α][i][K][(][x][i][,][ ·][) :=][ α][T][ K][(][·][, X][)][.]_

Now when K is invertible, it is trivial that any algorithm on objective function (2) (if it con
[P][n]

verges) converges at the unique minimal, that is, ˆα = K(X, X)[−][1]y, result in the RKHS functional
estimator
_fˆ(x) = K(x, X)[T]_ _K(X, X)[−][1]y_ (3)

where K(x, X)[T] = (K(x, x1), . . ., K(x, xn)). Estimator (3) is the minimum norm interpolant as
given by following problem:
arg min
_f_ _∈H[{∥][f]_ _[∥][H][ :][ f]_ [(][x][i][) =][ y][i][, i][ = 1][, . . ., n][}]

And its property has been studied in Liang & Rakhlin (2020).

In this paper, we compare the convergence direction of SGD and GD to ˆα. Specifically, we
consider a two-stage SGD with a phase transition from a larger step size to a decreased step size.
Note that this matches the training scheme people always use in practice for SGD algorithms:
decreasing the step size after training for a few epochs. For that purpose, in the following sections,
we define the one-step SGD/GD update and state our assumptions and notations for analysis.

3.2 ONE STEP SGD/GD UPDATE

In this paper, we consider the SGD algorithm as follows. For objective function (2), denote the
parameter estimation at tth step as αt, then SGD update αt+1 as

**_αt+1 = αt_** _ηt(Ki[T]t_ **_[α][t]_** _t_ [)][ ·][ K][i]t (4)
_−_ _[−]_ _[y][i]_


-----

where it is uniformly random sampled from {1, . . ., n}.

The GD update αt+1 as

**_αt+1 = αt −_** _[η]n [t]_ _[K]_ _[T][ (][K][α][t][ −]_ **_[y][) =][ α][t][ −]_** _[η]n [t]_ _[K][(][K][α][t][ −]_ **_[y][)]_** (5)


3.3 ASSUMPTIONS AND NOTATIONS

We state our assumption on the gram matrix in a unified format. Later we show in Appendix B that
some popular kernel families satisfy our assumption.
**Assumption 1 (Diagonal Dominant gram matrix). Denote by K = K(X, X) the gram matrix, we**
_assume that K is diagonal dominant. Specifically, suppose w.l.o.g. that K1,1_ _K2,2_ _. . ._
_Kn,n > 0, then we have for a small value τ that_ _≥_ _≥_ _≥_
_Ki,j_ _τ_ _Kn,n,_ _i_ = j
_|_ _| ≤_ _≪_ _∀_ _̸_

**Remark 2. Diagonal dominant gram matrix is common in kernel learning. Mathematically, one**
_can justify that a gram matrix is diagonal dominant by imposing proper assumptions on the kernel_
_function K(·, ·) and the data distribution. The following proposition shows diagonal dominance for_
_bilinear kernel. We can check for some other popular kernel to be diagonal dominant, which we_
_defer to Appendix B due to the page limit._
**Remark 3. Think of the kernel function as the inner product of high-dimensional features, the**
_resulting gram matrix is diagonal dominant when the high-dimension features are sparse. This_
_happens for a lot of practical problems (Sch¨olkopf et al., 2002; Weston et al., 2003), for example,_
_when linear or string kernels are applied to text data (Greene & Cunningham, 2006), when domain-_
_specific kernels are applied to image retrieval (Tao et al., 2004) and bioinformatics (Saigo et al.,_
_2004), and when the Global Alignment kernel is applied to most datasets (Cuturi et al., 2007; Cuturi,_
_2011)._
**Proposition 4 (Lemma 1 in Wu et al. (2021)). Consider the bilinear kernel K(x, x[′]) := ⟨x, x[′]⟩.**
_Assume the data xi, i = 1, . . ., n are i.i.d. uniformly distributed on the unit sphere S[d][−][1], where_
_d ≫_ _n. When d ≥_ 4 log(2n[2]/δ) for some δ ∈ (0, 1). Then with probability at least 1 − _δ, we have_

_|Ki,j| = |⟨xi, xj⟩| < ˜τ := O[˜]( √[1]d_ ) ≪ _Kn,n = 1, ∀i ̸= j._

It is meaningful to note that the diagonal dominance is undesired in classification and clustering
tasks. It indicates that the data pieces are dissimilar to each other as measured by the kernel
function, and thus generates very little information for classification/clustering. One may find a
lot of works on solving the issue of diagonal dominance in these cases, for example, Greene &
Cunningham (2006); Kandola et al. (2003). But for the regression task, the diagonal dominance,
in other words, the dissimilarity of data points, may have benefits. One can find similar conditions
such as Restricted Isometry Property and s−goodness that describes linearly dissimilar features
in a huge regression literature as Candes & Tao (2007); Cand`es (2008); Chen & Donoho (1994).
Such conditions are required for proving minimax optimality or exact recovery of a sparse signal
in sparse settings. In our case, we adopt the dissimilarity concept and apply it to data points in
high-dimensional nonlinear feature space. Later we will see that in the existence of diagonal
dominance, the directional bias drives SGD to select a good solution that generalizes well among
all solutions of the same level of empirical loss. In this way, our SGD estimator benefits from the
diagonal dominance.

**Notations. We use the following notations throughout the remaining of this work. For the gram**
matrix K, denote Ki,j be the element at ith row jth column of K. Denote λi = Ki,i = K(xi, xi),
and assume w.l.o.g. that λ1 _λ2_ _. . ._ _λn._ Denote the ith column of K as Ki, let
_K_ 1 = [K2, . . ., Kn]. Assume ≥ K is full rank, denote ≥ _≥_ _P_ 1 the projection onto column space of
_−_ _−_
_K_ 1, and P1 = I _P_ 1. And denote γ1 _. . ._ _γn > 0 eigenvalues of K in non-increasing order._
_−_ _−_ _−_ _≥_ _≥_

4 MAIN RESULT

The main results are presented in two subsections: Subsection 4.1 states the different directional
bias result of SGD and GD estimators; Subsection 4.2 shows that directional bias towards a certain


-----

directional leads to good generalization performance, and further applies this result to show that
SGD generalizes better than GD.

4.1 DIRECTIONAL BIAS

Since we assumed that K is full rank, then SGD and GD algorithm on objective function (2) converges to ˆα = K _[−][1]y (when they converge). We are interested in the direction at which αt converges_
to ˆα, i.e. the quantity
**_bt := αt −_** **_αˆ_**
With assumption 1 that the gram matrix is diagonal dominant, we prove that a two-stage SGD has
**_bt converge in the direction of the largest eigenvector of K._**
**Theorem 5 (Direction bias of SGD estimator). Assume Assumption 1 holds, run a two-stage SGD**
_with a fixed step size for each stage: stage 1 with step size η1 for steps 1, . . ., k1, stage 2 with step_
_size η2 for steps k1 + 1, . . ., k2, such that_
2 2

_λ[2]1_ _√nτ < η1 <_ _λ[2]2_ [+][ C][2]√nτ

_[−]_ _[C][1]_

1
_η2 <_ _λ[2]1_ [+][ C][3]√nτ

_where C1, C2, C3 are constants that are specified in the Appendix E. For a small ϵ > 0 such that_
_nτ < poly(ϵ) there exists k1 = O(log_ [1]ϵ [)][ and][ k][2][ such that]

_k2_ 2]
(1 2ϵ)γ1 _∥_ _γ1_
_−_ _≤_ _[E]E[[][∥][[K]b[b][SGD]k2[SGD]_ 2] _≤_

_∥_ _∥_

_That is, b[SGD]k2_ _is close to the direction of the largest eigenvector of K._

**Remark 6. One should assume τ in Assumption 1 to be small enough for ϵ to be very small if**
_one would like the resulting estimator b[SGD]k2_ _to have the direction that corresponds to the largest_
_eigenvalue of K. Later we will see that if one only wants different directional bias of SGD and GD_
_estimators, a moderate ϵ is allowed and then the assumption on τ is not that strong._

The proof of Theorem 5 is in Appendix E. Next, we see that GD has bt converge in the direction of
the smallest eigenvector of K, which contrasts with the directional bias of SGD.
**Theorem 7 (Direction bias of GD estimator). Assume Assumption 1 hold, run GD with a fixed step**
_size:_


_η <_


(λ1 + nτ )[2][,]


_for a small ϵ[′]_ _> 0, run k = O(log_ _ϵ[1][′][ )][ steps of GD, we have]_

_k_ 2
_γn_ _∥_ _√1 + ϵ[′]γn_
_≤_ _[∥][K]b[b][GD]k[GD]_ 2 _≤_

_∥_ _∥_

_That is, bt is close to the direction of the smallest eigenvector of K._
**Remark 8. The assumption (on τ** _) is mild for differentiating the directional bias of SGD and GD._
_Comparing Theorem 5 and 7, we see that as long as γn < (1 −_ 2ϵ)γ1, by taking k large enough we
_always have_
_Kb[GD]k_ 2 _k2_ 2
_∥_ _∥_ _< [E][∥][K][b][SGD]∥_

**_b[GD]k_** 2 _E_ **_b[SGD]k2_** 2
_∥_ _∥_ _∥_ _∥_

_the following subsection, we see that the directional bias towards a larger eigenvalue of the kernelThat is, one may expect b[SGD]k2_ _to be in the direction of larger eigenvalue compared with b[GD]k_ _. In_
_is good for generalization, which leads to our title that directional bias helps SGD to generalize in_
_kernel regression._

The proof of Theorem 7 is in Appendix F. Although there is assumption 1 in Theorem 7, it is just
used to bound the step size so that GD converges; the diagonal dominant structure of K is not
required for the directional bias for GD to hold. Moreover, the choice of ϵ[′] is independent of the
assumption on τ, then for an arbitrarily small ϵ[′] _> 0, we can always run GD long enough such that_
the theorem holds and the estimator b[GD]k is arbitrarily close to the smallest eigenvector.


-----

4.2 EFFECT OF DIRECTIONAL BIAS

In this subsection, the estimator that has a directional bias towards the largest eigenvalue of the
Hessian is shown to give the best parameter estimation error among all estimators that have the
same squared in-sample loss, see Theorem 9. Later we define a realizable problem setting of kernel
regression where the generalization error depends on a term similar to the parameter estimation
error, and in this way, the directional bias helps SGD to generalize.

**Theorem 9. Consider approximately minimizing the quadratic loss**

_L(w) =_ _Aw_ **_y_** 2
_∥_ _−_ _∥[2]_

_Assume there is a ground truth w[∗]_ _such that y = Aw[∗], for a fixed level of the quadratic loss, the_
_parameter estimation error ∥w −_ **_w[∗]∥2[2]_** _[has a lower bound]_

_a_
_∀w ∈{w : L(w) = a} : ∥w −_ **_w[∗]∥2[2]_** _[≥]_ _A[T]_ _A_ 2

_∥_ _∥_

_Moreover, the equality is obtained when w −_ **_w[∗]_** _is in the direction of the eigenvector that corre-_
_sponds to the largest eigenvalue of A[T]_ _A._

**Remark 10. Theorem 9 implies that the directional bias towards the largest eigenvalue is good**
_for parameter estimation. As discussed in Remark 8, the SGD estimator is biased towards a larger_
_eigenvalue compared to the GD estimator, then by Theorem 9 the SGD estimator better estimates the_
_true parameter and thus generalizes better. We formalize this statement in the following paragraphs._

The proof of Theorem 9 is in Appendix G.1.

Suppose _f_ such that y = f (x). Consider the generalization error LD(f ) := _f_ _f_
_∃_ _[∗]_ _∈H_ _[∗]_ _∥_ _−_ _[∗]∥H[2]_ [,]
for an algorithm output f _[alg], we decompose its generalization error as:_

_LD(f_ _[alg])_ inf + inf
_−_ _f_ _[L][D][(][f]_ [) =][ L][D][(][f][ alg][)][ −] _f[inf]_ _s_ _[L][D][(][f]_ [)] _f_ _s_ _[L][D][(][f]_ [)][ −] _f[inf]_ _[L][D][(][f]_ [)]
_∈H_ _∈H_ _∈H_ _∈H_
:=∆(f _[alg]), estimation error_ approximation error

where Hs is the hypothesis class that the output of the algorithm is restricted to. By formulation (| {z } | {z } 2),
we have our hypothesis class as

_Hs = {f ∈H : f = α[T]_ _K(·, X), α ∈R[n]}_

We define the a−level set of training loss:

_νa =_ _f_ _s : f = α[T]_ _K(_ _, X),_ [1] 2 [=][ a][}][,]
_{_ _∈H_ _·_ 2n _[∥][K][α][ −]_ **_[y][∥][2]_**


denote ∆[∗]a [:= inf] _[f]_ _[∈][ν]a_ [∆(][f] [)][.]

Note that the approximation error can not be improved by choice of algorithm unless we
change the hypothesis class, which is, changing the problem formulation in our case. So we just
minimize the estimation error for estimators that are in the a−level set. As shown in Appendix G.2,
one can check the estimation error is given by

_f ∈Hs : ∆(f_ ) = b[T] _Kb_

where b = α − **_αˆ. By similar reasoning as Theorem 9, the estimation error is minimized when b_**
is in the direction of the largest eigenvalue of K, so the directional bias towards a larger eigenvalue
helps to generalize in kernel regression. We formalize the statement for comparing the estimation
error of SGD and GD in the following theorem.

**Theorem 11 (Generalization performance). Follow Theorems 5 and 7, we have the following:**

-  The output of SGD has E[∆[1][/][2](f _[SGD])] ≤_ (1 + 4ϵ)(∆[∗]a[)][1][/][2][, where][ a][ is such that]
_E[_ _Kα[SGD]_ _y_ 2][2] = 2na and ϵ could be any positive small constant;
_∥_ _−_ _∥_

-  The output of GD has ∆(f _[GD]) ≥_ _M_ ∆[∗]a[, where][ a][ is the training loss of GD estimator, and]
_M =_ _γ[γ]n[1]_ [(1][ −] _[ϵ][′][)][ >][ 1][ is a large constant.]_


-----

Figure 1: Kernel regression on synthetic data. We simulate data from a nonlinear regression model
with Gaussian additive noise and fit kernel regression using a polynomial kernel. We run both SGD
and GD for two step size schemes, see details in Appendix H.1. In the first plot, we show the
directional bias by Rayleigh Quotient(RQ):= _[∥][K]b[b][∥]22[2]_ (same as Theorem 5 and 7). The SGD indeed

_∥_ _∥[2]_
converges in the direction of a larger RQ, which matches our Theorems 5 and 7. In the third plot we
show the prediction error of the solution paths, and the SGD does have lower prediction error than
GD, even GD has smaller training loss than SGD. This supports Theorem 11.

**Remark 12. One read from the theorem that the SGD estimator is (8ϵ + 16ϵ[2])−optimum, while**

_GD estimator is (M −_ 1)−sub-optimum. Combine with Theorem 7 that ϵ[′][ k]−[→∞]→ 0, we can take ϵ in
_Theorem 5 such that (1 + 4ϵ)[2]_ _< γ1/γn to have ∆(f_ _[SGD]) < ∆(f_ _[GD]) with high probability. This_
_finishes our claim that the SGD estimator generalize better than the GD estimator._

The detailed proof of Theorem 11 is left to Appendix G.2.

**Numeric Study. Figure 1 shows the simulation results of kernel regression, Figure 2 shows the**
results of a small ResNet-like Neural Network on FashionMNIST data (Xiao et al., 2017). Figure
1 and Figure 2a supports the directional bias results in Theorems 5 and 7, and Figure 2b validates
Theorem 11. For details of the experiments and more experiments, see Appendix H.
**Remark 13. The purpose of experiment using a Neural Network (Figure 2) is as following: first, the**
_Neural Network results support our finding on kernel regression, since Neural Network is related to_
_kernel regression through NTK theory (Jacot et al., 2018); second, our experiment indicates that our_
_result may be empirically true for the more general deep learning framework (Belkin et al., 2018),_
_since this experiment uses a complicated network that may not be simply explained by the kernel_
_method._

5 DISCUSSION AND FURTHER WORK

Our work takes one more step towards understanding the directional bias of SGD in kernel learning.
Here we discuss some implications of our results to deep learning.

**Implication for SGD scheme:** Our result shows the directional bias applies to SGD with
annealing step size. Specifically, the first stage of SGD with moderate step size should run long
enough, then in the second stage by decreasing step size we have the directional bias towards the
largest eigenvalue of the Hessian, which helps in benign overfitting. This explains a technique for
tuning the learning rate that people use in practice: start with a large step size, run long enough until
the error plateaus, then decrease the step size (He et al., 2016). Although this technique is always
used to speed convergence, we show that it also helps in benign overfitting, which becomes even
better.

**Implication for deep learning: Our assumption for analysis implies certain structures for deep**
learning models. Per our examples in Appendix B and our discussion in Remark 3, our assumption
holds when the feature space is high dimensional and/or when features are possibly sparse. This
matches the deep learning scenario where we have a highly overparameterized model and when the
trained parameter estimator becomes sparse. Besides, considering that some deep learning tasks can


-----

(a) Relative Rayleigh Quotient. (b) Test accuracy on FashionMNIST

Figure 2: The experiment of a small ResNet-like Neural Network on FashionMNIST. In (a), we
follow Wu et al. (2021) to use the Relative Rayleigh Quotient(RRQ) as the measurement of the
convergence direction, where higher RRQ means that the convergence direction is closer to the larger
eigenvalue direction of the Hessian. The SGD with moderate step size has higher RRQ than the GD
with either moderate step size or small step size, which supports the theory in Theorems 5 and 7. It is
also interesting to observe SGD with a small step size also has a different directional bias compared
with SGD with a moderate step size. In (b), we plot the testing accuracy from 20 repetitions of
experiments, the test accuracy (inside bracket) of SGD with moderate step size is higher than the
other cases, and we have Wilcoxon signed-rank test to confirm that the difference is significant at
0.01 level. The test accuracy validates Theorem 11. For more details of the experiments, the rank
test, and more experiments, see Appendix H.2.

be approximated by kernel learning (Jacot et al., 2018), our results help in explaining why the SGD
estimator can benign overfitting in an overparameterized deep learning.

Just as stated in Belkin et al. (2018), to understand deep learning one needs to understand kernel
learning. This work takes a step in understanding kernel learning, and we expect more steps that
go beyond this work towards understanding deep learning, possibly for some complicated structure
that could not be approximated by kernel learning.

**Reproducibility Statement:** For all theoretical results presented in this paper, we carefully
state and justify the assumption, we also include the proof in Appendix. For all experiments, we
state the implementation details in Appendix, and we include the necessary code and data for
reproducing our result in the supplementary material.

REFERENCES

Alnur Ali, Edgar Dobriban, and Ryan Tibshirani. The implicit regularization of stochastic gradient
flow for least squares. In International Conference on Machine Learning, pp. 233–244. PMLR,
2020.

Peter L. Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint.
_arXiv:2103.09177, 2021._

Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand kernel learning. In International Conference on Machine Learning, pp. 541–549. PMLR,
2018.

Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep
neural networks driven by an ornstein-uhlenbeck like process. In Jacob Abernethy and Shivani Agarwal (eds.), Proceedings of Thirty Third Conference on Learning Theory, volume 125
of Proceedings of Machine Learning Research, pp. 483–513. PMLR, 09–12 Jul 2020. URL

[http://proceedings.mlr.press/v125/blanc20a.html.](http://proceedings.mlr.press/v125/blanc20a.html)

Emmanuel Candes and Terence Tao. The dantzig selector: Statistical estimation when p is much
larger than n. The annals of Statistics, 35(6):2313–2351, 2007.


-----

Emmanuel J. Cand`es. The restricted isometry property and its implications for compressed
sensing. _Comptes Rendus Mathematique, 346(9):589–592, 2008._ ISSN 1631-073X. doi:
https://doi.org/10.1016/j.crma.2008.03.014. [URL https://www.sciencedirect.com/](https://www.sciencedirect.com/science/article/pii/S1631073X08000964)
[science/article/pii/S1631073X08000964.](https://www.sciencedirect.com/science/article/pii/S1631073X08000964)

Shaobing Chen and David Donoho. Basis pursuit. In Proceedings of 1994 28th Asilomar Conference
_on Signals, Systems and Computers, volume 1, pp. 41–44. IEEE, 1994._

Marco Cuturi. Fast global alignment kernels. In Proceedings of the 28th international conference
_on machine learning (ICML-11), pp. 929–936, 2011._

Marco Cuturi, Jean-Philippe Vert, Oystein Birkenes, and Tomoko Matsui. A kernel for time series
based on global alignments. In 2007 IEEE International Conference on Acoustics, Speech and
_Signal Processing-ICASSP’07, volume 2, pp. II–413. IEEE, 2007._

Michal Derezinski, Feynman T Liang, and Michael W Mahoney. Exact expressions for double descent and implicit regularization via surrogate random design. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
_in Neural Information Processing Systems,_ volume 33, pp. 5152–5164. Curran Asso[ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/](https://proceedings.neurips.cc/paper/2020/file/37740d59bb0eb7b4493725b2e0e5289b-Paper.pdf)
[37740d59bb0eb7b4493725b2e0e5289b-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/37740d59bb0eb7b4493725b2e0e5289b-Paper.pdf)

Jianqing Fan, Zhuoran Yang, and Mengxin Yu. Understanding implicit regularization in overparameterized nonlinear statistical model. arXiv:2007.08322, 2021.

Derek Greene and P´adraig Cunningham. Practical solutions to the problem of diagonal dominance
in kernel document clustering. In Proceedings of the 23rd international conference on Machine
_learning, pp. 377–384, 2006._

Hussein Hazimeh, Rahul Mazumder, and Peter Radchenko. Grouped variable selection with discrete
optimization: Computational and statistical perspectives. arXiv:2104.07084, 2021.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
_(CVPR), June 2016._

Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.

Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
[volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf)
[paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf.](https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf)

Jaz Kandola, Thore Graepel, and John Shawe-Taylor. Reducing kernel matrix diagonal dominance using semi-definite programming. In Learning Theory and Kernel Machines, pp. 288–302.
Springer, 2003.

Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “Ridgeless” regression can generalize. The Annals of Statistics, 48(3):1329–1347, June 2020. doi: 10.1214/19-AOS1849.

Tianyi Liu, Zhehui Chen, Enlu Zhou, and Tuo Zhao. A diffusion approximation theory of momentum sgd in nonconvex optimization. arXiv:1802.05155, 2018.

Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the
effectiveness of sgd in modern over-parametrized learning. arXiv:1712.06559, 2018.

Rahul Parhi and Robert D. Nowak. What kinds of functions do deep neural networks learn? insights
from variational spline theory. arXiv:2105.03361, 2021.

Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax-optimal rates for sparse additive
models over kernel classes via convex programming. Journal of Machine Learning Research, 13
(2), 2012.


-----

Hiroto Saigo, Jean-Philippe Vert, Nobuhisa Ueda, and Tatsuya Akutsu. Protein homology detection
using string alignment kernels. Bioinformatics, 20(11):1682–1689, 2004.

Bernhard Sch¨olkopf, Jason Weston, Eleazar Eskin, Christina Leslie, and William Stafford Noble.
A kernel approach for learning from almost orthogonal patterns. In European Conference on
_Machine Learning, pp. 511–528. Springer, 2002._

Samuel L Smith, Benoit Dherin, David Barrett, and Soham De. On the origin of implicit regularization in stochastic gradient descent. In International Conference on Learning Representations,
[2021. URL https://openreview.net/forum?id=rq_Qr0c1Hyo.](https://openreview.net/forum?id=rq_Qr0c1Hyo)

Hiroyuki Takeda, Sina Farsiu, and Peyman Milanfar. Kernel regression for image processing and
reconstruction. IEEE Transactions on image processing, 16(2):349–366, 2007.

Qingping Tao, Stephen Scott, NV Vinodchandran, Thomas Takeo Osugi, and Brandon Mueller. An
extended kernel for generalized multiple-instance learning. In 16th IEEE International Confer_ence on Tools with Artificial Intelligence, pp. 272–277. IEEE, 2004._

Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal
sparse recovery. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As[sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/5cf21ce30208cfffaa832c6e44bb567d-Paper.pdf)
[5cf21ce30208cfffaa832c6e44bb567d-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/5cf21ce30208cfffaa832c6e44bb567d-Paper.pdf)

Grace Wahba. Spline models for observational data. SIAM, 1990.

Jason Weston, Bernhard Sch¨olkopf, Eleazar Eskin, Christina Leslie, and William Stafford Noble.
Dealing with large diagonals in kernel matrices. Annals of the institute of statistical mathematics,
55(2):391–408, 2003.

Jingfeng Wu, Difan Zou, Vladimir Braverman, and Quanquan Gu. Direction matters: On the implicit bias of stochastic gradient descent with moderate learning rate. In International Confer_[ence on Learning Representations, 2021. URL https://openreview.net/forum?id=](https://openreview.net/forum?id=3X64RLgzY6O)_
[3X64RLgzY6O.](https://openreview.net/forum?id=3X64RLgzY6O)

Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv:1708.07747, 2017.

Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into deep learning.
_arXiv:2106.11342, 2021._

Peng Zhao, Yun Yang, and Qiao-Chu He. Implicit regularization via hadamard product overparametrization in high-dimensional linear regression. arXiv:1903.09367, 2019.


-----

A BACKGROUND ON RKHS

This section details background on RKHS in two subsections. The first subsection includes notations, theorems, and an example of RKHS, the second section reduces the kernel regression in
RKHS from infinite dimension to finite dimension, which gives our objective function (2) .

A.1 NONPARAMETRIC MODEL IN RKHS

In this subsection, we give the definition and notations for our model in RKHS, and its associated
norms, basis, etc. The definitions are similar to those in Raskutti et al. (2012).

Given n data pairs {xi, yi}i[n]=1[, where][ x][i][ ∈X ⊂R][p][ and][ y][i][ ∈R][, assume that][ y][i][s are as-]
sociated with xis through f (xi), where f ( ) is some unknown function in the reproducing kernel

_·_
Hilbert space (RKHS) of functions X →R, our goal is to estimate the function f (·) from the data.

Denote the RKHS where f lives as H, with reproducing kernel K : X × X →R+ (which
is known to us). And we associate the functions in H with probability measure Q, assume w.l.o.g.
that

_R[p][ f]_ [(][a][)][d][Q][(][a][) = 0][. By Mercer’s theorem,][ K][ has eigen-expansion:]

R


_K(a, b) =_


_γjφj(a)φj(b)_
_j=1_

X


Where {φj}j[∞]=1 [are orthonormal basis in][ L][2][(][Q][)][, w.r.t. the usual inner product in][ L][2][(][Q][)][ as]

_⟨g(·), h(·)⟩L2(Q) =_ _g(a)h(a)dQ(a)_
ZX

Now for any f ∈H, we can expand f (·) = _j=1_ _[c][j][φ][j][(][·][)][, where][ c][j][ =][ ⟨][f]_ [(][·][)][, φ][j][(][·][)][⟩][L][2][(][Q][)][. And for]
_f_ (·) = _j=1_ _[c][j][φ][j][(][·][)][,][ g][(][·][) =][ P]j[∞]=1_ _[c]j[′]_ _[φ][j][(][·][)][, by Parseval’s theorem]_

[P][∞]

[P][∞] _∞_

_⟨f_ (·), g(·)⟩L2(Q) = _cjc[′]j_

_j=1_

X


And we have another inner product that is defined for RKHS H as

_∞_ _cjc[′]j_

_f_ ( ), g( ) =
_⟨_ _·_ _·_ _⟩H_ _γj_

_j=1_

X

The reproducing property of RKHS says that ∀f ∈H, we have
_f_ ( ), K( _, x)_ = f (x)
_⟨_ _·_ _·_ _⟩H_

**Cubic Splines Formulate a RKHS. We go over an example of RKHS for better understanding.**
Consider the cubic spline of one dimension, we can show that the space of cubic splines is a RKHS.
One can also find the cubic spline example in Hazimeh et al. (2021). For more details on the
relationship between polynomial smoothing splines and RKHS, one can check Section 1.2 of Wahba
(1990) .

Assume w.l.o.g. that xi = [0, 1] . The cubic spline f on is continuous, has a continuous first-order derivative and square integrable second order derivative. By Taylor’s theorem with ∈X _⊂R_ _X_
remainder, we have

_t_
_f_ (t) = f (0) + tf _[′](0) +_ 0 (t − _u)f_ _[′′](u)du_
Z

1
= f (0) + tf (0) + (t _u)+f_ (u)du

_[′]_ 0 _−_ _[′′]_
Z

where (t − _u)+ = max{0, t −_ _u}. Let B be the set of cubic splines f on [0, 1] that satisfies the_
boundary condition f (0) = f _[′](0) = 0, then for f ∈B_

1
_f_ (t) = (t _u)+f_ (u)du

0 _−_ _[′′]_

Z


-----

Let G(t, u) = (t _u)+, then we claim that_ is RKHS with reproducing kernel
_−_ _B_

1
_K(s, t) =_ _G(s, u)G(t, u)du_

0

Z

and inner product


1
_⟨f, g⟩B =_ 0 _f_ _[′′](u)g[′′](u)du_
Z

as one can check the reproducing property


1
_f_ ( ), K( _, t)_ =
_⟨_ _·_ _·_ _⟩B_ 0
Z

1
=

0

Z

A.2 OPTIMIZATION PROBLEM CONSIDERED


_∂[2]K(u, t)_

_∂u[2]_ _f_ _[′′](u)du_

(t _u)+f_ (u)du = f (t)
_−_ _[′′]_


This subsection gives problem formulation of kernel regression. Given data pairs {xi, yi}i[n]=1 [and]
RKHS H, consider a loss function ℓ which is selected according to how y is connected with f (x),
we may estimate the model by


_ℓ(yi, f_ (xi)) (6)
_i=1_

X


min
_f_ _∈H_

= min
_cj_


_ℓ(yi,_
_i=1_

X


_cjφj(xi))_
_j=1_

X


Example for ℓ includes

-  Squared error loss ℓ(y, f ) = (y − _f_ )[2], which is usually used in regression;

-  0-1 loss ℓ(y, f ) = 1(y ∗ _f > 0), for binary classification;_

-  Logistic loss ℓ(y, f ) = log(1 + exp( _y_ _f_ )), also a loss function for classification, can
_−_ _∗_
be considered as a surrogate function of 0-1 loss, and is the same as negative log likelihood
function in logistic regression.


Let us come back to the nonparametric model part, to control the model smoothness, the usual
practice is to add a penalty to objective (6), result in


_ℓ(yi, f_ (xi)) + λpen(f )
_i=1_

X


min
_f_ _∈H_


A popular choice of pen(f ) is _f_
_∥_ _∥H[2]_ [, or any strictly increasing function of][ ∥][f] _[∥]H[2]_ [. Such method]
is explicitly controlling the model smoothness, and by Representer Theorem, it has solution of the
form


_αiK(_ _, xi)_ (7)

_·_
_i=1_

X


_f_ (·) =


Plug equation (7) into objective (6), we have the problem becomes


min
_αi′_


_ℓ(yi,_ _αi′_ _K(xi, xi′_ ))
_i=1_ _i[′]=1_

X X


Which gives the formulation (2) under loss function ℓ(y, f (x)) = (y − _f_ (x))[2]/2.


-----

B DIAGONAL DOMINANCE OF SOME POPULAR KERNELS

In this section, we justify Assumption 1 by figuring out a problem setting where some popular kernels give a diagonal dominant gram matrix. For simplicity, we assume the following data distribution
throughout this section:

**xi ∈** _R[d], i = 1, . . ., n, are normalized such that ∥xi∥2[2]_ [= 1;] (A.1)

The direction of xis are i.i.d. uniformly distributed on the unit sphere S[d][−][1]; (A.2)
_d ≫_ _n (overparameterized setting)._ (A.3)

Given assumption set (A), we can bound the inner product of data **_xi, xj_** with high probability as
_⟨_ _⟩_
follows:
**Lemma 14 (Lemma 1 in Wu et al. (2021)). Under assumption set (A), let d ≥** 4 log(2n[2]/δ) for
_some δ ∈_ (0, 1). Then with probability at least 1 − _δ, we have_

**_xi, xj_** _< ˜τ := [˜]( [1]_ ), _i_ = j
_|⟨_ _⟩|_ _O_ _√d_ _∀_ _̸_

_Proof. See proof of Lemma 1 in Wu et al. (2021)._

The bound on the inner product **_xi, xj_** induces bound on K(xi, xj) for some popular kernels. We
_⟨_ _⟩_
show the diagonal dominance for two groups of kernels in the following propositions, and list some
examples for kernels in each group.
**Proposition 15 (Inner product kernel). The inner product kernel is defined as a smooth transforma-**
_tion of inner product. We can write it as:_

_K(xi, xj) = g(_ **_xi, xj_** )
_⟨_ _⟩_

_Assume assumptions (A) hold, and assume the function g : [−1, 1] →_ _R satisfies:_

_g is convex;_ (B.1)
_g is L−smooth, that is, ∇g is L−Lipschitz continuous;_ (B.2)

_|g(0)| ≤_ _cτ˜ for a constant c, g[′](0) ≥_ 0. (B.3)

_we will have with probability 1 −_ _δ_

_Ki,j_ (c + g[′](0))˜τ + _[L]_ _τ_ [2] _for i_ = j (10)
_|_ _| ≤_ 2 [˜] _̸_

_where δ and ˜τ the same as in Lemma 14.When g[′](0)˜τ +_ _[L]2_ _τ[˜][2]_ _≪_ _g(1) for a small enough ˜τ_ _, the gram_

_matrix is diagonal dominant._

_Proof. We have the following with probability at least 1 −_ _δ by Lemma 14. For any off-diagonal_
elements of K:


_Ki,j = g(⟨xi, xj⟩)_

_g(0) + g[′](0)_ **_xi, xj_**
_≥_ _⟨_ _⟩_

_≥−(g[′](0) + c)˜τ_


and


_Ki,j = g(⟨xi, xj⟩)_

_g(0) + g[′](0)_ **_xi, xj_** + _[L]_
_≤_ _⟨_ _⟩_ 2 _[⟨][x][i][,][ x][j][⟩][2]_

_≤_ (c + g[′](0))˜τ + _[L]2 τ[˜][2]_

Thus |Ki,j| ≤ (c + g[′](0))˜τ + _[L]2_ _τ[˜][2]._

**Remark 16. We list some examples of inner product kernels that give diagonal dominant kernel**
_matrices:_


-----

-  Bilinear Kernel: K(x, x[′]) = ⟨x, x[′]⟩, then

_K(xi, xj)_ _τ˜_ _K(xn, xn) = 1._
_|_ _| ≤_ _≪_

-  Polynomial Kernel: K(x, x[′]) = (⟨x, x[′]⟩ + c)[m] _for m ∈_ N and c ∼O(˜τ ), then by
_Proposition 15_


_τ_ )
_K(xi, xj)_ (1 + m)˜τ + _[m][ ∗]_ [exp((][m][ −] [1)˜]
_|_ _| ≤_ 2


_τ˜[2]_


_when (1+_ _m)˜τ +_ _[m][∗][exp((]2[m][−][1)˜]τ_ ) _τ˜[2]_ (1+ _c)[m], we have diagonal dominant gram matrix._

_≪_

-  Hyperbolic Tangent Kernel (Sigmoid Kernel): K(x, x[′]) = tanh(α⟨x, x[′]⟩ + c), where
_α > 0, c ≥_ 0. Note that Hyperbolic Tangent Kernel does not satisfy all the assumptions in
_Proposition 15, one can still calculate_


_K(xi, xj)_ tanh(ατ˜ + c)
_|_ _| ≤_

_and_
_K(xk, xk) = tanh(α + c)_
_When tanh(ατ˜ + c) ≪_ tanh(α + c) (which is the case when α is large, and c, ˜τ are small
_enough), we have_ _K(xi, xj)_ _K(xn, xn) and the gram matrix is diagonal dominant._
_|_ _| ≪_

**Proposition 17 (Radial Basis Function (RBF) kernel). Radial Basis Function kernel depends on**
_two data points through their distance, which is of following form_

_K(xi, xj) = exp(−γ∥xi −_ **_xj∥2[2][)][, γ >][ 0]_**

_Assume assumptions (A) hold, when γ = −c0 log(˜τ_ ) for a constant c0, we have with probability
1 − _δ_

_|Ki,j| ≤_ _τ˜[2][c][0][(1][−]τ[˜]) ≪_ _Kn,n = 1, for i ̸= j_

_where δ and ˜τ the same as in Lemma 14. That is, the Gram matrix is diagonal dominant._

_Proof. Bound off-diagonal terms of K by Lemma 14:_


_Ki,j = exp(−γ∥xi −_ **_xj∥2[2][)]_**
_≤_ exp(2γτ˜ − 2γ)

= ˜τ [2][c][0][(1][−]τ[˜])

**Remark 18. We note some popular kernels that are related to Radial Basis Function kernel, and**
_show that they lead to diagonal dominance:_

-  Gaussian Kernel: K(x, x[′]) = exp( 2σ[2] 2 ). One can see that Gaussian Kernel is
_−_ _[∥][x][−][x][′][∥][2]_

_reparameterizing RBF Kernel by γ = 1/(2σ[2]). Thus Gaussian gram matrix is diagonal_
_dominant when σ[2]_ _∼O(−_ log(˜1 _τ_ ) [)][.]

-  Laplace Kernel: K(x, x[′]) = exp( _σ_ ) for σ > 0. The Laplace Kernel is very

_similar to Gaussian Kernel, and one can check by similar steps that when−_ _[∥][x][−][x][′][∥][2]_ _σ ∼O(−_ log(˜1 _τ_ ) [)][,]

_Laplace gram matrix is diagonal dominant._


C LEMMAS

This section includes two useful lemmas for characterizing the eigenvalues of a symmetric matrix.
**Lemma 19 (Gershgorin circle theorem, restated for symmetric matrix). Let A ∈R[n][×][n]** _be a sym-_
_metric matrix. Let Aij be the entry in the i−th row and the j−th column. Let_


_Aij_ _, i = 1, . . ., n_
_|_ _|_

Xj≠ _i_


_Ri(A) :=_


-----

_Consider n Gershgorin discs_

_Di(A) :=_ _z_ _,_ _z_ _Aii_ _Ri(A)_ _, i = 1, . . ., n_
_{_ _∈R_ _|_ _−_ _| ≤_ _}_

_The eigenvalues of A are in the union of Gershgorin discs_

_G(A) := ∪i[n]=1[D][i][(][A][)]_

_Furthermore, if the union of k of the n discs that comprise G(A) forms a set Gk(A) that is disjoint_
_from the remaining n_ _k discs, then Gk(A) contains exactly k eigenvalues of A, counted according_
_−_
_to their algebraic multiplicities._

_Proof. See Horn & Johnson (2012), Chap 6.1, Theorem 6.1.1._

**Lemma 20 (Cauchy interlacing theorem, restated for symmetric matrix). Let B ∈R[m][×][m]** _be a_

_B_ **_y_**
_symmetric matrix, let y_ _and a_ _, and let A =_ _. Then_
_∈R[n]_ _∈R_ **_y[T]_** _a_
 

_λ1(A)_ _λ1(B)_ _λ2(A)_ _. . ._ _λm(A)_ _λm(B)_ _λm+1(A)._
_≥_ _≥_ _≥_ _≥_ _≥_ _≥_

_Proof. See Horn & Johnson (2012), Chap 4.3, Theorem 4.3.17._

D SPECTRUM OF GRAM MATRIX

This section analyzes the eigen structure of the gram matrix.

**Lemma 21 (Characterizing K** [2]). Under Assumption 1, we have

_⟨Ki, Ki⟩∈_ [λ[2]i _[, λ]i[2]_ [+ (][n][ −] [1)][τ][ 2][]] (11)
_|⟨Ki, Kj⟩| ≤_ [2λ1 + (n − 2)τ ]τ, i ̸= j (12)

_Proof. For_ **_Ki, Ki_**
_⟨_ _⟩_


_⟨Ki, Ki⟩_ = Ki,i[2] [+] _Kl,i[2]_
Xl≠ _i_

_∈_ [λ[2]i _[, λ]i[2]_ [+ (][n][ −] [1)][τ][ 2][]]


And for **_Ki, Kj_** _, i_ = j
_⟨_ _⟩_ _̸_

**_Ki, Kj_** =
_|⟨_ _⟩|_ _|_


_Kl,iKl,j_
_|_
_l=1_

X


= |Ki,iKi,j + Kj,iKj,j +


_Kl,iKl,j_
_|_
_lX≠_ _i,j_


_Ki,iKi,j_ + _Kj,iKj,j_ +
_≤|_ _|_ _|_ _|_

_≤_ [λi + λj]τ + (n − 2)τ [2]

_≤_ [2λ1 + (n − 2)τ ]τ

**Lemma 22 (Eigenvalue of K). Under Assumption 1, we have**


_Kl,iKl,j_
_|_ _|_
_lX≠_ _i,j_


_γ1 ≤_ _λ1 + nτ_ (13)
_γn_ _λn_ _nτ_ (14)
_≥_ _−_

_If we further assume λj + nτ < λj_ 1 _nτ_ _, we will have_
_−_ _−_

_γj_ 1 _λj_ 1 _nτ > λj + nτ_ _γj._
_−_ _≥_ _−_ _−_ _≥_


-----

_Proof. Use Gershgorin circle theorem, calculate_


_Ri(K) =_


_Ki,j_ _nτ_
_|_ _| ≤_

Xj≠ _i_


then

_Di(K) ⊂_ [λi − _nτ, λi + nτ_ ].

By Gershgorin circle theorem, the lemma claim holds.


**Lemma 23 (Characterize P1K and P** 1K). Recall our definition: P 1 is the projection on column
_−_ _−_
_space of K_ 1 = [K2, K3, . . ., Kn], and P1 = I _P_ 1. We claim the following hold
_−_ _−_ _−_

_P1K = [P1K1, 0, . . ., 0]_ (15)
_P−1K = [P−1K1, K2, . . ., Kn]_ (16)

_Assume τ is small enough that nτ ≤O(1), λn −_ _nτ ≥_ _c1 > 0 and λ1 + nτ ≤_ _c2, let c3 :=_ _[c]c[2][2]1_ _[, then]_

_we have the following:_

_P_ 1K1 2 0, c3(2λ1 + (n 2)τ )[√]nτ (17)
_∥_ _−_ _∥_ _∈_ _−_

_∥P1K1∥2 ∈_  _λ[2]1_ _[−]_ _[c]3[2][[2][λ][1][ + (][n][ −]_ [2)][τ] []][2][nτ][ 2][, λ][1][ +][ √][nτ] _._ (18)
q 

_Proof. For i ̸= 1, we calculate_

_P_ 1Ki = Ki
_−_
_P1Ki = Ki_ _P_ 1Ki = 0
_−_ _−_

thus we have equations (15) and (16).


For ∥P−1K1∥2:

where


_∥P−1K1∥2_
= _K_ 1(K _[T]_ 1[K][−][1][)][−][1][K] _[T]_ 1[K][1][∥][2]
_∥_ _−_ _−_ _−_
_K_ 1(K _[T]_ 1[K][−][1][)][−][1][∥][2][∥][K] _[T]_ 1[K][1][∥][2]
_≤∥_ _−_ _−_ _−_

_K_ _[T]_ 1[K][1][∥]2[2]
_∥_ _−_

_n_

= (Ki[T] **_[K][1][)][2]_**

_i=2_

X


(12)
_≤_ (n − 1)[2λ1 + (n − 2)τ ][2]τ [2]

_≤[2λ1 + (n −_ 2)τ ][2]nτ [2]

and K _[T]_ 1[K][−][1] [has all eigenvalues in][ [][γ]n[2][, γ]1[2][]][ by Cauchy interlacing theorem (Lemma][ 20][), that is, all]
_−_
singular values of K 1 are in [c1, c2] by our assumption. Then
_−_

_∥K−1(K−[T]_ 1[K][−][1][)][−][1][∥][2] _[≤]_ _[c]c[2][2]1_ := c3


So we have

For _P1K1_ 2:
_∥_ _∥_


_∥P−1K1∥2 ≤∥K−1(K−[T]_ 1[K][−][1][)][−][1][∥][2][∥][K]−[T] 1[K][1][∥][2]
_≤_ _c3[2λ1 + (n −_ 2)τ ][√]nτ.

_P1K1_ 2 **_K1_** 2
_∥_ _∥_ _≤∥_ _∥_
_≤(λ[2]1_ [+ (][n][ −] [1)][τ][ 2][)][.][5]

_≤λ1 +_ _[√]nτ_


-----

and

_P1K1_ 2
_∥_ _∥[2]_
= **_K1_** 2 2
_∥_ _∥[2]_ _[−∥][P][−][1][K][1][∥][2]_
_λ[2]1_ 3[[2][λ][1] [+ (][n][ −] [2)][τ] []][2][nτ][ 2][.]
_≥_ _[−]_ _[c][2]_

**Lemma 24 (Spectrum of H** 1 := P 1KK _[T]_ _P_ 1). Assume
_−_ _−_ _−_

_c[2]3[[2][λ][1]_ [+ (][n][ −] [2)][τ] []][2][nτ][ 2][ + 2[2][λ][1] [+ (][n][ −] [2)][τ] []][nτ][ ≤] _[λ][2]n_

_We have the following:_

-  0 is an eigenvalue of H−1, corresponding eigenspace is the column space of P1;

-  Restricted to the column space of P 1, the eigenvalues of H 1 are all in the interval:
_−_ _−_
_λ[2]n_ [+ (][n][ −] [2)][τ] []][nτ, λ][2]2 [+ [2][λ][1] [+ (][n][ −] [1)][τ] []][nτ] _._

_[−]_ [[2][λ][1]
  

_Proof. The first claim is by construction of P1 and P_ 1.
_−_

For the second claim, note that H 1 has the same eigenvalues as
_−_

_H_ _[′]_ 1 [= (][P][−][1][K][)][T][ P][−][1][K]
_−_

Now the diagonal entries of H _[′]_ 1 [are:]
_−_


(H−[′] 1[)][ii] [=][ ∥][P][−][1][K][i][∥][2]2 [=]  _∥∥PK−i1∥K2[2]_ _[∈]1∥[[]2[2][λ]i[2][≤][, λ][c]i[2]3[2][[2][+ (][λ][1][n][+ (][ −]_ [1)][n][ −][τ][ 2][2)][]] _[τ]_ []][2][nτ][ 2] _, i, i ̸ = 1= 1_

And the off-diagonal entries of H _[′]_ 1 [are:]
_−_

_|(H−[′]_ 1[)][ij][|][ =][ |⟨][P][−][1][K][i][, P][−][1][K][j][⟩|][ =][ |⟨][K][i][,][ K][j][⟩| ≤] [[2][λ][1] [+ (][n][ −] [2)][τ] []][τ]

To use Gershgorin circle theorem, calculate


_Ri(H_ _[′]_ 1[) =]
_−_

Thus the Gershgorin discs:


_|(H−[′]_ 1[)][ij][|][ <][ [2][λ][1] [+ (][n][ −] [2)][τ] []][nτ]

Xj≠ _i_


_D1(H−[′]_ 1[)][ ∈] [(][∥][P][−][1][K][1][∥][2]2 _[−]_ [[2][λ][1] [+ (][n][ −] [2)][τ] []][nτ,][ ∥][P][−][1][K][1][∥][2]2 [+ [2][λ][1] [+ (][n][ −] [2)][τ] []][nτ] [)]

_Di(H−[′]_ 1[)][ ∈] [(][∥][K][i][∥][2]2 _[−]_ [[2][λ][1] [+ (][n][ −] [2)][τ] []][nτ,][ ∥][K][i][∥][2]2 [+ [2][λ][1] [+ (][n][ −] [2)][τ] []][nτ] [)]

when c[2]3[[2][λ][1] [+(][n] _[−]_ [2)][τ] []][2][nτ][ 2][ +[2][λ][1] [+(][n] _[−]_ [2)][τ] []][nτ][ ≤] _[λ][2]n_ [+(][n] _[−]_ [2)][τ] []][nτ] [, the first Gershgorin]
discs does not intersect with the others, so we have n − 1 nonzero eigenvalues in[−] [[2][λ][1]

_∪i[n]=2[D][i][(][H]−[′]_ 1[)][ ⊂] _λ[2]n_ _[−]_ [[2][λ][1] [+ (][n][ −] [2)][τ] []][nτ, λ][2]2 [+ [2][λ][1] [+ (][n][ −] [1)][τ] []][nτ] _._
  

E DIRECTIONAL BIAS OF SGD WITH MODERATE STEP SIZE

This section gives formal proof of Theorem 5 and specifies the constants. The proof is done in four
steps: Lemma 25 analyzes one update of SGD; Lemma 26 uses Lemma 25 to bound the first stage
updates of SGD with moderate step size; Lemma 27 again uses Lemma 25, and bounds the second
stage updates of SGD with small step size; finally, Theorem 28 combines Lemma 26 and Lemma 27
to formalize the directional bias of SGD, it is the same as Theorem 5, but restated using the constants
defined therein.


-----

**Lemma 25 (One step update of SGD). Under Assumption 1, denote At := E[∥P1bt∥2], Bt :=**
_E[_ _P_ 1bt 2], fix a constant c4 (λ1 + _nτ_ )(2λ1 + (n 2)τ )c3, then we have:
_∥_ _−_ _∥_ _≥_ _[√]_ _−_
_At+1 ≤_ _q1(η)At + ξ(η)Bt_ (19)
_At+1_ _q1(η)At_ _ξ(η)Bt_ (20)
_≥_ _−_
_Bt+1_ _q_ 1(η)Bt + ξ(η)At (21)
_≤_ _−_
_where_

_q1(η) =_ _[n][ −]_ [1] + [1] 2[|]

_n_ _n_ _[|][1][ −]_ _[η][∥][P][1][K][1][∥][2]_


_q−1(η) =_ s1 + _[∥]n[KP]P_ _[−]1[1]b[b]t[t][∥]22[2]_ [η[2](λ[2]2 [+ (][n][ −] [1)][τ][ 2][)][ −] [2][η][]]

_∥_ _−_ _∥[2]_

_ξ(η) = c4ηn[−][1][/][2]τ._


_Proof. One step of SGD update is:_
**_bt+1 = bt_** _ηKiKi[T]_ **_[b][t]_** [= [][I][ −] _[η][K][i][K]i[T]_ []][b][t]
_−_
where i is uniformly random sample from [1, . . ., n].

For inequalities (19) and (20), check
_P1bt+1 = P1[I_ _ηKiKi[T]_ []][b][t]
_−_
= P1bt _ηP1KiKi[T]_ [(][P][1] [+][ P][−][1][)][b][t]
_−_
= [I _ηP1KiKi[T]_ _[P][1][]][P][1][b][t]_ _i_ _[P][−][1][]][P][−][1][b][t]_
_−_ _[−]_ _[η][[][P][1][K][i][K][T]_
where P1Ki and P1bt are in the same 1 dimensional linear space, thus
_P1KiKi[T]_ _[P][1][P][1][b][t]_
= _P1Ki_ 2 _P1bt_ 2sign( _P1Ki, P1bt_ )P1Ki
_∥_ _∥_ _∥_ _∥_ _⟨_ _⟩_
= _P1Ki_ 2[P][1][b][t]
_∥_ _∥[2]_

[I _ηP1KiKi[T]_ _[P][1][]][P][1][b][t]_ [= [1][ −] _[η][∥][P][1][K][i][∥][2]2[]][P][1][b][t]_
_⇒_ _−_
and

[P1KiKi[T] _[P][−][1][]][P][−][1][b][t][∥][2]_
_∥_
_≤∥P1Ki∥2∥P−1Ki∥2∥P−1bt∥2._
Then
_E[_ _P1bt+1_ 2 **_bt]_** _E[_ 1 _η_ _P1Ki_ 2[|][]][∥][P][1][b][t][∥][2] [+][ ηE][[][∥][P][1][K][i][∥][2][∥][P][−][1][K][i][∥][2][]][∥][P][−][1][b][t][∥][2] (22)
_∥_ _∥_ _|_ _≤_ _|_ _−_ _∥_ _∥[2]_

_E[_ _P1bt+1_ 2 **_bt]_** _E[_ 1 _η_ _P1Ki_ 2[|][]][∥][P][1][b][t][∥][2] (23)
_∥_ _∥_ _|_ _≥_ _|_ _−_ _∥_ _∥[2]_ _[−]_ _[ηE][[][∥][P][1][K][i][∥][2][∥][P][−][1][K][i][∥][2][]][∥][P][−][1][b][t][∥][2]_
where
_E[|1 −_ _η∥P1Ki∥2[2][|][]]_

_n_

= [1] 1 _η_ _P1Ki_ 2[|]

_n_ _|_ _−_ _∥_ _∥[2]_

_i=1_

X


= _[n][ −]_ [1]


+ [1] 2[|]

_n_ _[|][1][ −]_ _[η][∥][P][1][K][1][∥][2]_


:=q1(η)
and
_ηE[∥P1Ki∥2∥P−1Ki∥2]_

_n_

=η _n[1]_ _∥P1Ki∥2∥P−1Ki∥2_

_i=1_

X

= _[η]_

_n_ _[∥][P][1][K][1][∥][2][∥][P][−][1][K][1][∥][2]_

_≤_ _n[η]_ [(][λ][1][ +][ √][nτ] [)][c][3][(2][λ][1][ + (][n][ −] [2)][τ] [)][√][nτ]

_√nτ := ξ(η)_ (24)

_≤_ _n[η]_ _[c][4]_


-----

where the first inequality by upper bounds (17) and (18), second inequality by nτ ≤O(1). Plug
the term (24) into inequalities (22) and (23), take expectation on both sides, we get claims (19) and
(20).

For inequality (21), check


_P−1bt+1 = P−1[I −_ _ηKiKi[T]_ []][b][t]
= [I − _ηP−1KiKi[T]_ _[P][−][1][]][P][−][1][b][t]_ _[−]_ _[η][[][P][−][1][K][i][K]i[T]_ _[P][1][]][P][1][b][t]_


Then we have


_E[∥P−1bt+1∥2|bt]_

_n_

_≤_ _n[1]_ _∥[I −_ _ηP−1KiKi[T]_ _[P][−][1][]][P][−][1][b][t][∥][2]_ [+][ ηE][[][∥][P][−][1][K][i][∥][2][∥][P][1][K][i][∥][2][]][∥][P][1][b][t][∥][2]

_i=1_

(24) Xn
_≤_ _n[1]_ _∥[I −_ _ηP−1KiKi[T]_ _[P][−][1][]][P][−][1][b][t][∥][2]_ [+][ ξ][(][η][)][∥][P][1][b][t][∥][2]

_i=1_

X

_n_

_∥[I −_ _ηP−1KiKi[T]_ _[P][−][1][]][P][−][1][b][t][∥][2]_
_i=1_

X


where

1
_n_

= [1]

_n_

= [1]

_n_

= [1]

_n_

= [1]


_∥[I −_ _ηP−1KiKi[T]_ _[P][−][1][]][P][−][1][b][t][∥]2[2]_

_∥P−1bt∥2[2]_ [+][ η][2][∥][P][−][1][K][i][K]i[T] _[P][−][1][P][−][1][b][t][∥]2[2]_ _[−]_ [2][η][⟨][P][−][1][b][t][, P][−][1][K][i][K]i[T] _[P][−][1][P][−][1][b][t][⟩]_

_∥P−1bt∥2[2]_ [+][ η][2][(][K]i[T] _[P][−][1][P][−][1][b][t][)][2][∥][P][−][1][K][i][∥]2[2]_ _[−]_ [2][η][(][K]i[T] _[P][−][1][P][−][1][b][t][)][2]_

1 + [(][K]i[T]∥[P]P[−]−[1]1[P]bt[−]∥[1]2[2][b][t][)][2] (η[2]∥P−1Ki∥2[2] _[−]_ [2][η][)][∥][P][−][1][b][t][∥][2]


_i=1_

_n_

_i=1_

Xn

_i=1_

X

_n_

_i=1_

X


_n_

(Ki[T] _[P][−][1][P][−][1][b][t][)][2]_

_≤vu1 + n[1]_ _i=1_ _∥P−1bt∥2[2]_ (η[2]∥P−1Ki∥2[2] _[−]_ [2][η][)][∥][P][−][1][b][t][∥][2]

u X
t

where the last inequality from Jensen’s inequality. Now the term


(Ki[T] _[P][−][1][P][−][1][b][t][)][2]_

_∥P−1bt∥2[2]_ (η[2]∥P−1Ki∥2[2] _[−]_ [2][η][)]


_i=1_

_n_

_i=1_

X


(11) _n_ (Ki[T] _[P][−][1][P][−][1][b][t][)][2]_

_η[2](λ[2]2_ [+ (][n][ −] [1)][τ][ 2][)][ −] [2][η]

_≤_ _n[1]_ _i=1_ _∥P−1bt∥2[2]_

X 

= _[∥][KP][−][1][b][t][∥]2[2]_ _η[2](λ[2]2_ [+ (][n][ −] [1)][τ][ 2][)][ −] [2][η]

_n∥P−1bt∥2[2]_

 


Let


_q−1(η) :=_ s1 + _[∥]n[KP]P_ _[−]1[1]b[b]t[t][∥]22[2]_ [η[2](λ[2]2 [+ (][n][ −] [1)][τ][ 2][)][ −] [2][η][]]

_∥_ _−_ _∥[2]_

combine all three inequalities above, take the expectation w.r.t. bt, we have claim (21).

**Lemma 26 (Long run behavior of SGD with moderate step size). Assume b0 is away from 0, λ[2]n** _[>]_
2 2
(2λ1 + (n 2)τ )nτ + c4√nτ _, λ2_ [+][ c][6]√nτ < λ1 _√nτ where c5, c6 are constants such that_
_−_ _[−]_ _[c][5]_

_c5_ _c[2]3[[2][λ][1]_ [+ (][n][ −] [2)][τ] []][2][√][nτ][ +][ c][4]
_≥_


-----

2
_√nτ_ _c4[n][−][.][5][τ/][[][λ]n[2]_ [+ (][n][ −] [2)][τ] [)][nτ] [] +][ λ][2]2[c][4][/][[][λ]n[2] [+ (][n][ −] [2)][τ] []][nτ] []]
_c6_ _−_ _[−]_ [(2][λ][1] 2 _[−]_ [[2][λ][1]
_≥_ 1 _c4√nτ/[λn_ [+ (][n][ −] [2)][τ] [)][nτ] []]

_−_ _[−]_ [(2][λ][1]

_Consider first k1 steps of SGD updates with step size η:_

2 2

_λ[2]1_ _√nτ < η <_ _λ[2]2_ [+][ c][6]√nτ

_[−]_ _[c][5]_

_Fix a β0_ _A0, then for 0 < ϵ < 1 and 0 < β < β0 such that_ _nτ_ _poly(ϵβ), there exists_
_k1 = O(log ≤_ _ϵβ[1]_ [)][ satisfying:] _[√]_ _≤_



-  Bk1 _ϵβ_
_≤_

-  Ak1 ≤∥b0∥2 ∗ _ρ[k]1[1]_ [+][ ϵβ/][2][ for some][ ρ][1][ >][ 1]

-  Ak > β0 for k = 0, . . ., k1.


_Proof. For this choice of η, denote q1 = q1(η), q_ 1 = q 1(η), ξ = ξ(η). By Lemma 21, we have
_−_ _−_

_Ak ≥_ _q1Ak−1 −_ _ξBk−1_
_Ak_ _q1_ _ξ_ _Ak−1_
_Bk_ _≤_ _ξ_ _q_ 1 _Bk_ 1
   _−_   _−_ 


Decompose the coefficient matrix as

_q1_ _ξ_ = cos θ _−_ sin θ _ρ1_ 0 cos θ sin θ
 _ξ_ _q−1_ sin θ cos θ   0 _ρ−1 −_ sin θ cos θ

Assume w.l.o.g. that sin θ ≥ 0 ( since otherwise we can take θ → _θ + π), then we have_

_k_

_Ak_ _q1_ _ξ_ _A0_
_Bk_ _≤_ _ξ_ _q_ 1 _B0_
   _−_   

= cos θ _−_ sin θ _ρ[k]1_ 0 cos θ sin θ _A0_
sin θ cos θ 0 _ρ[k]_ 1 sin θ cos θ _B0_
   _−_  −   

_A0(ρ[k]1_ [cos][2][ θ][ +][ ρ][k] 1 [sin][2][ θ][) +][ B][0][(][ρ]1[k] [cos][ θ][ sin][ θ][ −] _[ρ][k]_ 1 [cos][ θ][ sin][ θ][)]
= _−_ _−_
_B0(ρ[k]_ 1 [cos][2][ θ][ +][ ρ]1[k] [sin][2][ θ][) +][ A][0][(][ρ]1[k] [cos][ θ][ sin][ θ][ −] _[ρ][k]_ 1 [cos][ θ][ sin][ θ][)]
 _−_ _−_

_A0ρ[k]1_ [+ (][ρ]1[k] 1[) sin][ θ][(][B][0] [cos][ θ][ −] _[A][0]_ [sin][ θ][)]
= _B0ρ[k]_ 1 [+ (][ρ]1[k][−] _[ρ]−[k]_ 1[) sin][ θ][(][B][0] [sin][ θ][ +][ A][0] [cos][ θ][)]
 _−_ _[−]_ _[ρ]−[k]_ 

_A0ρ[k]1_ [+][ |][ρ]1[k] 1[|][ sin][ θ] _B0[2]_ [+][ A]0[2]

_[−]_ _[ρ]−[k]_
_≤_ B0ρ[k]−1 [+][ |][ρ]1[k] _[−]_ _[ρ]−[k]_ 1[|][ sin][ θ]p _B0[2]_ [+][ A]0[2]

_A0ρ[k]1_ [+][ |][ρ]1[k] 1[|][ sin][ θ][∥]p[b][0][∥][2]
= _B0ρ[k]_ 1 [+][ |][ρ]1[k][−] _[ρ]−[k]_ 1[|][ sin][ θ][∥][b][0][∥][2]
 _−_ _[−]_ _[ρ]−[k]_ 

We claim the following holds:


0 < ρ 1 < 1 < ρ1 _q1 + ξ_ (25a)
_−_ _≤_

_ρ[k][1]1[∥][b][0][∥][2][ ≤]_ _[ϵβ/][2]_ (25b)
_−_

_ρ1[k][1]_ (25c)

_[∥][b][0][∥][2][ sin][ θ][ ≤]_ _[ϵβ/][2]_
(B0 + ϵβ0/2)ξ < (q1 − 1)β0 (25d)

which we check later. Using inequalities (25), we can upper bound Bk1 as

_Bk1_ _B0ρ[k][1]1_ [+ (][ρ]1[k][1] 1[) sin][ θ][∥][b][0][∥][2]
_≤_ _−_ _[−]_ _[ρ]−[k][1]_

**_b0_** 2ρ[k][1]1 [+][ ρ]1[k][1] [sin][ θ][∥][b][0][∥][2]
_≤∥_ _∥_ _−_

(25b),(25c)
_≤_ _ϵβ_


-----

In addition, for k = 0, . . ., k1

_Bk_ _B0ρ[k]_ 1 [+ (][ρ]1[k] 1[) sin][ θ][∥][b][0][∥][2]
_≤_ _−_ _[−]_ _[ρ]−[k]_
_B0 + ρ[k]1[1]_ [sin][ θ][∥][b][0][∥][2]
_≤_

(25c)
_≤_ _B0 + ϵβ/2_

We now lower bound Ak by mathematical induction. We have A0 _β0, assume Ak_ 1 > β0, then
_≥_ _−_

_Ak ≥_ _q1Ak−1 −_ _ξBk−1_
_≥_ _q1β0 −_ _ξ(B0 + ϵβ/2)_

(25d)
_> q1β0 −_ (q1 − 1)β0 = β0

For upper bound Ak1, check


_Ak1_ _A0ρ[k]1[1]_ [+ (][ρ]1[k][1] 1[) sin][ θ][∥][b][0][∥][2]
_≤_ _[−]_ _[ρ]−[k][1]_

**_b0_** 2ρ[k]1[1] [+][ ρ]1[k][1] [sin][ θ][∥][b][0][∥][2]
_≤∥_ _∥_

(25c)
_≤∥b0∥2ρ[k]1[1]_ [+][ ϵβ/][2]

We have all lemma claims proved. Now it remains to check inequalities (25). First note that our
choice of the upper bound on η guarantees that q 1 < 1.
_−_

**For inequality (25a): By Gershgorin circle theorem, it suffices to show q** 1 + ξ < 1, q1 _ξ > 1,_
then we have ρ1 _q1_ _ξ > 1 and ρ_ 1 _q_ 1 + ξ < 1. In addition, we need the matrix to be− _−_
positive definite so that ≥ _− ρ_ 1 > 0, we just need− _≤_ _q−1q_ 1 > ξ[2] to make the matrix p.d..
_−_ _−_

_q_ 1 + ξ < 1
_−_

1 + 2 [η[2](λ[2]2 [+ (][n][ −] [1)][τ][ 2][)][ −] [2][η][]][ <][ 1][ −] _[c][4][ηn][−][1][/][2][τ]_

_⇐⇒s_ _[∥]n[KP]P_ _[−]1[1]b[b]t[t][∥]2[2]_

_∥_ _−_ _∥[2]_

= _[∥][KP][−][1][b][t][∥]2[2]_ _η[2](λ[2]2_ [+ (][n][ −] [1)][τ][ 2][)][ −] [2][η] _< c[2]4[η][2][n][−][1][τ][ 2][ −]_ [2][c][4][ηn][−][1][/][2][τ]
_⇐_ _n∥P−1bt∥2[2]_

 

Lemma= 2 24 _c4ηn[−][1][/][2]τ_ _c[2]4[η][2][n][−][1][τ][ 2][ +][ λ]n[2]_ _[−]_ [[2][λ][1] [+ (][n][ −] [2)][τ] []][nτ] 2η _η[2](λ[2]2_ [+ (][n][ −] [1)][τ][ 2][)]
_⇐_ _≤_ _n_ _−_

 


_n_ _[−]_ [[2][λ][1] [+ (][n][ −] [2)][τ] []][nτ] _λ[2]2_ [+ (][n][ −] [1)][τ][ 2][] _η_ _c[2]4[n][−][1][τ][ 2][η]_
_⇐⇒_ _[λ][2]_ _n_ _−_

2 _[λ]n[2]_ _[−]_ [[2][λ][1] [+ (][n][ −] [2)] _[τ]_ []][nτ] 2c4n[−][1][/][2]τ
_≤_ _n_ _−_

2
=η 2 1 − _c4√nτ/[λn_ _[−]_ [[2][λ][1] [+ (][n][ −] [2)][τ] []][nτ] []]
_⇐_ _≤_ _λ[2]2_ [+ (][n][ −] [1)][τ][ 2][ −] _[c]4[2][τ][ 2][/][[][λ]n[2]_ _[−]_ [[2][λ][1] [+ (][n][ −] [2)][τ] []][nτ] []]

2
_⇐⇒η ≤_ _λ[2]2_ [+][ (][n][−][1)][τ][ 2][−][c]4[2][τ][ 2][/][[][λ]n[2] _[−]1[[2]−[λ]c[1]4[+(]√nτ/[n][−][2)][λ[τ]2n[]][−][nτ][[2][]+][λ][1][λ][+(][2]2[[][c][n][4][−]√[2)]nτ/[τ]_ []][nτ][λ[]]2n[−][[2][λ][1][+(][n][−][2)][τ] []][nτ] []]]

which is true by our choice of η.

_q1_ _ξ > 1_
_−_

+ [1] 2[| −] _[c][4][ηn][−][1][/][2][τ >][ 1]_

_⇐⇒_ _[n][ −]n_ [1] _n_ _[|][1][ −]_ _[η][∥][P][1][K][1][∥][2]_

_⇐⇒1 + c4η[√]nτ < |1 −_ _η∥P1K1∥2[2][|]_

=η _P1K1_ 2
_⇐_ _∥_ _∥[2]_ _[−]_ [1][ >][ 1 +][ c][4][η][√][nτ]

2
_⇐=η >_ _P1K1_ 2 _√nτ_

_∥_ _∥[2]_ _[−]_ _[c][4]_

(18) 2
_⇐=η >_ _λ[2]1_ 3[[2][λ][1][ + (][n][ −] [2)][τ] []][2][nτ][ 2][ −] _[c][4]√nτ_

_[−]_ _[c][2]_


-----

which is true by our lower bound on η.
_q1q_ 1 > ξ[2]
_−_

_⇐=q−[2]_ 1 _[≥]_ _[ξ]_

1 + 2 _η[2](λ[2]2_ [+ (][n][ −] [1)][τ][ 2][)][ −] [2][η] _> c4ηn[−][1][/][2]τ_
_⇐⇒_ _[∥]n[KP]∥P−[−]1[1]b[b]t[t]∥[∥]2[2][2]_

 

2 [+ (2][λ][1] [+ (][n][ −] [1)][τ] [)][nτ]
=1 2η [λ][2] _> c4ηn[−][1][/][2]τ_
_⇐_ _−_ _n_

_n_
_⇐⇒η <_ 2λ[2]2 [+ 2(2][λ][1][ + (][n][ −] [1)][τ] [)][nτ][ +][ c][4]√nτ

which is true.

**For inequality (25b): It suffices to take**


_k1 = [log(][ϵβ/][(2][∥][b][0][∥][2][))]_ = (log [1]

log(ρ−1) _O_ _ϵβ_ [)]

**For inequality (25c):** We just need to show sin θ _<_ (ρ 1/ρ1)[k][1] . Calculate that
_ξ/(q1 −_ _q−1) =_ coscos[2] _θ θ− sinsin θ[2]_ _θ_ [, then] _−_

sin θ < (ρ−1/ρ1)[k][1]

_⇐=ξ/(q1 −_ _q−1) < 0.9(ρ−1/ρ1)[k][1]_

_⇐=ξ < 0.9(q1 −_ _q−1)(_ _[q]q[−]1[1] +[ −] ξ[ξ]_ [)][k][1]


_⇐=ξ < 0.9(q1 −_ _q−1)(ϵβ/(2∥b0∥2))1−_ log(log(qq−1+1 _−ξξ))_

=ξ < (q1 1)poly(ϵβ)
_⇐_ _−_

_⇐=[√]nτ ≤_ _poly(ϵβ)_

**For inequality (25d): Suffice to show**


_ξ <_ [(][q][1][ −] [1)][β][0]

_B0 + ϵβ0/2_
_⇐=[√]nτ ≤O(1)_

**Lemma 27 (Long run behavior of SGD with small step size). Under the same notations and as-**
_sumptions as Lemmawith step size_ _26 unless otherwise specified. Consider another k2 −_ _k1 steps of SGD update_


1
_η[′]_ _<_ _λ[2]1_ [+][ c][7]√nτ

_where the constant c7_ _nτ + c4. Then we have for k > k1:_
_≥_ _[√]_

-  Bk _ϵβ_
_≤_

_qAk_ 1 _, Ak_ 1 > β

-  Ak ≤  _β_ _−_ _, Ak−−1 < β_

_where q := q1(η[′]) + ξ(η[′])ϵ < 1._

_Proof. Denote q1[′]_ [=][ q][1][(][η][′][)][, q]−[′] 1 [=][ q][−][1][(][η][′][)][, ξ][′][ =][ ξ][(][η][′][)][, then][ q][ =][ q]1[′] [+][ ξ][′][ϵ][, denote][ B][ =][ ∥][b][0][∥][2] _[∗]_
_ρ[k]1[1]_ [+][ ϵβ/][2][. We have by proof of Lemma][ 26][ that][ q][′] 1 [+][ ξ <][ 1][. We claim the following holds:]
_−_

_q < 1_ (26a)

_ξ[′]B ≤_ (1 − _q1[′]_ [)][ϵβ] (26b)


-----

**Check inequality (26a):**


_q1[′]_ [+][ ξ][′][ϵ <][ 1]


= _[n][ −]_ [1] + [1] 2[|][ +][ c][4][η][′][n][−][1][/][2][τ <][ 1]

_n_ _n_ _[|][1][ −]_ _[η][′][∥][P][1][K][1][∥][2]_

_⇒|1 −_ _η[′]∥P1K1∥2[2][|][ <][ 1][ −]_ _[c][4][η][′][√][nτ]_

_⇒c4η[′][√]nτ < η[′]∥P1K1∥2[2]_ _[<][ 2][ −]_ _[c][4][η][′][√][nτ]_

2
_c4√nτ <_ _P1K1_ 2
_∥_ 2 _∥_
_⇒_  _η[′]_ _<_ _∥P1K1∥2[2][+][c][4]√nτ_

_√nτ <_ (1)
= _O_ 2
_η[′]_ _<_ _λ[2]1[+][nτ][ 2][+][c][4]√nτ_



which are true by assumption.

**Check inequality (26b):**

_ξ[′]B ≤_ (1 − _q1[′]_ [)][ϵβ]

_c4η[′]n[−][1][/][2]τ_ ( **_b0_** 2 _ρ1[k][1]_ [+][ ϵβ/][2)][ ≤] [(1][ −] _[n][ −]_ [1] 2[))][ϵβ]
_⇐⇒_ _∥_ _∥_ _∗_ _n_ _−_ _n[1]_ [(1][ −] _[η][′][∥][P][1][K][1][∥][2]_

_⇐⇒c4η[′][√]nτ_ (∥b0∥2 ∗ exp(k1)[log][ ρ][1] + ϵβ/2) ≤ _η[′]∥P1K1∥2[2][ϵβ]_

_⇐=[√]nτ ≤_ _poly(ϵβ)._


With (Ak1 26B) we can prove the lemma by mathematical induction. Suppose, then check _Bk−1 ≤_ _ϵβ and Ak−1 ≤_
_≤_

_Bk ≤_ _ξ[′]Ak−1 + q−[′]_ 1[B][k][−][1]
_ξ[′]B + q[′]_ 1[ϵβ]
_≤_ _−_

(26b)
_≤_ _ϵβ_

and


_Ak_ _q1[′]_ _[A][k][−][1]_ [+][ ξ][′][B][k][−][1]
_≤_
_q1[′]_ _[A][k][−][1]_ [+][ ξ][′][ϵβ]
_≤_

_≤_ (q1[′] [+][ ξ][′][ϵ][) max][{][A][k][−][1][, β][}]

_qAk_ 1 _, Ak_ 1 > β
_−_ _−_ _._
_≤_ _qβ < β_ _, Ak_ 1 < β
 _−_

We recap Theorem 5 using our notations in previous lemmas as follows:


**Theorem 28 (Directional bias of the two-stage SGD). Use the two stage SGD scheme as defined in**
_Lemma 26 and 27. Assume nτ < poly(ϵ), then there exists k1 = O(log_ [1]ϵ [)][ and][ k][2][ such that]

(1 2ϵ)γ1 _γ1_
_−_ _≤_ _[E]E[[][∥][[K]b[b]k2[k][2]_ _[∥]2][2][]]_ _≤_

_∥_ _∥_

_where γ1 is the largest eigenvalue of K._


_Proof. In Lemma 26 let β = β0, then for k1 =_ (log [1]ϵ [)][ we have][ B][k][1]

Lemma 27 we can early stop at k2 such that Ak O2 ≥ _β0 and Ak2+1 < β0[≤]. We then have[ϵβ][0][. For the 2nd stage, by]_

_Bk2_ _ϵβ0_ _ϵAk2_
_≤_ _≤_


-----

Then we check

_E_ _Kbk2_ 2
_∥_ _∥_

_E_ **_bk2_** 2
_∥_ _∥_

= _[E]_ _∥KP−1bk2_ _∥2[2]_ [+][ ∥][KP][1][b][k]2 _[∥][2]2_ [+ 2][⟨][K]1[T] _[P][−][1][b][k]2_ _[,][ K]1[T]_ _[P][1][b][k]2_ _[⟩]_

_E_ **_bk2_** 2

p _∥_ _∥_

_KP1bk2_ 2 2 _[∥][2]2_
_∥_ _∥[2]_ _[−]_ [2][∥][P][−][1][K][1][∥][2][∥][P][1][K][1][∥][2][∥][b][k]

_≥_ _[E]_ _E_ **_bk2_** 2
p _∥_ _∥_

_∥KP1bk2_ _∥2[2]_ _[−]_ _[E]_ 2∥P−1K1∥2∥P1K1∥2∥bk2 _∥2[2]_

_≥_ _[E]_ _E_ **_bk2_** 2
p p∥ _∥_

= _[E][∥][K]1[T]_ _[P][1][b][k]2_ _[∥][2]_ _[−]_ 2∥P−1K1∥2∥P1K1∥2E∥bk2 _∥2_

_E_ **_bk2_** 2

p _∥_ _∥_

(17),(18) _λ[2]1_ 3[[2][λ][1][ + (][n][ −] [2)][τ] []][2][nτ][ 2] _E∥P1bk2_ _∥2_
_≥_ q _[−]_ _[c][2]_ _E∥P1bk2_ _∥2 + E∥P−1bk2_ _∥2_

2(λ1 + _nτ_ )(c3(2λ1 + (n 2)τ )[√]nτ )

_−_ _[√]_ _−_
q _β0_

_λ[2]1_ 3[[2][λ][1][ + (][n][ −] [2)][τ] []][2][nτ][ 2] 2(λ1 + _nτ_ )(c3(2λ1 + (n 2)τ )[√]nτ )

_≥_ _[−]_ _[c][2]_ _ϵβ0 + β0_ _−_ _[√]_ _−_
q q

(13)
_≥_ (γ1 − _nτ −_ _c3(2λ1 + (n −_ 2)τ )[√]nτ )(1 − _ϵ) −_ 2(λ1 + _[√]nτ_ )(c3(2λ1 + (n − 2)τ )[√]nτ )

_γ1(1_ _ϵ)_ _γ1ϵ_ (By nτ < poly(ϵ)) q
_≥_ _−_ _−_
=γ1(1 2ϵ)
_−_

And the upper bound in the theorem is by definition of γ1.

F DIRECTIONAL BIAS OF GD WITH MODERATE OR SMALL STEP SIZE

This section includes the proof of Theorem 7. We first rewrite the GD updates as linear combination
of eigenvectors. Then the theorem is proved using the transformed variables and finally transformed
back to original parameters.

**The directional bias of GD does not require diagonal dominant gram matrix.**

**Reloading notations Denote the eigen decomposition of K:**

_K = GΓG[T]_ _, Γ = diag(γ1, . . ., γn), G = [g1, . . ., gn]_

where the eigenvectors gi’s are orthogonal. The GD update as


**_αt+1 = αt_**
_−_ _n[η]_ _[K][(][K][α][t][ −]_ **_[y][)]_**

Denote wt := G[T] (αt − **_αˆ), we can rewrite GD update in wt:_**

**_wt+1 = wt_**
_−_ _n[η]_ [Γ][2][w][t][ = (][I][ −] _n[η]_ [Γ][2][)][w][t]


We recap Theorem 7 to make reading easier as follows:

**Theorem 29 (Direction bias of GD). Assume α0 is away from 0, λn + 2nτ < λn** 1, GD with step
_−_
_size:_


_η <_


(λ1 + nτ )[2]


_For a small ϵ > 0, take k = O(log_ [1]ϵ [)][, we have]

**_α)_** 2
_γn ≤_ _[∥][K]α[(][α]k[k][ −]αˆ[ˆ]_ 2∥

_∥_ _−_ _∥_


1 + ϵγn


-----

_Proof. For i = 1, . . ., n, we have_

_wk[(][i][)]_ = (1 − _ηγi[2][/n][)][k][w]0[(][i][)]_

Denote qi = 1 − _ηγi[2][/n][, then][ 0][ < q][1][ ≤]_ _[. . .][ ≤]_ _[q][n][ <][ 1][ since]_

_n_ (13) _n_
0 < η <

(λ1 + nτ )[2] _≤_ _γ1[2]_ _≤_ _γ[n]i[2]_

Since λn + nτ < λn 1 _nτ_, we have γn < γn 1 by lemma 20, it follows that qn > qn 1. Denote
_q = qn_ 1/qn < 1, then− _−_ _−_ _−_
_−_
_n−1_
_i=1_ [(][w]k[(][i][)][)][2]
P (wk[(][n][)][)][2]

_ni=1−1_ _[q]i[2][k][(][w]0[(][i][)][)][2]_
=
P _qn[2][k][(][w]0[(][n][)][)][2]_

_in=1−1_ _[q]n[2][k]_ 1[(][w]0[(][i][)][)][2]
_≤_ P _qn[2][k][(][w]−0[(][n][)][)][2]_

_in=1−1[(][w]0[(][i][)][)][2]_
=q[2][k]
P (w0[(][n][)][)][2]


log _γ1[2]γn[2]ni[ϵ]=1 [(]−[w]10[(][(][n][w][)]0[(])[i][2][)]_ )[2]

Plog q = O(log [1]ϵ [)][, we have]

_ni=1−1[(][w]k[(][i][)][)][2]_ _n[ϵ]_
P (wk[(][n][)][)][2] _≤_ _[γ]γ[2]1[2]_


Let q[2][k] _≤_ _γ1[2]_ _γn[2]_ _[ϵ]ni=1[(]−[w]10[(][(][n][w][)])0[(][i][2][)][)][2][ ⇐]⇒_ _k ≥_ 2[1]

P

Thus


_∥K(αk −_ **_αˆ)∥2[2]_** = 2

**_αk_** **_αˆ_** 2 _[∥][Γ]w[w]k[k][∥]2[2]_
_∥_ _−_ _∥[2]_ _∥_ _n_ _∥[2]_

_i=1[(][w]k[(][i][)][)][2][γ]i[2]_
=
_n_
P _i=1[(][w]k[(][i][)][)][2]_
P(wk[(][n][)][)][2][γ]n[2] _ni=1−1[(][w]k[(][i][)][)][2][γ]i[2]_

=
_n_ _n_
_i=1[(][w]k[(][i][)][)][2][ +]_ P _i=1[(][w]k[(][i][)][)][2]_
P _in=1−1[(][w]k[(][i][)][)][2]P_

_γn[2]_ [+] _n_ 1
_≤_
P _i=1[(][w]k[(][i][)][)][2][ γ][2]_

_γn[2]_ [+][ γ]P1[2] _γn[2]_ _ϵ = γn[2][(1 +][ ϵ][)]_
_≤_ _γ1[2]_

thus
_K(αk_ **_αˆ)_** 2
_∥_ _−_ _∥_ _γn[2][(1 +][ ϵ][)]_

**_αk_** **_αˆ_** 2 _≤_

The lower bound of the theorem holds by definition of∥ _−_ _∥_ p γn.


G EFFECT OF DIRECTIONAL BIAS

In this section, we provide the proof for theorems in Section 4.2. There are two theorems there,
so we split this section into two subsections. Subsection G.1 proves Theorem 9, for a general
problem setting of squared error minimization, it provides a straightforward understanding for why
directional bias towards the largest eigenvalue of the Hessian is good for generalization. Section G.2
proves Theorem 11 by giving concrete generalization bounds of SGD and GD estimators in kernel
regression.


-----

G.1 PROOF OF THEOREM 9

Denote v = w − **_w[∗], rewrite the objective function as_**

min **_v_** 2
**_v_** _∥_ _∥[2]_

_s.t._ _∥Av∥2[2]_ [=][ a]

Denote the eigen decomposition of A[T] _A = QΓQ[T]_ where Q = [q1, . . ., qn], QQ[T] = Q[T] _Q = I_
and Γ = diag([ρ1, . . ., ρn]), ρ1 _. . ._ _ρn_ 0. Then
_≥_ _≥_ _≥_


_∥Av∥2[2]_ [=]


_ρi(qi[T]_ **_[v][)][2]_**
_i=1_

X


So


_∥Av∥2[2]_ _[≤]_ _[ρ][1][[]_ _i=1(qi[T]_ **_[v][)][2][] =][ ρ][1][v][T][ QQ][T][ v][ =][ ρ][1][∥][v][∥]2[2]_**

X


The equality is achieved when v is in the direction of q1, and ρ1 = ∥A[T] _A∥2. Take L(w) =_
_∥Av∥2[2]_ [=][ a][ then the theorem holds.]

G.2 PROOF OF THEOREM 11

**Calculate ∆[∗]a[:][ Denote][ f][ ∗]** [= ˆ]α[T] _K(·, X) + f[˜], then we have for a f ∈Hs, f = α[T]_ _K(·, X), let_
**_b = ˆα −_** **_α, then_**

_∥f_ _[∗]_ _−_ _f_ _∥H[2]_
= **_b[T]_** _K(_ _, X) + f[˜]_
_∥_ _·_ _∥H[2]_
= **_b[T]_** _K(_ _, X)_ [+][ ∥]f[˜] [+ 2][⟨][b][T][ K][(][·][, X][)][,][ ˜]f
_∥_ _·_ _∥H[2]_ _∥H[2]_ _⟩H_

where we can check


**_b[T]_** _K(_ _, X),_ _f[˜]_ =
_⟨_ _·_ _⟩H_

=

=

=

And we further calculate that


_bi_ _K(_ _, xi), f_ **_αˆ_** _[T]_ _K(_ _, X)_
_⟨_ _·_ _[∗]_ _−_ _·_ _⟩H_
_i=1_

X

_bi[_ _K(_ _, xi), f_ _K(_ _, xi), ˆα[T]_ _K(_ _, X)_ ]
_i=1_ _⟨_ _·_ _[∗]⟩H −⟨_ _·_ _·_ _⟩H_

X

_bi[f_ (xi) **_αˆ_** _[T]_ _K(xi, X)](By reproducing property)_

_[∗]_ _−_
_i=1_

X

_bi[yi_ _yi] = 0_
_i=1_ _−_

X


**_b[T]_** _K(_ _, X)_ [=][ ⟨] _biK(_ _, xi),_
_∥_ _·_ _∥H[2]_ _·_

_i=1_

X


_bjK(·, xj)⟩H_
_j=1_

X


= _bibj_ _K(_ _, xi), K(_ _, xj)_

_⟨_ _·_ _·_ _⟩_
_i,j=1_

X

_n_

= _bibjK(xi, xj) = b[T]_ _Kb_

_i,j=1_

X

_LD(f_ ) = b[T] _Kb + ∥f[˜]∥H[2]_

inf _f_
_f_ _s_ _[L][D][(][f]_ [) =][ ∥] [˜]∥H[2]
_∈H_


That is,

and


-----

It follows that

We claim that


∆(f ) = LD(f ) inf
_−_ _f_ _s_ _[L][D][(][f]_ [) =][ b][T][ K][b]
_∈H_


∆[∗]a [=] min _Kb_ 2 [= 2][na/γ][1]
**_b:_** 2[1]n 2[=][a][ b][T][ K][b][ = 1]γ1 _∥_ _∥[2]_

_[∥][K][b][∥][2]_

where the equality is obtained when b is in the direction of the largest eigenvector of K. To see
this, we check _Kb_ 2

[g1, . . ., gn] has orthogonal columns and ∥ _∥[2]_ _[≤]_ _[γ][1][b][T][ K][b][. Recall the eigendecomposition of] Γ = diag(γ1, . . ., γn). Then_ _[ K][ =][ G][Γ][G][T][ where][ G][ =]_


_∥Kb∥2[2]_ [=]

**_b[T]_** _Kb =_


_γi[2][(][g]i[T]_ **_[b][)][2]_**
_i=1_

X

_n_

_γi(gi[T]_ **_[b][)][2]_**
_i=1_

X


and

So we have


_γi(gi[T]_ **_[b][)][2][] =][ γ][1][b][T][ K][b]_**
_i=1_

X


_Kb_ 2
_∥_ _∥[2]_ _[≤]_ _[γ][1][[]_

This finishes our claim on ∆[∗]a[.]


**SGD output: By Theorem 5, the SGD output has**

(1 2ϵ)γ1E[ **_bk2_** 2] _E[_ _Kbk2_ 2]
_−_ _∥_ _∥_ _≤_ _∥_ _∥_

Thus


_E[∆[1][/][2](f_ _[SGD])] = E_


_γi(gi[T]_ **_[b][SGD][)][2]_**
_i=1_

X


_n_

_≤_ _[√]γ1E[(_ (gi[T] **_[b][SGD][)][2][)][1][/][2][]]_**

_i=1_

X

= _γ1E_ **_b[SGD]_** 2

_[√]_ _∥_ _∥_
_γ1E[_ _Kb[SGD]_ 2]/[(1 2ϵ)γ1]
_≤_ _[√]_ _∥_ _∥_ _−_

= 2na/γ1/(1 2ϵ)

_−_

p


1
= _a[)][1][/][2]_

1 2ϵ [(∆][∗]
_−_

_< (1 + 4ϵ)(∆[∗]a[)][1][/][2]_

last inequality by let ϵ < 1/4.

**GD output: By Theorem 7, the GD output has**


_∥Kbb[GD][GD]∥22[2]_ _≤_ (1 + ϵ[′])γn[2]

_∥_ _∥[2]_


-----

Thus


∆(f _[GD]) =_


_γi(gi[T]_ **_[b][GD][)][2]_**
_i=1_

X


(gi[T] **_[b][GD][)][2]_**
_i=1_

X


_γn_
_≥_


= γn **_b[GD]_** 2
_∥_ _∥[2]_
_≥_ _γn∥Kb[GD]∥2[2][/][[(1 +][ ϵ][′][)][γ]n[2][]]_

= 2na/[(1 + ϵ[′])γn]

_γ1_
= (1 + ϵ[′])γn ∆[∗]a

_> γ[γ]n[1]_ (1 − _ϵ[′])∆[∗]a_

:= M ∆[∗]a
where M > 1 by taking ϵ[′] _< 1_ _γn/γ1._
_−_

H EXPERIMENTS

We list the implementation details of the experiments at the end of Section 4 and include more
experiment results. For better presenting, we split into two subsections: Subsection H.1 includes the
details of simulation; Subsection H.2 is about the NN experiment on FashionMNIST, including the
data description, network structure, and algorithm details, also there are more experiment results in
Subsection H.2 that are not listed in Section 4 due to page limit.

H.1 SIMULATION

This subsection is corresponding to Figure 1.

**Data Generation. The training data is simulated as follows: Set n = 10, p = 100, simulate Xn** _p_
_×_
where elements of X are i.i.d. N (0, 1); denote ith row of X as xi, normalize xi such that it has

_i.i.d._
squared ℓ2 norm in [.49, 1]; set yi = _j=1_ [sin(][x][i,j][) +][ ϵ][i][ where][ ϵ][i] _N_ (0, .01). The testing data
_∼_
is simulated in exactly the same way, except that we only simulate n = 5 testing data.

**Kernel Function. We set the kernel function to be the polynomial kernel[P][p]**

_K(x1, x2) = (_ **_x1, x2_** + .01)[2]
_⟨_ _⟩_

**SGD and GD implementation. Both SGD and GD is run for small and moderate step sizes. The**
moderate step size scheme for SGD is: η1 = .1 for the first 50 steps, and η2 = .01 for the next 1000
steps; for GD is: η1 = .5 for the first 50 steps, and η2 = .05 for the next 1000 steps. The small step
size scheme for SGD is η = 0.01 for 1050 steps; for GD is η = 0.05 for 1050 steps. Note that the
step size for SGD is a fraction of that for GD, this matches our Theorem 5 and 7 that the step size of
GD is of magnitude n/2 times that of SGD.

H.2 NEURAL NETWORK ON FASHIONMNIST

This subsection is corresponding to Figure 2.

**Dataset. The original FashionMNIST consist of 60, 000 training data and 10, 000 testing data. We**
randomly sample 1, 500 data from original training data for training, and use all 10, 000 original
testing data for testing. All data entries are normalized to [0, 1].

**Network structure. we use a 6-layer ResNet-like (He et al., 2016) Neural Network, and the struc-**
ture is as follows

Input ⇒ 7 × 7 Conv ⇒ BatchNorm ⇒ ReLU ⇒ 3 × 3 MaxPool
_⇒_ ResBlock1 ⇒ ResBlock2 ⇒ Global AvePool ⇒ FC ⇒ output


-----

The Residual Blocks are as Figure 7.6.3 in Zhang et al. (2021) (without 1 × 1 convolution). Note
that each residual block contains two 3 × 3 convolutional layers, thus total number of layers is as
stated.

**Algorithm. We minimize the Cross Entropy Loss objective L(w) =** _n[1]_ _ni=1_ _[l][i][(][w][)][, where][ l][i][(][w][)][ is]_

the loss function at ith sample. One step SGD is as follows:
P

1
**_wt+1 = wt_** _ηt_ _li(wt)_
_−_ _|I|_ Xi∈I _∇_

where I is a randomly sampled subset of {1, . . ., n} (uniform random sample without replacement).
We choose the batch size |I| to be 25.

One step GD is as follows:
**_wt+1 = wt −_** _ηt∇L(wt)_
Both SGD and GD are run using two settings of step sizes ηt. The moderate step size setting is as
follows:

0.2, _t = 1, . . ., 5000_
_ηt =_
0.02, _t = 5001, . . ., 20000_


And the small step size setting has ηt = 0.02, t = 1, . . ., 20000.

**Comparison of convergence direction.** Since the loss surface is nonconvex and the Hessian
varies, we follow Wu et al. (2021) to measure the convergence direction by Relative Rayleigh Quotient(RRQ), which normalizes the Rayleigh Quotient by the maximum eigenvalue of the Hessian as
follows

_∇L(w)[⊤]_ _∇L(w)_

_RRQ(w) =_ _∥∇L(w)∥2_ _[· ∇][2][L][(][w][)][ ·]_ _∥∇L(w)∥2_

_L(w)_ 2
_∥∇[2]_ _∥_
where L(w) is the loss function on the whole training set. A high RRQ indicates that the convergence
direction of w is close to a larger eigenvector of the Hessian.

**Comparison of test accuracy. We set 20 different random seeds. For each random seed, we run:**
SGD with moderate step size, GD with moderate step size, SGD with small step size, GD with small
step size. For each algorithm, we evaluate its test accuracy once every 500 steps, and use the average
of the last 5 values as its test accuracy. We list the test accuracy in Table 1.

|Experiment|#1 #2 #3 #4 #5 #6 #7 #8 #9 #10|
|---|---|
|SGD + moderate LR|83.69 82.95 82.37 82.05 83.4 83.16 83.72 83.29 83.28 83.23|
|GD + moderate LR|80.93 80.79 80.79 81.80 81.68 81.12 82.43 81.63 80.94 81.54|
|SGD + small LR|82.00 81.72 81.34 81.92 82.63 82.67 82.99 82.22 80.78 82.10|
|GD + small LR|78.88 78.71 78.49 79.3 80.45 79.78 80.15 79.66 79.54 79.68|
|Experiment|#11 #12 #13 #14 #15 #16 #17 #18 #19 #20|
|SGD + moderate LR|83.12 82.92 83.58 83.47 82.35 83.57 83.59 82.43 84.21 83.12|
|GD + moderate LR|82.41 81.56 81.42 80.86 81.23 81.25 81.82 80.42 81.80 82.12|
|SGD + small LR|82.62 80.66 82.01 81.01 81.32 81.66 82.12 80.78 82.28 82.48|
|GD + small LR|80.08 78.29 79.93 79.36 78.9 79.69 80.2 79.62 79.98 79.69|



Table 1: Test Accuracy

We also use one-side Wilcoxon signed-rank test to check if the test accuracy of different algorithm
are significantly different, the result is in Table 2. All the p-values are significant at 0.01 level, so
we reject the null hypothesis and conclude that the SGD with moderate step size has test accuracy
significantly higher than all other algorithms.

|Null Hypothesis on Test Accuracy|p-value|
|---|---|
|SGD + moderate LR GD + moderate LR ≤|9.54 10−7 ×|
|SGD + moderate LR SGD + small LR ≤|9.54 10−7 ×|
|SGD + moderate LR GD + small LR ≤|9.54 10−7 ×|


Null Hypothesis on Test Accuracy p-value

SGD + moderate LR ≤ GD + moderate LR 9.54 × 10[−][7]

SGD + moderate LR ≤ SGD + small LR 9.54 × 10[−][7]

SGD + moderate LR ≤ GD + small LR 9.54 × 10[−][7]


Table 2: Wilcoxon signed-rank test result


-----

Figure 3: Use more step sizes in SGD/GD. The test accuracy is evaluated once every 500 iterations,
and inside the bracket is the average of the last 5 test accuracy values.

**Additional experiments. We conduct more experiments using different step sizes. The initial step**
size is taken in {1, 0.5, 0.2, 0.1, 0.02, 0.01, 0.005, 0.001}, and the step size is divided by a factor of
10 after 5000 steps. The test accuracy is in Figure 3, where we see that SGD with step size 0.2 has
the best test accuracy, and GD with step size 0.5 performs better than GD with any other step sizes,
but is still worse than the best SGD.


-----

