## AUGMENTED SLICED WASSERSTEIN DISTANCES

**Xiongjie Chen[1], Yongxin Yang[2,3], and Yunpeng Li[1]**


1University of Surrey, 2University of Edinburgh, 3Huawei Noah’s Ark Lab
1{xiongjie.chen, yunpeng.li}@surrey.ac.uk, 2yongxin.yang@ed.ac.uk

ABSTRACT


While theoretically appealing, the application of the Wasserstein distance to
large-scale machine learning problems has been hampered by its prohibitive
computational cost. The sliced Wasserstein distance and its variants improve the
computational efficiency through the random projection, yet they suffer from low
accuracy if the number of projections is not sufficiently large, because the majority
of projections result in trivially small values. In this work, we propose a new family
of distance metrics, called augmented sliced Wasserstein distances (ASWDs),
constructed by first mapping samples to higher-dimensional hypersurfaces parameterized by neural networks. It is derived from a key observation that (random) linear
projections of samples residing on these hypersurfaces would translate to much
more flexible nonlinear projections in the original sample space, so they can capture
complex structures of the data distribution. We show that the hypersurfaces can
be optimized by gradient ascent efficiently. We provide the condition under which
the ASWD is a valid metric and show that this can be obtained by an injective neural
network architecture. Numerical results demonstrate that the ASWD significantly
outperforms other Wasserstein variants for both synthetic and real-world problems.


1 INTRODUCTION

Comparing samples from two probability distributions is a fundamental problem in statistics and
machine learning. The optimal transport (OT) theory (Villani, 2008) provides a powerful and flexible
theoretical tool to compare degenerative distributions by accounting for the metric in the underlying
spaces. The Wasserstein distance, which arises from the optimal transport theory, has become an
increasingly popular choice in various machine learning domains ranging from generative models
to transfer learning (Gulrajani et al., 2017; Arjovsky et al., 2017; Kolouri et al., 2019b; Cuturi and
Doucet, 2014; Courty et al., 2016).

Despiteitsfavorableproperties, suchasrobustnesstodisjointsupportsandnumericalstability(Arjovsky
et al., 2017), the Wasserstein distance suffers from high computational complexity especially when the
sample size is large. Besides, the Wasserstein distance itself is the result of an optimization problem
— it is non-trivial to be integrated into an end-to-end training pipeline of deep neural networks, unless
one can make the solver for the optimization problem differentiable. Recent advances in computational
optimal transport methods focus on alternative OT-based metrics that are computationally efficient and
solvable via a differentiable optimizer (Peyré and Cuturi, 2019). Entropy regularization is introduced
in the Sinkhorn distance (Cuturi, 2013) and its variants (Altschuler et al., 2017; Dessein et al., 2018)
to smooth the optimal transport problem; as a result, iterative matrix scaling algorithms can be applied
to provide significantly faster solutions with improved sample complexity (Genevay et al., 2019).

An alternative approach is to approximate the Wasserstein distance through slicing, i.e. linearly
projecting, the distributions to be compared. The sliced Wasserstein distance (SWD) (Bonneel et al.,
2015) is defined as the expected value of Wasserstein distances between one-dimensional random
projections of high-dimensional distributions. The SWD shares similar theoretical properties with the
Wasserstein distance (Bonnotte, 2013) and is computationally efficient since the Wasserstein distance
in one-dimensional space has a closed-form solution based on sorting. (Deshpande et al., 2019) extends
the sliced Wasserstein distance to the max-sliced Wasserstein distance (Max-SWD), by finding a
single projection direction with the maximal distance between projected samples. The subspace robust
Wasserstein distance extends the idea of slicing to projecting distributions on linear subspaces (Paty and


-----

Figure 1: (a) and (b) are visualizations of projections for the ASWD and the SWD between two
2-dimensional Gaussians. (c) and (d) are distance histograms for the ASWD and the SWD between
two 100-dimensional Gaussians. Figure 1(a) shows that the injective neural network embedded in the
ASWD learns data patterns (in the X-Y plane) and produces well-separate projected values (Z-axis)
between distributions in a random projection direction. The high projection efficiency of the ASWD
is evident in Figure 1(c), as almost all random projection directions in a 100-dimensional space lead
to significant distances between 1-dimensional projections. In contrast, random linear mappings in
the SWD often produce closer 1-d projections (Z-axis) (Figure 1(b)); as a result, a large percentage of
random projection directions in the 100-d space result in trivially small distances (Figure 1(d)), leading
to a low projection efficiency in high-dimensional spaces.

Cuturi, 2019). However, the linear nature of these projections usually leads to low projection efficiency
of the resulted metrics in high-dimensional spaces (Deshpande et al., 2019; Kolouri et al., 2019a).

Different variants of the SWD have been proposed to improve the projection efficiency of the SWD,
either by introducing nonlinear projections or by optimizing the distribution of random projections.
Specifically, (Kolouri et al., 2019a) extends the connection between the sliced Wasserstein distance and
the Radon transform (Radon, 1917) to introduce generalized sliced Wasserstein distances (GSWDs)
by utilizing generalized Radon transforms (GRTs), which are defined by nonlinear defining functions
and lead to nonlinear projections. A variant named the GSWD-NN was proposed in (Kolouri et al.,
2019a) to generate nonlinear projections directly with neural network outputs, but it does not fit into the
theoretical framework of the GSWD and does not guarantee a valid metric. In contrast, the distributional
sliced Wasserstein distance (DSWD) and its nonlinear version, the distributional generalized sliced
Wasserstein distance (DGSWD), improve their projection efficiency by finding a distribution of
projections that maximizes the expected distances over these projections. The GSWD and the DGSWD
exhibit higher projection efficiency than the SWD in the experiment evaluation, yet they require the
specification of the particular form of defining functions from a limited class of candidates. However,
the selection of defining functions is usually a task-dependent problem and requires domain knowledge,
and the impact on performance from different defining functions is still unclear.

In this paper, we present the augmented sliced Wasserstein distance (ASWD), a distance metric
constructed by first mapping samples to hypersurfaces in an augmented space, which enables flexible
nonlinear slicing of data distributions for improved projection efficiency (See Figure 1). Our main
contributions include: (i) We exploit the capacity of nonlinear projections employed in the ASWD
by constructing injective mapping with arbitrary neural networks; (ii) We prove that the ASWD is a
valid distance metric; (iii) We provide a mechanism in which the hypersurface where high-dimensional
distributions are projected onto can be optimized and show that the optimization of hypersurfaces
can help improve the projection efficiency of slice-based Wasserstein distances. Hence, the ASWD
is data-adaptive, i.e. the hypersurfaces can be learned from data. This implies one does not need
to manually design a function from the limited class of candidates; (iv) We demonstrate superior
performance of the ASWD in numerical experiments for both synthetic and real-world datasets.

The remainder of the paper is organized as follows. Section 2 reviews the necessary background.
We present the proposed method and its numerical implementation in Section 3. Related work are
discussed in Section 4. Numerical experiment results are presented and discussed in Section 5. We
conclude the paper in Section 6.


-----

2 BACKGROUND

In this section, we provide a brief review of concepts related to the proposed work, including the
Wasserstein distance, (generalized) Radon transform and (generalized) sliced Wasserstein distances.

**Wasserstein distance: Let Pk(Ω) be a set of Borel probability measures with finite k-th moment**
on a Polish metric space (Ω,d) (Villani, 2008). Given two probability measures µ, ν _Pk(Ω), the_
_∈_
Wasserstein distance of order k _∈_ [1,+∞) between µ and ν is defined as:

[1]

_k_

_Wk(µ,ν)=_ inf _d(x,y)[k]dγ(x,y)_ _,_ (1)
γ∈Γ(µ,ν)ZΩ×Ω 

where d(·,·)[k] is the cost function, Γ(µ,ν) represents the set of all transportation plans γ, i.e. joint
distributions whose marginals are µ and ν, respectively.

While the Wasserstein distance is generally intractable for high-dimensional distributions, there are
several favorable cases where the optimal transport problem can be efficiently solved. If µ and ν
are continuous one-dimensional measures defined on a linear space equipped with the L[k] norm, the
Wasserstein distance between µ and ν has a closed-form solution (Peyré and Cuturi, 2019):

1 _k[1]_
_Wk(µ,ν)=_ 0 _|Fµ[−][1][(][z][)][−][F][ −]ν_ [1](z)|[k]dz _,_ (2)
Z 

where Fµ[−][1] and Fν[−][1] are inverse cumulative distribution functions (CDFs) of µ and ν, respectively.
In practice, Wasserstein distances Wk(˜µ, ˜ν) between one-dimensional empirical distributions
_µ˜ =_ _N[1]_ _Nn=1[δ][x]n_ [and][ ˜]ν = _N[1]_ _Nn=1[δ][y]n_ [can be computed by sorting one-dimensional samples from]

empirical distributions, and evaluating the distances between sorted samples (Kolouri et al., 2019b):

P P

_N_ _k[1]_

1
_Wk(˜µ,ν˜)=_ _N_ _|xIx[n]_ _−yIy[n]|[k]_ _,_ (3)
 _n=1_ 

X

where N is the number of samples, Ix[n] and Iy[n] are the indices of sorted samples satisfying
_xIx[n] ≤_ _xIx[n+1] and yIy[n] ≤_ _yIy[n+1], respectively._

**Radon transform and generalized Radon transform: The Radon transform (Radon, 1917) maps a**
function f (·) _∈_ _L[1](R[d]) to the space of functions defined over spaces of hyperplanes in R[d]. The Radon_
transform of f (·) is defined by line integrals of f (·) along all possible hyperplanes in R[d]:

_Rf_ (t,θ)= (4)

R[d] _[f]_ [(][x][)][δ][(][t][−⟨][x,θ][⟩][)][dx,]

Z

where t ∈ R and θ ∈ S[d][−][1] represent the parameters of hyperplanes {x ∈ R[d] _| ⟨x,θ⟩_ = t}, δ(·) is the
one-dimensional Dirac delta function, and ⟨·,·⟩ refers to the Euclidean inner product.

By replacing the inner product ⟨x,θ⟩ in Equation (4) with β(x,θ), a specific family of functions named
as defining function in (Kolouri et al., 2019a), the generalized Radon transform (GRT) (Beylkin, 1984)
is defined as integrals of f (·) along hypersurfaces defined by {x _∈_ R[d] _| β(x,θ)=_ _t}:_

_Gf_ (t,θ)= (5)

R[d] _[f]_ [(][x][)][δ][(][t][−][β][(][x,θ][))][dx,]

Z

where t ∈ R, θ ∈ Ωθ and Ωθ is a compact set of all feasible θ, e.g. Ωθ = S[d][−][1] for β(x,θ) = ⟨x,θ⟩. In
particular, a function β(x,θ) defined on X ×(R[d]\{0}) with X ⊆ R[d] is called a defining function of
GRTs if it satisfies conditions H.1 – H.4 given in (Kolouri et al., 2019a).

For probability measures µ ∈ _Pk(R[d]), the Radon transform and the GRT can be employed as_
push-forward operators, and the generated push-forward measures Rµ = _R#µ, Gµ =_ _G#µ are defined_
as follows (Bonneel et al., 2015):


_Rf_ (t,θ)=


_Gf_ (t,θ)=


(6)
R[d] _[δ][(][t][−⟨][x,θ][⟩][)][dµ,]_

Z

(7)
R[d] _[δ][(][t][−][β][(][x,θ][))][dµ.]_


_µ(t,θ)=_
_R_

_µ(t,θ)=_
_G_


-----

Notably, the Radon transform is a linear bijection (Helgason, 1980), and the sufficient conditions for
GRTs to be bijective are provided in (Homan and Zhou, 2017).

**Sliced Wasserstein distance and generalized sliced Wasserstein distance: By applying the Radon**
transform to µ and ν to obtain multiple projections, the sliced Wasserstein distance (SWD) decomposes
the high-dimensional Wasserstein distance into multiple one-dimensional Wasserstein distances which
can be efficiently evaluated (Bonneel et al., 2015). The k-SWD between µ and ν is defined by:

[1]

_k_

SWDk(µ,ν)= _k_ _µ(_ _,θ),_ _ν(_ _,θ)_ _dθ_ _,_ (8)

_R_ _·_ _R_ _·_

ZS[d][−][1] _[W][ k]_ 

  

where the Radon transform R defined by Equation (6) is adopted as the measure push-forward operator.
The GSWD generalizes the idea of SWD by slicing distributions with hypersurfaces rather than
hyperplanes (Kolouri et al., 2019a). The GSWD is defined as:

[1]

_k_

GSWDk(µ,ν)= _Wk[k]_ _µ(_ _,θ),_ _ν(_ _,θ)_ _dθ_ _,_ (9)

Ωθ _G_ _·_ _G_ _·_

Z 

  

where the GRT G defined by Equation (7) is used as the measure push-forward operator. From
Equation (3), with L random projections and N samples, the SWD and GSWD between µ and ν can
be approximated by:


_L_ _N_ _k[1]_

1
SWDk(µ,ν) _xIxl_ [[][n][]][,θ][l][⟩−⟨][y][I]y[l] [[][n][]][,θ][l][⟩|][k] _,_
_≈_ _NL_ _|⟨_
 _l=1_ _n=1_ 

XL XN _k[1]_

1
GSWDk(µ,ν) _β(xIxl_ [[][n][]][,θ][l][)][−][β][(][y][I]y[l] [[][n][]][,θ][l][)][|][k]
_≈_ _NL_ _|_
 _l=1_ _n=1_ 

XX


(10)

(11)


where Ix[l] [and][ I]y[l] [are sequences consisting of the indices of sorted samples which satisfy][ ⟨][x]Ix[l] [[][n][]][,θ][l][⟩≤]
_xIxl_ [[][n][+1]][, θ][l][⟩][,][ ⟨][y][I]y[l] [[][n][]][, θ][l][⟩≤⟨][y][I]y[l] [[][n][+1]][, θ][l][⟩] [in the SWD, and][ β][(][x][I]x[l] [[][n][]][, θ][l][)][ ≤] _[β][(][x][I]x[l]_ [[][n][+1]][, θ][l][)][,]
_⟨_
_β(yIyl_ [[][n][]][,θ][l][)][ ≤] _[β][(][y][I]y[l]_ [[][n][+1]][,θ][l][)][ in the GSWD. The approximation error in estimating SWDs using]
Equation (10) is derived in (Nadjahi et al., 2020). It is proved in (Bonnotte, 2013) that the SWD is a valid
distance metric. The GSWD is a valid metric except for its neural network variant (Kolouri et al., 2019a).

3 AUGMENTED SLICED WASSERSTEIN DISTANCES

In this section, we propose a new distance metric called the augmented sliced Wasserstein distance
(ASWD), which embeds flexible nonlinear projections in its construction. We also provide an
implementation recipe for the ASWD.

3.1 SPATIAL RADON TRANSFORM AND AUGMENTED SLICED WASSERSTEIN DISTANCE

In the definitions of the SWD and GSWD, the Radon transform (Radon, 1917) and the generalized
Radon transform (GRT) (Beylkin, 1984) are used as the push-forward operator for projecting
distributions to a one-dimensional space. However, it is not straightforward to design defining
functions β(x,θ) for the GRT, since one needs to first check if β(x,θ) satisfies the conditions to be
a defining function (Kolouri et al., 2019a; Beylkin, 1984), and then whether the corresponding GRT
is bijective or not (Homan and Zhou, 2017). In practice, the assumption of the transform can be relaxed,
as Theorem 1 shows that as long as the transform is injective, the corresponding ASWD metric is a
valid distance metric.

To help us define the augmented sliced Wasserstein distance, we first introduce the spatial Radon
_transform which includes the Radon transform and the polynomial GRT as special cases (See Remark 5)._
**Definition 1. Given a measurable injective mapping g(·):** R[d] _→_ R[d][θ] _and a function f_ (·) _∈_ _L[1](R[d]),_
_the spatial Radon transform of f_ (·) is defined as

_Hf_ (t,θ;g)= (12)

R[d] _[f]_ [(][x][)][δ][(][t][−⟨][g][(][x][)][,θ][)][⟩][dx,]

Z

_where t_ _∈_ R and θ _∈_ S[d][θ][−][1] _are the parameters of hypersurfaces {x_ _∈_ R[d] _| ⟨g(x),θ⟩_ = _t}._


-----

Similar to the Radon transform and the GRT, the spatial Radon transform can also be used to generate
push-forward measure Hµ = _H#µ for µ_ _∈_ _Pk(R[d]) as in Equations (6) and (7):_

_µ(t,θ;g)=_ (13)
_H_

R[d] _[δ][(][t][−⟨][g][(][x][)][,θ][⟩][)][dµ.]_

Z

**Remark 1. Note that the spatial Radon transform can be interpreted as applying the vanilla Radon**
_transform to ˆµg, where ˆµg refers to the push-forward measure g#µ, i.e given a measurable injective_
_mapping g(·):_ R[d] _→_ R[d][θ] _, the spatial Radon transform defined by Equation (13) can be rewritten as:_

_Hµ(t,θ;g)=_ _Ex∼µ[δ(t−⟨g(x),θ⟩)],_
= _Exˆ_ _µˆg_ [[][δ][(][t][−⟨]x,θ[ˆ] )]
_∼_ _⟩_

= _δ(t_ _x,θˆ_ )dµˆg
_−⟨_ _⟩_
Z


= _Rµˆg_ [(][t,θ][)][.] (14)

_Hence the spatial Radon transform inherits the theoretical properties of the Radon transform and_
_incorporates nonlinear projections through g(·)._

In what follows, we use µ _≡_ _ν to denote probability measures µ,ν ∈_ _Pk(R[d]) that satisfy µ(X_ )= _ν(X_ )
for ∀X ⊆ R[d].
**Lemma 1. Given an injective mapping g(·):** R[d] _→_ R[d][θ] _and two probability measures µ,ν ∈_ _P_ (R[d]),
_for all t_ _∈_ R and θ _∈_ S[d][θ][−][1], Hµ(t,θ;g) _≡Hν(t,θ;g) if and only if µ_ _≡_ _ν, i.e. the spatial Radon transform_
_is an injection on Pk(R[d]). Moreover, the spatial Radon transform is an injection on Pk(R[d]) if and_
_only if the mapping g(·) is an injection._

See Appendix A for the proof of Lemma 1.

We now introduce the augmented sliced Wasserstein distance, by utilizing the spatial Radon transform
as the measure push-forward operator:
**Definition 2. Given two probability measures µ,ν ∈** _Pk(R[d]) and an injective mapping g(·):_ R[d] _→_ R[d][θ] _,_
_the augmented sliced Wasserstein distance (ASWD) of order k_ _∈_ [1,+∞) is defined as:

[1]

_k_

ASWDk(µ,ν;g)= _k_ _µ(_ _,θ;g),_ _ν(_ _,θ;g)_ _dθ_ _,_ (15)

_H_ _·_ _H_ _·_

ZS[dθ] _[−][1]_ _[W][ k]_ 

  

_where θ_ _∈_ S[d][θ][−][1], Wk is the k-Wasserstein distance defined by Equation (1), and H refers to the spatial
_Radon transform defined by Equation (13)._
**Remark 2. Following the connection between the spatial Radon transform and the vanilla Radon**
_transform as shown in Equation (14), the ASWD can be rewritten as:_

[1]

_k_

ASWDk(µ,ν;g)= _k_ _µˆg_ [(][·][,θ][)][,][R]νˆg [(][·][,θ][)] _dθ_

_R_

ZS[dθ] _[−][1]_ _[W][ k]_ 

= SWDk(ˆµg,νˆg ),  (16)


_where ˆµg and ˆνg are probability measures on R[d][θ]_ _which satisfy g(x) ∼_ _µˆg for x ∼_ _µ and g(y) ∼_ _νˆg_
_for y_ _∼_ _ν._
**Theorem 1. The augmented sliced Wasserstein distance (ASWD) of order k ∈** [1,+∞) defined by
_Equation (15) with a mapping g(·):_ R[d] _→_ R[d][θ] _is a metric on Pk(R[d]) if and only if g(·) is injective._

The proof of Theorem 1 is provided in Appendix C. Theorem 1 shows that the ASWD is a metric given
a fixed injective mapping g(·). In practical applications, the mapping g(·) needs to be optimized to
project samples onto discriminating hypersurfaces. We show in Corollary 1.1 that the ASWD between
_µ and ν with the optimized g(·) is also a metric under mild conditions._
**Corollary 1.1. The augmented sliced Wasserstein distance (ASWD) of order k ∈** [1,+∞) between
_two probability measures µ,ν ∈_ _Pk(R[d]) defined by Equation (15) with the optimal mapping_

_g[∗](_ )=argmax ASWDk(µ,ν;g) _L(µ,ν,λ;g)_ (17)

_·_ _g_ _{_ _−_ _}_

1 1

_is a metric on Pk(R[d]), where L(µ,ν,λ;g)=_ _λ(Exk∼µ_ _||g(x)||2[k]_ +Eyk∼ν _||g(y)||2[k]_ ) for λ _∈_ (1,+∞).

   


-----

The proof of Corollary 1.1 is provided in Appendix D.

**Remark 3.for the ASWD when Corollary g( 1.1·) is optimized for each pair of measures, as shown in Appendix shows that given measures µ1,µ2,µ3 ∈** _Pk(R[d]), the triangle inequality holds D. It is worth_
_noting that λ>_ 1 is a sufficient condition for the ASWD to be a metric – as further discussed in Remark 6,
0 _<λ_ _≤_ 1 can also lead to finite ||g(x)||2 in various scenarios, resulting in valid metrics. The discussion
_on the impact of λ on the performance of the ASWD in practice can be found in Appendix G.2._

3.2 NUMERICAL IMPLEMENTATION

We discuss in this section how to realize injective mapping g(·) with neural networks due to their
expressiveness and optimize it with gradient based methods.

**Injective neural networks: As stated in Lemma 1 and Theorem 1, the injectivity of g(·) is the sufficient**
_and necessary condition for the ASWD being a valid metric. Thus we need specific architecture designs_
on implementing g(·) by neural networks. One option is the family of invertible neural networks
(Behrmann et al., 2019; Karami et al., 2019), which are both injective and surjective. However, the
running cost of those models is usually much higher than that of vanilla neural networks. We propose
an alternative approach by concatenating the input x of an arbitrary neural network to its output φω(x):

_gω(x)=[x,φω(x)]._ (18)

It is trivial to show that gω(x) is injective, since different inputs will lead to different outputs. Although
embarrassingly simple, this idea of concatenating the input and output of neural networks has found
success in preserving information with dense blocks in the DenseNet (Huang et al., 2017), where the
input of each layer is injective to the output of all preceding layers.

**Optimization objective: We aim to slice distributions with maximally discriminating hypersurfaces**
between two distributions while avoiding the projected samples being arbitrarily large, so that the
projected samples between the compared distributions are finite and most dissimilar regarding the
ASWD, as shown in Figure 1. Similar ideas have been employed to identify important projection
directions (Deshpande et al., 2019; Kolouri et al., 2019a; Paty and Cuturi, 2019) or a discriminative
ground metric (Salimans et al., 2018) in optimal transport metrics. For the ASWD, the parameterized
injective neural network gω( ) is optimized by maximizing the following objective:

_·_

[1]

_k_

(µ,ν;gω,λ)= _k_ _µ(_ _,θ;gω),_ _ν(_ _,θ;gω)_ _dθ_ _L(µ,ν,λ;gω),_ (19)
_L_ _H_ _·_ _H_ _·_ _−_
ZS[dθ] _[−][1]_ _[W][ k]_ 

  1  1

where λ> 0 and the regularization term L(µ,ν,λ;gω)= _λ(Exk∼µ_ _||gω(x)||2[k]_ +Eyk∼ν _||gω(y)||2[k]_ ) on

the magnitude of the neural network’s output is used, otherwise the projections may be arbitrarily large.

   

**Remark 4. The regularization coefficient λ adjusts the introduced non-linearity in the evaluation of**
_the ASWD by controlling the norm of φω(_ ) in Equation (18). In particular, when λ _, the nonlinear_

_·_ _→∞_
_term φω(_ ) shrinks to 0. The intrinsic dimension of the augmented space, i.e. the number of non-zero

_·_
_dimensions in the augmented space, is hence explicitly controlled by the flexible choice of φω(_ ) and

_·_
_implicitly regularized by L(µ,ν,λ;gω)._

By plugging the optimized gω,λ[∗] [(][·][) = argmax]gω (L(µ, ν; gω, λ)) into Equation (15), we obtain the

empirical version of the ASWD. Pseudocode is provided in Appendix E.

4 RELATED WORK

Recent work on slice-based Wasserstein distances mainly focused on improving their projection
efficiency, leading to a reduced number of projections needed to capture the structure of data
distributions (Kolouri et al., 2019a; Nguyen et al., 2021). The GSWD proposes using nonlinear
projections to achieve this goal, and it has been proved to be a valid distance metric if and only if
they adopt injective GRTs, which only include the circular functions and a finite number of harmonic
polynomial functions with odd degrees as their feasible defining functions (Ehrenpreis, 2003). While
the GSWD has shown impressive performance in various applications (Kolouri et al., 2019a), its
defining function is restricted to the aforementioned limited class of candidates. In addition, the


-----

selection of defining function is usually task-dependent and needs domain knowledge, and the impact
on performance from different defining functions is still unclear.

To tackle those limitations, (Kolouri et al., 2019a) proposed the GSWD-NN, which directly takes
the outputs of a neural network as its projection results without using the standard Radon transform
or GRTs. However, this brings three side effects: 1) The number of projections, which equals the
number of nodes in the neural network’s output layer, is fixed, thus new neural networks are needed
if one wants to change the number of projections. 2) There is no random projections involved in the
GSWD-NN, as the projection results are determined by the inputs and weights of the neural network. 3)
The GSWD-NN is a pseudo-metric since it uses a vanilla neural network, rather than Radon transform
or GRTs, as its push-forward operator. Therefore, the GSWD-NN does not fit into the theoretical
framework of GSWD and does not inherit its geometric properties.

Another notable variant of the SWD is the distributional sliced Wasserstein distance (DSWD) (Nguyen
et al., 2021). By finding a distribution of projections that maximizes the expected distances over these
projections, the DSWD can slice distributions from multiple directions while having high projection
efficiency. Injective GRTs are also used to extend the DSWD to the distributional generalized sliced
Wasserstein distance (DGSWD) (Nguyen et al., 2021). Experiment results show that the DSWD and
the DGSWD have superior performance in generative modelling tasks (Nguyen et al., 2021). However,
neither the DSWD nor the DGSWD have solved the problem with the GSWD, i.e. they are still not
able to produce nonlinear projections adaptively.

Ourcontributiondiffersfrompreviousworkinthreeways: 1)TheASWDisdata-adaptive, i.e. thehypersurfaces where high-dimensional distributions are projected onto can be learned from data. This implies
one does not need to specify a defining function from limited choices. 2) Unlike GSWD-NN, the ASWD
takes a novel direction to incorporate neural networks into the framework of sliced-based Wasserstein
distanceswhilemaintainingthepropertiesofslicedWassersteindistances. 3)Previousworkonintroducing nonlinear projections into Radon transform either is restricted to only a few candidates of defining
functions (GRTs) or breaks the framework of Radon transforms (neural networks in GSWD-NN), in
contrast, the spatial Radon transform provides a novel way of defining nonlinear Radon-type transforms.

5 EXPERIMENTS

In this section, we describe the experiments that we have conducted to evaluate performance of the
proposed distance metric. The GSWD leads to the best performance in a sliced Wasserstein flow
problem reported in (Kolouri et al., 2019a) and the DSWD outperforms the compared methods in
the generative modeling task examined in (Nguyen et al., 2021) on CIFAR 10 (Krizhevsky, 2009),
CelebA (Liu et al., 2015), and MNIST (LeCun et al., 1998) datasets (Appendix H.2). Hence, we
compare performance of the ASWD with the state-of-the-art distance metrics in the same examples
and report results as below[1]. We provide additional experiment results in the appendices, including a
sliced Wasserstein autoencoder (SWAE) (Kolouri et al., 2019b) using the ASWD (Appendix I), image
color transferring (Appendix J) and sliced Wasserstein barycenters (Appendix K).

To examine the robustness of the ASWD, throughout the experiments, we adopt the injective network
architecture given in Equation (18) and set φω to be a single fully-connected layer neural network
whose output dimension equals its input dimension, with a ReLU layer as its activation function. The
order k is set to be 2 in all experiments.

5.1 SLICED WASSERSTEIN FLOWS

We first consider the problem of evolving a source distribution µ to a target distribution ν by minimizing
slice-based Wasserstein distances between µ and ν in the sliced Wasserstein flow task reported in
(Kolouri et al., 2019a).
_∂tµt =_ _−∇SWD(µt,ν),_ (20)
where µt refers to the updated source distribution at each iteration t. The SWD in Equation (20) can
be replaced by other sliced-Wasserstein distances to be evaluated. As in (Kolouri et al., 2019a), the
2-Wasserstein distance was used as the metric for evaluating performance of different distance metrics
in this task. The set of hyperparameter values used in this experiment can be found in Appendix F.1.

[1Code to reproduce experiment results is available at : https://github.com/xiongjiechen/ASWD.](https://github.com/xiongjiechen/ASWD)


-----

Figure 2: The first and third columns are target distributions. The second and fourth columns are log
2-Wasserstein distances between the target distribution and the source distribution. The horizontal
axis show the number of training iterations. Solid lines and shaded areas represent the average values
and 95% confidence intervals of log 2-Wasserstein distances over 50 runs. A more extensive set of
experimental results can be found in Appendix G.1.

Without loss of generality, we initialize µ0 to be the standard normal distribution N (0,I). We repeat
each experiment 50 times and record the 2-Wasserstein distance between µ and ν at every iteration. In
Figure 2, we plot the 2-Wasserstein distances between the source and target distributions as a function
of the training epochs and the 8-Gaussian, the Knot, the Moon, and the Swiss roll distributions are
respective target distributions. For clarity, Figure 2 displays the experiment results from the 6 best
performing distance metrics, including the ASWD, the DSWD, the SWD, the GSWD-NN 1, which
directly generates projections through a one layer MLP, as well as the GSWD with the polynomial
of degree 3, circular defining functions, out of the 12 distance metrics we compared.

We observe from Figure 2 that the ASWD not only leads to smaller 2-Wasserstein distances, but
also converges faster by achieving better results with fewer iterations than the other methods in these
four target distributions. A complete set of experimental results with 12 compared distance metrics
and 8 target distributions are included in Appendix G.1. The ASWD outperforms the compared
state-of-the-art sliced-based Wasserstein distance metrics with 7 out of the 8 target distributions except
for the 25-Gaussian. This is achieved through the simple injective network architecture given in
Equation (18) and a one layer fully-connected neural network with equal input and output dimensions
throughout the experiments. In addition, ablation study is conducted to study the effect of injective
neural networks, the regularization coefficient λ, the choice of the dimensionality dθ of the augmented
space, and the optimization of hypersurfaces in the ASWD. Details can be found in Appendix G.2.

5.2 GENERATIVE MODELING

In this experiment, we use sliced-based Wasserstein distances for a generative modeling task described
in (Nguyen et al., 2021). The task is to generate images using generative adversarial networks (GANs)
(Goodfellow et al., 2014) trained on either the CIFAR10 dataset (64×64 resolution) (Krizhevsky,
2009) or the CelebA dataset (64×64 resolution) (Liu et al., 2015). Denote the hidden layer and the
output layer of the discriminator by hψ and DΨ, and the generator by GΦ, we train GAN models with
the following objectives:

min SWD(hψ(pr),hψ(GΦ(pz))), (21)
Φ

max (22)
Ψ,ψ [E][x][∼][p][r] [[log(][D][Ψ][(][h][ψ][(][x][)))]+][E][z][∼][p][z] [[log(1][−][D][Ψ][(][h][ψ][(][G][Φ][(][z][))))]][,]

where pz is the prior of latent variable z and pr is the distribution of real data. The SWD in Equation
(21) is replaced by the ASWD and other variants of the SWD to compare their performance. The


-----

Figure 3: FID scores of generative models trained with different metrics on CIFAR10 (left) and CelebA
(right) datasets with L =1000 projections. The error bar represents the standard deviation of the FID
scores at the specified training epoch among 10 simulation runs.

Table 1: FID scores of generative models trained with different distance metrics. Smaller scores
indicate better image qualities. L is the number of projections, we run each experiment 10 times and
report the average values and standard errors of FID scores for CIFAR10 dataset and CELEBA dataset.
The running time per training iteration for one batch containing 512 samples is computed based on a
computer with an Intel (R) Xeon (R) Gold 5218 CPU 2.3 GHz and 16GB of RAM, and a RTX 6000
graphic card with 22GB memories.

CELEBA

|Col1|Col2|Col3|CIFAR10|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
|L|SWD (Bonneel et al., 2015)||GSWD (Kolouri et al., 2019a)||DSWD (Nguyen et al., 2021)||ASWD||
||FID|t (s/it)|FID|t (s/it)|FID|t (s/it)|FID|t (s/it)|
|10 100 1000|121.4±7.0 104.6±5.2 102.3±5.3|0.34 0.35 0.36|108.3±5.6 105.2±3.2 98.2±5.1|0.41 0.74 2.22|74.2 ± 3.1 66.5 ± 3.9 62.3 ± 5.7|0.55 0.57 1.30|65.7±3.2 62.5±1.9 59.3±3.2|0.58 0.60 1.38|


|10 100 1000|94.8±2.5 88.7±5.7 86.5±4.1|0.35 0.36 0.38|95.1±4.2 86.7±3.5 85.2±6.3|0.40 0.75 2.19|86.0 ± 1.4 76.1 ± 3.5 71.3±4.7|0.53 0.55 1.28|81.2±1.3 73.2±2.6 67.4±2.1|0.59 0.61 1.38|
|---|---|---|---|---|---|---|---|---|



GSWD with the polynomial defining function and the DGSWD is not included in this experiment due
to its excessively high computational cost in high-dimensional space. The Fréchet Inception Distance
(FID score) (Heusel et al., 2017) is used to assess the quality of generated images. More details on
the network structures and the parameter setup used in this experiment are available in Appendix F.2.

We run 200 and 100 training epochs to train the GAN models on the CIFAR10 and the CelebA dataset,
respectively. Each experiment is repeated for 10 times and results are reported in Table 1. With the same
number of projections and a similar computation cost, the ASWD leads to significantly improved FID
scores among all evaluated distances metrics on both datasets, which implies that images generated with
the ASWD are of higher qualities. Figure 3 plots the FID scores recorded during the training process.
The GAN model trained with the ASWD exhibits a faster convergence as it reaches smaller FID scores
with fewer epochs. Randomly selected samples of generated images are presented in Appendix H.1.

6 CONCLUSION

We proposed a novel variant of the sliced Wasserstein distance, namely the augmented sliced
Wasserstein distance (ASWD), which is flexible, has a high projection efficiency, and generalizes well.
The ASWD adaptively updates the hypersurfaces used to slice compared distributions by learning from
data. We proved that the ASWD is a valid distance metric and presented its numerical implementation.
We reported empirical performance of the ASWD over state-of-the-art sliced Wasserstein metrics
in various numerical experiments. We showed that ASWD with a simple injective neural network
architecture can lead to the smallest distance errors over the majority of datasets in a sliced Wasserstein
flow task and superior performance in generative modeling tasks involving GANs and VAEs. We
have also evaluated the applications of the ASWD in downstream tasks including color transferring
and Wasserstein barycenters. What remains to be explored includes the topological properties of the
ASWD. We leave this topic as an interesting future research direction.


-----

REFERENCES

J. Altschuler, J. Niles-Weed, and P. Rigollet. Near-linear time approximation algorithms for optimal
transport via Sinkhorn iteration. In Proc. Advances in Neural Information Processing Systems
_(NeurIPS), pages 1964–1974, Long Beach, California, USA, 2017._

M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In Proc.
_International Conference on Machine Learning (ICML), pages 214–223, Sydney, Australia, 2017._

J. Behrmann, W. Grathwohl, R. T. Q. Chen, D. Duvenaud, and J. Jacobsen. Invertible residual networks.
In Proc. International Conference on Machine Learning (ICML), pages 573–582, Long Beach,
California, USA, 2019.

E. Bernton, P. E. Jacob, M. Gerber, and C. P. Robert. On parameter estimation with the wasserstein
distance. Information and Inference: A Journal of the IMA, 8(4):657–676, 2019.

G. Beylkin. The inversion problem and applications of the generalized Radon transform.
_Communications on Pure and Applied Mathematics, 37(5):579–599, 1984._

N. Bonneel, J. Rabin, G. Peyré, and H. Pfister. Sliced and Radon Wasserstein barycenters of measures.
_Journal of Mathematical Imaging and Vision, 51(1):22–45, 2015._

N. Bonnotte. Unidimensional and evolution methods for optimal transportation. PhD thesis, Paris
11, 2013.

N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain adaptation.
_IEEE Transactions on Pattern Analysis and Machine Intelligence (IPAMI), 39(9):1853–1865, 2016._

M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Proc. Advances in Neu_ral Information Processing Systems (NeurIPS), pages 2292–2300, Lake Tahoe, Nevada, USA, 2013._

M. Cuturi and A. Doucet. Fast computation of Wasserstein barycenters. In Proc. International
_Conference on Machine Learning (ICML), pages 685–693, Beijing, China, 2014._

I. Deshpande, Y. Hu, R. Sun, A. Pyrros, et al. Max-sliced Wasserstein distance and its use for GANs. In
_Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 10648–10656,_
Long Beach, California, USA, 2019.

A. Dessein, N. Papadakis, and J. Rouas. Regularized optimal transport and the rot mover’s distance.
_The Journal of Machine Learning Research (JMLR), 19(1):590–642, 2018._

L. Ehrenpreis. The universality of the Radon transform, chapter 5, pages 299–363. Oxford University
Press, Oxford, UK, 2003.

S. Ferradans, N. Papadakis, G. Peyré, and J. Aujol. Regularized discrete optimal transport. SIAM
_Journal on Imaging Sciences, 7(3):1853–1882, 2014._

A. Genevay, L. Chizat, F. Bach, M. Cuturi, and G. Peyré. Sample complexity of Sinkhorn divergences.
In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 1574–1583,
Okinawa, Japan, 2019.

I. Goodfellow, A. J. Pouget, M. Mirza, B. Xu, F. D. Warde, et al. Generative adversarial nets. In
_Proc. Advances in Neural Information Processing Systems (NeurIPS), pages 2672–2680, Montréal,_
Canada, 2014.

I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of Wasserstein
GANs. In Proc. Advances in neural information processing systems (NeurIPS), pages 5767–5777,
Long Beach, California, USA, 2017.

S. Helgason. The Radon transform, volume 2. Basel, Switzerland: Springer, 1980.

M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a two
time-scale update rule converge to a local Nash equilibrium. In Proc. Advances in neural information
_processing systems (NeurIPS), pages 6626–6637, Long Beach, California, USA, 2017._


-----

A. Homan and H. Zhou. Injectivity and stability for a generic class of generalized Radon transforms.
_The Journal of Geometric Analysis, 27(2):1515–1529, 2017._

G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional
networks. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
4700–4708, Hawaii, USA, 2017.

M. Karami, D. Schuurmans, J. Sohl-Dickstein, L. Dinh, and D. Duckworth. Invertible convolutional
flow. In Proc. Advances in Neural Information Processing Systems (NeurIPS), pages 5636–5646,
Vancouver, Canada, 2019.

S. Kolouri, K. Nadjahi, U. Simsekli, R. Badeau, and G. Rohde. Generalized sliced Wasserstein
distances. In Proc. Advances in Neural Information Processing Systems (NeurIPS), pages 261–272,
Vancouver, Canada, 2019a.

S. Kolouri, P. E. Pope, C. E. Martin, and G. K. Rohde. Sliced-Wasserstein autoencoders. In Proc. In_ternational Conference on Learning Representations (ICLR), New Orleans, Louisiana, USA, 2019b._

A. Krizhevsky. Learning multiple layers of features from tiny images. Tech Report, 2009.

Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proc. IEEE
_International Conference on Computer Vision (ICCV), pages 3730–3738, Las Condes, Chile, 2015._

B. Muzellec and M. Cuturi. Subspace detours: Building transport plans that are optimal on subspace projections. Proc. Advances in Neural Information Processing Systems (NeurIPS), 32:6917–6928, 2019.

K. Nadjahi et al. Statistical and topological properties of sliced probability divergences. Proc.
_Advances in Neural Information Processing Systems (NeurIPS), 33, 2020._

K. Nguyen, N. Ho, T. Pham, and H. Bui. Distributional sliced-Wasserstein and applications to
generative modeling. In Proc. International Conference on Learning Representations (ICLR),
Vienna, Austria, 2021.

F. Paty and M. Cuturi. Subspace robust Wasserstein distances. In Proc. International Conference
_on Machine Learning (ICML), pages 5072–5081, Long Beach, California, USA, 2019._

G. Peyré and M. Cuturi. Computational optimal transport. Foundations and Trends® in Machine
_Learning, 11(5-6):355–607, 2019._

J. Radon. Uber die bestimmung von funktionen durch ihre integralwerte laengs gewisser
mannigfaltigkeiten. Ber. Verh. Saechs. Akad. Wiss. Leipzig Math. Phys. Kl., 69:262, 1917.

D. Rezende and S. Mohamed. Variational inference with normalizing flows. In International
_Conference on Machine Learning (ICML), pages 1530–1538, Lille, France, 2015._

T. Salimans, H. Zhang, A. Radford, and D. Metaxas. Improving GANs using optimal transport. In
_Proc. International Conference on Learning Representations (ICLR), Vancouver, Canada, 2018._

C. Villani. Optimal Transport: old and new, volume 338. Berlin, Germany: Springer Science &
Business Media, 2008.


-----

APPENDIX A PROOF OF THE LEMMA 1

We prove that the spatial Radon transform defined with a measurable mapping g(·): R[d] _→_ R[d][θ] is an
injection on Pk(R[d]) if and only if g(·) is injective. In the following contents, we use Pk(R[d]) to denote a
set of Borel probability measures with finiteffunctions1(·): _X → f1R(· and) : X f →2(·R): andX → f2R( that satisfy·) : X →_ R k that satisfy f-th moment on1(x)= _f2 f(x1() Rx for)[d] ̸=, and ∀ fx2∈( fxX1) ≡ for certain, andf2 is used to denote functions f1 ̸≡_ _f x2 ∈ is used to denoteX. In addition,_
for probability measures µ and ν, we use µ _≡_ _ν to denote µ,ν ∈_ _Pk(R[d]) that satisfy µ(X_ )= _ν(X_ ) for
_∀X ⊆_ R[d], and µ _̸≡_ _ν to denote µ,ν ∈_ _Pk(R[d]) that satisfy µ(X_ ) ≠ _ν(X_ ) for certain X ⊆ R[d].

_Proof. By using proof by contradiction, we first prove that if g(·) is injective, the corresponding_
spatial Radon transform is injective. If the spatial Radon transform defined with an injective mapping
_g(·) : R[d]_ _→_ R[d][θ] is not injective, there exist µ, ν ∈ _Pk(R[d]), µ ̸≡_ _ν, such that Hµ(t,θ;g) ≡Hν(t,θ;g)_
for ∀t _∈_ R and ∀θ _∈_ S[d][θ][−][1].

From Equation (14), for ∀t _∈_ R and ∀θ _∈_ S[d][θ][−][1], the spatial Radon transform of µ can be written as:

_Hµ(t,θ;g)=_ _Rµˆg_ [(][t,θ][)][,] (23)

_Hν(t,θ;g)=_ _Rνˆg_ [(][t,θ][)][,] (24)

where ˆµg and ˆνg refer to the push-forward measures g#µ and g#ν, respectively. From the assumption
_Hµ(t,θ;g) ≡Hν(t,θ;g) and Equations (23) and (24), we know Rµˆg_ [(][t,θ][)][ ≡R]νˆg [(][t,θ][)][ for][ ∀][t][ ∈] [R][ and]
_∀θ_ _∈_ S[d][θ][−][1], which implies ˆµg ≡ _νˆg since the Radon transform is injective._

Since g(·) is injective, for all measurable X ⊆ R[d], x _∈X if and only if ˆx_ = _g(x)_ _∈_ _g(X_ ), which implies
_P_ (x _∈X_ )= _P_ (ˆx _∈_ _g(X_ )), P (y _∈X_ )= _P_ (ˆy _∈_ _g(X_ )). Therefore,


_dµ,_ (25)

_dν._ (26)


_dµˆg =_

Zg(X )

_dνˆg =_

Zg(X )

Since ˆµg _νˆg, from Equations (25) and (26) we have:_
_≡_



_[dν][ for][ ∀X ⊆]_ [R][d][. Hence, for][ ∀X ⊆] [R][d][:]
_X_



_[dµ]_ [=]
_X_


_d(µ−ν)=0,_ (27)

ZX

which implies µ _ν, contradicting with the assumption µ_ _ν. Therefore, if_ _µ_ _ν, µ_ _ν. In_
addition, from the definition of the spatial Radon transform in Equation ( ≡ _̸≡_ 13), it is trivial to show that H _≡H_ _≡_
if µ ≡ _ν, Hµ(t,θ;g) ≡Hν(t,θ;g). Therefore, Hµ ≡Hν if and only if µ ≡_ _ν, i.e. the spatial Radon_
transform H defined with an injective mapping g(·): R[d] _→_ R[d][θ] is injective.

We now prove that if the spatial Radon transform defined with a mapping g(·) : R[d] _→_ R[d][θ] is
injective, g(·) must be injective. Again, we use proof by contradiction. If g(·) is not injective, there
existµ1(X x)=0,y0X ∈[δ][(]R[x][−][d] such that[x][0][)][dx][ and] x[ ν]0 ̸[1]=[(][Y] y[)=]0 andY g[δ][(]([y]x[−]0) =[y][0][)] g[dy]([, where]y0). For Dirac measures[ X] _[,][Y ⊆]_ [R][d][, we know] µ1[ µ] and[1][ ̸≡] ν[ν][1]1[ as] defined by[ x][0][ ̸][=] _[y][0][.]_

We define variablesR _x ∼_ _µ1 and y ∼_ _ν1R. Then for variables ˆx = g(x) ∼_ _µ2 and ˆy = g(y) ∼_ _ν2, where_
_µ2 and ν2 are push-forward measures g#µ1 and g#ν1, it is trivial to derive for ∀X_ _,Y ⊆_ R[d]


_δ(ˆx_ _g(x0))dx,ˆ_ (28)
_g(X_ ) _−_

_δ(ˆy_ _g(y0))dy,ˆ_ (29)

Zg(Y) _−_


_µ2(g(_ ))=
_X_

_ν2(g(_ ))=
_Y_


which implies µ2 ≡ _ν2 as g(x0)=_ _g(y0)._

From Equations (23), (24), (28) and (29), for ∀t _∈_ R and ∀θ _∈_ S[d][θ][−][1]:

_µ1_ (t,θ;g)= _µ2_ (t,θ),
_H_ _R_
= _ν2_ (t,θ),
_R_
= _ν1_ (t,θ;g), (30)
_H_


-----

contradicting with the assumption that the spatial Radon transform is injective. Therefore, if the spatial
Radon transform is injective, g(·) must be injective. We conclude that the spatial Radon transform
is injective if and only if the mapping g(·) is an injection on Pk(R[d]).


-----

APPENDIX B SPECIAL CASES OF SPATIAL RADON TRANSFORMS


**Remark 5. The spatial Radon transform degenerates to the vanilla Radon transform when the mapping**
_g(·) is an identity mapping. In addition, the spatial Radon transform is equivalent to the polynomial_
_GRT (Ehrenpreis, 2003) when g(x)=(x[α][1]_ _,...,x[α][dα]_ ), where α is multi-indices αi =(ηi,1,...,ηi,d) _∈_ N[d]

_satisfying |αi|_ = [P][d]j=1[η][i,j][ =] _[m][.][ m][ is the degree of the polynomial function,][ x][α][i][ =]_ [Q][d]j=1[x]j[η][i,j] _given an_
_input x_ =(x1,...,xd) _∈_ R[d], and dα is the number of all possible multi-indices αi that satisfies |αi| = _m._

We provide a proof for the claim in Remark 5.

_Proof. Given a probability measure µ_ _∈_ _P_ (R[d]), the spatial Radon transform of µ is defined as:

_µ(t,θ;g)=_ (31)
_H_

R[d] _[δ][(][t][−⟨][g][(][x][)][,θ][⟩][)][dµ,]_

Z

where t ∈ R and θ ∈ S[d][θ][−][1] are the parameters of hypersurfaces in R[d]. When the mapping g(·) is an
identity mapping, i.e. g(x) = x for ∀x ∈ R[d], the spatial Radon transform degenerates to the vanilla
Radon transform:


_µ(t,θ;g)=_
_H_

R[d] _[δ][(][t][−⟨][x,θ][⟩][)][dµ]_

Z

= _µ(t,θ)._ (32)
_R_


For GRTs, (Ehrenpreis, 2003) provides a class of injective GRTs named polynomial GRTs by adopting
homogeneous polynomial functions with an odd degree m as the defining function:

_dα_


_θix[α][i]_ )dµ,
_i=1_

X


_µ(t,θ)=_
_G_


R[d] _[δ][(][t][−]_


s.t. _αi_ = _m,_ (33)
_|_ _|_

where αi =(ηi,1,...,ηi,d) _∈_ N[d], |αi| = [P][d]j=1[η][i,j][,][ x][α][i][ =] [Q][d]j=1[x]j[η][i,j] for x =(x1,...,xd) _∈_ R[d], dα is the
number of all possible multi-indices αi that satisfies |αi| = _m, and θ_ =(θ1,...,θdα ) _∈_ S[d][α][−][1].

In spatial Radon transform, for ∀x _∈_ R[d], when the mapping g(·) is defined as:

_g(x)=(x[α][1]_ _,...,x[α][dα]_ ), (34)

the spatial Radon transform is equivalent to the polynomial GRT defined in Equation (33):


_µ(t,θ;g)=_
_H_


R[d] _[δ][(][t][−⟨][g][(][x][)][,θ][⟩][)][dµ]_

_dα_


_θix[α][i]_ )dµ. (35)
_i=1_

X


R[d] _[δ][(][t][−]_


-----

APPENDIX C PROOF OF THEOREM 1

We provide a proof that the ASWD defined with a mapping g(·): R[d] _→_ R[d][θ] is a metric on Pk(R[d]), if and
only if g(·) is injective. In what follows, we denote a set of Borel probability measures with finite k-th
moment on R[d] by Pk(R[d]), and use µ,ν ∈ _Pk(R[d]) to refer to two probability measures defined on R[d]._

_Proof. Symmetry: Since the k-Wasserstein distance is a metric thus symmetric (Villani, 2008):_


_Wk_ _µ(_ _,θ;g),_ _ν(_ _,θ;g)_ = _Wk_ _ν(_ _,θ;g),_ _µ(_ _,θ;g)_ _._ (36)
_H_ _·_ _H_ _·_ _H_ _·_ _H_ _·_
     


Therefore,



[1]

_k_

ASWDk(µ,ν;g)= _k_ _µ(_ _,θ;g),_ _ν(_ _,θ;g)_ _dθ_

_H_ _·_ _H_ _·_

ZS[dθ] _[−][1]_ _[W][ k] _   _k[1]_

= _k_ _ν(_ _,θ;g),_ _µ(_ _,θ;g)_ _dθ_ = ASWDk(ν,µ;g).

_H_ _·_ _H_ _·_

ZS[dθ] _[−][1]_ _[W][ k]_ 

  

**Triangle inequality: Given an injective mapping g(·) : R[d]** _→_ R[d][θ] and probability measures
the following inequality holds:µ1, µ2, µ3 ∈ _Pk(R[d]), since the k-Wasserstein distance satisfies the triangle inequality (Villani, 2008),_

[1]

_k_

ASWDk(µ1,µ3;g)= _k_ _µ1_ ( _,θ;g),_ _µ3_ ( _,θ;g)_ _dθ_

_H_ _·_ _H_ _·_

ZS[dθ] _[−][1]_ _[W][ k]_ 

  

_Wk(_ _µ1_ ( _,θ;g),_ _µ2_ ( _,θ;g))_

_≤_ _H_ _·_ _H_ _·_

S[dθ] _[−][1]_

Z   _k_ _k[1]_

+Wk( _µ2_ ( _,θ;g),_ _µ3_ ( _,θ;g))_ _dθ_
_H_ _·_ _H_ _·_

 _k[1]_

_k_ _µ1_ ( _,θ;g),_ _µ2_ ( _,θ;g)_ _dθ_

_≤_ _H_ _·_ _H_ _·_
ZS[dθ] _[−][1]_ _[W][ k] _   _k[1]_

+ _k_ _µ2_ ( _,θ;g),_ _µ3_ ( _,θ;g)_ _dθ_

_H_ _·_ _H_ _·_

ZS[dθ] _[−][1]_ _[W][ k]_ 

= ASWDk(µ1,µ2 ;g)+ASWDk(µ2,µ3;g),


where the second inequality is due to the Minkowski inequality in L[k](S[d][θ][−][1]).

**Identity of indiscernibles: Since Wk(µ,µ)=0 for ∀µ** _∈_ _Pk(R[d]),we have_

[1]

_k_

ASWDk(µ,µ;g)= _k_ _µ(_ _,θ;g),_ _µ(_ _,θ;g)_ _dθ_

_H_ _·_ _H_ _·_

ZS[dθ] _[−][1]_ _[W][ k]_ 

  


=0, (37)


for ∀µ ∈ _Pk(R[d]). Conversely, for ∀µ,ν ∈_ _Pk(R[d]), if ASWDk(µ,ν;g) = 0, from the definition of the_
ASWD:



[1]

_k_

ASWDk(µ,ν;g)= _k_ _µ(_ _,θ;g),_ _ν(_ _,θ;g)_ _dθ_

_H_ _·_ _H_ _·_

ZS[dθ] _[−][1]_ _[W][ k]_ 

  


=0, (38)


Due to the non-negativity of k-th Wasserstein distance as it is a metric on Pk(R[d]) and the continuity
of Wk(·,·) on Pk(R[d]) (Villani, 2008), Wk _Hµ(·,θ;g),Hν(·,θ;g)_ =0 holds for ∀θ _∈_ S[d][θ][−][1] if and only
if _µ(_ _,θ;g)_ _ν(_ _,θ;g). Again, given the spatial Radon transform is injective when g(_ ) is injective
_H_ _·_ _≡H_ _·_    _·_
(see the proof in Appendix A), _µ(_ _,θ;g)_ _ν(_ _,θ;g) implies µ_ _ν if g(_ ) is injective.
_H_ _·_ _≡H_ _·_ _≡_ _·_

In addition, if g(·) is not injective, the spatial Radon transform is not injective (see the proof in Appendix
A), then ∃µ, ν ∈ _Pk(R[d]), µ_ _̸≡_ _ν such that Hµ_ (·,θ;g) _≡Hν_ (·,θ;g), which implies ASWDk(µ,ν;g)=0
for µ _̸≡_ _ν. Therefore, the ASWD satisfies the identity of indiscernibles if and only if g(·) is injective._

**Non-negativity: The three axioms of a distance metric, i.e. symmetry, triangle inequality, and identity**
of indiscernibles imply the non-negativity of the ASWD. Since the Wasserstein distance is non-negative,


-----

for ∀µ,ν ∈ _Pk(R[d]), it can also be straightforwardly proved the ASWD between µ and ν is non-negative:_

[1]

_k_

ASWDk(µ,ν;g)= _k_ _µ(_ _,θ;g),_ _ν(_ _,θ;g)_ _dθ_

_H_ _·_ _H_ _·_

ZS[dθ] _[−][1]_ _[W][ k] _ _k[1]_  

_≥_ =0. (39)
ZS[dθ] _[−][1]_ [0][k][dθ]

Therefore, the ASWD is a metric on Pk(R[d]) if and only if g(·) is injective.


-----

APPENDIX D PROOF OF COROLLARY 1.1

We first introduce Lemma 2 to support the proof of Corollary 1.1.
**Lemma 2. For λ ∈** (1, +∞), the optimal mapping g[∗](·) defined by Equation (17) satisfies
_||g[∗](x)||2 <_ _∞_ _for ∀x_ _∈_ R[d] _∼_ _µ and ∀x_ _∈_ R[d] _∼_ _ν._

_Proof. Recall that in Equation (16) the ASWD can be rewritten as:_

ASWDk(µ,ν;g)= SWDk(ˆµg,νˆg)

[1]

_k_

= _k_ _µˆg_ [(][·][,θ][)][,][R]νˆg [(][·][,θ][)] _dθ_ _,_ (40)

_R_

ZS[dθ] _[−][1]_ _[W][ k]_ 

  

where transformed variables ˆx = g(x) ∼ _µˆg for x ∼_ _µ and ˆy = g(y) ∼_ _νˆg for y ∼_ _ν, respectively._
Combining the equation above with Equation (2):

[1]

_k_

ASWDk(µ,ν;g)= _k_ _µˆg_ [(][·][,θ][)][,][R]νˆg [(][·][,θ][)] _dθ_

_R_

ZS[dθ] _[−][1]_ _[W] 1[ k] _   _k[1]_

= ZS[dθ] _[−][1]_ Z 10 _|FP[−]θ[1]#ˆµg_ [(][z][)][−][F][ −]Pθ[1]#ˆνg [(][z][)][|][k][dzdθ]k  _k[1]_

_≤_ ZS[dθ] _[−][1]_ Z0 _|FR[−]θ[1](ˆµg)[(][z][)][|][+][|][F][ −]Rθ[1](ˆνg)[(][z][)][|]_ _dzdθ_ _,_ (41)

  

where # denotes the push forward operator, Pθ : _x_ _∈_ R[d][θ] _→⟨x,θ⟩∈_ R, and Rθ(ˆµg)= _Pθ#ˆµg, Rθ(ˆνg)=_
_Pθ#ˆνg refer to one-dimensional measures obtained by slicing ˆµg, ˆνg with a unit vector θ, FR[−]θ[1](ˆµg)_ [and]

_FR[−]θ[1](ˆνg)_ [are inverse cumulative distribution functions (CDFs) of][ R][θ][(ˆ]µg) and Rθ(ˆνg), respectively.

By repeatedly applying the Minkowski’s inequality to Equation (41), we obtain the following
inequalities:


1 _k_ _k[1]_

ASWDk(µ,ν;g) _≤_ ZS[dθ] _[−][1]_ Z0   1|FR[−]θ[1](ˆµg)[(][z][)][|][+][|][F][ −]Rθ[1](ˆνk[1]g)[(][z][)][|] 1dzdθ _k[1]_ _k[1]_

_≤_ ZS[dθ] _[−][1]_ Z 1 0 _|FR[−]θ[1](ˆµg)[(][z][)][|][k][dz]k[1]_ +Z0 _|FR 1[−]θ[1](ˆνg)[(][z][)][|][k][dz]_ [][k]dθk[1]

_≤_ ZS[dθ] _[−][1]_ Z0 _|FR[−]θ[1](ˆµg)[(][z][)][|][k][dzdθ]_ +ZS[dθ] _[−][1]_ Z0 _|FR[−]θ[1](ˆνg)[(][z][)][|][k][dzdθ]_ _._


(42)


Let s = _x,θˆ_, then z = _FRθ(ˆµg)[(][s][)][,][ dz][ =]_ _[dF]Rθ(ˆµg)[(][s][)][:]_
_⟨_ _⟩_
1

0 _|FR[−]θ[1](ˆµg)[(][z][)][|][k][dz][ =]_

Z Z


_s_ _dFRθ(ˆµg)[(][s][)]_
_|_ _|[k]_


= _x,θˆ_ _dµˆg_

_|⟨_ _⟩|[k]_
R[dθ]

Z

= (43)

R[d] _[|⟨][g][(][x][)][,θ][⟩|][k][dµ,]_

Z

where the last two equations are obtained through the definitions of the push-forward operators.
Therefore, the following inequalities hold:

1 _k[1]_ 1 _k[1]_

ASWDk(µ,ν;g) _≤_ ZS[dθ] _[−][1]_ Z0 _|FR[−]θ[1](ˆµg)[(][z][)][|][k][dzdθ][1]_ +ZS[dθ] _[−][1]_ Z0 _|FR[−]θ[1](ˆνg)[(][z][)][|][k][dzdθ][1]_ 

_k_ _k_

= +
Z1S[dθ] _[−][1]_ ZR[d] _[|⟨][g][(][x][)][,θ][⟩|]1_ _[k][dµdθ]_ ZS[dθ] _[−][1]_ ZR[d] _[|⟨][g][(][y][)][,θ][⟩|][k][dνdθ]_

_≤_ Exk∼µ _||g(x)||2[k]_ +Eyk∼ν _||g(y)||2[k]_ _._ (44)

   


-----

Then we obtain the following inequalities for the optimization objective:

ASWDk(µ,ν;g) _L(µ,ν,λ;g)_

1 _−_ 1 1 1

_≤_ Exk∼µ _||g1(x)||2[k]_ +Eyk∼ν _||g1(y)||2[k]_ _−λ(Exk∼µ_ _||g(x)||2[k]_ +Eyk∼ν _||g(y)||2[k]_ )

_k_ _k_

= 1−λ  Ex∼µ _||g(x)||2[k]_ + Ey∼ν _||g(y)||2[k]_ _._     (45)
       

When we setthe optimization objective approaches negative infinity, implying λ ∈ (1,+∞), if ∃x ∈ R[d] _∼_ _µ or y ∈_ R[d] _∼_ _ν such that || gg(·()x is not the optimal mapping.)||2 →∞_ or ||g(y)||2 →∞,
Therefore, by adopting Equation (17) as the optimization objective, the optimal mapping g[∗](·) satisfies
_||g[∗](x)||2 <_ _∞_ for ∀x _∈_ R[d] _∼_ _µ and ∀x_ _∈_ R[d] _∼_ _ν._


**Remark 6. It is worth noting that λ> 1 in Corollary 1.1 is a sufficient condition for the supremum**
_of the optimization objective to be non-positive and ||g[∗](x)||2 <_ _∞_ _for ∀x_ _∈_ R[d] _∼_ _µ and ∀x_ _∈_ R[d] _∼_ _ν._
0 < λ ≤ 1 can also lead to finite ||g(x)||2 in various scenarios. Specifically, the upper bound of the
_optimization objective given in Equation (44) is obtained by applying:_

_g(x),θ_ = _g(x)_ 2 cos(α) _g(x)_ 2, (46)
_|⟨_ _⟩|_ _||_ _||_ _|_ _|≤||_ _||_

_where α is the angle between θ and g(x). In high-dimensional spaces, Equation (46) gives a very loose_
_bound since in high-dimensional spaces the majority of sampled θ would be nearly orthogonal to g(x)_
_and cos(α) is nearly zero with high probability (Kolouri et al., 2019a). Empirically we found that across_
_all the experiment results, λ in a candidate set of {0.01,0.05,0.1,0.5,1,10,100} all lead to finite g[∗](·)._

We now prove Corollary 1.1, i.e ASWDk(µ,ν;g[∗]) is a metric on Pk(R[d]), where g[∗](·) is the optimal
mapping defined by Equation (17) for λ _∈_ (1,+∞).

_Proof. Symmetry: Since the k-Wasserstein distance is a metric thus symmetric (Villani, 2008):_

_Wk_ _µ(_ _,θ;g[∗]),_ _ν(_ _,θ;g[∗])_ = _Wk_ _ν(_ _,θ;g[∗]),_ _µ(_ _,θ;g[∗])_ _._ (47)
_H_ _·_ _H_ _·_ _H_ _·_ _H_ _·_

Therefore,      



[1]

_k_

ASWDk(µ,ν;g[∗])= _k_ _µ(_ _,θ;g[∗]),_ _ν(_ _,θ;g[∗])_ _dθ_

_H_ _·_ _H_ _·_

ZS[dθ] _[−][1]_ _[W][ k] _   _k[1]_

= _k_ _ν(_ _,θ;g[∗]),_ _µ(_ _,θ;g[∗])_ _dθ_

_H_ _·_ _H_ _·_

ZS[dθ] _[−][1]_ _[W][ k]_ 

  


= ASWDk(ν,µ;g[∗]).


**Triangle inequality: From Lemma 2, when λ ∈** (1, +∞), the optimal mapping g[∗](·) satisfies
_||g[∗](x)||2 <_ _∞_ for ∀x _∈_ R[d] _∼_ _µ and ∀x_ _∈_ R[d] _∼_ _ν, hence ASWDk(ν,µ;g[∗]) is finite due to Equation (16)._
We then prove that ASWDk(ν,µ;g[∗]) satisfies the triangle inequality.

Denote by g1[∗][,][ g]2[∗][, and][ g]3[∗] [optimal mappings that result in the supremum of Equation (][19][) between]
_µ1 and µ2, µ1 and µ3, µ2 and µ3, respectively, since ASWDk(µ1,µ2;g1[∗][)][,][ ASWD][k][(][µ][1][,µ][3][;][g]2[∗][)][, and]_
ASWDk(µ2,µ3;g3[∗][)][ are finite, the following equations hold:]

ASWDk(µ1,µ2;g1[∗][)] _[≤]_ [ASWD][k][(][µ][1][,µ][3][;][g]1[∗][)+][ASWD][k][(][µ][2][,µ][3][;][g]1[∗][)] (48)
_≤_ supg _g_ (49)

_[{][ASWD][k][(][µ][1][,µ][3][;][g][)][}][+sup][{][ASWD][k][(][µ][2][,µ][3][;][g][)][}]_

= ASWDk(µ1,µ3;g2[∗][)+][ASWD][k][(][µ][2][,µ][3][;][g]3[∗][)][,] (50)

where the first inequality are from the metric property of the ASWD.


**Identity of indiscernibles: Since Wk(µ,µ)=0 for ∀µ** _∈_ _Pk(R[d]),we have_

[1]

_k_

ASWDk(µ,µ;g[∗])= _k_ _µ(_ _,θ;g[∗]),_ _µ(_ _,θ;g[∗])_ _dθ_

_H_ _·_ _H_ _·_

ZS[dθ] _[−][1]_ _[W][ k]_ 

  


=0, (51)


-----

for ∀µ _∈_ _Pk(R[d]). Conversely, for ∀µ,ν ∈_ _Pk(R[d]), if ASWDk(µ,ν;g[∗])=0, from Equation (15):_

[1]

_k_

ASWDk(µ,ν;g[∗])= _k_ _µ(_ _,θ;g[∗]),_ _ν(_ _,θ;g[∗])_ _dθ_ =0. (52)

_H_ _·_ _H_ _·_

ZS[dθ] _[−][1]_ _[W][ k]_ 

  

Due to the non-negativity of k-th Wasserstein distance as it is a metric on Pk(R[d]) and the continuity
of Wk(·,·) on Pk(R[d]) (Villani, 2008), Wk _Hµ(·,θ;g[∗]), Hν(·,θ;g[∗])_ =0 holds for ∀θ _∈_ S[d][θ][−][1], which
implies _µ(_ _,θ;g[∗])_ _ν(_ _,θ;g[∗]) for_ _θ_ S[d][θ][−][1]. Therefore, given the spatial Radon transform is
_H_ _·_ _≡H_ _·_ _∀_ _∈ _ 
injective when g[∗]( ) is injective, _µ(_ _,θ;g[∗])_ _ν(_ _,θ;g[∗]) implies µ_ _ν._

_·_ _H_ _·_ _≡H_ _·_ _≡_

**Non-negativity: Since the Wasserstein distance is non-negative, for ∀µ,ν ∈** _Pk(R[d]), the ASWD_
defined with optimal mappings g(·) between µ and ν is also non-negative:

[1]

_k_

ASWDk(µ,ν;g[∗])= _k_ _µ(_ _,θ;g[∗]),_ _ν(_ _,θ;g[∗])_ _dθ_

_H_ _·_ _H_ _·_

ZS[dθ] _[−][1]_ _[W][ k] _ _k[1]_  

_≥_ =0. (53)
ZS[dθ] _[−][1]_ [0][k][dθ]

Therefore, the ASWD defined with the optimal mapping g[∗](·) is non-negative, symmetric, and satisfies
the triangle inequality and the identity of indiscernibles, i.e. the ASWD defined with optimal mappings
_g[∗](·) is also a metric._


-----

APPENDIX E PSEUDOCODE FOR THE EMPIRICAL VERSION OF THE ASWD

**Algorithm 1 The augmented sliced Wasserstein distance. All of the for loops can be parallelized.**

**Require:Require: Randomly initialized injective neural network Sets of samples {xn ∈** R[d]}n[N]=1[,][ {][y][n] _[∈]_ [R][d][}]n[N]=1 g[;] _ω(·):_ R[d] _→_ R[d][θ] ;
**Require: Number of projections L, hyperparameter λ, learning rate ϵ, number of iterations M** ;

1: Initialize D =0,Lλ =0,m =1;
2: while ω has not converged and m _≤_ _M do_
3: Draw a set of samples {θl}l[L]=1 [from][ ∈] [S][d][θ][−][1][;]

4: **for n** =1 to N do

5: Compute gω(xn) and gω(yn);

1 1

6: Calculate the regularization term Lλ _Lλ_ +λ ( _[||][g][ω][(]N[x][n][)][||]2[k]_ ) _k +(_ _[||][g][ω][(]N[y][n][)][||]2[k]_ ) _k_ ;

7: **end for** _←_

8: **for l** =1 to L do  []

9: Compute β(xn,θl)= _gω(xn),θl_, β(yn,θl)= _gω(yn),θl_ for each n;

10: Sort β(xn,θl) and β(⟨yn,θl) in ascending order s.t.⟩ _⟨_ _β(x⟩Ixl_ [[][n][]][,θ][l][)][ ≤] _[β][(][x][I]x[l]_ [[][n][+1]][,θ][l][)][ and]
_β(yIyl_ [[][n][]][,θ][l][)] _[≤]_ _[β][(][y][I]y[l]_ [[][n][+1]][,θ][l][)][;]

_N_ 1

11: Calculate the ASWD: D _D+(_ _L[1]_ _n=1[|][β][(][x][I]x[l]_ [[][n][]][,θ][l][)][−][β][(][y][I]y[l] [[][n][]][,θ][l][)][|][k][)] _k ;_
_←_

12: **end for**

P

13: _D_ _Lλ;_
_L←_ _−_

14: Update ω by gradient ascent ω _ω+ϵ_ _ω_ ;
_←_ _·∇_ _L_

15: Reset D =0,Lλ =0, update m _←_ _m+1;_

16: end while
17: Draw a set of samples {θl}l[L]=1 [from][ ∈] [S][d][θ][−][1][;]

18: for n =1 to N do
19: Compute gω(xn) and gω(yn);

20: end for
21: for l =1 to L do
22: Compute β(xn,θl)= _gω(xn),θl_, β(yn,θl)= _gω(yn),θl_ for each n;

23: Sort β(xn, θl) and β⟨(yn, θl) in ascending order s.t.⟩ _⟨_ _β⟩(xIxl_ [[][n][]][, θ][l][)][ ≤] _[β][(][x][I]x[l]_ [[][n][+1]][, θ][l][)][ and]
_β(yIyl_ [[][n][]][,θ][l][)] _[≤]_ _[β][(][y][I]y[l]_ [[][n][+1]][,θ][l][)][;]

_N_ 1

24: Calculate the ASWD: D _D+(_ _L[1]_ _n=1[|][β][(][x][I]x[l]_ [[][n][]][,θ][l][)][−][β][(][y][I]y[l] [[][n][]][,θ][l][)][|][k][)] _k ;_
_←_

25: end for

P

26: Output: Augmented sliced Wasserstein distance D.

In Algorithm 1, Equation (17) is used as the optimization objective, where the regularization term

1 1

_L(µ,ν,λ;gω)=_ _λ(Exk∼µ_ _||gω(x)||2[k]_ +Eyk∼ν _||gω(y)||2[k]_ ) is used. This particular choice of the regu
larization term facilitates the proofs to Lemma 2 and subsequently to Corollary 1.1 with details in Appendix D. In fact, we have also examined other types of regularization terms such as the    _L2 norm of the_
output of g(·), and empirically they produce similar numerical results as the current regularization term.


-----

APPENDIX F EXPERIMENTAL SETUPS

F.1 HYPERPARAMETERS IN THE SLICED WASSERSTEIN FLOW EXPERIMENT

We randomly generate 500 samples both for target distributions and source distributions. We initialize
the source distributions µ0 as standard normal distributions N (0,I), where I is a 2-dimensional
identity matrix. We update source distributions using Adam optimizer, and set the learning rate=0.002.
For all methods, we set the order k = 2. When testing the ASWD, the number of iterations M in
Algorithm 1 is set to 10.

In the sliced Wasserstein flow experiment the mapping g[∗](·) optimized by maximizing Equation (17)
was found to be finite for all values of λ in the set of {0.01, 0.05, 0.1, 0.5, 1, 10, 100}, which is a
sufficient condition for the ASWD to be a valid metric as shown in the proof of corollary 1.1 provided
in Appendix D. In addition, numerical results presented in Appendix G.2 indicate that empirical errors
in the experiment are not sensitive to the choice of λ in the candidate set {0.01, 0.05, 0.1, 0.5}. The
reported results in the main paper are produced with λ =0.1.

F.2 NETWORK ARCHITECTURE IN THE GENERATIVE MODELING EXPERIMENT

Denote a convolutional layer whose kernel size is s with C kernels by ConvC(s _s), and a_
_×_
fully-connected layer whose input and output layer havenetwork structure used in the generative modeling experiment is configured to be the same as described s1 and s2 neurons by FC(s1 × s2). The
in (Nguyen et al., 2021):

_hψ :(64×64×3)_ _→_ Conv64(4×4) _→_ LeakyReLU(0.2) _→_
Conv128(4 4) BatchNormalization LeakyReLU(0.2)
_×_ _→_ _→_ _→_
Conv256(4 4) BatchNormalization LeakyReLU(0.2)
_×_ _→_ _→_ _→_

Output
Conv512(4 4) BatchNormalization Tanh (512 4 4)
_×_ _→_ _→_ _−−−→_ _×_ _×_

Output
_DΨ :_ Conv1(4×4) _→_ Sigmoid _−−−→_ (1×1×1)

_GΦ :_ _z_ R[100] ConvTranspose512(4 4)
_∈_ _→_ _×_ _→_
BatchNormalization ReLU ConvTranspose256(4 4)
_→_ _→_ _×_ _→_
BatchNormalization ReLU ConvTranspose128(4 4)
_→_ _→_ _×_ _→_
BatchNormalization ReLU ConvTranspose64(4 4)
_→_ _→_ _×_ _→_
BatchNormalization ConvTranspose3(4 4) Tanh
_→_ _×_ _→_

Ouput
_−−−→_ (64×64×3)

Output
_φ_ : FC(8192×8192) _−−−→_ (8192)-dimensional vector

We train the models with the Adam optimizer, and set the batch size to 512. Following the setup
in (Nguyen et al., 2021), the learning rate is set to 0.0005 and beta=(0.5, 0.999) for both CIFAR10
dataset and CelebA dataset. For all methods, we set the order k to 2. For the ASWD, the number
of iterations M in Algorithm 1 is set to 5. The hyperparameter λ is set to 1.01 to guarantee that the
ASWD being a valid metric and introduce slightly larger regularization of the optimization objective
due to the small output values from the feature layer hψ.


-----

APPENDIX G ADDITIONAL
RESULTS IN THE SLICED WASSERSTEIN FLOW EXPERIMENT

G.1 FULL EXPERIMENTAL RESULTS ON THE SLICED WASSERSTEIN EXPERIMENT

Figure 4 shows the full experimental results on the sliced Wasserstein flow experiment.

Figure 4: Full experimental results on the sliced Wasserstein flow example. The first and third columns
are target distributions. The second and fourth columns are log 2-Wasserstein distances between the
target distributions and the source distributions. The horizontal axis shows the number of training
iterations. Solid lines and shaded areas represent the average values and 95% confidence intervals of
log 2-Wasserstein distances over 50 runs.


-----

G.2 ABLATION STUDY

**Impact of the injectivity and optimization of the mapping**

In this ablation study, we compare ASWDs constructed by different mappings to GSWDs with
different predefined defining functions, and investigate the effects of the optimization and injectivity
of the adopted mapping gω( ) used in the ASWDs. In what follows, “ASWD-vanilla" is used to denote

_·_
ASWDs that employ randomly initialized neural network φω( ) to parameterize the injective mapping

_·_
_gω(_ ) = [ _,φω(_ )], i.e. the mapping gω( ) is not optimized in the ASWD-vanilla and the results of

_·_ _·_ _·_ _·_
ASWD-vanilla reported in Figure 5 are obtained by slicing with random hypersurfaces. Furthermore,
the “ASWD-non-injective" refers to ASWDs that do not use the injectivity trick, i.e. the mapping
_gω(_ )= _φω(_ ) is not guaranteed to be injective. In addition, the “ASWD-vanilla-non-injective" adopts

_·_ _·_
both setups in the “ASWD-vanilla" and "ASWD-non-injective", resulting in a random non-injective
mapping gω( ). The reported experiment results in this ablation study is calculated over 50 runs, and

_·_
the neural network φω( ) is reinitialized randomly in each run.

_·_

From Figure 5, it can be observed that the ASWD-vanilla shows comparable performance to GSWDs
defined by polynomial and circular defining functions, which implies GSWDs with predefined defining
functions are as uninformative as slicing distributions with random hypersurfaces constructed by the
ASWD. In GSWDs, the hypersurfaces are predefined and cannot be optimized since they are determined
by the functional forms of the defining functions. On the contrary, we found that the optimization
of hypersurfaces in the ASWD framework can help improve the performance of the slice-based
Wasserstein distance. As in Figure 5, the ASWD and the ASWD-non-injective present significantly
better performance than methods that do not optimize their hypersurfaces (ASWD-vanilla, ASWDvanilla-non-injective, and GSWDs). In terms of the impact of the injectivity of the mapping gω, in this
experiment, the ASWD-vanilla exhibits smaller 2-Wasserstein distances than the ASWD-vanilla-noninjective in all tested distributions, and the ASWD leads to more stable training than the ASWD-noninjective. Therefore, theinjectivityofthemapping _gω(_ ) doesnotonlyguaranteetheASWDtobeavalid

_·_
distance metric as proved in Section 3, but also better empirical performance in this experiment setup.

**Impact of the regularization coefficient**

We also evaluated the sensitivity of performance of the ASWD with respect to the regularization
coefficient λ. The ASWD is evaluated with different values of λ and compared with other slice-based
Wasserstein metrics in this ablation study. The numerical results presented in Figure 6 indicates that
different values of λ in the candidate set {0.01, 0.05, 0.1, 0.5} lead similar performance of the ASWD,
i.e the performance of the ASWD is not sensitive to λ. Additionally, the ASWDs with different values
of λ in the candidate set outperform the other evaluated slice-based Wasserstein metrics.

We have also evaluated the performance of the ASWD when the range of λ is much larger than in
the candidate set. Specifically, as presented in Figure 7, when λ is set to be large values, e.g 10 or
100, the resulted ASWD leads to decreased performance on par with the SWD. This is consistent with
our expectation that excessive regularization will eliminate nonlinearity as discussed in Remark 2,
leading to similar performance with the SWD.

In addition, the effect of the regularization term on the performance of Max-GSWD-NN was also
investigated in this ablation study. The performance of the Max-GSWD-NN and the Max-GSWD-NN
trained with the regularization term used in the ASWD are compared in Figure 8. From the numerical
results presented in Figure 8, the Max-GSWD-NN 1 with regularization leads to performance similar
to the Max-GSWD-NN 1 without regularization, implying that the performance gap between the
ASWD and the Max-GSWD-NN is not due to the introduction of the regularization term.

**Choice of injective mapping**

We reported in Figure 9 the performance of the ASWD defined with other types of injective mappings
other than Equation (18). In particular, we examined two invertible mappings, including the planar
flow and radial flow (Rezende and Mohamed, 2015), as alternatives to the injective mapping defined by
Equation(18). ThenumericalresultspresentedinFigure 9 showthattheASWDdefinedwithplanarflow
and radial flow produced better performance than GSWD variants in most setups. They exhibit slightly
worse performance compared with the ASWD with injective mapping defined in Equation (18), possibly
due to the additional restriction in invertible mapping imposed by the planar flow and radial flow.

**Choice of the dimensionality dθ of the augmented space**


-----

To investigate how the dimensionality dθ of the augmented space affects the performance of the
ASWD, different choices of dθ are employed in the ASWD. Specifically, the injective network
architecture gω(x) = [x,φω(x)] : R[d] _→_ R[d][θ] given in Equation (18) is adopted and φω is set to be
single fully-connected neural networks whose output dimension equals {1, 2, 3, 4} times its input
dimension, i.e dθ = _{2d,3d,4d,5d}, respectively, where d is the dimensionality of x. The numerical_
results are presented in Figure 10, and it can be found that the ASWDs present similar results across
different choices of dθ. It can also be observed in Figure 10 that the ASWDs with different choices of
_dθ consistently produce better performance than the other evaluated slice-based Wasserstein metrics._

Figure 5: Ablation study on the impact from injective neural networks and the optimization of
hypersurfaces on the ASWD. ASWDs with different mappings are compared to GSWDs with different
defining functions. The first and third columns show target distributions. The second and fourth
columns plot log 2-Wasserstein distances between the target distributions and the source distributions.
In the second and fourth columns, the horizontal axis shows the number of training iterations. Solid
lines and shaded areas represent the average values and 95% confidence intervals of log 2-Wasserstein
distances over 50 runs.


-----

Figure 6: Ablation study on the impact of the regularization coefficient λ. The performance of the
ASWDs with different values of λ are compared with other slice-based Wasserstein metrics. The first
and third columns show target distributions. The second and fourth columns plot log 2-Wasserstein
distances between the target distributions and the source distributions. In the second and fourth columns,
the horizontal axis shows the number of training iterations. Solid lines and shaded areas represent the
average values and 95% confidence intervals of log 2-Wasserstein distances over 50 runs.


-----

Figure 7: Ablation study on the impact from large values of λ. The performance of the ASWDs with
large values of λ, e.g 10 and 100, are compared with the SWD. The first and third columns show target
distributions. The second and fourth columns plot log 2-Wasserstein distances between the target
distributions and the source distributions. In the second and fourth columns, the horizontal axis shows
the number of training iterations. Solid lines and shaded areas represent the average values and 95%
confidence intervals of log 2-Wasserstein distances over 50 runs.


-----

Figure 8: Ablation study on the impact from the regularization term on the performance of the MaxGSW-NN. The first and third columns show target distributions. The second and fourth columns plot
log 2-Wasserstein distances between the target distributions and the source distributions. In the second
and fourth columns, the horizontal axis shows the number of training iterations. Solid lines and shaded
areas represent the average values and 95% confidence intervals of log 2-Wasserstein distances over 50
runs.


-----

Figure 9: Ablation study on the impact from the choice of injective networks. The performance of the
ASWDs with different types of injective networks are compared with other slice-based Wasserstein
metrics. The first and third columns show target distributions. The second and fourth columns plot log
2-Wasserstein distances between the target distributions and the source distributions. In the second and
fourth columns, the horizontal axis shows the number of training iterations. Solid lines and shaded
areas represent the average values and 95% confidence intervals of log 2-Wasserstein distances over 50
runs.


-----

Figure 10: Ablation study on the choice of the dimensionality dθ of the augmented space. The performance of the ASWDs with different choices of dθ are compared with other slice-based Wasserstein
metrics. The first and third columns show target distributions. The second and fourth columns plot log
2-Wasserstein distances between the target distributions and the source distributions. In the second and
fourth columns, the horizontal axis shows the number of training iterations. Solid lines and shaded
areas represent the average values and 95% confidence intervals of log 2-Wasserstein distances over 50
runs.


-----

APPENDIX H ADDITIONAL
RESULTS IN THE GENERATIVE MODELING EXPERIMENT

H.1 SAMPLES OF GENERATED IMAGES OF CIFAR10 AND CELEBA DATASETS

(a) CelebA (L =10) (b) CelebA (L =100) (c) CelebA (L =1000)

(d) CIFAR10 (L =10) (e) CIFAR10 (L =100) (f) CIFAR10 (L =1000)

Figure 11: Visualized experimental results of the ASWD on CelebA and CIFAR10 dataset with 10,
100, 1000 projections. The first row shows randomly selected samples of generated CelebA images,
the second row shows randomly selected samples of generated CIFAR10 images.

H.2 EXPERIMENT RESULTS ON MNIST DATASET

In the generative modelling experiment on the MNIST dataset, we train a generator by minimizing
different slice-based Wasserstein metrics, including the ASWD, the DSWD, the GSWD (circular),
and the SWD. Denote by GΦ the generator, the training objective of the experiment can be formulated
as (Bernton et al., 2019):
min (54)
Φ [E][x][∼][p][r][,z][∼][p][z] [[][SWD][(][x,G][Φ][(][z][))]][,]

where pz and pr are the prior of latent variable z and the real data distribution, respectively. In other
words, the SWD, or other slice-based Wasserstein metrics, can be considered as a discriminator in this
framework. By replacing the SWD with the ASWD, the DSWD, and the GSWD, we compare the performance of learned generative models trained with different metrics. In this experiment, different methods
are compared using different number of projections L = _{10,1000}. The 2-Wasserstein distance and_
the SWD between generated images and real images are used as metrics for evaluating performances of
different generative models. The experiment results are presented in Figure 12. Quality of generated
**images and convergence rate: It can be observed from Figure 12 that the ASWD outperforms all the**
other methods regarding both the 2-Wasserstein distance and the SWD between generated and real
images. In particular, the generative model trained with the ASWD produces smaller 2-Wasserstein
distances within less iteration, which implies the generated images are of higher quality and the ASWD
leads to higher convergence rates of generative models. In addition, the ASWD shows that it is able to
generate higher quality images than the SWD and the GSWD with 1000 projections using only as less


-----

(a) (b)

Figure 12: Visualized experimental results of different slice-based Wasserstein metrics on the MNIST
dataset with 10, 1000 projections. (a) Comparison between the SWD, the GSWD, the DSWD, and the
ASWD using the 2-Wasserstein distance between fake and real images as the evaluation metric. (b)
Comparison between the SWD, the GSWD, the DSWD, and the ASWD using the SWD between fake
and real images as the evaluation metric.

as 10 projections. In other words, the ASWD has higher projection efficiency than the other slice-based
Wasserstein metrics. The ASWD also has the smallest SWD distance as shown in Figure 12. Although
the SWD converges slightly faster than the ASWD in terms of the SWD between fake and real images,
this is due to the training objective and the evaluated metric are the same for the SWD. Randomly
selected images generated by different slice-based Wasserstein metrics are presented in Figure 13.

**Computation cost of the ASWD: The execution time per mini-batch (512 samples) of different**
methods are compared in Figure 14a. We evaluate the SWD by varying the number of projections
in the set {10, 1000, 10000} and all the other methods in the set {10, 1000}. We found that although
the SWD requires much fewer computational time than the DSWD and the ASWD, the quality of
generated data is poor even when the number of projections L increases to 10000. The GSWD is
also computationally efficient when using a 10 projections, but it requires the highest execution time
and generates the highest 2-Wasserstein distance among all compared methods when the number
of projections increases to 1000. The huge difference in the execution time of the GSWD with 10
and 1000 projections is due to the GSWD needs to calculate distance matrices of shape N ×L, where
_N and L are the number of samples and projections respectively, which is more computationally_
expensive than calculating inner products when the number of projections L increases. The DSWD
requires a similar computational time as the ASWD in this example, while the ASWD generates higher
quality images in terms of 2-Wasserstein distances.

Besides, we have also evaluated the effect of batch size on the computation cost of different slice-based
Wasserstein metrics. Due to the out-of-memory error caused by the excessively high computation cost
of the GSWDs in high-dimensional space, the GSWD is not included in this comparison. Specifically,
the ASWD, the DSWD, and the SWD are compared in this experiment, and the computation time
of the evaluated methods with L = 10,1000 projections and N = {2[13],2[14],2[15],2[16]} samples are
reported in Figure 14b. From the results presented in Figure 14b, it can be observed that, similar to the
computational complexity of the DSWD and the SWD, the computational complexity of the ASWD
empirically tends to scale in O(N logN ).


-----

ASWD-10 SWD-10 DSWD-10 GSWD-10

ASWD-1000 SWD-1000 DSWD-1000 GSWD-1000

Figure 13: Randomly selected samples generated by different metrics, 10 and 1000 refer to the number
of projections.

(a) (b)

Figure 14: (a) The execution time of different methods and the 2-Wasserstein distance between the real
images and the fake images generated by their corresponding models. Each dot of the curve of SWD
corresponds to the performance of the SWD with the number of projections L = {10,1000,10000},
in sequence. Each dot of the other curves correspond to the performance of the other methods with
the number of projections L = _{10,1000}, in sequence. (b) Computation cost of calculating different_
metrics, the horizontal axis is the number of samples from the compared distributions, and the horizontal
axis is the averaged time consumption for calculating the distances. It can be found that the evaluated
metrics tend to scale in O(N logN ).


-----

APPENDIX I SLICED WASSERSTEIN AUTOENCODERS

We train an autoencoder using the framework proposed in (Kolouri et al., 2019b), where an encoder
and a decoder are jointly trained by minimizing the following objective:

min (55)
_φ,ψ_ [BCE][(][ψ][(][φ][(][x][))][,x][)+][L][1][(][ψ][(][φ][(][x][))][,x][)+][SWD][(][p][z][,φ][(][x][))][,]

where φ is the encoder, ψ is the decoder, pz is the prior distribution of latent variable, BCE(·,·) is
the binary cross entropy loss between reconstructed images and real images, and L1( _,_ ) is the L1

_·_ _·_
loss between reconstructed images and real images. We train this model using different slice-based
Wasserstein metrics, including the ASWD, the DSWD, the SWD, and the GSWD. Here we use the
ring distribution as the prior distribution as shown in Figure 16. We report the binary cross entropy
loss during test time and the 2-Wasserstein distance between prior and the encoded latent variable
_φ(x) in Figure 15. The slice-based Wasserstein metrics used as the third term in Equation (55) is also_
recorded at each iteration and presented in Figure 15 in order to analyze the factors that causes the
differences in the performance of models trained with different slice-based Wasserstein metrics.

It can be observed from the first two columns of Figure 15 that while the model trained with the
ASWD, SWD, and DSWD lead to similar 2-Wassertein distance between the encoded latent variable
distribution and the prior distribution, which implies that they have similar coverage of the prior
distribution as shown in Figure 16, the model trained with the ASWD converges slightly faster to
smaller binary cross entropy loss than the others. As shown in Figure 15(b) and 15(c), since the obtained
GSWDs are trivially small compared with the other metrics, models trained with GSWDs only focuses
on the reconstruction loss and therefore produce reconstructed images of higher qualities in terms of the
binary cross entropy. However, although models trained with the GSWD polynomial and the GSWD
circular lead to smaller binary cross entropy loss than the ASWD, their latent distributions present
very different data structures from the specified prior distribution, which can be problematic in certain
applications where the support of the latent distribution is required to be within a particular range.

Some MNIST images randomly generated by SWAEs trained with different metrics are given in Figure

17.

(a) (b) (c)

Figure 15: Convergence behavior of SWAEs trained with different slice-based Wasserstein metrics. (a)
The 2-Wasserstein distance between the prior distribution pz and the distribution of encoded feature
_φ(x). (b) The binary cross entropy loss between the reconstruction and real data. (c) The slice-based_
Wasserstein metric used to train the model.


-----

(a) ASWD latent space (b) DSWD latent space (c) SWD latent space

(d) GSWD (poly 3) latent space (e) GSWD (circular) latent (f) Prior distribution

(c) SWD latent space

1.00

0.75

0.50

0.25

0.00

0.25

0.50

0.75

1.00

1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00

(f) Prior distribution


Figure 16: Comparisons between the encoded latent space generated by different slice-based Wasserstein metrics.

(a) ASWD samples (b) DSWD samples (c) SWD samples

(d) GSWD (poly 3) samples (e) GSWD (circular) samples (f) MNIST samples

Figure 17: MNIST images randomly generated by SWAEs trained with different metrics.


-----

APPENDIX J COLOR TRANSFERRING

Color transferring can be formulated as an optimal transport problem (Bonneel et al., 2015; Radon,

1917). In this task, the color palette of a source image is transferred to that of a target image, while
keeping the content of source image unchanged. To achieve this, the optimal transport can be used
to find the alignment between image pixels by calculating the optimal mapping of color palettes. In
this experiment, instead of solving the optimal mapping in the original space, we first project the
distribution onto one-dimensional spaces and average the alignment between one-dimensional samples
as an approximation of the optimal mapping in the original space. After obtaining the approximation,
we replace pixels of the source image with the averaged corresponding pixels in the target image.
To reduce the computational cost, we utilize the approach proposed in (Muzellec and Cuturi, 2019),
where the K-means algorithm is used to cluster the pixels of both source and target images, and then
we implement color transfer for the quantized images whose pixels are consist of the centers of 3000
clusters rather than the original source and target images.

We present the results of color transferring in Figure 18. It can be observed that the ASWD and the
DSWD produce sharper images than the SWD and the GSWD (polynomial), we conjecture that is
because the ASWD and the DSWD can generate better alignment of pixels. The GSWD (circular)
tends to generate high contrast images, but sometimes produces images with low color diversity as
in Figure 18(a).The Max-SWD has the highest contrast among all methods, but this is due to it only
uses a single projection to obtain the transport mapping, thus there is no need to average different
pixels from the target image. A disadvantage of the Max-SWD is that the transferred images generated
by Max-SWD is not smooth enough and do not look realistic. The ASWD can generate smooth and
realistic images than the SWD and the Max-SWD, even when the number of projections is as small
as 10. Transferred images obtained by transferring colors from the source to target using standard
optimal transport maps is also presented in Figure 18 for reference (Ferradans et al., 2014).


-----

(a)

(b)

(c)

(d)

Figure 18: Top rows are source images and target images, lower rows show transferred images obtained by using different methods with different number of projections. Source and
[target images are from (Bonneel et al., 2015) and https://github.com/chia56028/](https://github.com/chia56028/Color-Transfer-between-Images)


-----

APPENDIX K SLICED WASSERSTEIN BARYCENTER

Sliced Wasserstein distances can also be applied in the barycenter calculation and shape interpolation (Bonneel et al., 2015). Here we compare barycenters produced by different slice-based
Wasserstein metrics, including the GSWD (circular and polynomial), the ASWD, the SWD, and the
DSWD. Specifically, we compute barycenters of different shapes consisting of point clouds, as shown
in Figure 19. Each object in Figure 19 corresponds to a specific barycenter with different weights.
The Wasserstein barycenters are also presented in Figure 19 for reference, which provide geometrically
meaningful barycenters at the expense of significantly higher computational cost .

weightsFormally, a sliced-Wasserstein barycenter of objects w =[w1,w2,···,wN ∈ R] is defined as: _µ = {µ1,µ2,···_ _,µN ∈_ _Pk(R[d])} assigned with_


Bar(µ,w)= argmin
_µ∈Pk(R[d])_


_wiSWD(µ,µi)._ (56)
_i=1_

X


In this experiment, we set N =3 and compute barycenters corresponding to different weights. The
results are given in Figure 19.

From Figure 19, it can be observed that the ASWD produces similar barycenters as that of the SWD,
which are sharper than the DSWD, and more meaningful than the GSWD (polynomial). The flexibility
of the injective neural networks g(·) and its optimization in the ASWD can be potentially combined
with specific requirements in particular tasks to generate calibrated barycenters - we leave this as a
future research direction.

(a) ASWD barycenters (b) GSWD (circular) barycenters (c) GSWD (polynomial) barycenters

(d) DSWD barycenters (e) SWD barycenters (f) Wasserstein barycenter

Figure 19: Sliced Wasserstein barycenters generated by the ASWD, the GSWD, the DSWD, and the
SWD, and the Wasserstein barycenter.


-----

