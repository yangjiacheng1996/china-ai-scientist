# KOOPMAN Q-LEARNING: OFFLINE REINFORCEMENT LEARNING VIA SYMMETRIES OF DYNAMICS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Offline reinforcement learning leverages large datasets to train policies without
interactions with the environment. The learned policies may then be deployed in
real-world settings where interactions are costly or dangerous. Current algorithms
over-fit to the training dataset and as a consequence perform poorly when deployed
to out-of-distribution generalizations of the environment. We aim to address these
limitations by learning a Koopman latent representation which allows us to infer
symmetries of the system’s underlying dynamic. The latter is then utilized to
extend the otherwise static offline dataset during training; this constitutes a novel
data augmentation framework which reflects the system’s dynamic and is thus to be
interpreted as an exploration of the environments phase space. To obtain the symmetries we employ Koopman theory in which nonlinear dynamics are represented
in terms of a linear operator acting on the space of measurement functions of the
system and thus symmetries of the dynamics may be inferred directly. We provide
novel theoretical results on the existence and nature of symmetries relevant for
control systems such as reinforcement learning settings. Moreover, we empirically
evaluate our method on several benchmark offline reinforcement learning tasks
and datasets including D4RL, Metaworld and Robosuite and find that by using
ourframework we consistently improve the state-of-the-art for Q-learning methods.

1 INTRODUCTION

The recent impressive advances in reinforcement learning (RL) range from robotics, to strategy games
and recommendation systems (Kalashnikov et al., 2018; Li et al., 2010). Reinforcement learning is
canonically regarded as an active learning process - also referred to as online RL - where the agent
interacts with the environment at each training run. In contrast, offline RL algorithms learn from large,
previously collected static datasets, and thus do not rely on environment interactions (Agarwal et al.,
2020a; Ernst et al., 2005; Fujimoto et al., 2019). Online data collection is performed by simulations
or by means of real world interactions e.g. robotics and in either scenario interactions maybe costly
and/or dangerous.

In principle offline datasets only need to be collected once which alleviates the before-mentioned
short-comings of costly online interactions. Offline datasets are typically collected using behavioral
policies for the specific task ranging from, random policies, or near-optimal policies to human
demonstrations. In particular, being able to leverage the latter is a major advantage of offline RL
over online approaches, and then the learned policies can be deployed or finetuned on the desired
environment. Offline RL has successfully been applied to learn agents that outperform the behavioral
policy used to collect the data (Kumar et al., 2020; Wu et al., 2019; Agarwal et al., 2020b; Ernst et al.,
2005). However algorithms admit major shortcomings in regard to over-fitting and overestimating the
true state-action values of the distribution. One solution was recently propsed by Sinha et al. (2021),
where they tested several data augmentation schemes to improve the performance and generalization
capabilities of the learned policies.

However, despite the recent progress, learning from offline demonstrations is a tedious endeavour as
the dataset typically does not cover the full state-action space. Moreover, offline RL algorithms per
definition do not admit the possibility for further environment exploration to refine their distributions
towards an optimal policy. It was argued previously that is basically impossible for an offline RL
agent to learn an optimal policy as the generalization to near data generically leads to compounding


-----

errors such as overestimation bias (Kumar et al., 2020). In this paper, we look at offline RL through
the lens of Koopman spectral theory in which nonlinear dynamics are represented in terms of a
linear operator acting on the space of measurement functions of the system. Through which the
representation the symmetries of the dynamics may be inferred directly, and can then be used to guide
data augmentation strategies see Figure 1. We further provide theoretical results on the existence on
nature of symmetries relevant for control systems such as reinforcement learning. More specifically,
we apply Koopman spectral theory by: first learning symmetries of the system’s underlying dynamic
in a self-supervised fashion from the static dataset, and second employing the latter to extend the
offline dataset at training time by out-of-distribution values. As this reflects the system’s dynamics
the additional data is to be interpreted as an exploration of the environment’s phase space.

Some prior works have explored symmetry of the state-action space in the context of Markov Decision
Processes (MDP’s) (Higgins et al., 2018; Balaraman & Andrew, 2004; van der Pol et al., 2020) since
many control tasks exhibit apparent symmetries e.g. the classic cart-pole task which is symmetric
across y-axis. However, the paradigm we introduce in this work is of a different nature entirely. The
distinction is twofold: first, the symmetries are learned in a self-supervised way and are in general
not apparent to the developer; second: we concern with symmetry transformation from state tuples
(st, st+1) (˜st, ˜st+1) which leave the action invariant inferred from the dynamics inherited by the
_→_
behavioral policy of the underlying offline data. In other words we seek to derive a neighbourhood
around a MDP tuple in the offline dataset in which the behavioral policy is likely to choose the same
action based on its dynamics in the environment. In practice the Koopman latent space representation
is learned in a self-supervised manner by training to predict the next state using a VAE model (Kingma
& Welling, 2013).

To summarize, in this paper, we propose Koopman Forward (Conservative) Q-learning (KFC): a
model-free Q-learning algorithm which uses the symmetries in the dynamics of the environment
to guide data augmentation strategies. We also provide thorough theoretical justifications for KFC.
Finally, we empirically test our approach on several challenging benchmark datasets from D4RL (Fu
et al., 2021), MetaWorld (Yu et al., 2019) and Robosuite (Zhu et al., 2020) and find that by using
KFC we can improve the state-of-the-art on most benchmark offline reinforcement learning tasks.

2 PRELIMINARIES AND BACKGROUND

2.1 OFFLINE RL & CONSERVATIVE Q-LEARNING

Reinforcement learning algorithms train policies to maximize the cumulative reward received by an
agent who interacts with an environment. Formally the setting is given by a Markov decision process
( _,_ _, ρ, r, γ), with state space_, action space, and ρ(st+1 _st, at) the transition density function_
_S_ _A_ _S_ _A_ _|_
from the current state and action to the next state. Moreover, γ is the discount factor and r(St) the
reward function. At any discrete time the agent chooses an action at according to its underlying
policy πθ(at _st) based on the information of the current state st_ where the policy is parametrized ∈A
by θ. We focus on the Actor-Critic methods for continuous control tasks in the following. In deep| _∈S_
RL the parameters θ are the weights in a deep neural network function approximation of the policy
or Actor as well as the state-action value function Q or Critic, respectively, and are optimized by
gradient decent. The agent i.e. the Actor-Critic is trained to maximize the expected γ-discounted
cumulative reward Eπ[[P][T]t=0 _[γ][t][ r][π][(][s][t][, a][t][)]][, with respect to the policy network i.e. its parameters][ θ][.]_
For notational simplicity we omit the explicit dependency of the latter in the remainder of this work.
Furthermore the state-action value function Q(st, at), returns the value of performing a given action
_at while being in the state st. The Q-function is trained by minimizing the so called Bellman error as_

2[i]
_Qˆi+1_ arg min _rt + γ_ _Q[ˆ]i(st+1, at+1)_ _Qi(st, at)_ _,_ (1)
_←_ _Q_ [E] _−_
h  

This is commonly referred to as the i[th] policy evaluation step where the hat denotes the target
Q-function. In offline RL one aims to learn an optimal policy for the given the dataset D =

_k=1,...,#data-points_ _st, at, rt, st+1_ _k_ [as the option for exploration of the MDP is not available. The]

policy is optimized to maximize the state-action value function via the policy improvement

S   

_πi+1_ arg max Est ˆQ _st, πi(at_ _st)_ _._ (2)
_←_ _π_ _∼D_ _|_
   


-----

Learn Koopman Latent Space Representation Apply Symmetry Shifts to Offline RL Data

Koopman operator

Derive Symmetries of Dynamics Input in Q-Learning Algorithm

randomly symmetry shifted

is symmetry commutes with states in  - ball policy optimisation step


**Figure 1: Overview of Koopman Q-learning. The states of the data point (st, st+1, at) are shifted along**
symmetry trajectories parametrized by ϵ1,2,3 for constant at. Symmetry transformations can be combined to
reach other specific subset of the ϵ-ball region.

Note that behavioural policies including sub-optimal or randomized ones may be used to generate the
static dataset D. In that case offline RL algorithms face difficulties in the learning process.
**CQL algorithm: CQL is built on top of a Soft-Actor Critic algorithm (SAC) (Haarnoja et al., 2018),**
which employs soft-policy iteration of a stochastic policy (Haarnoja et al., 2017). A policy entropy
regularization term is added to the policy improvement step in Eq. (2) as

_πi+1_ arg max Est _Qˆ_ _st, πi(at_ _st)_ _α log πi(at_ _st)_ _._ (3)
_←_ _π_ _∼D_ _|_ _−_ _|_

where α either is a fixed hyperparameter or may be chosen to be trainable. CQL reduces theh    i
overestimation of state-values - in particular those out-of distribution from D. It achieves this
by regularizing the Q-function in Eq. (1) by a term minimizing its values over out of distribution
randomly sampled action. In the following at+1 is given by the prediction of the policy πi(at+1|st+1)
and ˜α is a hyperparameter balancing the regulizer term. The policy optimisation step is given by

2[]

_Qˆi+1_ arg min E _rt + γ_ _Q[ˆ]i(st+1, at+1)_ _Qi(st, at)_ (4)
_←_ _Q_  _st,at,st+1∼D_  _−_ 

+ ˜α E log exp _Qi(st, a)_ E _Qi(st, a)_
_st_ _−_ _a_ _π(st)_
_∼D_ _a_ _∼_

h X     [i!]

2.2 KOOPMAN THEORY

Historically, the Koopman theoretic perspective of dynamical systems was introduced to describe the
evolution of measurements of Hamiltonian systems (Koopman, 1931; Mezi´c, 2005). The underlying
dynamic of most modern reinforcement learning tasks is of nonlinear nature, i.e. the agents actions
lead to changes of it state described by a complex non-linear dynamical system. In contrast to linear
systems which are completely characterized by their spectral decomposition non-linear systems lack
such a unified characterisation. The Koopman operator theoretic framework describes nonlinear
dynamics via a linear infinite-dimensional Koopman operator and thus inherits certain tools applicable
to linear control systems (Mauroy et al., 2020; Kaiser et al., 2021). In practice one aims to find
a finite-dimensional representation of the Koopman operator which is equivalent to obtaining a
coordinate transformations in which the nonlinear dynamics are approximately linear. A general
non-affine control system is governed by the system of non-linear ordinary differential equations
(ODEs) as
_s˙ = f_ (s, a) (5)
where s, is the n-dimensional state vector, a the m-dimensional action vector with (s, a) ∈S × A
the state-action-space. Moreover, ˙s = ∂ts is the time derivative, and f is some general non-linear - at
least C[1]-differentiable - vector valued function. For a discrete time system, Eq. (5) takes the form
_st+1 = F_ (st, at) (6)
where st denotes the state at time t where F is at least C[1]-differentiable vector valued function.

**Definition 1 (Koopman operator) Let K(S × A) be the (Banach) space of all measurement func-**
_tions (observables). Then the Koopman operator K : K(S × A) →_ K(S × A) is defined by
_g(st, at) = g_ _F_ (st, at), at+1 = g(st+1, at+1), _g_ K( ) (7)
_K_ _∀_ _∈_ _S × A_
_where g :_ R.
_S × A →_   


-----

Many systems can be modeled by a bilinearisation where the action enters the controlling equations 5
linearly as f (s, a) = f0(s) + _i_ _[f][i][(][s][)][a][i][ for][ f][i][, i][ = 0][, . . ., m][ i.e.][ C][1][-differentiable-vector valued]_
functions. In that case the action of the Koopman operator takes the simple form

_m_

[P][m]

_g(st+1) = K(a)g(st) =_ _K0 +_ _Kiai_ _g(st), ∀g ∈_ K(S), (8)

_i_

 X 

where K(S) is a (Banach) space of measurement functions K0, Ki decompose the Koopman operator
in a the free and forcing-term, respectively. Details on the existence of a representation as in
equation 8 are discussed in Goswami & Paley (2017). Associated with a Koopman operator is its
eigenspectrum, that is, the eigenvalues λ, and the corresponding eigenfunctions φλ(s, a), such that

[ _φλ](s, a) = λφλ(s, a). In practice one derives a finite set of observable ⃗g = (g1, . . ., gN_ ) in which
_K_
case the approximation to the Koopman operator admits a finite-dimensional matrix representation.
The N × _N matrix representing the Koopman operator may be diagonalized by a matrix U containing_
the eigen-vectors of K as columns. In which case the eigen-functions are derived by ⃗φ = U⃗g and one
infers from Eq. (5) that ˙φi = λiφi, for i = 1, . . ., N with eigenvalues λi=1,...,N . These ODEs
admit simple solutions for their time-evolution namely the exponential functions exp(λit)..

3 THE KOOPMAN FORWARD FRAMEWORK

We initiate our discussion with a focus on symmetries in dynamical control systems in Subsection 3.1
where we additionally present some theoretical results. We then proceed in Subsection 3.2 by
presenting the Koopman forward framework for Q-learning based on CQL. Moreover, we discuss the
algorithm as well as the Koopman Forward model’s deep neural network architecture.
**Overview: Our theoretical results culminate in Theorem 3.4 and 3.5, which provides a road-map**
on how specific symmetries of the dynamical control system are to be inferred from a VAE forward
prediction model. In particular, Theorem 3.4 guarantees that the procedure leads to symmetries of
the system (at least locally) and Theorem 3.5 that actual new data points can be derived by applying
symmetry transformations to existing ones. Practically the VAE model parametrized by a neural net
is trained data from one of many behavior policies and is thus learning an approximate dynamics.
The theoretical limitations are twofold, firstly the theorems only hold for dynamical systems with
differentiable state-transitions; secondly, we employ a Bilinearisation Ansatz for the Koopman
operator of the system. Practically many RL environments incorporate dynamics with discontinuous
“contact” events where the Bilinearisation Ansatz may not be applicable. However, empirically we
find that our approach nevertheless is successful for environments with “contact” events. The latter
does not affect the performance significantly (see Appendix D.1).

3.1 SYMMETRIES OF DYNAMICAL CONTROL SYSTEM

In general, a symmetry group Σ of the state space may be any subgroup of the group of isometrics
of the Euclidean space E[n]. We restrict oneself to an Euclidean state space, however in general one
may consider Riemannian manifolds, see e.g. Giannakis (2019). In particular, in our work we also
consider Σ invariant compact subsets of Euclidean space. Moreover, relevant for a system of ODEs
are local Lie symmetries as well as symmetries of ODEs.

**Definition 2 (Local Lie Group) A parametrized set of transformations σ[ϵ]** : S →S with s 7→ _s˜(s, ϵ)_
_for ϵ ∈_ (ϵlow, ϵhigh) where ϵlow < 0 < ϵhigh is a one-parameter local Lie group if

_1. σ[0]_ _is the identity map i.e for ϵ = 0 such that ˜s(s, 0) = s._

_2. σ[ϵ][1]_ _σ[ϵ][2]_ = σ[ϵ][2] _σ[ϵ][1]_ = σ[ϵ][1][+][ϵ][2] _for every_ _ϵ1_ _,_ _ϵ2_ 1.
_|_ _|_ _|_ _| ≪_

_3. ˜s(s, ϵ) admits a Taylor series expansion in ϵ, i.e. in a neighbourhood of s determined by_
_ϵ = 0 as ˜s(s, ϵ) = s + ϵ ζ(s) + O(ϵ[2])._

The points (1) and (2) in definition 2 imply the existence of an inverse element σ[ϵ][−][1] = σ[−][ϵ] for
_|ϵ| ≪_ 1. Moreover, note that a system of ODEs may be represented by the set of its solutions.

**Definition 3 (Symmetries of ODEs) A symmetry of a system of ODEs on a locally-smooth structure**
_(such as E[n]) is a locally-defined diffeomorphism that maps the set of all solutions to itself._


-----

**Definition 4 (Equivariant Dynamical System) Consider the dynamical system ˙s = f** (s) and let
Σ be a group acting on the state-space S. Then the system is called Σ-equivariant if f (σ · s) =
_σ · f_ (s), _for s ∈S,_ _∀σ ∈_ Σ. For a discrete time dynamical system st+1 = F (st) one defines
_equivariance analogously, namely if F_ (σ _st) = σ_ _F_ (st), for st _,_ _σ_ Σ.
_·_ _·_ _∈S_ _∀_ _∈_

Note that as a local Lie group may satisfy the group axioms for sufficiently small parameters values it
may not be a group on the entire set. And moreover not every diffeomorphism is also an isometry.
Let us start by introducing symmetries in the simple context of dynamical system without control
(Sinha et al., 2020) given by ˙s = f (s) where we use analog notations as in Eq. (5). The Koopman
operator of equivariant dynamical system is reviewed in the appendix A. Let us next turn to the case
relevant for RL, namely control systems. In the remainder of this section we focus on dynamical
systems given as in Eq. (6) and Eq. (8).

**Definition 5 (Action Equivariant Dynamical System) Let** Σ[¯] _be a group acting on the state-action-_
_space S × A of a general control system as in Eq. (6) such that it acts as the identity operation on A_
_i.e. σ · (st, at) = (σ|S · st, at), ∀σ ∈_ Σ[¯] _. Then the system is called_ Σ[¯] _-action-equivariant if_

_F_ (σ · (st, at)) = σ|S · F (st, at), for (st, at) ∈S × A, ∀σ ∈ Σ[¯] _._ (9)

**Lemma 3.1 The map ¯∗** : Σ[¯] _× K(S × A) →_ K(S × A) given by (σ¯∗g)(s, a) 7−→ _g(σ[−][1]_ _· s, a)_
_defines a group action on the Koopman space of observables K(S × A)._

**Theorem 3.2 A Koopman operator K of a** Σ[¯] _-action-equivariant system st+1 = F_ (st, at) satisfies

[σ¯( _g)](st, at) = [_ (σ¯g)](st, at) . (10)
_∗_ _K_ _K_ _∗_

_σIn particular, it is easy to see that the biliniarisation in Eq. (|S · fi(s),_ _∀i = 0, . . ., m. Let us thus turn our focus on the relevant case of a control system8) is Σ-action-equivariant if fi(σ|S · s) =_
_st+1 = F[˜](st, at) which admits a Koopman operator description as_

_g(st+1) =_ (at)g(st), for at _,_ _g_ K( ), (11)
_K_ _∈A_ _∀_ _∈_ _S_

where (a) _a_ is a family of operators with analytical dependence on a . Note that the
_{K_ _}_ _∈A_ _∈A_
bilinearisation in Eq. (8) is a special case of Eq. (11). Furthermore, let _U_ (a) _a_ be a family
_{_ _}_ _∈A_
of invertible operators s.t. U (a) : K(S) → F(S × A) is a mapping to the (Banach) space of
eigenfunctions F(S×A) with eigenfunctions φ(s, a) := U (a)g(s) which obeys U (a)K(a)U (a)[−][1] =
Λ(a), with Λ(a)φ(s, a) = λφ(a)φ(s, a) and where λφ(a) : A → R. The existence of such operators
on an infinite dimensional space requires the Koopman operator in Eq. (11) to be self-adjoint or a
finite-dimensional (approximate) matrix representation to be diagonalizable.[1]

**Lemma 3.3 The map** _ϕ[ˆ] : Σ[¯]_ K( ) _U_ (a) _a_ K( ) given by
_×_ _S_ _× {_ _}_ _∈A →_ _S_

(σaˆg)(s) _U_ (a) _σ¯(U_ (a)g) (s) = g(σ[−][1] _s),_ (12)
_∗_ _7−→_ _[−][1]_ _∗_ _·_
   []

_defines a group action on the Koopman space of observables K(S). Where ¯∗_ _is defined analog to_
_Lemma 3.1 but acting on F(S × A) instead of K(S × A) by (σ¯∗φ)(s, a) 7−→_ _φ(σ[−][1]_ _· s, a). We refer_
_to a control system in Eq. (11) admitting a_ _ϕ[ˆ]-symmetry as_ Σ[ˆ] _-action-equivariant._

**Theorem 3.4 Let st+1 = F[˜](st, at) be a** Σ[ˆ] _-action-equivariant control system with a symmetry action_
_as in Lemma 3.3 which furthermore admits a Koopman operator representation as_

_g(st+1) =_ (at)g(st), for at _,_ _g_ K( ) . (13)
_K_ _∈A_ _∀_ _∈_ _S_

_Then_
_σat_ ˆ (at)g (st) = (at) _σat_ ˆg (st) . (14)
_∗_ _K_ _K_ _∗_

_Moreover, a control system obeying equations_    _13 and_ _14 is Σ[ˆ]_ _-action-equivariant locally if_ _g[−][1]_ _exists_
_for a neighborhood of st, i.e. then σ_ _F[˜](st, at) = F[˜](σ_ (st, at)).
_·_ _·_

1See Appendix A for the details. We found that, in practice the Koopman operator leaned by the neural nets
is diagonalizable almost everywhere.


-----

Let us next provide theoretical statements on how data-points may be shifted by symmetry transformations of solutions of ODEs. To establish an easier connection to the next section we introduce the
notation Let E : S → K(S) and D : K(S) →S denote the C[1]-differentiable encoder and decoder to
and from the finite-dimensional Koopman space approximation, respectively, i.e. E _◦D = D◦E = id._

**Theorem 3.5 Let st+1 = F[˜](st, at) be a control system as in equation 13 and σat an operator**
_obeying equation 14. Then σa[ϵ]_ _t_ [:] _st, st+1, at_ _7−→_ _s˜t, ˜st+1, at_ _with_

_s˜t = D_ 1 + ϵ σa t ˆ∗E(st) , ˜st+1  = D 1 + _ϵ σat_ ˆ∗E(st+1) (15)

_is a one-parameter local Lie symmetry group of ODEs. In other words one can use a symmetry  _       
_transformation to shift both st as well as st+1 such that ˜st+1 = F[˜](˜st, at)._ [2]

3.2 KFC ALGORITHM


In the previous section we laid the theoretical foundation for the KFC-algorithm by providing a
raod-map on how to derive symmetries of dynamical control systems based on a Koopman latent
space representation. The goal is to generate new data points for the RL algorithm at training-time as
Eq. (15) in Theorem 3.5. The reward rt is not part of the symmetry shift process and will just remain
unchanged as an assertion.
**On the power of using symmetries: Let us emphasize the practical advantage of employing**
symmetries to generate augmented data points. It is evident from Theorem 3.5 that a symmetry
transformation shifts both st as well as st+1 which evades the necessity of forecasting states. Thus
the use of an inaccurate fore-cast model is avoided and the accuracy and generalisation capabilities
of the VAE are fully utilized. The magnitude of the induced shift is controlled by the parameter
_ϵ ≪_ 1 such that |s − _s˜| = O(ϵ) to limit out-of-distribution generalisation errors . Algorithmically_
the symmetry maps are derived in two distinct ways which we denote KFC and KFC++. The latter,
constitute a simple starting point to extract symmetries from our setup.[3]
**Limitations: Theorems 3.4 and 3.5 imply that if an operator commutes with the Koopman operator**
we can find a local symmetry group. However, global conditions may not be easily inferred. Moreover,
in practice one has that D ◦ _E ≈_ _id. Thus all assumptions of theorem 3.5 hold approximately._

**The Koopman forward model: The KFC algorithm requires pre-training of a Koopman forward**
model F : S × A →S which is closely related to a VAE architecture as

_D_ _E(st)_ = st if c = 0: VAE

(st, at) = (16)
_F_ _[c]_ D  0 + _i=1_ _E(st)_ = st+1 if c = 1: forward prediction

 _K_ _[K][i][ a][t,i]_

where both of E and   D are approximated by Multi-Layer-Perceptrons (MLP’s) and the bilinear[P][m]  
Koopman-space operator approximation are implemented by a single fully connected layer for
_i, i = 0, . . ., m, respectively. The model is trained on batches of the offline data-set tuples_
_K_
(st+1, st, at) and optimized via an additive loss-function of the VAE and the forward prediction part
of the model. We refer the reader to Appendix C for details.

We integrate our framework into a specific Q-learning algorithm (CQL). In practice the Koopman
latent space representation is N-dimensional (finite). The Koopman operator and the symmetry
generators admit matrix representations; matrix multiplication replaces the mapping ˆ∗. Following in
the footsteps of (Sinha et al., 2021) our approach leaves the policy improvement Eq. (3) unchanged
but modifies the policy optimisation step Eq. (4) as

2[]

_Qˆi+1 ←_ arg minQ  _st,at,sEt+1∼D_ rt + γ _Q[ˆ]iσa[ϵ]_ _t_  s˜t+1|st+1, at+1 _−_ _Qiσa[ϵ]_ _t_  s˜t|st, at

+ ˜α E log exp _Qi(st, a)_ E _Qi(st, a)_ (17)
_st_ _−_ _a_ _π(st)_
_∼D_ _a_ _∼_

h X     [i!]

2No assumptions on the Koopman operator are imposed. Moreover, note that the equivalent theorem holds
when ϵ σat _I=1_ _[ϵ][I][ σ]a[I]t_ [to be a local N-parameter Lie group.]

3More elaborate studies employing the extended literate on Koopman spectral analysis are desirable. See → [P]
appendix B for details on numerical errors.


-----

The state-space symmetry generating function σa[ϵ] [:][ S →S][ depends on the normally distributed]
random variables ϵ. Note that we only modify the Bellman error of Eq. (17) and leave the CQL
specific regulizer untouched. We study two distinct cases which differ on an algorithmic level[4]

**KFC σa[ϵ]** **t** [:][ s][ 7→] _s[˜] = D_ 1 + ϵσat _E(s)_ **KFC++ σa[ϵ]** **t** [:][ s][ 7→] _s[˜] = D_ 1 + σat (⃗ϵ) _E(s)_ _,_
  where σ _at_ (⃗ϵ) = Re _U_ (at) diag(ϵ1, . . ., ϵN ) U [−][1](at) _,_  (18)

and U (at) diagonalizes the Koopman operator with eigenvalues  _λi, i = 1, . . ., N i.e. the latter_
can be expressed as (at) = U (at)diag(λ1, . . ., λN )U (at). Note that we abuse our notation as
_K_ _[−][1]_
_E(s)_ _⃗g(s) = [g1(s), . . ., gN_ (s)] i.e. the encoder provides the Koopman space observables. From
_≡_
theorem 3.5 one infers that in order for Eq. (18) to be a local Lie symmetry group of the approximate
ODEs captured by the neural net Eq. (16) one needs σat to commute with the approximate Koopman
operator in Eq. (16).
For the KFC option in Eq. (18) the symmetry generator matrix σa is obtained by solving the equation
_σFor KFC++ we compute the eigen-vectors ofat · K(at) −K(at) · σat = 0 which may be accomplished by employing a Sylvester algorithm ( K(a) which then constitute the columns of U_ (a). Thussyl).
in particular one infers that [σat (⃗ϵ), (at)] = 0. The latter, is solved by construction of σa(⃗ϵ) which
_K_
commutes with the Koopman operator for all values of the random variables.[5] The advantage of KFC
is that it is computationally less expensive than KFC++, however it provides less freedom to explore
different symmetry directions than the latter. We employ our symmetry shift only with probability
_pK; otherwise we use a random normally distributed state shift._

4 EMPIRICAL EVALUATION

In this section, we will first experiment with the popular D4RL benchmark commonly used for offline
RL (Fu et al., 2021). The benchmark covers various different tasks such as locomotion tasks with
Mujoco Gym (Brockman et al., 2016), tasks that require hierarchical planning such as antmaze, and
other robotics tasks such as kitchen and adroit (Rajeswaran et al., 2017). Furthermore, similar to
S4RL (Sinha et al., 2021), we perform experiments on 6 different challenging robotics tasks from
MetaWorld (Yu et al., 2019) and RoboSuite (Zhu et al., 2020). We compare KFC to the baseline
CQL algorithm (Kumar et al., 2020), and two best performing augmentation variants from S4RL,
S4RL-N and S4RL-adv (Sinha et al., 2021). We use the exact same hyperparameters as proposed in
the respective papers. Furthermore, similar to S4RL, we build KFC on top of CQL (Kumar et al.,
2020) to ensure conservative Q-estimates during for policy evaluation.

4.1 D4RL BENCHMARKS

We present results in the benchmark D4RL test suite and report the normalized return in Table 1.
We see that both KFC and KFC++ consistently outperform both the baseline CQL and S4RL across
multiple tasks and data distributions. Outperforming S4RL-N and S4RL-adv on various different
types of environments suggests that KFC and KFC++ fundamentally improves the data augmentation
strategies discussed in S4RL. KFC-variants also improve the performance of learned agents on
challenging environments such as antmaze: which requires hierarchical planning, kitchen and adroit
tasks: which are sparse reward and have large action spaces. Similarly, KFC-variants also perform
well on difficult data distributions such as “medium-replay”: which is a collected by simply using
all the data that the policy encountered while training base SAC policy, and “-human” which is
collected using human demonstrations on robotic tasks which results in a non-Markovian behaviour
policy (more details can be found in the D4RL manuscript (Fu et al., 2021)). Furthermore, to our
knowledge, the results for KFC++ are state-of-the-art in policy learning from D4RL datasets for most
environments and data distributions.

We do note that S4RL outperforms KFC on the “-random” split of the data distributions, which is
expected as KFC depends on learning a simple dynamics model of the data to use to guide the data
augmentation strategy. Since the “-random” split consists of random actions in the environment, our
simple model is unable to learn a useful dynamics model. For ablation studies see appendix D.

4Eq. (18) holds for both st as well as st+1 thus we have deliberately dropped the subscript. Moreover, note
that in case (II) the symmetry transformations are of the form σat (⃗ϵ) = _I=1_ _[ϵ][I][ σ]a[I]t_ [.]

5[ _,_ ] denotes the commutator of two matrices. The Koopman operators eigenvalues and eignevectors

_·_ _·_ 
generically are C-valued. However, the definition in Eq. (18) ensures that[P] σa(⃗ϵ) is a R-valued matrix.


-----

**Table 1: We experiment with the full set of the D4RL tasks and report the mean normalized episodic returns**
over 5 random seeds using the same protocol as Fu et al. (2021). We compare against 3 competitive baselines
including CQL and the two best performing S4RL-data augmentation strategies. We see that KFC and KFC++
consistently outperforms the baselines. We use the baseline numbers reported in Sinha et al. (2021).

|Franka|kitchen-complete kitchen-partial|43.8 77.1 88.1 49.8 74.8 83.6|94.1 94.9 92.3 95.9|
|---|---|---|---|


**Domain** **Task Name** **CQL** **S4RL-(N** **)** **S4RL-(Adv)** **KFC** **KFC++**

antmaze-umaze 74.0 91.3 94.1 **96.9** **99.8**

antmaze-umaze-diverse 84.0 87.8 88.0 **91.2** **91.1**

antmaze-medium-play 61.2 61.9 61.6 60.0 **63.1**

AntMaze

antmaze-medium-diverse 53.7 78.1 82.3 87.1 **90.5**

antmaze-large-play 15.8 24.4 **25.1** 24.8 **25.6**

antmaze-large-diverse 14.9 27.0 26.2 **33.1** **34.0**

cheetah-random 35.4 **52.3** **53.9** 48.6 49.2

cheetah-medium 44.4 48.8 48.6 55.9 **59.1**

cheetah-medium-replay 42.0 51.4 51.7 **58.1** **58.9**

cheetah-medium-expert 62.4 **79.0** 78.1 **79.9** **79.8**

hopper-random **10.8** **10.8** **10.7** **10.4** **10.7**

hopper-medium 58.0 78.9 81.3 90.6 **94.2**

Gym

hopper-medium-replay 29.5 35.4 36.8 **48.6** **49.0**

hopper-medium-expert 111.0 113.5 117.9 121.0 **125.5**

walker-random 7.0 **24.9** **25.1** 19.1 17.6

walker-medium 79.2 93.6 93.1 102.1 **108.0**

walker-medium-replay 21.1 30.3 35.0 **48.0** 46.1

walker-medium-expert 98.7 112.2 107.1 114.0 **115.3**

pen-human 37.5 44.4 51.2 **61.3** 60.0

pen-cloned 39.2 57.1 58.2 **71.3** 68.4

hammer-human 4.4 5.9 6.3 7.0 **9.4**

hammer-cloned 2.1 2.7 2.9 3.0 **4.2**

Adroit

door-human 9.9 27.0 35.3 44.1 **46.1**

door-cloned 0.4 2.1 0.8 3.6 **5.6**

relocate-human **0.2** **0.2** **0.2** **0.2** **0.2**

relocate-cloned **-0.1** **-0.1** **-0.1** **-0.1** **-0.1**


**(a) MetaWorld Environments** **(b) RoboSuite Environments**

**Figure 2: Results on challenging dexterous robotics environments using data collected using a similar strategy**
as S4RL (Sinha et al., 2021). We report the % of goals that the agent is able to reach during evaluation, where
the goal is set by the environments. We see that KFC and KFC++ consistently outperforms both CQL and
**the two best performing S4RL variants.**

4.2 METAWORLD AND ROBOSUITE BENCHMARKS

To further test the ability of KFC, we perform additional experiments on challenging robotic tasks.
Following (Sinha et al., 2021), we perform additional experiments with 4 MetaWorld environments
(Yu et al., 2019) and 2 RoboSuite environments (Zhu et al., 2020). We followed the same method to


-----

collect the data as described in Appendix F of S4RL (Sinha et al., 2021), and report the mean percent
of goals reached, where the condition of reaching the goal is defined by the environment.

We report the results in Figure 2, where we see that using KFC to guide the data augmentation strategy
for a base CQL agent, we are able to learn an agent that performs significantly better. Furthermore,
we see that for more challenging tasks such as “push” and “door-close” in the MetaWorld, KFC++
outperforms the base CQL algorithm and the S4RL agent by a significant margin. These set of
experiments further highlight the ability of KFC to guide the data augmentation strategy.

5 RELATED WORKS

The use of data augmentation techniques in Q-learning has been discussed recently (Laskin et al.,
2020b;a; Sinha et al., 2021). In particular, our work shares strong parallels with (Sinha et al.,
2021). Our modification of the policy evaluation step of the CQL algorithm (Kumar et al., 2020) is
analogous to the one in Sinha et al. (2021). However, the latter randomly augments the data while
our augmentation framework is based on symmetry state shifts. Regarding the connection to world
models (Ha & Schmidhuber, 2018). Here a VAE is used to decode the state information while a
recurrent separate neural network predicts future states. Their latent representation is not of Koopman
type. Also no symmetries and data-augmentations are derived.

Algebraic symmetries of the state-action space in Markov Decision Processes (MDP) originate
(Balaraman & Andrew, 2004) an were discussed recently in the context of RL in (van der Pol et al.,
2020). Their goal is to preserve the essential algebraic homomorphism symmetry structure of the
original MDP while finding a more compact representation. The symmetry maps considered in our
work are more general and are utilized in a different way. Symmetry-based representation learning
(Higgins et al., 2018) refers to the study of symmetries of the environment manifested in the latent
representation. The symmetries in our case are derived form the Koopman operator not the latent
representation directly. In (Caselles-Dupré et al., 2019) the authors discuss representation learning of
symmetries (Higgins et al., 2018) allowing for interactions with the environment. A Forward-VAE
model which is similar to our Koopman-Forward VAE model is employed. However, our approach
is based on theoretical results providing a road-map to derive explicit symmetries of the dynamical
systems as well as their utilisation for state-shifts.

In (Sinha et al., 2020) the authors extend the Koopman operator from a local to a global description
using symmetries of the dynamics. They do not discuss action-equivariant dynamical control systems
nor data augmentation. In (Salova et al., 2019) the imprint of known symmetries on the blockdiagonal Koopman space representation for non-control dynamical systems is discussed. This is
close to the spirit of disentanglement (Higgins et al., 2018). Our results are on control setups and
deriving symmetries. On another front, the application of Koopman theory in control or reinforcement
learning has also been discussed recently. For example, Li et al. (2020) propose to use compositional
Koopman operators using graph neural networks to learn dynamics that can quickly adapt to new
environments of unknown physical parameters and produce control signals to achieve a specified
goal. Kaiser et al. (2021) discuss the use of Koopman eigenfunction as a transformation of the state
into a globally linear space where the classical control techniques is applicable. To the best of our
knowledge, this paper is the first to discuss Koopman latent space for data augmentation.

6 CONCLUSIONS

In this work we proposed a symmetry-based data augmentation technique derived from a Koopman
latent space representation. It enables a meaningful extension of offline RL datasets describing
dynamical systems, i.e. further "exploration" without additional environment interactions. The
approach is based on our theoretical results on symmetries of dynamical control systems and symmetry
shifts of data. Both hold for systems with differentiable state transitions and with a Bilinearisation
Ansatz for the Koopman operator. However, the empirical results show that the framework is
successfully applicable beyond those limitations. We empirically evaluated our method on several
benchmark offline reinforcement learning tasks D4RL, Metaworld and Robosuite and find that by
using our framework we consistently improve the state-of-the-art of Q-learning algorithms.


-----

REFERENCES

[Sylvester algorithm - scipy.org. URL https://docs.scipy.org/doc/scipy-0.15.1/](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.linalg.solve_sylvester.html)
[reference/generated/scipy.linalg.solve_sylvester.html.](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.linalg.solve_sylvester.html)

Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
reinforcement learning, 2020a.

Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
reinforcement learning. In International Conference on Machine Learning, 2020b.

Ravindran Balaraman and G. Barto Andrew. Approximate homomorphisms: A framework for nonexact minimization in markov decision processes. In In International Conference on Knowledge
_Based Computer Systems, 2004._

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.

Steven L. Brunton, Marko Budiši´c, Eurika Kaiser, and J. Nathan Kutz. Modern koopman theory for
dynamical systems, 2021.

Hugo Caselles-Dupré, Michael Garcia Ortiz, and David Filliat. Symmetry-based disentangled
representation learning requires interaction with environments. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information
_Processing Systems, volume 32. Curran Associates, Inc., 2019._

Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
_Journal of Machine Learning Research, 6(18):503–556, 2005._

Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning, 2021.

Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pp. 2052–2062. PMLR, 09–15 Jun 2019.

Dimitrios Giannakis. Data-driven spectral decomposition and forecasting of ergodic dynamical
systems. Applied and Computational Harmonic Analysis, 47(2):338–396, 2019. ISSN 1063[5203. doi: https://doi.org/10.1016/j.acha.2017.09.001. URL https://www.sciencedirect.](https://www.sciencedirect.com/science/article/pii/S1063520317300982)
[com/science/article/pii/S1063520317300982.](https://www.sciencedirect.com/science/article/pii/S1063520317300982)

Debdipta Goswami and Derek A. Paley. Global bilinearization and controllability of control-affine
nonlinear systems: A koopman spectral approach. 2017 IEEE 56th Annual Conference on Decision
_and Control (CDC), pp. 6107–6112, 2017._

David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso[ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/](https://proceedings.neurips.cc/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf)
[2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf.](https://proceedings.neurips.cc/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf)

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
_International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning_
_Research, pp. 1352–1361. PMLR, 06–11 Aug 2017._

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pp. 1861–1870. PMLR, 10–15 Jul 2018.

Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and
Alexander Lerchner. Towards a definition of disentangled representations, 2018.


-----

Eurika Kaiser, J. Nathan Kutz, and Steven L. Brunton. Data-driven discovery of Koopman eigenfunctions for control. Machine Learning: Science and Technology, 2:035023, 2021.

Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. Scalable
deep reinforcement learning for vision-based robotic manipulation. In Aude Billard, Anca Dragan,
Jan Peters, and Jun Morimoto (eds.), Proceedings of The 2nd Conference on Robot Learning,
volume 87 of Proceedings of Machine Learning Research, pp. 651–673. PMLR, 29–31 Oct 2018.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint_
_arXiv:1312.6114, 2013._

Bernard Osgood Koopman. Hamiltonian systems and transformation in Hilbert space. Proceedings
_of the National Academy of Sciences of the United States of America, 17(5):315–318, 1931._

Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1179–1191. Curran
Associates, Inc., 2020.

Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised representations for reinforcement learning. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th
_International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning_
_Research, pp. 5639–5650. PMLR, 13–18 Jul 2020a._

Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
19884–19895. Curran Associates, Inc., 2020b.

Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference on
_World wide web, pp. 661–670, 2010._

Yunzhu Li, Hao He, Jiajun Wu, Dina Katabi, and Antonio Torralba. Learning compositional Koopman
operators for model-based control. In Proc. of the 8th Int’l Conf. on Learning Representation
_(ICLR’20), 2020._

Alexandre Mauroy, Igor Mezi´c, and Yoshihiko Susuki. The Koopman Operator in Systems and
_Control: Concepts, Methodologies, and Applications. Springer, 2020._

I. Mezi´c. Spectral properties of dynamical systems, model reduction and decompositions. Nonlinear
_Dynamics, 41:309–325, 2005._

Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
_Icml, 2010._

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026–8037, 2019.

Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel
Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement
learning and demonstrations. arXiv preprint arXiv:1709.10087, 2017.

Anastasiya Salova, Jeffrey Emenheiser, Adam Rupe, James P. Crutchfield, and Raissa M. D’Souza.
Koopman operator and its approximations for systems with symmetries. Chaos: An Interdisci_plinary Journal of Nonlinear Science, 29(9):093128, 2019. doi: 10.1063/1.5099091._

Samarth Sinha, Ajay Mandlekar, and Animesh Garg. S4RL: Surprisingly Simple Self-Supervision
for Offline Reinforcement Learning. In Conference on Robot Learning, 2021.


-----

Subhrajit Sinha, Sai P. Nandanoori, and Enoch Yeung. Koopman operator methods for global phase
space exploration of equivariant dynamical systems, 2020.

Elise van der Pol, Daniel Worrall, Herke van Hoof, Frans Oliehoek, and Max Welling. Mdp homomorphic networks: Group symmetries in reinforcement learning. In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,
volume 33, pp. 4199–4210. Curran Associates, Inc., 2020.

Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning,
2019.

Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey
Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.
_arXiv preprint arXiv:1910.10897, 2019._

Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Martín-Martín. robosuite: A modular
simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293, 2020.


-----

A KOOPMAN THEORY AND SYMMETRIES

This section provides some additional introductory material on Koopman theory and symmetries of
dynamical systems.

**Lemma A.1 The map ∗** : Σ × K(S) → K(S) given by (σ ∗ _g)(s) 7−→_ _g(σ[−][1]_ _· s) defines a group_
_action on the Koopman space of observables K(S)._

**Theorem A.2 Let K be the Koopman operator associated with a Σ-equivariant system st+1 = F** (st).
_Then_

[σ ∗ (Kg)](s) = [K(σ ∗ _g)](s) ._ (19)
Theorem A.2 states that for a Σ-equivariant system any symmetry transformation commutes with the
Koopman operator. For the proof see (Sinha et al., 2020).

To proceed let us re-evaluate some information from the main text of this work. Let _U_ (a) _a_ be
_{_ _}_ _∈A_
a family of invertible operators s.t. U (a) : K(S) → F(S × A) is a mapping to the (Banach) space
of eigenfunctions φ(s, a) := U (a)g(s) ∈ F(S × A). Which moreover obeys U (a)K(a)U (a)[−][1] =
Λ(a), with Λ(a)φ(s, a) = λφ(a)φ(s, a) and where λφ(a) : A → R. The existence of such operators
puts further restriction on the Koopman operator in Eq. (11). However, as our algorithm employs the
finite-dimensional approximation of the Koopman operator i.e. its matrix representation this amounts
simple for K(a) to be diagonalizable and U (a) is the matrix containing its eigen-vectors as columns.
To evolve a better understanding on the required criteria for the infinite-dimensional case we employ
an alternative formulation of the so called spectral theorem below.

**Theorem A.3 (Spectral Theorem) Let K be a bounded self-adjoint operator on a Hilbert space H.**
_Then there is a measure space (S, Σ, µ) and a real-valued essentially bounded measurable function_
_λ on S and a unitary operator U : H →_ _L[2]µ[(][S][)][ i.e.][ U][ ∗][U][ =][ U U][ ∗]_ [=][ id][ such that]

_U Λ U_ _[∗]_ = K, with Λφ (s) = λ(s)φ(s) . (20)

In other words every bounded self-adjoint operator is unitarily equivalent to a multiplication operator. 
In contrast to the finite-dimensional case we need to slightly alter our criteria to U (a)K(a)U (a)[−][1] =
Λ(a), with Λ(a)φ(s, a) = λφ(a, s)φ(s, a) and where λφ(s, a) : S × A → R. Concludingly, a
sufficient condition for our criteria to hold in terms of operators on Hilbert spaces is that the Koopman
operator K(a) is self-adjoint i.e. that
_K(a) = K(a)[∗]_ _._ (21)

B PROOFS

In this section we provide the proofs of the theoretical results of Section 3.

**Proof of Lemma A.1 and Theorem A.2** The proofs of the lemma as well as the theorem can be
found in (Sinha et al., 2020).

**Proof of Lemma 3.1:** We aim to show that map ¯∗ : Σ[¯] _× K(S × A) →_ K(S × A) given by
(σ¯∗g)(s, a) 7−→ _g(σ[−][1]_ _·_ _s, a) defines a group action on the Koopman space of observables K(S ×A),_
where Σ[¯] defines the symmetry group of definition 5. Firstly, let g K( ) and σ0 Σ be the
identity element, then we see that it provides existence of an identity element of ∈ _S × A_ ¯∗ by _∈_ [¯]
(σ0¯g)(s, a) = g _σ0[−][1]_ _s, a_ = g(s, a) .
_∗_ _·_
Secondly, let σ1, σ2 Σ and denoting the group operation i.e.   _σ1_ _σ2 = σ3_ Σ. Then
_∈_ [¯] _⊙_ _⊙_ _∈_ [¯]
_σ2¯∗_ _σ1¯∗g_ (s, a) = σ2¯∗ _g_ _σ1[−][1]_ _· s, a_ = g _σ2[−][1]_ _· (σ1[−][1]_ _· s), a_ = g (σ2[−][1] _⊙_ _σ1[−][1][)][ ·][ s][)][, a]_
   [] (I)    []      

= g (σ1 _σ2)[−][1]_ _s, a_ = _σ1_ _σ2_ ¯g (s, a) = _σ3¯g_ (s, a),
_⊙_ _·_ _⊙_ _∗_ _∗_

where in (I) we have used the invertibility of the group property of      Σ[¯]. Lastly it follows analogously 
that for σ, σ[−][1] _∈_ Σ[¯] that
_σ¯_ _σ[−][1]¯g_ (s, a) = _σ[−][1]_ _σ_ ¯g (s, a) = _σ0¯g_ (s, a) = g(s, a) .
_∗_ _∗_ _⊙_ _∗_ _∗_

Thus the existence of an inverse is established which concludes to show the group property of   []       ¯.
_∗_


-----

**Proof of TheoremΣ¯** -action-equivariant system 3.2: We aim to show that with st+1 = F (st, at). Then K being the Koopman operator associated with a

_σ¯∗_ _Kg_ (s, a) = _K_ _σ¯∗g_ (s, a) .

First of all note that by the definition of the Koopman operator of non-affine control systems it obeysh    [i] h   [i]

[ _g](st, at) = g(F_ (st, at), at+1) .
_K_

Using the latter one thus infers that

(I)
_σ¯_ _g_ (st, at) = σ¯g(F (st, at), at+1) = g(σ[−][1] _F_ (st, at), at+1) = g(F (σ[−][1] _st, at), at+1),_
_∗_ _K_ _∗_ _·_ _·_
h   [i]

where in (I) we have used that it is a Σ[¯] -action-equivariant system. Moreover, one finds that

_g(F_ (σ[−][1] _st, at), at+1) =_ _g(σ[−][1]_ _st, at) =_ _σ¯g_ (st, at),

_·_ _K_ _·_ _K_ _∗_

which concludes the proof. h    [i]

**Proof of Lemma 3.3:** We aim to show that the map _ϕ[ˆ] : Σ[¯]_ K( ) _U_ (a) _a_ K( ) given
by _×_ _S_ _× {_ _}_ _∈A →_ _S_
_σaˆg_ (s) _U_ (a) _σ¯(U_ (a)g) (s) = g _σ[−][1]_ _s_ _,_
_∗_ _7−→_ _[−][1]_ _∗_ _·_

defines a group action on the Koopman space of observables      [] K(S) . Where ¯∗ is defined analog to
Lemma 3.1 but acting on F(S × A) instead of K(S × A) by (σ¯∗φ)(s, a) 7−→ _φ(σ[−][1]_ _· s, a). First of_
all note that
_U_ _[−][1](a)_ _σ¯ ∗_ (U (a)g) (s) = U _[−][1](a)_ _σ¯ ∗_ _φ(s, a)_ = U _[−][1](a)φ_ _σ[−][1]_ _· s, a_ = g _σ[−][1]_ _· s_ (22)
   []         

We proceed analogously as in the proof of Lemma 3.1 above. Firstly, let g K( ) and σ0 Σ be
_∈_ _S_ _∈_ [¯]
the identity element then on infers from Eq. (22) that it provides existence of an identity element of _ϕ[ˆ]_
by
(σa,0ˆg)(s) = g _σ0[−][1]_ _s_ = g(s),
_∗_ _·_
where we have used the notation
  

(σa,iˆg)(s) = _U_ (a) _σi¯(U_ (a)g) (s), for i = 0, . . . .
_∗_ _[−][1]_ _∗_
   []

Secondly, let σ1, σ2 ∈ Σ[¯] and ⊙ denoting the group operation i.e. σ1 ⊙ _σ2 = σ3 ∈_ Σ[¯] . Then
_σa,2ˆ∗_ _σa,1ˆ∗g_ (s) = σa,2ˆ∗ _g_ _σ1[−][1]_ _· s_ = g _σ2[−][1]_ _· (σ1[−][1]_ _· s)_ = g (σ2[−][1] _⊙_ _σ1[−][1][)][ ·][ s]_
   [] (I)   []      

= g (σ1 _σ2)[−][1]_ _s_ = g _σ3[−][1]_ _s_ = _σa,3¯g_ (s),
_⊙_ _·_ _·_ _∗_

where in (I) we have used the invertibility of the group property of     Σ. Lastly, it follows analogously 
that for σ, σ[−][1] _∈_ Σ[¯] that
_σˆ_ _σ[−][1]ˆg_ (s) = _σ[−][1]_ _σ_ ˆg (s) = _σ0ˆg_ (s) = g(s) .
_∗_ _∗_ _⊙_ _∗_ _∗_
   []      

Thus the existence of an inverse is established which concludes to show the group property of _ϕ[ˆ]._

**Proof of Theorem 3.4:** We aim to show twofold.

=⇒: Firstly, that with st+1 = F[˜](st, at) be a Σ[ˆ] -action-equivariant control system with a symmetry
action as in Lemma 3.3 which furthermore admits a Koopman operator representation as


_g(st+1) =_ (at)g(st), for at _,_ _g_ K( ) .
_K_ _∈A_ _∀_ _∈_ _S_

Then
_σat_ ˆ (at)g (st) = (at) _σat_ ˆg (st) .
_∗_ _K_ _K_ _∗_
h   [i] h   [i]


-----

_⇐=:_ Secondly, the converse. Namely, if a control system st+1 = F[˜](st, at) obeys Eqs. (13) and
(14), then it is Σ[ˆ] -action-equivariant, i.e. σ _F[˜](st, at) = F[˜](σ_ (st, at)). For notational simplicity we
_·_ _·_
drop the subscripts referring to time in the remainder of this proof i.e. st _s and at_ _a._
_→_ _→_

Let us start with the first implication i.e. =⇒.

First of all note that by the definition of the Koopman operator one has
_K(a)g_ (s) = g ˜F (s, a) _._

Using the latter one infers that     
_σaˆ_ (a)g (s) = _σaˆg( F[˜](s, a)),_
_∗_ _K_ _∗_
h   [i]

= _U_ _[−][1](a)_ _σ¯∗_ _U_ (a)g ˜F (s, a) _,_
     []

= _U_ _[−][1](a)_ _σ¯∗φ_ ˜F (s, a), a _,_
   []

= _U_ _[−][1](a)_ _φ_ _σ[−][1]_ _·_ _F[˜](s, a), a_ _,_
   []

= _U_ _[−][1](a)_ _φ_ ˜F _σ[−][1]_ _· s, a_ _, a_ _,_
      []

= _g_ ˜F _σ[−][1]s, a_ _._

Moreover, one derives that     

_g_ ˜F _σ[−][1]s, a_ = _K(a)g_ _σ[−][1]s_ _,_
     = _K(a) U _ _[−][1](a) U_ (a) _g_ _σ[−][1]s_ _,_

=id

  

= _K(a) U|_ _[−][1](a{z) φ_ _σ[−]}_ [1]s, a _,_
  

= _K(a)_ _U_ _[−][1](a)_ _σ¯∗φ_ _s, a_ _,_
     []

= _K(a)_ _U_ _[−][1](a)_ _σ¯∗_ _U_ (a)g(s) _,_
     []

= (a) _σaˆg_ (s),
_K_ _∗_

which concludes the proof of the first part of the theorem. Let us next show the converse implicationh   [i]
i.e. ⇐=. For this case it is practical to use the discrete time-system notation explicitly. Let σ ∈ Σ be
a symmetry of the state-space and be ˜st = σ · st and ˜st+1 = σ · st+1 the σ-shifted states. Then

_g_ _st+1_ = _g_ ˜F (st, at) _,_
   =  (at) g(st),

_K_

= (at) g _σ[−][1]_ ˜st _,_
_K_ _·_
  

= (at) _σaˆg_ _s˜t_ _,_
_K_ _∗_
   []

= (at) _σaˆg_ (˜st),
_K_ _∗_

(I) h   [i]
= _σaˆ_ (at)g (˜st) .
_∗_ _K_

Thus in particular h   [i]

_Lemma 3.3_
_σa[−][1][ˆ]∗g_ _st+1_ = _σa[−][1][ˆ]∗_ _σaˆ∗_ _K(at)g_ (˜st) = _K(at)g_ (˜st),
   h     [i] h i


-----

where in (I) we have used that the symmetry operator commutes with the Koopman operator. Moreover, one finds that

_σa[−][1][ˆ]∗g_ _st+1_ = g _σ[−][1][][−][1]_ _· st+1_ = g _σ · st+1_ = g _s˜t+1_ _,_

from which one concludes that           

_g(˜st+1) =_ (at)g (˜st) = g ˜F (˜st, at) _._
_K_

Finally, we use the invertibility of the Koopman space observables i.e.     _g[−][1](g(s)) = s to infer_

_s˜t+1 = F[˜](˜st, at) ._

However, in general s the Koopman space observables are not invertible globally. The "inverse
function theorem" guarantees the existence of a local inverse if g(s) is C[1] differentiable for maps
between manifolds of equal dimensions. However, we assume inevitability locally.

_s˜t+1 = σ · st+1_ _⇒_ _F˜(˜st, at) = σ ·_ _F[˜](st, at)_ _⇒_ _F˜(σ · st, at) = σ ·_ _F[˜](st, at),_ (23)

which at last concludes our proof of the second part of the theorem.

**Extension of Theorem 3.5:** Moreover, one may account for practical limitations i.e. an error by
the assumption [σa[ϵ] _t_ _[,][ K][(][a][t][)] =][ ϵ][a]_ [1][. One then finds that][ ˜]st+1 = F[˜](˜st, at) + (ϵa). Thus the error
_O_
becomes suppressed when ϵa _ϵ. The error may be due to practical limitations of capturing the true_
dynamics as well as the symmetry map. ≪

**Proof of Theorem 3.5:** We will show here theorem 3.5 as well as the extension mentioned in the
paragraph prior to this proof. Let st+1 = F[˜](st, at) be a Σ[ˆ] -action-equivariant control system as in
Theorem 3.4. For symmetry maps

_s˜t = D_ 1 + ϵ σat ˆ∗E(st) _, ˜st+1 = D_ 1 + ϵ σat ˆ∗E(st+1) _._ (24)

we aim to show that one can use a symmetry transformation to shift both         st as well as _st+1_

_σa[ϵ]_ _t_ [:] _st, st+1, at_ _7−→_ _s˜t, ˜st+1, at_ _, s.t. ˜st+1 = F[˜](˜st, at) ._ (25)

From equation 24 and the definition of the Koopman operator one infers that      [6]

_s˜t+1 = D_ _σat_ ˆ∗E(st+1) = D _σat_ ˆ∗K(at)E(st) _._ (26)
   

By using that [σat _,_ (at)] = 0 and equation 24 one finds that
_K_

_s˜t+1 = D_ _K(at)σat_ ˆ∗E(st) = D _K(at) E(D(_ _σat_ ˆ∗E(st))) = D _K(at)E(˜st)_ _,_ (27)
   =1   

which concludes the proof of the first part of the theorem with| {z } _σat_ (1 + ϵ σat ).
**Local diffeomorphism. Per definition the maps D, E are differentiable and invertible which implies →**
that they provide a local diffeomorphism from and to Koopman-space. Also the linear map i.e. a
matrix multiplication by (1 + ϵ σat ) is a diffeomorphism. Thus the symmetry map σa[ϵ] _t_ [i.e. Eq. (][15][)]
constitutes a local diffeomorphism. The above proof culminating in Eq. (24) implies that we have a
local diffeomorphism mapping solutions of the ODEs to solutions, thus a symmetry of the system of
ODEs.
**Limitations. However D, E to be invertible is a strong assumption as it implies that the Koopman**
space approximation admit the same dimension as the state space. Note that however in practice as
we only require D ◦ _E ≈_ _id one may choose other Koopman space dimensions._
**Local Lie group. What is left to show is that the symmetry of the system of ODEs locally is a Lie**
group. In definition 2 we need to show points (1)-(3).

1. For ϵ = 0 one finds that σa[0]t [is the identity map i.e for such that]


_s˜t(st, 0) = D_ (1 + 0 _σat_ )E(st) = D(E(st)) = st (28)
_·_

6For notational simplicity we study the general case  (1 + ϵ σat ) _σat_ .
_→_


-----

2.


_σ[ϵ][1]_ _σ[ϵ][2]_ = _D_ (1 + ϵ1σat )(1 + ϵ2σat )E(st)
 


(29)


= _D_ (1 + ϵ1σat + ϵ2σat + O(ϵ1 · ϵ2))E(st)

= _D (1 + (ϵ1 + ϵ2)σat +_ (ϵ1 _ϵ2))E(st)_ 
_O_ _·_

= _σ[ϵ][1][+][ϵ][2]_

  

for every _ϵ1_ _,_ _ϵ2_ 1.
_|_ _|_ _|_ _| ≪_

3. ˜st(st, ϵ) admits a Taylor series expansion in ϵ, i.e. in a neighbourhood of s determined by
_ϵ = 0. We may Taylor expand D around the point E(st)_ _g as_
_≡_


_N_

_∂D_
(σat _E(st))I_ _∂gI_ + O(ϵ[2]) (30)
_I=1_

X


_s˜t(st, ϵ) = D(E(st)) + ϵ_


thus
_s˜t(st, ϵ) = st + ϵζ(st) + O(ϵ[2])_ (31)

for ζ(st) = _I=1[(][σ][a]t_ _[E][(][s][t][))][I ∂D]∂gI_ [.]

4. The existence of an inverse element (σa[ϵ] _t_ [)][−][1][ =][ σ]a[−]t[ϵ] [follows from (1) and (2).]

[P][N]

**Numerical errors. Let us next turn to the second part of the theorem incorporating for practical**
numerical errors. We aim to show that under the assumption [σa[ϵ] _t_ _[,][ K][(][a][t][)] =][ ϵ][a]_ [1][ one finds][ ˜]st+1 =
_F˜(˜st, at) +_ (ϵa). Note that σat (1 + ϵ σat ) admits an expansion in the parameter ϵ. For ϵ 1
one can Taylor expand the differentiable functions O _→_ _D, E to find that_ _≪_


_∂D_
_gI[σ]_ _∂gI_


_s˜t = st + δ(ϵ), with δ(ϵ) = ϵ_


_E(st)_ [+][ O][(][ϵ][2][)][,] (32)


where g[σ] = σat ˆE(st) and g = E(st), and with the index I = 1, . . ., N . The analog expression
_∗_
holds for t + 1 i.e. ˜st+1 = st+1 + O(ϵ). If at = at(st) is differentiable function of the states[7] one
may Taylor expand the Koopman operator using equation 32 as[8]


_δn(ϵ) a[′]in[(][s][t][)][ K][i]_ [=][ K][(][a][t][(][s][t][)) +][ ϵ][ ∆+][ O][(][ϵ][2][)][ .] (33)


_K(at(˜st)) = K(at(st)) +_


where and a[′]in[(][s][t][) =] _∂∂ast,nt_ [and][ n][ = 1][, . . ., dim][(][s][t][)][ and with]

_[|][s][t]_

_∂Dn_

∆= _gI[σ]_ _∂gI_ _E(st)[a]in[′]_ [(][s][t][)][ K][i] _[.]_ (34)

_i_ _n_ _I_

X X X

However, by the definition of the symmetry map at(˜st) = at(st), thus the Koopman operators in
equation 33 match ∀ϵ. By using the assumption on the error one infers from equation 26 that

_s˜t+1 = D_ (1 + ϵσat )ˆ∗K(at)E(st) = _D_ _K(at) + ϵa 1_ (1 + ϵσat )ˆ∗E(st)
     

= _D_ _K(at)) (1 + ϵσat_ )ˆ∗E(st) + ϵa E(st) + O(ϵaϵ)
 

= _D_ _K(at)E(˜st) + ϵa E(st) + O(ϵaϵ)_ _,_ (35)
 


One may Taylor expand D by making use of ϵa 1 as
_≪_

_s˜t+1 = D_ (at)E(˜st) + ϵa _EI_ (st) _[∂D]_
_K_ _∂gI_
   X


_a[)][ .]_ (36)
(at)E(˜st) [+][ O][(][ϵ][a][ϵ][) +][ O][(][ϵ][2]
_K_


7Although neural networks modeling the policies modeling the data-distribution contain non-differentiable
components they are differentiable almost everywhere.
8The reader may wonder about the explicit use of the index δn(ϵ) expressing the dimension of the state.
it is necessary here although we have simply used st as an implicit vector without explicitly specifying any
components throughout the work.


-----

Thus under the assumption that the numerical error is given by a violation of the commutation relation
one finds ˜st+1 = F[˜](˜st, at) + O(ϵa) where

_ϵa_ _EI_ (st) _[∂D]_ (37)

_I_ _∂gI_ _K(at)E(˜st)_ _[∼O][(][ϵ][a][)][ .]_

X

By comparison with equation 32 one infers that the ϵ-expansion is a good approximation to the real
dynamic if ϵa _ϵ. This concludes the proof of theorem 3.5. Let us note that the proof for state shifts_
arising from a sum of symmetry transformations as ≪

_N_ _N_

_s˜t = D_ 1 + _ϵI σa[I]t_ ˆE(st) _, ˜st+1 = D_ 1 + _ϵI σa[I]t_ ˆE(st+1) _._ (38)

_∗_ _∗_
_I=1_ _I=1_

   X      X  

is completely analogous, and an analogous theorem holds. We choose the number of symmetries to
run over the dimension of the Koopman latent space N as it is relevant for the KFC++ setup.

C IMPLEMENTATION DETAILS

**The Koopman forward model The KFC algorithm requires pre-training of a Koopman forward**
model F : S × A →S as

_D_ _E(st)_ = st if c = 0 corresponds to a VAE.

(st, at) =
_F_ _[c]_ D  0 + _i=1_ _E(st)_ = st+1 if c = 1: forward prediction model.

 _K_ _[K][i][ a][t,i]_

     (39)
where both of E and D are approximated by Multi-Layer-Perceptrons (MLP’s) and the bilinear[P][m]
Koopman-space operator approximation are implemented by a single fully connected layer for
_i=0,...,m, respectively. The model is trained on batches of the offline data-set tuples (st+1, st, at)_
_K_
and optimized via the loss-function

¯l2(F [1](st, at), st+1) + γ2 [¯]l2(F [0](st + sϵ, at), st + sϵ), (40)

where sϵ is a normally distributed random sate vector shift with zero mean and a standard deviation
of 6 10[−][2] where the [¯]l2-loss is the Huberloss. For training the Koopman forward model in Eq. (16)
_·_
we split the dataset in a randomly selected training and validation set with ratios 70%/30%. We then
loop over the dataset to generate the required symmetry transformations and extend the replay buffer
dataset with the latter according to Eq. (18) as

_symmetries_
**KFC :** _st, st+1, at, rt_ _st, st+1, at, rt, σat_ _,_ (41)
_7−→_

_symmetries_
**KFC++ :**  st, st+1, at, rt _7−→_  st, st+1, at, rt, U (at), U (at)[−][1][] _._

It is more economic to compute the symmetry transformation as a pre-processing step rather than at    
runtime of the RL algorithm.

Let us reemphasize a couple of points addressed already in the main text to make this section selfcontained. Note that we have built upon the implementation of CQL (Kumar et al., 2020), which
is based on SAC (Haarnoja et al., 2018). We also use the same hyperparameters for all the
**experiments as presented in the CQL and S4RL papers for baselines.**

**Koopman forward VAE-model: For the encoder as well as the decoder we choose a three layer**
MLP architecture, with hidden-dimensions [512, 512, N ] and [512, 512, m], respectively. Where
_N = 32 is the Koopman latent space dimension and m is the state space dimensions following our_
notation from section 2. The activation functions are ReLU (Nair & Hinton, 2010). We use an ADAM
optimizer with learning rate 3 · 10[−][4]. We train the model for 75 epochs. The batch size is chosen to
be 256. Lastly, we employ a random normally distributed state-augmentation for the VAE training i.e.
_s →_ _s + sϵ with sϵ ∈N_ (0, 1 · 10[−][2]).

**Hyper-parameters KFC: We take over the hyper-parameter settings from the CQL paper (Kumar**
et al., 2020) except for the fact that we do not use automatic entropy tuning of the policy optimisation
step Eq. (3) but instead a fixed value of α = 0.2. The remaining hyper-parameters of the conservative


-----

Q-learning algorithm are as follows: γ = 0.99 and τ = 5 · 10[−][3] for the discount factor and target
smoothing coefficient, respectively. Moreover, the policy learning rate is 1 · 10[−][4] and the value
function learning rate is 3 · 10[−][4] for the ADAM optimizers. We enable Lagrange training for ˜α with
threshold of 10.0 and learning rate of 3 · 10[−][4] where the optimisation is done by an ADAM optimizer.
The minimum Q-weight is set to 10.0 and the number of random samples of the Q-minimizer is 10.
For more specifics on these quantities see (Kumar et al., 2020). The model architectures of the policy
as well as the value function are three layer MLP’s with hidden dimension 256 and ReLU -activation.
The batch size is chosen to be 256. Moreover, the algorithm performs behavioral cloning for the first
40k training-steps, i.e. time-steps in an online RL notation.

The KFC specific choices are as follows: the split of random to Koopman-symmetry state shifts
is 20/80 i.e. pK = 80%, whereas the option for the symmetry-type is either "Eigenspace" or
"Sylvester" referring to case (I) and (II) of Eq. (18), respectively. Regarding the random variables in
the symmetry transformation generation process, we choose ϵi (0, 1 10[−][4]) for case (II) and
_ϵ ∈N_ (0, 5 · 10[−][5]) for case (I) in Eq. (18) after normalizing σa by its mean. While for the random ∈N _·_
shift we use ˜ϵ ∈N (0, 3 · 10[−][3]) which is the hyper-parameter used in (Sinha et al., 2021).

**Computational setup: We performed the empirical experiments on a system with PyTorch 1.9.0a**
(Paszke et al., 2019) The hardware was as follows: NVIDIA DGX-2 with 16 V100 GPUs and 96
cores of Intel(R) Xeon(R) Platinum 8168 CPUs and NVIDIA DGX-1 with 8 A100 GPUs with 80
cores of Intel(R) Xeon(R) E5-2698 v4 CPUs. The models are trained on a single V100 or A100 GPU.

D ABLATION STUDY

|Task Name|S4RL-(N) KFC KFC++ KFC++-contact|
|---|---|


|cheetah-random cheetah-medium cheetah-medium-replay cheetah-medium-expert|52.3 48.6 49.2 49.4 48.8 55.9 59.1 61.4 51.4 58.1 58.9 59.3 79.0 79.9 79.8 79.8|
|---|---|


|hopper-random hopper-medium hopper-medium-replay hopper-medium-expert|10.8 10.4 10.7 10.7 78.9 90.6 94.2 95.0 35.4 48.6 49.0 49.1 113.5 121.0 125.5 129|
|---|---|


|walker-random walker-medium walker-medium-replay walker-medium-expert|24.9 19.1 17.6 18.3 93.6 102.1 108.0 108.3 30.3 48.0 46.1 48.5 112.2 114.0 115 118.1|
|---|---|



**Table 2: We study the effect of combining S4RL-N based augmentation training on “contact” event transitions**
(state transitions where the agent makes contact with a surface, read D.1 for more details), and KFC++ augmentation training on non-“contact” events. We see that KFC++-contact performs similarly and in most cases,
slightly better than the KFC++ baseline, which is expected. We experiment with the Open AI gym subset of the
D4RL tasks and report the mean normalized returns over 5 random seeds.

D.1 TRAINING ON NON CONTACT EVENTS

To test the theoretical limitations in a practical setting one interesting experiment is to use the S4RL_N augmentation scheme when the current state is a “contact” state and to choose KFC++ otherwise._
We define a “contact” state as one where the agent makes contact with another surface; in practice
we looked at the state dimensions which characterize the velocity of the agent parts, and if there is a
sign difference between st and st+1, then contact with a surface must have been made. The details
regarding which dimension maps to agent limb velocities is available in the official implementation
of the Open AI Gym suite (Brockman et al., 2016).

Note that the theoretical limitations are twofold, firstly the theorems 3.4 and 3.5 only hold for
dynamical systems with differentiable state-transitions; secondly, we employ a Bilinearisation Ansatz
for the Koopman operator. Contact events lead to non-continuous state transitions and moreover


-----

would require incorporating for external forces in our Koopman description, thus both theoretical
assumptions are practically violated in our main empirical evaluations of KFC and KFC++. By
performing this ablation study which is more suitable for our Ansatz, i.e. the KFC++ part is
only trained on "differentiable" trajectories without contact events. In other words, the Koopman
description is unlikely to give good information regarding st+1 if there is a “contact event”, we
simply use S4RL-N when training the policies, which we expect to give slightly better performance
than the KFC++ baseline.

We report the results in Table 2, where we perform results on the OpenAI Gym subset of the D4RL
benchmark test suite. We denote the proposed scheme as KFC++-contact and compare to both
S4RL-N and KFC++. We see that KFC++-contact performs comparably and slightly better in
some instances, compared to KFC++. This is expected as contact events may be rare, or the data
augmentation given by the KFC++ model during contact events is pseudo-random and itself is
normally distributed, which may suggest that KFC++ behaves similarly to KFC++-contact. However,
we do not say this conclusively.

D.2 MAGNITUDE OF KFC AUGMENTATION

To further try to understand the effect of using KFC for guiding data augmentation, we measure the
mean L2-norm of the strength of the augmentation during training. We measured the mean L2-norm
between the original state and the augmented state, and found the distance to be 2.6 × 10[−][4]. For
comparison, S4RL uses a fixed magnitude of 1 × 10[−][4] (Sinha et al., 2021) for all experiments with
S4RL-Adv. Since S4RL-Adv does a gradient ascent step towards the direction with the highest
change, its reasonable for the magnitude of the step to be smaller in comparison. An adversarial step
too large may be detrimental to training as the step can be too strong. KFC minimizes the need for
making such a hyperparameter search decision.

D.3 SYMMETRIES

In this section we provide a quantitative analysis of the symmetry shifts generated by KFC++ in
comparison to random shifts of the latent states. For the latter we choose the variance such that
absolute state shift
∆S := _s˜t_ _st_ + _s˜t+1_ _st+1_ (42)
_|_ _−_ _|_ _|_ _−_ _|_
averaged over all sampled data points (D4RL) is comparable between the two setups. Moreover we
simply use the encoder We then compare how well the shifted states are in alignment with the actual
dynamic using the online Mujoco environment .[9] To do so we set the environment state to the value
_s˜t and use the action at to perform the environment step which we denote by st+1 = M_ (st, at)[10]
The performance metric is then given by

∆E := _st+1_ _M_ (˜st, at) _._ (43)
_|_ _−_ _|_

Note that ∆E is simply the error to the actual dynamic of the environment. We take the model trained
on the hopper-medium-expert dataset from section appendix D.1 to compute the symmetry shift.
Moreover, for the comparison in equation 43 we split the state space into positions and velocities
of the state vector, i.e. the first five elements of st+1 _M_ (˜st, at) give position while the remaining
ones given velocities. The norm is taken subsequently. For the results see figure − D.3.

We conclude that there is a qualitative difference between the distributions obtained. Most notable
the symmetry shift allows for a controlled shift by a larger ∆S while still maintaining a small error
∆E. This amounts to a wide but accurate exploration of the environments state-space. Although we
cannot say this conclusively we expect this to be the reason for the performance gains of KFC++ over
random shifts.

D.4 PREDICTION MODEL

As a further ablation study it is of interest to compare the symmetry induced state shifts tuple
(˜st, ˜st+1) to one obtained by the forward prediction of our VAE-forward model equation 16.

9Note that this requires a minor modification of the environment code by a function which allows to set states
to specific values.
10M (st, at) denotes the step done by the active gym environment.


-----

**(a) Hopper position state shifts** **(b) Hopper velocity state shifts**

**Figure 3: Symmetry shifts vs. random latent space shifts of same magnitude in the mean of ∆S compared by**
its accuracy in the online evaluation in the gym Hopper-v3 environment. The first line shows the result where st
and st+1 are shifted by the same N-dimensional random variable. For the ablation study in line two we sample
twice from the random variable for the shifts st of st+1, respectively.

**KFC++prediction: In particular we use the forward prediction on ˜st obtained by the KFC++ shift,**
i.e. ˜st+1 = F _[c][=1](˜st, at). We refer to this setup in the following as KFC++prediction. See table 3_

for the results.
**Fwd-prediction: Moreover, we study the case where we ˜st is obtained by a shift with a normally**
distributed random variable N (0, 6 · 10[−][3]) and then simply forward predict as ˜st+1 = F _[c][=1](˜st, at)._
This is comparable to conventional model based approaches employing our simple VAE forward
model. This constitutes an ideal systematic comparison as the model, hyper-parameters and training
procedure are identical to the one used in the KFC variants. Moreover, the variance of the random
variable is such that the distance to the augmented states is comparable to the ones in KFC, see
appendix D.2. See table 4 for the results.

We conclude that KFC++prediction falls behind both KFC as well as KFC++. This was expected
as the symmetry shift in the latter alters the original data points by means of the VAE which admits
not only a much higher accuracy but also advanced generalisation capabilities to out of distribution
values.
More interesting however is that Fwd-prediction falls behind both KFC as well as KFC++ as
well as KFC++prediction. This is a strong indicator that the symmetry shifts provide superior
out-of-distribution data for training a Q-learning algorithm.

D.5 DISCUSSION

Although our KFC framework is suited best for the description of environments described by control
dynamical systems the learned (in self-supervised manner) linear Koopman latent space representation
may be applicable to a much wider set of tasks. However, there are notable shortcomings to the
current implementation both conceptually as well as practically. The bilinearisation in Eq. (16) of the
latent space theoretically assumes the dynamical system to be governed by Eq. (8), which is rather
restrictive. Although a general non-affine control system admits a bilinearisation (Brunton et al., 2021)


-----

|Task Name|KFC KFC++ KFC++-prediction|
|---|---|


|cheetah-random cheetah-medium cheetah-medium-replay cheetah-medium-expert|48.6 49.2 46.5 55.9 59.1 53.7 58.1 58.9 55.3 79.9 79.8 76.3|
|---|---|


|hopper-random hopper-medium hopper-medium-replay hopper-medium-expert|10.4 10.7 10.8 90.6 94.2 90.5 48.6 49.0 44.2 121.0 125.5 121.2|
|---|---|


|walker-random walker-medium walker-medium-replay walker-medium-expert|19.1 17.6 15.6 102.1 108.0 105.3 48.0 46.1 45.2 114.0 115.6 114.5|
|---|---|


**Table 3: Results with prediction model KFC++-prediction on the Open AI Gym subset of the D4RL tasks. We**
report the mean normalized episodic rewards over 5 random seeds similar to the original D4RL paper Fu et al.
(2021).

|Franka|kitchen-complete kitchen-partial|94.1 94.9 92.3 95.9|90.0 84.6|
|---|---|---|---|


**Domain** **Task Name** **KFC** **KFC++** **Fwd-prediction**

antmaze-umaze **96.9** **99.8** 92.7

antmaze-umaze-diverse **91.2** **91.1** 90.1

antmaze-medium-play 60.0 **63.1** 60.8

AntMaze

antmaze-medium-diverse 87.1 **90.5** 88.0

antmaze-large-play 24.8 **25.6** 23.1

antmaze-large-diverse **33.1** **34.0** 29.3

cheetah-random 48.6 49.2 **50**

cheetah-medium 55.9 **59.1** 50.1

cheetah-medium-replay **58.1** **58.9** 56.4

cheetah-medium-expert **79.9** **79.8** 71.3

hopper-random **10.4** **10.7** **10.4**

hopper-medium 90.6 **94.2** 82.3

Gym

hopper-medium-replay **48.6** **49.0** 40.8

hopper-medium-expert 121.0 **125.5** 120.3

walker-random **19.1** 17.6 18.4

walker-medium 102.1 **108.0** 103.2

walker-medium-replay **48.0** 46.1 41.7

walker-medium-expert 114.0 **115.3** 111.8

pen-human **61.3** 60.0 49.4

pen-cloned **71.3** 68.4 50.2

hammer-human 7.0 **9.4** 6.1

hammer-cloned 3.0 **4.2** 4.2

Adroit

door-human 44.1 **46.1** 41.8

door-cloned 3.6 **5.6** 1.2

relocate-human **0.2** **0.2** 0.2

relocate-cloned **-0.1** **-0.1** **-0.1**


**Table 4: Results with prediction model Fwd-prediction on the D4RL tasks. We report the mean normalized**
episodic rewards over 5 random seeds similar to the original D4RL paper Fu et al. (2021).

it generically requires the observables to depend on the action-space variables implicitly. Secondly,
the Koopman operator formalism is theoretically defined by its action on an infinite dimensional


-----

observable space. The finite-dimensional approximation i.e. the latent space representation of the
Koopman forward model in Eq. (16) lacks accuracy due to that. On the practical side our formalism
requires data pre-processing which is computationally expensive, i.e. solving the Sylvester or
eigenvalue problem for every data-point. Moreover, the Koopman forward model in Eq. (16) serves as
a function approximation to two distinct tasks. Thus one faces a twofold accuracy vs. over-estimation
problem which needs to be balanced. The systematic error in the VAE directly imprints itself on the
state-data shift in Eqs. (15) and (18) and may thus conceal any potential benefit of the symmetry
considerations. Lastly, the dynamical symmetries do not infer the reward. Thus the underlying
working assumption is that the reward should not vary much in Eq. (15).

**A note on the simplicity of the current algorithm:** Let us stress a crucial point. Algorithmically
the symmetry maps are derived in two distinct ways KFC and KFC++. The latter, constitute a simple
starting point to extract symmetries from our setup. More elaborate studies employing the extended
literate on Koopman spectral analysis are desirable. Moreover, it is desirable to extend our framework
to more complex latent space descriptions such as e.g. world models. Both on a theoretical as well
as a practical level. It is our opinion that by doing so there is significant room for improvement
both in the accuracy of the derived symmetry transformations as well their induced performance
gains of Q-learning algorithms. Note that currently our VAE model is of very simple nature and the
symmetries are extracted in a rather uneducated way. While the Sylvester algorithm simply converges
to one out of many symmetry transformations for the KFC++ algorithm we omit all the information
of the imaginary part, let alone utilize concrete spectral information.


-----

