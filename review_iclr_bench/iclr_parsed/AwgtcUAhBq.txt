# DOMAIN ADVERSARIAL TRAINING A GAME PERSPECTIVE

**David Acuna[123], Marc T. Law[2], Guojun Zhang[34], Sanja Fidler[123]**

1University of Toronto 2NVIDIA 3Vector Institute 4University of Waterloo

ABSTRACT

The dominant line of work in domain adaptation has focused on learning invariant
representations using domain-adversarial training. In this paper, we interpret this
approach from a game theoretical perspective. Defining optimal solutions in
domain-adversarial training as local Nash equilibria, we show that gradient descent
in domain-adversarial training can violate the asymptotic convergence guarantees
of the optimizer, oftentimes hindering the transfer performance. Our analysis leads
us to replace gradient descent with high-order ODE solvers (i.e., Runge–Kutta), for
which we derive asymptotic convergence guarantees. This family of optimizers is
significantly more stable and allows more aggressive learning rates, leading to high
performance gains when used as a drop-in replacement over standard optimizers.
Our experiments show that in conjunction with state-of-the-art domain-adversarial
methods, we achieve up to 3.5% improvement with less than half of training
iterations. Our optimizers are easy to implement, free of additional parameters,
and can be plugged into any domain-adversarial framework.

1 INTRODUCTION

Unsupervised domain adaptation (UDA) deals with the lack of labeled data in a target domain by
transferring knowledge from a labeled source domain (i.e., a related dataset with different distribution
where abundant labeled data already exists). The paramount importance of this paradigm has led
to remarkable advances in the field in terms of both theory and algorithms (Ben-David et al., 2007;
2010a;b; Mansour et al., 2009). Several state-of-the-art algorithms tackle UDA by learning domaininvariant representations in an adversarial fashion (Shu et al., 2018; Long et al., 2018; Saito et al.,
2018; Hoffman et al., 2018; Zhang et al., 2019; Acuna et al., 2021). Their goal is to fool an auxiliary
classifier that operates in a representation space and aims to classify whether the datapoint belongs to
either the source or the target domain. This idea, called Domain-Adversarial Learning (DAL), was
introduced by Ganin et al. (2016) and can be more formally understood as minimizing the discrepancy
between source and target domain in a representation space (Acuna et al., 2021).

Despite DAL being a dominant approach for UDA, alternative solutions have been sought as DAL is
noticeably unstable and difficult to train in practice (Sener et al., 2016; Sun et al., 2019; Chang et al.,
2019). One major cause of instability is the adversarial nature of the learning algorithm which results
from the introduction of the Gradient Reversal Layer (GRL, Ganin et al., 2016) (Figure 1). GRL flips
the sign of the gradient during the backward pass, which has profound implications on the training
dynamics and asymptotic behavior of the learning algorithm. Indeed, GRL transforms gradient
descent into a competitive gradient-based algorithm which may converge to periodic orbits and
other non-trivial limiting behavior that arise for instance in chaotic systems (Mazumdar et al., 2020).
Surprisingly, little attention has been paid to this fact, and specifically to the adversarial component
and interaction among the three different networks in the algorithm. In particular, three fundamental
questions have not been answered from an algorithmic point of view, 1) What is optimality in DAL?
_2) What makes DAL difficult to train and 3) How can we mitigate this problem?_

In this work, we aim to answer these questions by interpreting the DAL framework through the
lens of game theory. Specifically, we use tools developed by the game theoretical community in
Ba¸sar & Olsder (1998); Letcher et al. (2019); Mazumdar et al. (2020) and draw inspiration from
the existing two-player zero-sum game interpretations of Generative Adversarial Networks (GANs)


-----

(Goodfellow et al., 2014). We emphasize that in DAL, however, we have three rather than two
networks interacting with each other, with partial cooperation and competition. We propose a natural
three-player game interpretation for DAL, which is not necessarily equivalent to two-player zero-sum
game interpretations (see Example 1), which we coin as the Domain-Adversarial Game. We also
propose to interpret and characterize optimal solutions in DAL as local Nash Equilibria (see Section 3).
This characterization introduces a proper mathematical definition of algorithmic optimality for DAL.
It also provides sufficient conditions for optimality that drives the algorithmic analysis.

With our proposed game perspective in mind, a simple optimization solution would be to use
the Gradient Descent (GD) algorithm, which is the de facto solution but known to be unstable.
Alternatively, we could also use other popular gradient based optimizers proposed in the context of
differentiable games (e.g. Korpelevich, 1976; Mescheder et al., 2017). However, we notice that these
do not outperform GD in practice (see § 6). To understand why, we analyze the asymptotic behavior
of gradient-based algorithms in the proposed domain-adversarial game (§ 4). The main result of § 4.2
(Theorem 2) shows that GD with GRL (i.e., the existing solution for DAL) violates the asymptotic
convergence guarantees to local NE unless an upper bound is placed on the learning rate, which
may explain its training instability and sensitivity to optimizer parameters. In § 4.3, Appendix B.2
and Appendix E, we also provide a similar analysis for the popular game optimization algorithms
mentioned above. We emphasize however that while some of our results may be of independent
interest for learning in general games, our focus is DAL. § 4.3 and § 6 show both theoretically and
experimentally that the limitations mentioned above disappear if standard optimizers are replaced with
ODE solvers of at least second order. These are straightforward to implement as drop-in replacements
to existing optimizers. They also lead to more stable algorithms, allow for more aggressive learning
rates and provide notable performance gains. n3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUWZsNiUbgrf8ipXVS9q6rbuKzUbvM4inACp3AOHlxDe6hDk1gPAMr/DmPDovzrvzsWgtOPnMfyB8/kDzceM7w=</latexit>g _hˆhuG1nwuVZsgVmy8KM0kwIdPXyUBozlCOLaFMC3srYRHVlKENqGxD8BZfXiat85p3WXPvL6r1myKOEhzDCZyB1dQhztoQBMYPMIzvMKbkzgvzrvzMW9dcYqZI/gD5/MHKgqO2w=</latexit>_

2 PRELIMINARIES sCKroQgsWXl0njvBJcVvz7i3L1Jo+jAMdwAmcQwBVU4Q5qUAcKj/AMr/DmKe/Fe/c+5q0rXj5zBH/gf4AgK6PFA=</latexit>

We focus on the UDA scenario and follow the formulation
from Acuna et al. (2021). This makes our analysis general and _Z_ PR10uOaUStGjiDV3N1K6A1UusCKroQgsWXl0njvBJcVvz7i3L1Jo+jAMdwAmcQwBVU4Q5qUAcKj/AMr/DmKe/Fe/c+5q0rXj5zBH/gf4AgK6PFA=</latexit>
applicable to most state-of-the-art DAL algorithms (e.g., Ganin et al. QY/Uby8V/I6iR5Uu1Mm4kRTQRaLBgnHOsJpC7jPJCWaTwBIpm5FZMRSCDadJUzJXz9FP9PmqWie150brxC7XJZRxYdoiN0glxUQTV0jeqogQhK0AN6Qs/WvfVovVivi2jGWs4coB+w3j4BX0GS6w=</latexit> DLGQ9qzVGJBtZ/lC0/RiVCFMXKPmlQrv6eyLDQeiICmxTYjPSiNxP/83qpia78jMkNVS+UdRypGJ0ex6FDJFieETSzBRzO6KyAgrTIztqGJL8BZPXibts7p3UXfvzmuN6KOMhzBMZyCB5fQgFtoQgsICHiGV3hzlPivDsf82jJKWYO4Q+czx+ZdZBG</latexit> _r_
(2016); Saito et al. (2018); Zhang et al. (2019)). We assume that the _−λr_ _R(GRL)λ_
learner has access to a source dataset (S) with labeled examples and
a target dataset (T) with unlabeled examples, where the source inputs Figure 1: We study domain_x[s]i_ [are sampled i.i.d. from a (source) distribution][ P][s][ and the target]

perspective. In DAL (Ganin

n3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUWZsNiUbgrf8ipXVS9q6rbuKzUbvM4inACp3AOHlxDe6hDk1gPAMr/DmPDovzrvzsWgtOPnMfyB8/kDzceM7w=</latexit>g _hˆhuG1nwuVZsgVmy8KM0kwIdPXyUBozlCOLaFMC3srYRHVlKENqGxD8BZfXiat85p3WXPvL6r1myKOEhzDCZyB1dQhztoQBMYPMIzvMKbkzgvzrvzMW9dcYqZI/gD5/MHKgqO2w=</latexit>_

sCKroQgsWXl0njvBJcVvz7i3L1Jo+jAMdwAmcQwBVU4Q5qUAcKj/AMr/DmKe/Fe/c+5q0rXj5zBH/gf4AgK6PFA=</latexit>
_r_ Vb3Luru3XmtcV3UYjOIZT8OASGnALTWgBAwXP8ApvDjovzrvzMR8tOcXOIfyB8/kDl52RdA=</latexit>

_hˆ2jOUI4soUwLeythEdWUoY2oZEPwFl9eJs3zqndZde8vKrWbeRxFOIJjOAMPrqAGd1CHBjCQ8Ayv8OY8Oi/Ou/Mxay0485lD+APn8wfj5o82</latexit>_ _[0]_

_Z_ PR10uOaUStGjiDV3N1K6A1UusCKroQgsWXl0njvBJcVvz7i3L1Jo+jAMdwAmcQwBVU4Q5qUAcKj/AMr/DmKe/Fe/c+5q0rXj5zBH/gf4AgK6PFA=</latexit>

QY/Uby8V/I6iR5Uu1Mm4kRTQRaLBgnHOsJpC7jPJCWaTwBIpm5FZMRSCDadJUzJXz9FP9PmqWie150brxC7XJZRxYdoiN0glxUQTV0jeqogQhK0AN6Qs/WvfVovVivi2jGWs4coB+w3j4BX0GS6w=</latexit> DLGQ9qzVGJBtZ/lC0/RiVCFMXKPmlQrv6eyLDQeiICmxTYjPSiNxP/83qpia78jMkNVS+UdRypGJ0ex6FDJFieETSzBRzO6KyAgrTIztqGJL8BZPXibts7p3UXfvzmuN6KOMhzBMZyCB5fQgFtoQgsICHiGV3hzlPivDsf82jJKWYO4Q+czx+ZdZBG</latexit> _r_
_−λr_ _R(GRL)λ_

Figure 1: We study domainadversarial training from a game

inputs x[t]i [are sampled i.i.d. from a (target) distribution][ P][t][, both over] et al. (2016)), three networks in. We have = 0, 1 for binary classification, and = 1, ..., k
_X_ _Y_ _{_ _}_ _Y_ _{_ _}_ teract with each other: the feain the multiclass case. The risk of a hypothesis h : X →Y w.r.t. the ture extractor (g), the domain
labeling function f, using a loss function ℓ : Y ×Y → R+ under dis- classifier (h[ˆ] _[′]) and the classifier_
tribution is defined as: R[ℓ]
_D_ _D[(][h, f]_ [) :=][ E][D][[][ℓ][(][h][(][x][)][, f] [(][x][))]][. For sim-] (h[ˆ]). During backpropagation,
plicity, we define RS[ℓ] [(][h][) :=][ R]P[ℓ] _s_ [(][h, f][s][)][ and][ R]T[ℓ] [(][h][) :=][ R]P[ℓ] _t_ [(][h, f][t][)][.] the GRL flips the sign of the graThe hypothesis class of h is denoted by H. dient with respect to g.

**UDA aims to minimize the risk in the target domain while only having access to labeled data in the**
source domain. This risk is upper bounded in terms of the risk of the source domain, the discrepancy
between the two distributions and the joint hypothesis error λ[∗]:
**Theorem 1. (Acuna et al. (2021)) Let us note ℓ** : Y × Y → [0, 1], λ[∗] := minh∈H RS[ℓ] [(][h][) +][ R]T[ℓ] [(][h][)][,]
_and D[φ]h,H[(][P][s][||][P][t][) := sup]h[′]∈H_ _[|][E][x][∼][P][s]_ [[][ℓ][(][h][(][x][)][, h][′][(][x][))]][ −] [E][x][∼][P][t][[][φ][∗][(][ℓ][(][h][(][x][)][, h][′][(][x][)))][|][.][ We have:]

_RT[ℓ]_ [(][h][)][ ≤] _[R]S[ℓ]_ [(][h][) +][ D][φ]h, (1)
_H[(][P][s][||][P][t][) +][ λ][∗][.]_

The functionis typical in UDA, we assume that the hypothesis class is complex enough and both φ : R+ → R defines a particular f -divergence and φ[∗] is its (Fenchel) conjugate. As fs and ft are
similar in such a way that the non-estimable term (λ[∗]) is negligible and can be ignored.

**Domain-Adversarial Training (see Figure 1) aims to find a hypothesis h ∈H that jointly minimizes**
the first two terms of Theorem 1. To this end, the hypothesis h is interpreted as the composition of
_h = h[ˆ] ◦_ _g with g : X →Z, and_ _h[ˆ] : Z →Y. Another function class_ _H[ˆ] is then defined to formulate_
_H := {h[ˆ] ◦_ _g : h[ˆ] ∈_ _H[ˆ], g ∈G}. The algorithm tries to find the function g ∈G such that_ _h[ˆ] ◦_ _g_
minimizes the risk of the source domain (i.e. the first term in Theorem 1), and its composition with _h[ˆ]_
and _h[ˆ][′]_ minimizes the divergence of the two distributions (i.e. the second term in Theorem 1).


-----

Algorithmically, the computation of the divergence function in Theorem 1 is estimated by a so-called
_domain classifier_ _h[ˆ][′]_ whose role is to detect whether the datapoint g(xi) belongs to the
_∈_ _H[ˆ]_ _∈Z_
source or to the target domain. When there does not exist a function _h[ˆ][′]_ _∈_ _H[ˆ] that can properly_
distinguish between g(x[s]i [)][ and][ g][(][x][t]i[)][,][ g][ is said to be invariant to the domains.]

Learning is performed using GD and the GRL (denoted by Rλ) on the following objective:

min Ex _ps_ [ℓ(h[ˆ] _g, y)]_ _αds,t(h,[ˆ]_ _h[ˆ][′], Rλ(g)),_ (2)
_hˆ∈H[ˆ],g∈G,h[ˆ]_ _[′]∈H[ˆ]_ _∼_ _◦_ _−_

where ds,t(h,[ˆ] _h[ˆ][′], g) := Ex_ _ps_ [ℓ[ˆ](h[ˆ][′] _g,_ _h[ˆ]_ _g)]_ Ex _pt[(φ[∗]_ _ℓ)(h[ˆ][′]_ _g,_ _h[ˆ]_ _g)]. Mathematically, the_
_∼_ _◦_ _◦_ _−_ _∼_ _◦_ [ˆ] _◦_ _◦_
GRL Rλ is treated as a “pseudo-function” defined by two (incompatible) equations describing its
forward and back-propagation behavior (Ganin & Lempitsky, 2015; Ganin et al., 2016). Specifically,

_Rλ(x) := x_ and _dRλ(x)/dx :=_ _λ,_ (3)
_−_

where λ and α are hyper-parameters that control the tradeoff between achieving small source error
and learning an invariant representation. The surrogate loss ℓ : Y × Y → R (e.g., cross-entropy) is
used to minimize the empirical risk in the source domain. The choice of function _ℓ[ˆ]_ : Y × Y → R
and of conjugate φ[∗] of the f -divergence defines the particular algorithm (Ganin et al., 2016; Saito
et al., 2018; Zhang et al., 2019; Acuna et al., 2021). From eq. 2, we can notice that GRL introduces
an adversarial scheme. We next interpret eq. 2 as a three-player game where the players are _h,[ˆ]_ _h[ˆ][′]_ and
_g, and study its continuous gradient dynamics._

3 A GAME PERSPECTIVE ON DAL

We now interpret DAL from a game-theoretical perspective. In § 3.1, we rewrite the DAL objective
as a three-player game. In this view, each of the feature extractor and two classifiers is a player. This
allows us to define optimality in terms of local Nash Equilibrium (see Def. 2 in Appendices). In § 3.2,
we introduce the vector field, the game Hessian and the tools that allow us to characterize local NE
for the players. This characterization leads to our analysis of the continuous dynamics in § 4.

3.1 DOMAIN-ADVERSARIAL GAME

We now rewrite and analyze the DAL problem in eq. 2 as a three-player game. Let _H[ˆ],_ _H[ˆ][′]_ and G
be classes of neural network functions and define ω1 ⊆ Ω1 and ω3 ⊆ Ω3 as a vector composed of
the parameters of the classifier and domain classifier networks _h[ˆ] ∈_ _H[ˆ] and_ _h[ˆ][′]_ _∈_ _H[ˆ], respectively._
Similarly, letis denoted bynetwork be a player ω Ω= Ω2 ⊆ Ω and its parameter choice to be its individual strategy (here continuous). The goal12 × be the parameters of the feature extractor network Ω2 × Ω3 and the joint parameter set is ω = (ω1 g, ω ∈G2, ω. Their joint domain3). Let each neural
of each player is then to selfishly minimize its own cost function Ji : Ω _→_ R. We use the subscript

_i_ [to refer to all parameters/players but][ i][. With the notation introduced, we can now formally]
_−_
define the Domain-Adversarial Game as the three-player game ( _, Ωi, Ji) where_ := 1, 2, 3,
_G_ _I_ _I_ _{_ _}_
dim(Ω) = _i=1_ [dim(Ω][i][) =][ d][,][ Ω][i][ ⊆] [R][d][i][ and:]

_J1(ω1, ω−1) := ℓ(ω1, ω2) + αds,t(ω)_

[P][3]

_J2(ω2, ω−2) := ℓ(ω1, ω2) + αλds,t(ω)_ (4)
_J3(ω3, ω−3) := −_ _αds,t(ω),_

We use the shorthand ℓ(ω1, ω2) for Ex,y _ps_ [ℓ(ω1 _ω2(x), y)], and ωi’s refer to the feature extractor_
_∼_ _◦_
_g and the classifiers (h[ˆ] and_ _h[ˆ][′]). Similar notation follows for ds,t. Here, we assume that each Ji is_
smooth in each of its arguments ωi Ωi.
_∈_

The gradient field of Equation (2) and the game’s vector field (see § 3.2) are equivalent, making the
original interpretation of DAL and our three-player formulation equivalent. However, it is worth
noting that our intepretation does not explicitly require the use of Rλ in ds,t in Equation (4). We can
write optimality conditions of the above problem through the concept of Nash Equilibrium:
**Definition 1. (Nash Equilibrium (NE)) A point ω[∗]** _∈_ Ω _is said to be a Nash Equilibrium of the_
_Domain-Adversarial Game if_ _i_ 1, 2, 3 _,_ _ωi_ Ωi, we have: Ji(ωi[∗][, ω][∗] _i[)][ ≤]_ _[J][i][(][ω][i][, ω][∗]_ _i[)][.]_
_∀_ _∈{_ _}_ _∀_ _∈_ _−_ _−_

In our scenario, the losses are not convex/concave. NE then does not necessarily exist and, in general,
finding NE is analogous to, but much harder than, finding global minima in neural networks – which


-----

is unrealistic using gradient-based methods (Letcher et al., 2019). Thus, we focus on local NE which
relaxes the NE to a local neighborhood B(w[∗], δ) := {||w − _w[∗]|| < δ} with δ > 0 (see Definition 2)._

Intuitively, a NE means that no player has the incentive to change its own strategy (here parameters
of the neural network) because it will not generate any additional pay off (here it will not minimize
its cost function). We emphasize that each player only has access to its own strategy set. In other
words, the player J1 cannot change the parameters ω2, ω3. It only has access to ω1 ∈ Ω1.

While the motivation of the three-player game follows naturally from the original formulation of DAL
where three networks interact with each other (see Figure 1), the optimization problem (2) could also
be interpreted as the minimax objective of a two-player zero-sum game. Thus, a natural question
arises: can we interpret the domain-adversarial game as a two player zero-sum game? This can be
done for example by defining ω12[∗] [:= (][ω]1[∗][, ω]2[∗][)][, and considering the cost of the two players][ (][ω][12][, ω][3][)]
as J12 = J and J3 = −J where J(ω12, ω3) := Eps [ℓ(ω1, ω2)] + ds,t(ω). In general, however, the
_solution of the two-player game (ω12[∗]_ _[, ω]3[∗][)][ is not equal to the NE solution of the three-player game]_
(in the following counterexample (see Ba¸ω1[∗][, ω]2[∗][, ω]3[∗][)][. This is because the team optimal solution]sar & Olsder (1998) for more details):[ ω]12[∗] _[̸][= (][ω]1[∗][, ω]2[∗][)][ in general. We illustrate this]_


**Example 1. Let the function J(ω) :=** 2[1] _ω1[2]_ [+ 4][ω][1][ω][2] [+][ ω]2[2] 3 _._

_[−]_ _[ω][2]_

_(a) Suppose the three-player game ω = ( ω1, ω2, ω3) with J1 = J2 = J and J3 =_ _J. Each Ji is_
_−_
_strictly convex in ωi. The NE solution of the game ω[∗]_ = (0, 0, 0) is unique.
_(b) Suppose the two-player game ω = (ω12, ω3) with J12 = J and J3 = −J. The solution ω[∗]_ _from_
_(a) is not a NE solution. To see this, let ˆω := (_ 1, 1, 0). We have that J12(ˆω) = 1 < J12(ω[∗]) = 0.
_−_ _−_
_This contradicts Definition 1. One can verify that there is no NE in this two-player scenario._

3.2 CHARACTERIZATION OF THE DOMAIN-ADVERSARIAL GAME
We now introduce the game’s vector field (also called pseudo-gradient) and the pseudo-gradient’s
Jacobian. We also provide a characterization of local NE based on them (see § 3). These are the core
concepts used in our analysis (§ 4). We first define the game’s vector field v(w), and its Jacobian
_H(ω) (also called the game Hessian (Letcher et al., 2019)):_

_v(ω) := (∇ω1_ _J1, ∇ω2_ _J2, ∇ω3_ _J3) ∈_ R[d], _H(ω) := ∇v(ω) ∈_ R[d][×][d] (5)
Note that the vector field v(w) and the three-player formulation naturally capture the behavior
introduced by the GRL in the original formulation. Specifically, v(ω) is identical to the gradient with
respect to the parameters of the original DAL objective with GRL (Equation (2)). Therefore, in both
cases the behavior of GD is identical. Assuming the same initial conditions, they will reach the same
solution. This shows the equivalence between our perspective and the original DAL formulation. We
emphasize that by equivalence, we mean the same dynamics, and the same intermediate and final
solutions. Another fact worth emphasizing is that H(ω) is asymmetric. This is in contrast with the
Hessian in supervised learning. Before proceeding with a characterization of local NE in terms of
_v(w) and H(ω), we first define sufficient and necessary conditions for local NEs:_
**Proposition 1. (Necessary condition). Suppose each Ji is twice continuously differentiable in each**
_ωi, any local NE ω[∗]_ _satisfies: i) ∇ωi_ _Ji(ω[∗]) = 0 and ii) ∀i ∈{1, 2, 3}, ∇ω[2]_ _i,ωi_ _[J][i][(][ω][∗][)][ ⪰]_ [0][.]

**Proposition 2. (Sufficient condition). Suppose each Ji is twice continuously differentiable in each**
_ωi. ωi[∗]_ _[is a][ local NE][ if][ i)][ ∇][ω]i_ _[J][i][(][ω][∗][) = 0][ and][ ii)][ ∀][i,][ ∇][2]ωi,ωi_ _[J][i][(][ω][∗][)][ ≻]_ [0][.]

The necessary and sufficient conditions from Propositions 1 and 2 are reminiscent of conditions for
local optimality in continuous optimization (Nocedal & Wright, 2006). Similar conditions were
also proposed in Ratliff et al. (2016) where the sufficient condition defines the differential Nash
_equilibrium. We can now characterize a local NE in terms of v(w) and H(ω):_
**Proposition 3. (Strict Local NE) w is a strict local NE if v(w) = 0 and H(ω) + H(ω)[⊤]** _≻_ 0.
The sufficient condition implies that the NE is structurally stable (Ratliff et al., 2016). Structural
stability is important as it implies that slightly biased estimators of the gradient (e.g., due to sampling
noise) will not have vastly different behaviors in neighborhoods of equilibria (Mazumdar et al., 2020).
In the following, we focus on the strict local NE (i.e., ω[∗] for which Proposition 3 is satisfied).

4 LEARNING ALGORITHMS

We defined optimality as the local NE and provided sufficient conditions in terms of the pseudogradient and its Jacobian. In this section, we assume the existence of the strict local NE (Prop. 3)


-----

in the neighborhood of the current point (e.g., initialization), and analyze the continuous gradient
dynamics of the Domain-Adversarial Game (eq. 4 and eq. 5). We show that given the sufficient
conditions from Prop. 3, asymptotic convergence to a local NE is guaranteed through an application
of Hurwitz condition (Khalil, 2002). Most importantly, we show that using GD with the GRL could
violate those guarantees unless its learning rate is upper bounded (see Thm. 2 an Cor. 1). This
is in sharp contrast with known results from supervised learning where the implicit regularization
introduced by GD has been shown to be desirable (Barrett & Dherin, 2021). We also analyze the use
of higher-order ODE solvers for DAL and show that the above restrictions are not required if GD is
replaced with them. Finally, we compare our resulting optimizers with recently algorithms in the
context of games.

Our algorithmic analysis is based on the continuous gradient-play dynamics and the derivation of the
_modified or high-resolution ODE of popular integrators (e.g., GD/Euler Method and Runge-Kutta)._
This type of analysis is also known in the numerical integration community as backward error analysis
(Hairer et al., 2006) and has recently been used to understand the implicit regularization effect of GD
in supervised learning (Barrett & Dherin, 2021). High resolution ODEs have also been used in Shi
et al. (2018) to understand the acceleration effect of optimization algorithms, and more recently in
Lu (2020). As in Shi et al. (2018); Lu (2020); Barrett & Dherin (2021), our derivation of the high
resolution ODEs is in the full-batch setting. The derivation of the stochastic dynamics of stochastic
discrete time algorithms is significantly more complicated and is beyond the scope of this work.

We experimentally demonstrate that our results are also valid when there is stochasticity due to
sampling noise in the mini-batch. We emphasize that our analysis does not put any constraint or
structure on the players’ cost functions as opposed to Azizian et al. (2020); Zhang & Yu (2020). In our
problem, the game is neither bilinear nor necessarily strongly monotone. See proofs in appendices.

4.1 CONTINUOUS GRADIENT DYNAMICS

Given v(ω) the continuous gradient dynamics can be written as:

_ω˙_ (t) = −v(ω). (6)

For later reasons and to distinguish between eq. 6 and the gradient flow, we will refer to these as the
gradient-play dynamics as in Ba¸sar & Olsder (1998); Mazumdar et al. (2020). These dynamics are
well studied and understood when the game is either a potential or a purely adversarial game (see
definitions in appendices). While eq. 2 may look like a single objective, the introduction of the GRL
(Rλ), makes a fundamental difference between our case and the dynamics that are analyzed in the
single-objective gradient-based learning and optimization literature. We summarize this below:
**Proposition 4. The domain-adversarial game is neither a potential nor necessarily a purely adver-**
_sarial game. Moreover, its gradient dynamics are not equivalent to the gradient flow._

Fortunately, we can directly apply the Hurwitz condition (Khalil, 2002) (also known as the condition
for asymptotic stability, see Appendix A.1) to derive sufficient conditions for which the continuous
dynamics of the gradient play would converge.
**Lemma 1. (Hurwitz condition) Let ∇v(w[∗]) be the Jacobian of the vector field at a stationary**
_point w[∗]_ _where v(w[∗]) = 0. If the real part of every eigenvalue λ of ∇v(w[∗]) (i.e. in the spectrum_
_Sp(∇v(w[∗]))) is positive then the continuous gradient dynamics are asymptotically stable._
In this work, we assume the algorithms are initialized in a neighborhood of a strict local NE ω[∗].
Therefore, Lemma 1 provides sufficient conditions for the asymptotic convergence of the gradientplay dynamics to a local NE. In practice this assumption may not hold, and it is computationally
hard to verify. Despite this, our experiments show noticeable performance gains in several tasks,
benchmarks and network architectures (see § 6).

4.2 ANALYSIS OF GD WITH THE GRL

We showed above that given the existence of a strict local NE, the gradient-play dynamics are
attracted to the strict local NE. A natural question then arises: If under this assumption local
_asymptotic convergence is guaranteed, what could make DAL notoriously hard to train and unstable?_
In practice, we do not have access to an explicit solution of the ODE. Thus, we rely on integration
algorithms to approximate the solution. One simple approach is to use the Euler method:

_w[+]_ = w − _ηv(w)._ (7)


-----

This is commonly known as GD. The equivalence between v(w) (game’s vector field) and the gradient
of Equation (2) (original DAL formulation) follows from the use of the GRL (Rλ). We remind the
reader that the GRL is a “pseudo-function” defined by two (incompatible) equations describing its
forward and back-propagation behavior, i.e., a flip in the gradient’s sign for the backward pass (see
Figure 1, Section 2 and Ganin et al. (2016)). Equation (7) is then the default algorithm used in DAL.
Now, to provide an answer to the motivating question of this section, we propose to analyze the
high-resolution ODE of this numerical integrator (i.e., Euler) and in turn its asymptotic behavior.
This is similar to deriving the modified continuous dynamics for which the integrator produces the
exact solution (Hairer et al., 2006) and applying Hurwitz condition on the high-resolution ODE.
**Theorem 2. The high resolution ODE of GD with the GRL up to O(η) is:**

_w˙_ = −v(w)− _[η]2_ _[∇][v][(][w][)][v][(][w][)]_ (8)

_Moreover, this is asymptotically stable (see Appendix A.1) at a stationary point w[∗]_ _(Proposition 3) iff_
_for all eigenvalue written as λ = a + ib ∈_ _Sp(−∇v(w[∗])), we have 0 > η(a[2]_ _−_ _b[2])/2 > a._

A striking difference between Equation (6) and Equation (8) is made clear (additional term marked in
red). This additional term is a result of the discretization of the gradient-play dynamics using Euler’s
method (i.e. GD) and leads to a different Jacobian of the dynamics. This term was recently shown to
be beneficial for standard supervised learning (Barrett & Dherin, 2021), where ∇v(ω[∗]) is symmetric
and thus only has real eigenvalues. In our scenario, this term is undesirable. In fact, this additional
term puts an upper bound on the learning rate η. The following corollary formalizes this:
**Corollary 1. The high resolution ODE of GD with GRL in Equation (8) is asymptotically stable only**
_if the learning rate η is in the interval: 0 < η <_ _b[2]−2aa[2][,][ for all][ λ][ =][ a][ +][ ib][ ∈]_ _[Sp][(][−∇][v][(][w][∗][))][ with]_

_−_
_large imaginary part (i.e., such that |a| < |b|)._
To have good convergence properties, the imaginary part of the eigenvalues of −∇v(w[∗]) must be
small enough. Therefore, if some eigenvalue λ = a + ib satisfies a < 0 and b[2] _≫_ _a[2]_ _≫−2a, the_
learning rates should be chosen to be very small. This is verified in Section 6 and in Example 2.
**Example 2. Consider the three-player game where ℓ(w1, w2) = w1[2]** [+ 2][w][1][w][2] [+][ w]2[2][,][ λ][ = 1][ and]

2 2 0
_deigenvalues ofs,t(w2, w3) = A w are2[2]_ [+ 99] −2 and[w][2][w] −[3] _[−]3 ±[w] 23[2][. Then]i√2449[ ˙]w. From Corollary 1, = −v(w) becomes: η should be ˙w = Aw = 0 < η < −−02 − −99 64. −2− ×992_ 10. The[−][3].

4.3 HIGHER ORDER ODE SOLVERS

The limitation described above exists because GD with the GRL can be understood as a discretization
of the gradient-play dynamics using Euler’s Method. Thus, it only approximates the continuous
dynamics up to O(η). To obtain a better approximation, we consider Runge-Kutta (RK) methods of
order two and beyond (Butcher, 1996). For example, take the improved Euler’s method (a particular
RK method of second order) that can be written as:

_w[+]_ = w − _[η]2_ [(][v][(][w][) +][ v][(][w][ −] _[ηv][(][w][))][)][.]_ (9)

Comparing Equation (9) (i.e., update rule of RK2) with Equation (7) (i.e., update rule of GD), one
can see that the RK2 method is straightforward to implement in standard deep learning frameworks.
Moreover, it does not introduce additional hyper-parameters. More importantly, such discrete
dynamics approximate the continuous ODE of Equation (6) to a higher precision. In Appendix C, we
provide asymptotic guarantees for the high resolution ODE of general RK methods, their generalized
expression and the algorithm pseudo-code. See also PyTorch pseudo-code in Appendix E.

**Limitation. A disadvantage of using high-order solvers is that they require additional extra steps.**
Specifically, one extra step in the case of RK2 (computation of the additional second term in
Equation (9)). In our implementation, however, this was less than 2x slower in wall-clock time
(see Appendix E.5 for more details and wall-clock comparison). Moreover, if not initialized in the
neighborhood of a local NE, high-order solvers and gradient-based methods might also converge to a
non-NE as described in Mazumdar et al. (2019) although this is likely a rare case.

**Comparison vs other game optimization algorithms. DAL has not been previously interpreted**
from a game perspective. Our interpretation allows us to bring recently proposed algorithms to the
context of differentiable games (Zhang & Yu, 2020; Azizian et al., 2020) to DAL. Classic examples are
the Extra-Gradient (EG) method (Korpelevich, 1976) and Consensus Optimization (CO) (Mescheder
et al., 2017). In Appendix B.2 we analyze the continuous dynamics of the EG method, and show that


-----

we cannot take the learning rate of EG to be large either. Thus, we obtain a similar conclusion as
Corollary 1. Then, in practice for DAL, stability for EG comes at the price of slow convergence due
to the use of small learning rates. We experimentally show this in Figure 3. With respect to CO, we
show in Appendix C that this algorithm can be interpreted in the limit as an approximation of the RK2
solver. In practice, if its additional hyper-parameter (γ) is tuned thoroughly, CO may approximate the
continuous dynamics better than GD and EG. We believe this may be the reason why CO slightly
outperforms GD and EG (see Appendix E.4). In all cases, RK solvers outperform GD, EG and CO.
This is in line with our theoretical analysis since they better approximate the continuous dynamics
(Hairer et al., 2006). It is worth noting that many other optimizers have recently been proposed in
the context of games e.g., Gidel et al. (2019a); Hsieh et al. (2020); Lorraine et al. (2021a;b). Some of
them are modifications of the EG method that we compared to e.g. Extra-Adam (Gidel et al., 2019a)
or double step-size EG (Hsieh et al., 2020). More practical modifications in terms of adaptive step
size could also be applied on top of RK solvers as done in Qin et al. (2020). A comparison of all
existing game optimizers in DAL, and a better theoretical understanding of such modification on RK
solvers are beyond the scope of this work. However, we believe it is an interesting and unexplored
research direction that our game perspective on DAL enables.

5 RELATED WORK
To the best of our knowledge, DAL has not been previously analyzed from a game perspective.
Moreover, the stability of the optimizer and the implications of introducing the GRL has not been
analyzed either. Here, we compare our results with the general literature.

**Gradient-Based Learning in Games. Ratliff et al. (2016) proposed a characterization of local Nash**
Equilibrium providing sufficient and necessary conditions for its existence. Mazumdar et al. (2020)
proposed a general framework to analyze the limiting behavior of the gradient-play algorithms in
games using tools from dynamical systems. Our work builds on top of this characterization but
specializes them to the domain-adversarial problem. We propose a more stable learning algorithm
that better approximates the gradient-play dynamics. Our resulting algorithm does not introduce
explicit adjustments or modify the learning dynamics, nor does it require the computation of the
several Hessian vector products or new hyperparameters. This is in contrast with general algorithms
previously analyzed in the context of differentiable games (Azizian et al., 2020; Letcher et al., 2019).

**Integration Methods and ML. Scieur et al. (2017) showed that accelerated optimization methods**
can be interpreted as integration schemes of the gradient flow equation. Zhang et al. (2018) showed
that the use of high order RK integrators can achieve acceleration in convex functions. In the context
of two-players game (i.e GANs), Gemp & Mahadevan (2018) consider using a second-order ODE
integrator. More recently, Qin et al. (2020) proposed to combine RK solvers with regularization on the
generators’ gradient norm. Chen et al. (2018) interpreted the residual connection in modern networks
as the Euler’s integration of a continuous systems. In our case, we notice that the combination of
GD with the GRL can be interpreted as the Euler’s discretization of the continuous gradient play
dynamics, which could prevent asymptotic convergence guarantees. We then study the discretization
step of popular ODE solvers and provide simple guarantees for stability. Moreover, our analysis is
based on a novel three-player game interpretation of the domain-adaptation problem. This is also
different from a single potential function or two-player games (i.e. GANs).

**Two-Player Zero-Sum Games have recently received significant attention in the machine learning**
literature due to the popularity of Generative Adversarial Networks (GANs) (Goodfellow et al.,
2014). For example, several algorithms have been proposed and analyzed (Mescheder et al., 2017;
Mertikopoulos et al., 2019; Gidel et al., 2019a;b; Zhang & Yu, 2020; Hsieh et al., 2020), in both
deterministic and stochastic scenarios. In our problem, we have a general three-player games resulting
of a novel game interpretation of the domain-adversarial problem. It is worth noting that while Gidel
et al. (2019a) focused on GANs, their convergence theory and methods for stochastic variational
inequalities could also be applied to three-players games and thus DAL using our perspective.

6 EXPERIMENTAL RESULTS

We conduct an extensive experimental analysis. We compare with default optimizers used in domainadversarial training such as GD, GD with Nesterov Momentum (GD-NM) (as in Sutskever et al.
(2013)) and Adam (Kingma & Ba, 2014). We also compare against recently proposed optimizers in
the context of differentiable games such as EG (Korpelevich, 1976) and CO (Mescheder et al., 2017).
We focus our experimental analysis on the original domain-adversarial framework of Ganin et al.


-----

(2016) (DANN). However, in section 6.2, we also show the versatility and efficacy of our approach
improving the performance of recently proposed SoTA DAL framework (e.g., f -DAL (Acuna et al.,
2021) combined with Implicit Alignment (Jiang et al., 2020)).

6.1 EXPERIMENTAL ANALYSIS ON DIGITS


**Implementation Details. Our first experimen-**
tal analysis is based on the digits benchmark
with models trained from scratch (i.e., with random initialization). This benchmark constitutes
of two digits datasets MNIST (CC BY-SA 3.0)
and USPS (LeCun et al., 1998; Long et al.,
2018) with two transfer tasks (M → U and U
_→_ M). We adopt the splits and evaluation protocol from Long et al. (2018) and follow their
standard implementation details.


|Grad. Descent Nesterov Mome|ntum|
|---|---|
|Adam Ours(RK2)||
|Ours(RK4)||
|||


16 24

# of Epochs


0.60

0.45

0.30

0.15


96

92

88 Grad. Descent

Nesterov Momentum

84 Adam

Ours(RK2)

Transfer Performance 80 Ours(RK4)

0 8 16 24

# of Epochs


For GD-NM, we use the default momentum
value (0.9). We follow the same approach for the
additional hyper-parameters of Adam. Hyper
,Ours (RK2)

parameters such as learning rate, learning sched
Epoch: 0 Epoch: 12 Epoch: 24

Figure 2: Our method vs popular optimizers on the

ule and adaptation coefficient (λ) are determined Digits Benchmark. (Top-Left) Loss in target domain.
for all optimizers by running a dense grid search (Top-Right) Transfer performance. (Bottom) t-SNE
and selecting the best hyper-parameters on the Visualization of the last layer representations during
transfer task M→U. As usual in UDA, the best training. Our method converges faster, has better percriteria are determined based on best transfer formance and produces more aligned features faster.
accuracy. The same parameters are then used
for the other task (i.e., U→M). We use the same seed and identically initialize the network weights
for all optimizers. This analysis is conducted on Jax (Bradbury et al., 2018) (see Appendix D).

**Comparison vs optimizers used in DAL. Figure 2 (top) illus-** 96
trates the training dynamics for the loss in the target domain

88

and the performance transfer. As expected, our optimizer converges faster and achieves noticeable performance gains. A 80 Extra-Gradient
core idea of DAL is to learn domain-invariant representations, Consensus Opt
thus we plot in Figure 2 (bottom) t-SNE (Van der Maaten & Transfer Performance 72 Ours(RK2)
Hinton, 2008) visualizations of the last layer features of the 64

|Col1|Col2|Col3|
|---|---|---|
||||
||||
|||Extra-Gradient Consensus Opt|
|||Ours(RK2) Ours(RK4)|

network. We show this over a sequence of epochs for GD with 0 8 # of Epochs16 24
GRL vs RK2. A different color is used for the source and target

Figure 3: Comparison among op
datasets. In the comparison vs Adam, we emphasize that Adam
computes adaptive learning rates which our method does not. timization algorithms (MDANN. _→U) with_
That said, Figure 2 shows that our two methods RK2 and RK4
outperform all baselines in terms of both convergence and transfer performance. In Figure 7, we
show how unstable these standard optimizers are when more aggressive step sizes are used. This is
in line with our theoretical analysis. Experimentally, it can be seen that in DAL, GD is more stable
than GD-NM and Adam, with the latter being the most unstable. This sheds lights on why well tuned
GD-NM is often preferred over Adam in DAL.

**Comparison vs game optimization algorithms. We now compare RK solvers vs other recently**
proposed game optimization algorithms. Specifically, we compare vs the EG method (Korpelevich,
1976) and CO (Mescheder et al., 2017). In every case, we perform a dense grid under the same
budget for all the optimizer and report the best selection (see Appendix E for details). In line with
our theoretical analysis of the continuous dynamics of the EG, we notice that the EG method is not
able to train with learning rates bigger than 0.006, as a result it performs signficantly worse than any
other optimizer (including simple GD). Also inline with our theoretical analysis, CO performs better
than EG and all other popular gradient algorithms used in DAL. This is because CO can be seen as
an approximation of Heun’s Method (RK2). More details on this in supplementary.

**Robustness to hyper-parameters. Figure 4 shows transfer performance of our method for different**
choices of hyper-parameters while highlighting (green line) the best score of the best performing GD
hyperparameters on the same dataset. Our method is robust to a wide variety of hyperparameters.


-----

Figure 5: Sensitivity to Sam_pling Noise . Different amounts_
of sampling noise controlled by
the batch size (64, 128, 160)
(Visda).

2.00

1.75

1.50

1.25


LR LR Schedule Method adapt Transfer Acc.

96 96

94 94

92 92

0.1 0.3 0.4 const poly RK2 RK4 0.1 0.5 1.0 92 94 96


Figure 4: Robustness to hyperparameters. We compare the transfer performance of our method for different hyperarameters in the task M→ U in the
Digits benchmark. Green line shows the best score for the best performing
hyperparameters of GD. Blue star corresponds to the best solution. Our
method performs well for a wide variety of hyperparameters.


72.5

70.0

67.5

65.0


Method M→U U→M Avg

GD-NMAdamGD 90.092.891.8 ± ± ± 0.4 0.3 0.3 93.496.894.4 ± ± ± 0.7 0.2 0.4 91.794.893.1

Ours(RK4)Ours(RK2) **95.195.0 ± ± 0.1 1.4** **97.597.3 ± ± 0.2 0.1** **96.396.1**


|arameters.|Col2|
|---|---|
|||
|||
|||
|Grad. Descent (Nest Ours (RK2)|erov)|


8000 16000 24000

Grad. Descent (Nesterov)
Ours (RK2)

# of Iterations

|Col1|Col2|Col3|
|---|---|---|
|Grad. Descen Nesterov Mo|t LR:0.3 mentun|LR:0.1|
||||
||||
||||


_±_ _±_ 0 8000 16000 24000 0 6000 12000 18000 24000

Grad. Descent LR:0.3
Nesterov Momentun LR:0.1

# of Iterations # of Iteration(s)

Table 3: Accuracy (%) on Digits Figure 6: Transfer Perfor- Figure 7: Stability anal. _on Digits._
(DANN). mance on Visda (DANN). Most aggressive step size before diver
gence. Adam diverges for η > 0.001.

6.2 COMPARISON IN COMPLEX ADAPTATION TASKS
Method Sim Real

We evaluate the performance of our algorithm with Resnet-50 (He _→_
et al., 2016) on more challenging adaptation benchmarks. Specif- GD-NM 71.7 ± 0.7
ically, this analysis is conducted on Visda-2017 benchmark (Peng

Ours(RK2) **73.8** 0.3

et al., 2017). This is a simulation-to-real dataset with two different _±_
domains: (S) synthetic renderings of 3D models and (R) real images. For this experiment, we use PyTorch (Paszke et al., 2019), our Table 1: Accuracy (DANN) on
evaluation protocol follows Zhang et al. (2019) and uses ResNet-50 Visda 2017 with ResNet-50.
as the backbone network. For the optimizer parameters, we tune
thoroughly GD-NM, which is the optimizer used in this setting (Long et al., 2018; Zhang et al., 2019;
Jiang et al., 2020; Acuna et al., 2021). For ours, we keep the hyper-parameters, but increase the
learning rate (0.2), and the batch size to 128. In this task, our approach corresponds to the improved
Euler’s method (RK2). Table 1 shows the comparison. Figure 6 compares the training dynamics of
our method vs GD-NM. In Figure 5, we evaluate the sensitivity of our method (in terms of transfer
performance) to sampling noise as controlled by the batch size.

**Improving SoTA DAL frameworks. We use this**

Method Sim Real

complex visual adaptation task to showcase the ap- _→_
plicability of our method to SoTA DAL frameworks. _f_ -DAL - GD-NM 72.9 (29.5K iter)
Specifically, we let the DA method being f -DAL _f_ -DAL - RK2 (Ours) 76.4 (10.5K iter)
Pearson as in Acuna et al. (2021) with Implicit Align
_f_ -DAL - GD-NM 72.9 (29.5K iter)
_f_ -DAL - RK2 (Ours) 76.4 (10.5K iter)

Table 2: Comparison using SoTA DA adversarial

ment Jiang et al. (2020). We use the tuned parameters

frameworks with ResNet-50 on Visda.

and optimizer from Acuna et al. (2021); Jiang et al.
(2020) as the baseline. In our case, we only increase the learning rate (0.2). Table 2 shows that our
method achieves peak results (+3.5%) in 10.5K iterations (vs 29.5K iterations for GD-NM).
**Natural Language Processing Tasks. We also evaluate our approach on natural language processing**
tasks on the Amazon product reviews dataset (Blitzer et al., 2006). We show noticeable gains by
replacing the GD with either RK2 or RK4. Results and details can be found in Appendix E.1.
7 CONCLUSIONS
We analyzed DAL from a game-theoretical perspective where optimality is defined as local NE. From
this view, we showed that standard optimizers in DAL can violate the asymptotic guarantees of the
gradient-play dynamics, requiring careful tuning and small learning rates. Based on our analysis, we
proposed to replace existing optimizers with higher-order ODE solvers. We showed both theoretically
and experimentally that these are more stable and allow for higher learning rates, leading to noticeable
improvements in terms of the transfer performance and the number of training iterations. We showed
that these ODE solvers can be used as a drop-in replacement and outperformed strong baselines.

**Acknowledgements. We would like to thank James Lucas, Jonathan Lorraine, Tianshi Cao, Rafid**
Mahmood, Mark Brophy and the anonymous reviewers for feedback on earlier versions of this work.


-----

REFERENCES

David Acuna, Guojun Zhang, Marc T. Law, and Sanja Fidler. f -domain adversarial learning: Theory and
algorithms. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pp.
66–75. PMLR, 2021.

Waïss Azizian, Ioannis Mitliagkas, Simon Lacoste-Julien, and Gauthier Gidel. A tight and unified analysis of
gradient-based methods for a whole spectrum of differentiable games. In Proceedings of the Twenty Third
_International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine_
_Learning Research, pp. 2863–2873. PMLR, 26–28 Aug 2020._

David Barrett and Benoit Dherin. Implicit gradient regularization. In International Conference on Learning
_[Representations, 2021. URL https://openreview.net/forum?id=3q5IqUrkcF.](https://openreview.net/forum?id=3q5IqUrkcF)_

Tamer Ba¸sar and Geert Jan Olsder. Dynamic Noncooperative Game Theory, 2nd Edition. Society for Industrial
[and Applied Mathematics, 1998. doi: 10.1137/1.9781611971132. URL https://epubs.siam.org/](https://epubs.siam.org/doi/abs/10.1137/1.9781611971132)
[doi/abs/10.1137/1.9781611971132.](https://epubs.siam.org/doi/abs/10.1137/1.9781611971132)

Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain
adaptation. In Advances in neural information processing systems, pp. 137–144, 2007.

Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan.
A theory of learning from different domains. Machine learning, 79(1-2):151–175, 2010a.

Shai Ben-David, Tyler Lu, Teresa Luu, and Dávid Pál. Impossibility theorems for domain adaptation. In
_Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 129–136,_
2010b.

Hugo Berard, Gauthier Gidel, Amjad Almahairi, Pascal Vincent, and Simon Lacoste-Julien. A closer look at
the optimization landscapes of generative adversarial networks. In International Conference on Learning
_[Representations, 2020. URL https://openreview.net/forum?id=HJeVnCEKwH.](https://openreview.net/forum?id=HJeVnCEKwH)_

John Blitzer, Ryan McDonald, and Fernando Pereira. Domain adaptation with structural correspondence learning.
In Proceedings of the 2006 conference on empirical methods in natural language processing, pp. 120–128,
2006.

James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George
Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable
[transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.](http://github.com/google/jax)

John Charles Butcher. A history of runge-kutta methods. Applied numerical mathematics, 20(3):247–260, 1996.

Woong-Gi Chang, Tackgeun You, Seonguk Seo, Suha Kwak, and Bohyung Han. Domain-specific batch
normalization for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer
_Vision and Pattern Recognition, pp. 7354–7362, 2019._

Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential
equations. In Advances in Neural Information Processing Systems, volume 31, 2018.

Nicolas Courty, Rémi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution optimal
transportation for domain adaptation. In Advances in Neural Information Processing Systems, volume 30,
2017.

Sofien Dhouib, Ievgen Redko, and Carole Lartizien. Margin-aware adversarial domain adaptation with optimal
transport. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International Conference on
_Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 2514–2524. PMLR, 13–18_
[Jul 2020. URL https://proceedings.mlr.press/v119/dhouib20b.html.](https://proceedings.mlr.press/v119/dhouib20b.html)

Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International
_conference on machine learning, pp. 1180–1189. PMLR, 2015._

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario
Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine
_Learning Research, 17(1):2096–2030, 2016._

Ian Gemp and Sridhar Mahadevan. Global convergence to the equilibrium of gans using variational inequalities.
_arXiv preprint arXiv:1808.01531, 2018._

Gauthier Gidel, Hugo Berard, Gaëtan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A variational
inequality perspective on generative adversarial networks. In ICLR, 2019a.


-----

Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Rémi Le Priol, Gabriel Huang, Simon LacosteJulien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics. In The 22nd International
_Conference on Artificial Intelligence and Statistics, pp. 1802–1811. PMLR, 2019b._

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
_systems, pp. 2672–2680, 2014._

Ernst Hairer, Marlis Hochbruck, Arieh Iserles, and Christian Lubich. Geometric numerical integration. Ober_wolfach Reports, 3(1):805–882, 2006._

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
_Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016._

Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor
Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In International conference on machine
_learning, pp. 1989–1998. PMLR, 2018._

Yu-Guan Hsieh, Franck Iutzeler, Jérôme Malick, and Panayotis Mertikopoulos. Explore aggressively, update
conservatively: Stochastic extragradient methods with variable stepsize scaling. In Advances in Neural
_Information Processing Systems, volume 33, pp. 16223–16234, 2020._

Xiang Jiang, Qicheng Lao, Stan Matwin, and Mohammad Havaei. Implicit class-conditioned domain alignment
for unsupervised domain adaptation. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th Interna_tional Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 4816–_
[4827. PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.press/v119/jiang20d.html.](http://proceedings.mlr.press/v119/jiang20d.html)

Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-nonconcave minimax
optimization? In International Conference on Machine Learning, pp. 4880–4889. PMLR, 2020.

[Mingsheng Long Junguang Jiang, Bo Fu. Transfer-learning-library. https://github.com/thuml/](https://github.com/thuml/Transfer-Learning-Library)
[Transfer-Learning-Library, 2020.](https://github.com/thuml/Transfer-Learning-Library)

Hassan K Khalil. Nonlinear systems. Prentice-Hall, 2002.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.

GM Korpelevich. The extragradient method for finding saddle points and other problems. Matecon, 12:747–756,
1976.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Jason D. Lee, Max Simchowitz, Michael I. Jordan, and Benjamin Recht. Gradient descent only converges to
minimizers. In 29th Annual Conference on Learning Theory, volume 49, pp. 1246–1257. PMLR, 2016.

Alistair Letcher, David Balduzzi, Sébastien Racanière, James Martens, Jakob Foerster, Karl Tuyls, and Thore
Graepel. Differentiable game mechanics. Journal of Machine Learning Research, 20(84):1–40, 2019. URL
[http://jmlr.org/papers/v20/19-008.html.](http://jmlr.org/papers/v20/19-008.html)

Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Moritz Hardt, Ben Recht, and Ameet
Talwalkar. Massively parallel hyperparameter tuning. 2018.

Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E Gonzalez, and Ion Stoica. Tune: A
research platform for distributed model selection and training. arXiv preprint arXiv:1807.05118, 2018.

Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation.
In Advances in Neural Information Processing Systems, pp. 1640–1650, 2018.

Jonathan Lorraine, David Acuna, Paul Vicol, and David Duvenaud. Complex momentum for optimization in
games, 2021a.

Jonathan Lorraine, Paul Vicol, Jack Parker-Holder, Tal Kachman, Luke Metz, and Jakob Foerster. Lyapunov
exponents for diversity in differentiable games. arXiv preprint arXiv:2112.14570, 2021b.

Haihao Lu. An o (sr)-resolution ode framework for discrete-time optimization algorithms and applications to
convex-concave saddle-point problems. arXiv preprint arXiv:2001.08826, 2020.


-----

Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and
algorithms. In Proceedings of The 22nd Annual Conference on Learning Theory (COLT 2009), Montréal,
[Canada, 2009. URL http://www.cs.nyu.edu/~mohri/postscript/nadap.pdf.](http://www.cs.nyu.edu/~mohri/postscript/nadap.pdf)

Eric Mazumdar, Lillian J Ratliff, and S Shankar Sastry. On gradient-based learning in continuous games. SIAM
_Journal on Mathematics of Data Science, 2(1):103–131, 2020._

Eric V Mazumdar, Michael I Jordan, and S Shankar Sastry. On finding local nash equilibria (and only local nash
equilibria) in zero-sum games. arXiv preprint arXiv:1901.00838, 2019.

Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar, and
Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile.
[In International Conference on Learning Representations, 2019. URL https://openreview.net/](https://openreview.net/forum?id=Bkg8jjC9KQ)
[forum?id=Bkg8jjC9KQ.](https://openreview.net/forum?id=Bkg8jjC9KQ)

Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. Advances in neural information
_processing systems, 30:1825–1835, 2017._

Dov Monderer and Lloyd S Shapley. Potential games. Games and economic behavior, 14(1):124–143, 1996.

Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media, 2006.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d’Alché Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Sys_[tems 32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/](http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf)_
[9015-pytorch-an-imperative-style-high-performance-deep-learning-library.](http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf)
[pdf.](http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf)

Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual
domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017.

Chongli Qin, Yan Wu, Jost Tobias Springenberg, Andy Brock, Jeff Donahue, Timothy Lillicrap, and Pushmeet
Kohli. Training generative adversarial networks by solving ordinary differential equations. In Advances in
_Neural Information Processing Systems, volume 33, pp. 5599–5609, 2020._

Lillian J Ratliff, Samuel A Burden, and S Shankar Sastry. On the characterization of local nash equilibria in
continuous games. IEEE Transactions on Automatic Control, 61(8):2301–2307, 2016.

Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrepancy for
unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
_Recognition, pp. 3723–3732, 2018._

Damien Scieur, Vincent Roulet, Francis Bach, and Alexandre d'Aspremont. Integration methods and optimization
algorithms. In Advances in Neural Information Processing Systems, volume 30, 2017.

Ozan Sener, Hyun Oh Song, Ashutosh Saxena, and Silvio Savarese. Learning transferrable representations for unsupervised domain adaptation. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon,
and R. Garnett (eds.), _Advances in Neural Information Processing Systems,_ volume 29. Curran Associates, Inc., 2016. [URL https://proceedings.neurips.cc/paper/2016/file/](https://proceedings.neurips.cc/paper/2016/file/b59c67bf196a4758191e42f76670ceba-Paper.pdf)
[b59c67bf196a4758191e42f76670ceba-Paper.pdf.](https://proceedings.neurips.cc/paper/2016/file/b59c67bf196a4758191e42f76670ceba-Paper.pdf)

Bin Shi, Simon S Du, Michael I Jordan, and Weijie J Su. Understanding the acceleration phenomenon via
high-resolution differential equations. arXiv preprint arXiv:1810.08907, 2018.

Rui Shu, Hung Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation.
In International Conference on Learning Representations, 2018.

Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros. Unsupervised domain adaptation through selfsupervision. arXiv preprint arXiv:1909.11825, 2019.

Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and
momentum in deep learning. In International conference on machine learning, pp. 1139–1147. PMLR, 2013.

Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning
_research, 9(11), 2008._


-----

Guojun Zhang and Yaoliang Yu. Convergence of gradient-based methods on bilinear games. In International
_Conference on Learning Representations, 2020._

Jingzhao Zhang, Aryan Mokhtari, Suvrit Sra, and Ali Jadbabaie. Direct runge-kutta discretization achieves
acceleration. arXiv preprint arXiv:1805.00521, 2018.

Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain
adaptation. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International
_Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 7404–7413,_
[Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http://proceedings.mlr.press/](http://proceedings.mlr.press/v97/zhang19i.html)
[v97/zhang19i.html.](http://proceedings.mlr.press/v97/zhang19i.html)


-----

SUPPLEMENTARY MATERIAL

CONTENTS

**1** **Introduction** **1**

**2** **Preliminaries** **2**

**3** **A Game Perspective on DAL** **3**

3.1 Domain-Adversarial Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

3.2 Characterization of the Domain-Adversarial Game . . . . . . . . . . . . . . . . . . . . . . . 4

**4** **Learning Algorithms** **4**

4.1 Continuous Gradient Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

4.2 Analysis of GD with the GRL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

4.3 Higher order ODE Solvers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

**5** **Related Work** **7**

**6** **Experimental Results** **7**

6.1 Experimental Analysis on Digits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8

6.2 Comparison in complex adaptation tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

**7** **Conclusions** **9**

**A Concepts in Game Theory** **15**

A.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

A.2 Games Characterizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

A.3 Case of Study in DANN. Original Formulation from Ganin et al. (2016) . . . . . . . . . . . . 15

**B** **Derivation of high-resolution ODEs** **17**

B.1 High-resolution ODE of second-order Runge–Kutta method . . . . . . . . . . . . . . . . . . 17

B.2 Continuous dynamics of Extra-Gradient (EG) . . . . . . . . . . . . . . . . . . . . . . . . . . 17

B.3 High-resolution ODE of classic fourth-order Runge–Kutta method (RK4) . . . . . . . . . . . 18

**C Proofs and additional theoretical results** **20**

C.1 Proposed Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

C.2 CO approximates RK2 (Heun’s Method) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

**D Experimental Setup Additional Details** **22**

**E** **Additional Experiments** **23**

E.1 Natural Language Processing Tasks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

E.2 Sensitivity to Sampling Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

E.3 Additional Comparison vs Game Optimization Algorithms . . . . . . . . . . . . . . . . . . . 24

E.4 CO vs Gradient Descent and Extra-Gradient Algorithms . . . . . . . . . . . . . . . . . . . . 24


-----

E.5 Wall-Clock Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

**F** **PyTorch PseudoCode of RK2 Solver** **25**

A CONCEPTS IN GAME THEORY

A.1 DEFINITIONS

**Definition 2. (Local Nash Equilibrium) : A point (wi[∗][, w][∗]** _i[)][ ∈]_ [Ω] _[is said to be a local Nash Equilibrium of the]_
_−_
_domain-adversarial game if there exists some δ > 0 such that:_

_i_ 1, 2, 3 _, Ji(wi[∗][, w][∗]_ _i[)][ ≤]_ _[J]i[(][w]i[, w][∗]_ _i[)][,]_ s.t. _ωi_ _ωi[∗]_ 2 _[< δ]_ (10)
_∀_ _∈{_ _}_ _−_ _−_ _||_ _−_ _[||]_

Intuitively, this is restricting the concept of NE to a local neighborhood B(x[∗], δ) := {||x − _x[∗]||2 < δ} with_
_δ > 0._

A more practical characterization of the NE can be given in terms of the Best Response Map of each player
which we now define.

**Definition 3. (Best Response Map (BR)) The best response map BRi : Ω−i ⇒** Ωi of player i is defined as:

_BRi(ω_ _i) := arg min_ _Ji(ωi, ω_ _i),_ (11)
_−_ _ωi∈Ωi_ _−_

here the symbol ⇒ emphasizes that the best response map is generally a set map and not a singleton, thus it
is not a function in general. In other words, there may be a subset of element in Ωi for which Ji(., ω _i) is a_
_−_
minimum.

The notion of NE can be defined in terms of the generalized BR : Ω ⇒ Ω map. This can be thought as an
stacked vector where the i-th element of BR is BRi(ω _i)._
_−_

**Proposition 5. A point wi[∗]**
_map. That is,_ _[∈]_ [Ω] _[is said to be a NE of the game if it is a fixed point of the generalized][ BR][ : Ω]_ [⇒] [Ω]
_ω[∗]_ _BR(ω[∗]) =_ _i_ 1, 2, 3 _,_ _ωi[∗]_ _i[(][ω][∗]_ _i[)]_ (12)
_∈_ _⇒∀_ _∈{_ _}_ _[∈]_ _[BR]_ _−_

_Proof. This follows from the definitions of BR map and NE._

**Definition 4. (Asymptotically Stable) A point ω is said to be a locally asymptotically stable point of the**
_continuous dynamics ˙ω = f_ (ω) if Re(λ) < 0 for all λ ∈ _Sp(∇f_ (ω)), where Sp(∇f (ω)) is the spectrum of
_∇f_ (ω).

Definition 4 is also known as the Hurwitz condition Khalil (2002).

**Definition 5. A stationary point x of a C** [2] _function φ : R[n]_ _→_ R is said to be a strict saddle point if:

_• λmin(∇xx[2]_ _[φ][(][x][∗][))][ <][ 0][ and,]_

_• λ(∇xx[2]_ _[φ][(][x][∗][))][ >][ 0][, for any other][ λ][ ∈]_ _[sp][(][∇][2]xx[φ][(][x][∗][))]_

A.2 GAMES CHARACTERIZATIONS

**Potential Games. Potential Games were introduced in Monderer & Shapley (1996) and can be defined as**
a type of game for which there exists an implicit potential function φ : Ω _→_ _R such that ∇φ(ω) = v(ω)._
Consequently, a necessary and sufficient condition for the game to be potential is the Jacobian of the vector field
_∇v(ω) being symmetric (see 3.3 in Mazumdar et al. (2020) and Monderer & Shapley (1996))._

**Purely Adversarial Games. This particular type of game refers to the other extreme in which H(ω) is a**
non-symmetric matrix with purely imaginary eigenvalues. If the game Hessian is skew-symmetric these have
also been called Hamiltonian Games Letcher et al. (2019).

A.3 CASE OF STUDY IN DANN. ORIGINAL FORMULATION FROM GANIN ET AL. (2016)

As mentioned in the main text (Section 2), our analysis is compatible with both the original and more recent
formulation of domain-adversarial training such as Zhang et al. (2019); Acuna et al. (2021). In this section, we
specifically derive additional results for DANN Ganin et al. (2016).


-----

In order to obtain the original formulation of DANN, let us define _ℓ[ˆ](_, b) = log(σ(b)) and φ[∗](t) = −_ log(1−e[t])
in Equation (2). This corresponds to the Jensen-Shannon divergence (JS) (up to a constant shift that does not
affect optimization). We can then rewrite ds,t as:

_ds,t = Ex∼ps log σ ◦_ _h[ˆ]_ _[′]_ _◦_ _g(x) + Ex∼pt log(1 −_ _σ ◦_ _h[ˆ]_ _[′]_ _◦_ _g(x))_ (13)


where σ(x) :=


1

1+e[−][x][ . To simplify the notation, we write][ H][ :=][ σ][ ◦H][.]


We can now re-define the pseudo-gradient v(w) of the game as the gradient of each player loss with respect to
its parameters. Letting α = 1, we get from Equation (4).

_v(ω) := (∇ω1_ _ℓ, ∇ω2_ (ℓ + λds,t), −∇ω3 _ds,t) ∈_ R[d]. (14)

The following propositions characterize local NE in terms of the pseudo-gradient v(w) and its Jacobian H(ω).
**Proposition 6. (Local NE) Suppose v(w) = 0 and**

_ω1_ _[ℓ]_ _ω1,ω2_ _[ℓ]_
_∇[2]_ _∇[2]_ 0, _ω3_ _[d]s,t_ (15)
_ω1,ω2_ _[ℓ]_ _ω2_ [(][ℓ] [+][ λd]s,t[)] _≻_ _∇[2]_ _[≺]_ [0][,]
∇[2] _∇[2]_ 

_then w is an isolated local NE._

The proof is simple and follows from Propositions 1 and 2, the definition of the vector field v(ω) and the
condition H + H _[⊤]_ _≻_ 0.

**Cooperation with Competition. By examining the matrix H(ω), one can see that, in our scenario, the game is**
neither a potential game nor a purely adversarial game. However, we can write the vector field in the following
form:

_ω1_ _ℓ_ 0
_∇_

_v(w) =_ _ω2_ _ℓ_ + _λ_ _ω2_ _dst_ (16)

∇   _∇_ 

0 _ω3_ _dst_

_−∇_

   

_∇φ(ω)_ _vˆ(ω)_

| {z } | {z }

where the first part corresponds to the gradient of the potential function φ(ω) = ℓ(ω1, ω2). The second part, on
the other hand, corresponds to a function ˆv(w) whose Jacobian is a non-symmetric matrix. Analyzing them
separately leads to either a potential or an adversarial game respectively. We define this particular type of game
as cooperation (i.e., in the potential term) with competition (i.e., the adversarial term).

It is worth noting that, while the spectrum of the game Hessian for the first term has only real eigenvalues, the
second term can have complex eigenvalues with a large imaginary component. Indeed, it can be shown that this
second term approximates the one obtained for a GAN using the non-saturating loss proposed by Goodfellow
et al. (2014) (e.g. λ = 1). In other words, the second term can be written as the pseudo-gradient of the two
player zero-sum game minω2 maxω3 dst. Building on this key observation and the work of Mescheder et al.
(2017); Berard et al. (2020) (Figure 4), where it was experimentally shown that the eigenvalues of the game
Hessian for GANs have indeed a large imaginary component around stationary points, we can assume that the
spectrum of the game Hessian in our case also have eigenvalues with a large imaginary component around the
stationary points. This observation can also be used with Corollary 1 to further motivate the use of higher-order
ODE solvers instead of GD with the GRL.
**Example 3. Consider the three-player game Equation (16) where ℓ(w1, w2) = w1[2]** [+ 2][w]1[w]2 [+][ w]2[2][,][ λ][ = 1][ and]
_ds,t(w2, w3) = w2[2]_ [+ 99][w]2[w]3 _[−]_ _[w]3[2][. The gradient play dynamics][ ˙]w = −v(w) becomes:_

_−2_ _−2_ 0

_w˙_ = Aw =  _−2_ _−4_ _−99_  _w._

0 99 _−2_
 

_The eigenvalues of A are −2 and −3 ± 2i√2449. From Corollary 1, η should be 0 < η < 6.2 · 10[−][3]._

**Is the three-player game formulation desired? In domain adaptation, optimization is a means to an end. The**
final goal is to minimize the upper bound from Theorem 1 to ensure better performance in the target domain. One
might then wonder whether interpreting optimality in terms of NE is desirable. In our context, NE means finding
the optimal g[∗], _h[ˆ]_ _[∗]_ and _h[ˆ]_ _′∗_ of the cost functions defined in Equation (4). This in turns leads to minimizing the
upper bound in Theorem 1.

**Remark on sequential games: Recently, Jin et al. (2020) introduced a notion of local min-max optimality**
for two-player’s game exploiting the sequential nature of some problems in adversarial ML (i.e GANs). In
domain-adversarial learning, updates are usually performed in a simultaneous manner using the GRL. Thus, we
focus here on the general case where the order of the players is not known.


-----

B DERIVATION OF HIGH-RESOLUTION ODES

**Lemma 2. The high resolution ODE of resulting of the GD algorithm with the GRL is:**

_w˙_ = _v(w)_ (17)
_−_ _−_ _[η]2_

_[∇][v][(][w][)][v][(][w][) +][ O][(][η][2][)][,]_

_Proof. This follows from Corollary 1 of Lu (2020)._

B.1 HIGH-RESOLUTION ODE OF SECOND-ORDER RUNGE–KUTTA METHOD

The high-resolution ODE was discussed in Shi et al. (2018); Lu (2020). For discrete algorithms with the
following update:

_w[+]_ = w + f (η, w), (18)

we can think of the trajectory as a discretization of the continuous dynamics w : [0, +∞) → R[d], and in
Equation (18), we have w = w(t), w[+] = w(t + η). Here, with slight abuse of notation we also use w for the
continuous function of dynamics.

We derive high-resolution ODE of the second-order Runge–Kutta method:

_wk+1/2 = wk_
_−_ 2[η]α _[v][(][w][k][)][, w][k][+1][ =][ w][k][ −]_ _[η][((1][ −]_ _[α][)][v][(][w][k][) +][ αv][(][w][k][+1][/][2][))][,]_

where 0 < α ≤ 1 and α is a constant. If α = 1/2, we obtain Heun’s method; if α = 1, we obtain the midpoint
method; if α = 2/3, we obtain the Ralston’s method. Combining the two equations, we have:
_wk+1 −_ _wk_ = (1 _α)v(wk)_ _αv(wk_ (19)

_η_ _−_ _−_ _−_ _−_ 2[η]α _[v][(][w][k][))][.]_


Using the Taylor expansion:

_v(wk −_ 2[η]α _[v][(][w][k][))]_ = _v(wk) −_ 2[η]α _[∇][v][(][w][k][)][⊤][v][(][w][k][) +][ O][(][η][2][)]_

Plugging it back into Equation (19) and using the Taylor expansion wk+1 = wk + η ˙w + η[2]w/¨ 2, we have:


_w˙_ + [1] _w =_ _v(w) + [1]_ (20)

2 _[η][ ¨]_ _−_ 2 _[∇][v][(][w][)][⊤][v][(][w][) +][ O][(][η][2][)][.]_

Now we make the assumption that we have the high-resolution ODE that:

_w˙_ = f0(w) + ηf1(w) + O(η[2]). (21)

Taking the derivative over t we have:

_w¨ = ∇f0(w)f0(w) + O(η)._ (22)

Combining Equation (20), Equation (21) and Equation (22), we obtain that:

_f0(w) = −v(w), f1(w) = 0,_ (23)

i.e., the high resolution ODE of second-order Runge–Kutta method is:

_w˙_ = −v(w) + O(η[2]). (24)

B.2 CONTINUOUS DYNAMICS OF EXTRA-GRADIENT (EG)

The continuous dynamics of Gradient Descent Ascent (GDA), Extra-Gradient (EG) and Heun’s method can be
summarized as follows:
_w˙_ = v(w) + α∇v(w)v(w)
For GDA, we have α = −η/2; for EG, we have α = η/2 (Lu, 2020); for Heun’s method, ˙w = v(w) + O(η[2]).
The Jacobian of the dynamics at the stationary point is ∇v(w) + α∇v(w)[2]. Take λ = a + ib ∈ _Sp(∇v(w))._
The eigenvalue of the Jacobian of the dynamics is:

_α(a + ib)[2]_ + a + ib = a + α(a[2] _−_ _b[2]) + i(b + 2ab)α._ (25)

We want the real part to be negative, i.e.:


_a + α(a[2]_ _−_ _b[2]) < 0,_ (26)

_a(1 + αa) < αb[2]._ (27)


and thus:


for EG, α = η/2 and the dynamics diverges if a(1+(η/2)a) ≥ _ηb[2]/2. When η is large, and η(a[2]−b[2])/2 ≥−a_
then it diverges. However, the high-resolution ODE of second-order Runge–Kutta methods only requires a < 0.


-----

B.3 HIGH-RESOLUTION ODE OF CLASSIC FOURTH-ORDER RUNGE–KUTTA METHOD (RK4)

In this subsection, we derive the high-resolution ODE of the classic fourth-order Runge–Kutta method. We
prove the following result:
**Theorem 3. The high-resolution ODE of the classic fourth-order Runge–Kutta method (RK4):**

_w[+]_ = w − _[η]6 [(][v][(][w][) + 2][v][2][(][w][) + 2][v][3][(][w][) +][ v][4][(][w][))][,]_ (28)


_where_

_is_


_v2(w) = v(w_ (29)
_−_ _[η]2_ _[v][(][w][))][, v][3][(][w][) =][ v][(][w][ −]_ _[η]2_ _[v][2][(][w][))][, v][4][(][w][) =][ v][(][w][ −]_ _[ηv][3][(][w][))][,]_


_w˙_ = −v(w) + O(η[4]). (30)

_Proof. We use the following Taylor expansion:_

_v(w + δ) = v(w) =_ _v(w)δ + [1]_ (31)
_∇_ 2 _[∇][2][v][(][w][)(][δ, δ][) + 1]6_ _[∇][3][v][(][w][)(][δ, δ, δ][) +][ O][(][∥][δ][4][∥][)][,]_

where ∇[2]v(w) : R[d] _× R[d]_ _→_ R[d] is a symmetric bilinear form, and ∇[3]v(w) : R[d] _× R[d]_ _× R[d]_ _→_ R[d] is a
symmetric trilinear form. With the formula we have:

_v4(w) = v(w) −_ _η∇v(w)v3(w) +_ _[η]2[2]_ _[∇][2][v][(][w][)(][v][3][(][w][)][, v][3][(][w][))][ −]_ _[η]6[3]_ _[∇][3][v][(][w][)(][v][3][(][w][)][, v][3][(][w][)][, v][3][(][w][)) +][ O](32)[(][η][4][)][,]_

_v3(w) = v(w) −_ _[η]2_ 8 48 _[∇][3][v][(][w][)(][v][2][(][w][)][, v][2][(][w][)][, v][2][(][w][)) +][ O][(][η][4][)][,]_

_[∇][v][(][w][)][v][2][(][w][) +][ η][2]_ _[∇][2][v][(][w][)(][v][2][(][w][)][, v][2][(][w][))][ −]_ _[η][3]_ (33)


_v2(w) = v(w) −_ _[η]2_ 8 48 _[∇][3][v][(][w][)(][v][(][w][)][, v][(][w][)][, v][(][w][)) +][ O][(][η][4][)][.]_

_[∇][v][(][w][)][v][(][w][) +][ η][2]_ _[∇][2][v][(][w][)(][v][(][w][)][, v][(][w][))][ −]_ _[η][3]_ (34)

Putting them together we have:


_v4(w) + 2v3(w) + 2v2(w) + v(w) = 6v(w)_ _η_ _v(w)(v3(w) + v2(w) + v(w))_
_−_ _∇_

+ _[η]2[2]_ _∇[2]v(w)(v3(w), v3(w)) + [1]2_ _[∇][2][v][(][w][)(][v][2][(][w][)][, v][2][(][w][)) + 1]2_ _[∇][2][v][(][w][)(][v][(][w][)][, v][(][w][))]_




(35)

_−_ _[η]4[3]_

_[∇][3][v][(][w][)(][v][(][w][)][, v][(][w][)][, v][(][w][)) +][ O][(][η][4][)][,]_

_v3(w) + v2(w) + v(w) = 3v(w) −_ _[η]2_ 4

_[∇][v][(][w][)(][v][2][(][w][) +][ v][(][w][)) +][ η][2]_ _[∇][2][v][(][w][)(][v][(][w][)][, v][(][w][)) +][ O][(][η][3][)](36)[,]_

_v2(w) + v(w) = 2v(w)_ (37)
_−_ _[η]2_

_[∇][v][(][w][)][v][(][w][) +][ O][(][η][2][)][.]_

Bringing Equation (37) into Equation (36), we obtain:


_v3(w) + v2(w) + v(w) = 3v(w) −_ _η∇v(w)v(w) +_ _[η]4[2]_ _[∇][2][v][(][w][)(][v][(][w][)][, v][(][w][)) +][ η]4 [2]_ [(][∇][v][(][w][))][2][v][(][w][) +][ O][(][η][3][)][.][ (38)]

Putting Equation (38) and Equation (37) together, we have:

_η_ _v(w)(v3(w) + v2(w) + v(w)) =_ 3η _v(w)v(w) + η[2](_ _v(w))[2]v(w)_
_−_ _∇_ _−_ _∇_ _∇_

_−_ _[η]4[3]_ 4 [(][∇][v][(][w][))][3][v][(][w][) +][ O][(][η][4][)][.]

_[∇][v][(][w][)][∇][2][v][(][w][)(][v][(][w][)][, v][(][w][))][ −]_ _[η][3]_ (39)


Moreover, we have

_∇[2]v(w)(v2(w), v2(w)) = ∇[2]v(w)(v −_ _[η]2_ 2

_[∇][v][(][w][)][v][(][w][)][, v][ −]_ _[η]_ _[∇][v][(][w][)][v][(][w][)) +][ O][(][η][2][)]_

= ∇[2]v(w)(v(w), v(w)) − _η∇[2]v(w)(∇v(w)v(w), v(w)) + O(η[2]),_ (40)

and similarly


_∇[2]v(w)(v3(w), v3(w)) = ∇[2]v(w)(v(w), v(w)) −_ _η∇[2]v(w)(∇v(w)v(w), v(w)) + O(η[2])._ (41)


-----

Bringing Equation (39) and Equation (40) into Equation (35) results in

_v4(w) + 2v3(w) + 2v2(w) + v(w) = 6v(w)_ 3η _v(w)v(w) + η[2]((_ _v(w))[2]v(w) +_ _v(w)(v(w), v(w))_
_−_ _∇_ _∇_ _∇[2]_

_−_ _[η]4 [3]_ [((][∇][v][(][w][))][3][v][(][w][) + 3][∇][2][v][(][w][)(][∇][v][(][w][)][v][(][w][)][, v][(][w][))]

+ ∇v(w)∇[2]v(w)(v(w), v(w)) + ∇[3]v(w)(v(w), v(w), v(w))) + O(η[4]).
(42)

Let us now derive the high-resolution ODE. From Equation (28), we have:


_w[+]_ _−_ _w_


= (43)
_−_ 6[1] [(][v][4][(][w][) + 2][v][3][(][w][) + 2][v][2][(][w][) +][ v][(][w][))][.]


Let us assume that w[+] = w(t + η) and w = w(t). Expanding the left we have:


_w˙_ + _[η]_ _w +_ _[η][2]_

2 [¨] 6


... ....
_w +_ _[η][3]_ _w._ (44)

24


Let us assume that the high-resolution ODE up to O(η[4]) has the form:

_w˙_ = f0(w) + ηf1(w) + η[2]f2(w) + η[3]f3(w) + O(η[4]). (45)

Taking derivatives on both sides, we have:

_w¨ = (_ _f0(w) + η_ _f1(w) + η[2]_ _f2(w) + η[3]_ _f3(w)) ˙w + O(η[4])_
_∇_ _∇_ _∇_ _∇_

= _f0(w)f0(w) + η(_ _f1(w)f0(w) +_ _f1(w)f0(w)) + η[2](_ _f1(w)f1(w) +_ _f0(w)f2(w) +_ _f2(w)f0(w))_
_∇_ _∇_ _∇_ _∇_ _∇_ _∇_

+ O(η[3]). (46)


Comparing the order O(1) on both sides of Equation (43) we have:

_f0(w) =_ _v(w), f1(w) + [1]_ (47)
_−_ 2 _[∇][f][0][(][w][)][f][0][(][w][) = 1]2_ _[∇][v][(][w][)][v][(][w][)][,]_


which gives f0(w) = −v(w) and f1(w) = 0. Bringing it back to Equation (46) we obtain:

_w¨ = ∇v(w)v(w) + O(η[2])._ (48)

We take the derivatives on both sides of Equation (48) to get:
...
_w = ∇(∇v(w)v(w)) ˙w + O(η[2])_

= −∇(∇v(w)v(w))v(w) + O(η[2]). (49)

Let us now compute ∇(∇v(w)v(w))v(w). We note that ∇(∇v(w)v(w)) is a linear form and

_∇v(w + δ)v(w + δ) = (∇v(w) + ∇[2]v(w)δ + o(∥δ∥))(v(w) + ∇v(w)δ + o(∥δ∥))_

= ∇v(w)v(w) + ∇[2]v(w)(δ, v(w)) + ∇[2]v(w)δ + o(∥δ∥). (50)

Therefore, we have
...
_w = −∇(∇v(w)v(w))v(w) = −∇[2]v(w)(v(w), v(w)) −∇[2]v(w)v(w) + O(η[2])._ (51)

With Equation (51) we can compare O(η[2]) on both sides on Equation (43) and obtain

_f2(w) = 0._ (52)

Finally, we take the derivative of Equation (51). Since

_∇[2]v(w + δ)(v(w + δ), v(w + δ)) = (∇[2]v(w) + ∇[3]v(w)δ)(v(w) + ∇v(w)δ, v(w) + ∇v(w)δ)_

= ∇[2]v(w)(v(w), v(w)) + ∇[3]v(w)(δ, v(w), v(w)) + 2∇[2]v(w)(v(w), ∇v(w)δ)
+ o(∥δ∥), (53)

we have:


_∇(∇[2]v(w)(v(w), v(w)))v(w) = ∇[3]v(w)(v(w), v(w), v(w)) + 2∇[2]v(w)(v(w), ∇v(w)v(w))._ (54)

Similarly, since

(∇v(w + δ))(∇v(w + δ))v(w + δ) = (∇v(w) + ∇[2]v(w)δ)(∇v(w) + ∇[2]v(w)δ)(v(w) + ∇v(w)δ)

= (∇v(w))[2]v(w) + ∇[2]v(w)(δ, ∇v(w)v(w))

+ ∇v(w)∇[2]v(w)(δ, v(w)) + ∇v(w))[3]δ + (∥δ∥) (55)


-----

_∇(∇v(w))[2]v(w)(v(w)) = ∇[2]v(w)(v(w), ∇v(w)v(w)) + ∇v(w)∇[2]v(w)(v(w), v(w)) + ∇v(w)[3]v(w). (56)_

Combining Equation (51), Equation (54) and Equation (56) gives:
....
_w = ∇w[...] ˙w = ∇v(w)[3]v(w) + ∇[3]v(w)(v(w), v(w), v(w)) + 3∇[2]v(w)(v(w), ∇v(w)v(w))_

+ ∇v(w)∇[2]v(w)(v(w), v(w)). (57)

Combining Equation (45), Equation (46), Equation (51) and Equation (57) and comparing the O(η[3]) term of
Equation (43), we obtain

_f3(w) = 0._ (58)

Therefore, the high-resolution ODE is:

_w˙_ = −v(w) + O(η[4]). (59)

C PROOFS AND ADDITIONAL THEORETICAL RESULTS

Several proofs in this section are based on the derivations of the high resolution ODEs that were proven in
Appendix B.

**Theorem 4. Suppose φ : R[n]** _→_ R and φ is C [2]. Suppose also any stationary point x[∗] _of φ s.t. ∇φ(x[∗]) = 0 is_
_either:_

_1. a strict local minimum, i.e. ∇xx[2]_ _[φ][(][x][∗][)][ ≻]_ [0][;]

_2. a strict saddle. (Definition 5)_

_Then, the gradient flow is attracted towards a local minimum of φ._


_Proof._


_x˙_ = −∇φ(x) (60)


Define g(x) := −∇φ(x).

a) Let us first show that strict saddles are non-asymptotically stable fixed points. Thus, gradient descent is not
attracted to them.


_g(x) ≈−∇xx[2]_ _[φ][(][x][)(][x][ −]_ _[x][∗][)]_ since ∇φ(x[∗]) = 0 (61)

_g(z) ≈−Λ(z −_ _z[∗])_ where z := U _[T]_ _x_ (62)

This impliesasymptotically stable, as desired. g(z)i ≈ _λmin(∇xx[2]_ _[φ][(][x][∗][))(][z][ −]_ _[z][∗][)]i_ _[>][ 0][. Thus, we just showed that strict saddles are non-]_

Let us now show that all the local minima are asymptotically stable, and gradient descent is then attracted to
them.

b) Define V (x) := φ(x) − _φ(x[∗]) to be a Lyapunov function. We have x[∗]_ is asymptotically stable if:

_∇V (x)[T]_ _g(x) < 0_ _∀x ̸= x[∗]_ (63)

From which the result follows. (e.g ∇V (x)[T] _g(x) = −||g(x)||2[2][).]_

Similar results with a different proof can be derived from Lee et al. (2016).


**Theorem 2. The high resolution ODE of GD with the GRL up to O(η) is:**

_w˙_ = _v(w)_ (64)
_−_ _−_ _[η]2_

_[∇][v][(][w][)][v][(][w][)]_

_Moreover, this is asymptotically stable (see Appendix A.1) at a stationary point w[∗]_ _(Proposition 3) iff for all_
_eigenvalue written as λ = a + ib ∈_ _Sp(−∇v(w[∗])), we have 0 > η(a[2]_ _−_ _b[2])/2 > a._

_Proof. This theorem can be obtained by computing the Jacobian of the ODE dynamics and using the Hurwitz_
condition Lemma 1. Specifically, we have the Jacobian of the dynamics at the stationary point is:


_v(ω[∗])_ (65)
_−∇_ _−_ _[η]2_

_[∇][v][(][ω][∗][)][2]_


-----

Take λ = a + ib ∈ _Sp(−∇v(ω[∗])), we must have:_

_ℜ[a + ib −_ _[η]2 [(][−][a][ −]_ _[ib][)][2][]][ <][ 0][,][ where][ ℜ]_ [stands for the real part.] (66)


then,

as desired.


_a −_ _[η]2 [(][a][2][ −]_ _[b][2][)][ <][ 0]_ (67)


**Corollary 1. For the high resolution ODE of GD with GRL (i.e., Equation (64)) to be asymptotically stable, the**
_learning rate η should be upper bounded by:_

2a
0 < η < _−_ (68)

_b[2]_ _a[2][,]_
_−_

_for all λ = a + ib ∈_ _Sp(−∇v(w[∗])) with large imaginary part (i.e. such that |a| < |b|)._

_Proof. With |a| < |b| where b is the imaginary part of λ ∈_ C, an algebraic manipulation of Theorem 2.
i.e. η(b[2] _−_ _a[2]) < −2a leads to the desired result._

**Theorem 3. The high resolution ODE of any RK2 method up to O(η), ˙w = −v(w), is asymptotically stable if**
_for all eigenvalues λ = a + ib ∈_ _Sp(−∇v(w[∗])), we have a < 0._

_Proof. This theorem can be obtained by computing the Jacobian of the ODE dynamics (see Appendix B for the_
derivation) and using the Hurwitz condition Lemma 1. For details, see the proof of Theorem 2.

Unlike the high resolution ODE of GD with GRL in Corollary 1, up to O(η), there is no upper bound constraint
on the learning rate. Therefore, at the cost of an additional extra-gradient computation, we are allowed to take
a more aggressive step. We observe in practice that this leads to both faster convergence and better transfer
performance.

**Lemma 3. Suppose the game G(I, Ωi, Ji) is a potential game with potential function φ. Suppose the minimizers**
_of φ are either a local minimum or a strict saddle (see Definition 5). The gradient play dynamics converges to a_
_local Nash Equilibrium._

_Proof. A potential game can be analyzed in terms of the implicit function φ(w) since ∇φ(w) = v(w). From_
Theorem 4, we know the gradient flow in φ(w) converges to a point w[∗] that is a local minimum. Thus, this
follows from Proposition 6.

**Proposition 4. The domain-adversarial game is neither a potential nor necessarily a purely adversarial game.**
_Moreover, its gradient dynamics are not equivalent to the gradient flow._

_Proof. From appendix A.2, a sufficient and necessary condition for a game to be potential is ∇v(ω) being_
symmetric. Computing ∇v(ω) using Equation (4) leads to ∇v(ω) being asymmetric, from which the first
part of the result follows. Another way to see this, it is to notice a there is no potential function φ satisfying
_∇φ(ω) = v(ω) (because of the flip in the sign in ds,t). Since the game is not a potential game then the vector_
field is not equal to the gradient flow (see also for more details Mazumdar et al. (2020). ) To see the game
is not necessarily a purely adversarial game, it suffices to show an example for which this does not happen.
See Appendix A.3 and example therein.

C.1 PROPOSED LEARNING ALGORITHM

**Algorithm 1 Pseudo-Code of the Proposed Learning Algorithm**

**Input: source data Ds, target data Dt, player losses Ji, network weights {ωi}i[3]=1[, learning rate][ η]**

**for t = 1 to T −** 1 do

_xv =s, y [gradients ∼_ _Ds, x(xts ∼, xDt, yt_ _s, Ji) for i in 1..3 ]._
_ω = Gradient Descent(v, w, η)_
_ω = Runge-KuttaSolver(v, w, η, steps=1)_

**end for**


-----

C.2 CO APPROXIMATES RK2 (HEUN’S METHOD)

We have the update rule of CO Mescheder et al. (2017):

_ω[+]_ = ω [ηv(ω) + γ [1] 2[]] (69)
_−_ 2 _[∇||][v][(][ω][)][||][2]_


where 2[1] _[∇||][v][(][ω][)][||]2[2]_ [=][ ∇][v][(][w][)][⊤][v][(][w][)]

From RK2 (Heun‘s Method), we have:

_w[+]_ = w − _[η]2 [(][v][(][w][) +][ v][(][w][ −]_ _[ηv][(][w][))][)][.]_ (70)

= w − _[η]2 [(2][v][(][w][)][ −]_ _[η][∇][v][(][w][)][v][(][w][) +][ O][(][η][2][))]_ (71)


If −∇v(w) = ∇v(w)[⊤] =⇒∇v(w) skew-symmetric, we can see that Equation (71) approximates the CO
optimization method from Mescheder et al. (2017). This assumption is however not necessarily true in our case
which may explain why RK2 was better in experiments. Experimentally, for CO, the best transfer performance
was obtained for γ = 0.0001. We also observed that CO is very sensitive to the choice of γ.

D EXPERIMENTAL SETUP ADDITIONAL DETAILS

Our algorithm is implemented in Jax Bradbury et al. (2018) (Digits, NLP benchmark) and PyTorch (Visual Task).
Experiments are conducted on a NVIDIA Titan V and V100 GPU Cards.

The digits benchmark constitutes of two digits datasets MNIST and USPS LeCun et al. (1998); Long et al.
(2018) with two transfer tasks (M → U and U → M). We adopt the splits and evaluation protocol from Long
et al. (2018) which uses 60,000 and 7,291 training images and the standard test set of 10,000 and 2,007 test
images for MNIST and USPS, respectively. We follow the standard implementation details of Long et al. (2018)
for this benchmark. Therefore, we use LeNet LeCun et al. (1998) as the backbone architecture, dropout (0.5),
fix the batch size to 32, the number of iterations per epoch to 937 and use weight decay (0.005). For GD-NM,
we use the default momentum value (0.9). We follow the same approach for the additional hyper-parameters of
Adam.

**Grid Search Details for Comparison vs Standard Optimizers. For the learning rate, learning schedule and**
adaptation coefficient (λ), we run the following grid search and select the best hyper-parameters for each
optimizer.

-  learning rate: 0.0001, 0.001, 0.01, 0.03, 0.1, 0.3, 0.4.

-  learning scheduler: none, polynomial.

-  adaptation coefficient (λ): 0.1, 0.5, 1.

The grid is run using Ray Tune Liaw et al. (2018) and on the transfer task M→U. To avoid expending resources
in bad runs we use a HyperBandScheduler Li et al. (2018). The same hyper-parameters are used for the other
task (i.e U → M). The best criteria correspond to the transfer accuracy (as it is typically done in UDA).

**Grid Search Details for Comparison vs Game Optimization Algorithms. Similar to the previous experiment,**
for the comparison vs game optimization algorithms, we run the following grid search in order to select their
best parameters for the comparison. In the case of CO, we additionally add γ extra-hypermeter to the grid as we
experimentally found this optimizer to be sensitive to this parameter.

-  learning rate: 0.0001, 0.001, 0.006, 0.01, 0.03, 0.1, 0.3, 0.4.

-  learning scheduler: none, polynomial.

-  Adaptation coefficient (λ): 0.1, 0.5, 1.

-  CO Only. Additional (γ): 10, 1.0, 0.1, 0.01, 0.001, 0.001, 0.0001

We show the best results of this grid search in Table 6. For EG, we observe training diverges for learning rates
greater than 0.006.

**Experiment on Visda 2017. For the visual tasks our implementation is built on PyTorch(1.5.1) and on top of**
the public available implementations of Long et al. (2018); Zhang et al. (2019); Junguang Jiang (2020). We did
not run grid search here as it would be very computationally expensive. For the optimizer parameters, we tune
thoroughly GD with Nesterov Momentum following the recommendation and implementation from Long et al.
(2018); Zhang et al. (2019). For our method, we keep the exact same hyper-parameters of GD with NM, but


-----

increase the learning rate. We do not experiment with RK4 since the performance was similar to RK2 in the
other benchmarks (see Tables 3 and 4), and it is more computationally expensive.

**Experiment on Visda 2017 with SoTA algorithms. For the comparison using SoTA algorithms, we use the**
well-tuned hyper-parameters from Jiang et al. (2020) and set the discrepancy measure to be f -DAL Pearson
as in Acuna et al. (2021) since this empirically was shown to be better than both JS and γ-JS/MDD. We use
Implicit Alignment Jiang et al. (2020) as this method also allows to account for the dissimilarity between the
labels marginals. We did not run grid search here as it would be very computationally expensive. In our case, we
only increase the learning rate to 0.2.

**Experiment on Natural Language. In this experiment, we use the Amazon product reviews dataset Blitzer**
et al. (2006) that contains online reviews of products collected on the Amazon website. We follow the splits and
evaluation protocol from Courty et al. (2017); Dhouib et al. (2020) and choose 4 of its subsets corresponding to
different product categories, namely: books (B), dvd (D), electronics (E) and kitchen (K). As in Courty et al.
(2017); Dhouib et al. (2020), from where we took the baseline result, the network architecture is a two layer
MLP with sigmoid function. In our case, hyper-parameters are kept the same but the learning rate is increased
by a factor of 10.

**Implementation on different frameworks. We use both Jax and Pytorch in our paper for simplicity. Since**
the proposed method only requires changing some lines of code, it can be easily implemented in different
frameworks. Prototyping in Jax small scale experiments such as the one in the Digits datasets is very easy. We
then use PyTorch in order to integrate our optimizers with large-scale SoTA UDA methods (their codebase
was implemented in PyTorch). This shows the versatility and simplicity of implementation of high-order ODE
solvers. We did not see any issue while working with either Jax or Pytorch. Our choice of framework was simply
based on modifying existing source code to evaluate our optimizer.

**Remark on error bars. For experimental analysis involving a grid-search, we use the same seed and identically**
initialize the networks. That is, we give every optimizer the same fair opportunity to win as reported above.
Note that reporting avg and std from a grid-search significantly increases the computational demand so we did
not do this. That said, we report avg and std for our main experiments using the best parameters found on the
grid-search.

E ADDITIONAL EXPERIMENTS

E.1 NATURAL LANGUAGE PROCESSING TASKS.

We also evaluate our approach on natural language processing tasks. We use the Amazon product reviews dataset
(Blitzer et al., 2006). We follow the splits and evaluation protocol from Courty et al. (2017); Dhouib et al. (2020)
and choose 4 of its subsets corresponding to different product categories, namely: books (B), dvd (D), electronics
(E) and kitchen (K). In our case, hyper-parameters are the same provided by authors but the learning rate is
increased by a factor of 10. Table 4 shows noticeable gains by replacing the optimizer with either RK2 or RK4.

B→D B→ E B→ K D→B D →E D→K E → B E → D E → K K → B K → D K → E Avg

DANN (Dhouib et al., 2020) 80.6 74.7 76.7 74.7 73.8 76.5 71.8 72.6 85 71.8 73 84.7 76.3

with RK2 **82.3** 74.3 75.7 **80.2** **79.2** **79.6** 72.3 74.2 86.5 72.6 73.6 **86.2** 78.1
with RK4 81.6 **75.0** **78.1** 79.4 78.3 80.0 **74.1** **74.6** **86.7** **73.5** **74.6** 86.0 **78.5**

Table 4: Accuracy (%) on the Amazon Reviews Sentiment Analysis Dataset (NLP)

E.2 SENSITIVITY TO SAMPLING NOISE

Table 5: Sensitivity to Sampling Noise controlled by the batch size in the Visda Dataset. Resnet-50

Batch Size Avg Std


64 67.82 0.29
128 67.50 0.11
160 67.42 0.24


GD


64 73.20 0.36
Ours(RK2) 128 73.81 0.26
160 **74.18** 0.15


-----

Table 6: Comparison vs Game Optimization Algorithms (best result from the grid search)


Algorithm M→ U

GD 90.0
GD-NM 93.2

EG Korpelevich (1976) 86.7
CO Mescheder et al. (2017) 93.8

**Ours(RK2)** 95.3
**Ours(RK4)** **95.9**

E.3 ADDITIONAL COMPARISON VS GAME OPTIMIZATION ALGORITHMS

In Table 5, we show the sensitivity of our method to sampling Noise controlled by the mini-batch size. Specifically, we show results for 64, 128, 160 ( with 160 being the bigger size we could fit in GPU memory). We also
add results for GD. We observed that our method performs slightly better when the batch size was bigger.

Table 6 shows the results of the grid-search for the comparison vs game optimization algorithms. For each
optimizer, these results correspond to their best (in terms of transfer performance) determined from the gridsearch explained in Appendix E. We also add GD for comparison. Our method notably outperforms the
baselines.

E.4 CO VS GRADIENT DESCENT AND EXTRA-GRADIENT ALGORITHMS

In Table 6 we can observe that RK solvers outperform the baselines. Interestingly, we can also observe that CO
outperforms both EG and GD-NM. In this section, we provide some intuition about why this may be the case in
practice and conduct an additional experimental on this direction.

In the analysis presented in Section 4, we show that a better discretization implies better convergence to a
local optimality (assuming ω[∗] exists, and that it is a strict local NE). Specifically, under those assumptions,
we show that in both GD and EG methods we should put an upper bound to their learning rate (see Section 4
and appendix B.2). In Appendix C.2, on the other hand, we show CO can be interpreted in the limit as an
approximation of the RK2 solver. Based on this, we believe in practice CO may approximate the continuous
dynamics better than GD and EG. Particularly, if we have an additional hyperparameter (γ) to tune thoroughly.
We believe this might also be the reason of why we found CO to be very sensitive to the γ parameter. In Table 7,
we compare the performance before and after removing the best performing γ from the grid search. We can see
for values others than the best γ, CO does not outperform GD-NM either.

Table 7: Performance of CO vs others. CO(γ =1e-3) corresponds to the best result after removing
1e-4 from the grid search.

Method M →U

GD 90.0
GD-NM 93.2

CO (γ = 1e-4) 93.8
CO (γ = 1e-3) 91.6

RK2 **95.3**


E.5 WALL-CLOCK COMPARISON

Table 8: Average Time Per Iteration Comparison (Wall-Clock Comparison)

Algorithm Avg time per iteration

GD-NM 0.26 ± 0.003
Ours(RK2) 0.49 ± 0.008


-----

In Table 8, we show experimental results of a wall-clock comparison between RK2 and GD-NM. Specifically,
we compute the average over 100 runs on a NVIDIA GPU: TITAN V, CudaVersion 10.1 and PyTorch version:
1.5.1. As we can see, the average increase in time is less than 2x slower in wall-clock time.This is in line with the
claims made in our submission. We additionally emphasize that this time can be improved with more efficient
implementations (e.g. Cuda). Moreover, many more efficient high-order ODE solvers exist and could be used in
the future. Our work also inspires future research in this direction.

F PYTORCH PSEUDOCODE OF RK2 SOLVER

Below, we show how to implement the RK2 solver in PyTorch by simply modifying the existing implementation
of SGD.

_# Based on https://github.com/pytorch/pytorch/blob/release/1.5/torch/optim/sgd.py_
**class RK2(torch.optim.optimizer):**

_# ......_
**def update_step(self,fwd_bwd_fn):**

_"""_
_fwd_bwd_fn: fn that computes y=model(x); loss(y,yhat).backward()_
_example:_

_# training loop_
_x, yhat=sample_from_dataloader()_
_optimizer.zero_grad()_

_def fwd_bwd_fn():_

_y=model(x)_
_loss=compute_loss(y,yhat)_
_loss.backward()_
_return loss,y_

_fwd_bwd_fn()_
_optimizer.update(fwd_bwd_fn)_

_"""_
_#_ _We want to compute the update equation:_

_#_ _w - n/2* v(w) - n/2 * v(w-n*v(w))_

temp_step = {}
**with torch.no_grad():**

**for jj_, group in enumerate(self.param_groups):**

_#ignore weight-decay for simplicity._

**for ii_, p in enumerate(group['params']):**

**if p.grad is None:**

**continue**

d_p = p.grad _# v(w)_

_# first part of the step._
_# w - n/2*v(w)_
p1 = p.data.clone().detach()
p1.add_(d_p, alpha=-0.5 * group['lr'])

_#storing for later used._
temp_step[f"{jj_}_{ii_}"] = p1

_# Computing now w-n*v(w) for the second part of the step_
p.add_(d_p, alpha=-1.0 * group['lr'])

_# set to none_
p.grad = None _# self.zero_grad()_

_# extra step evaluation._
_# this function does model forward,compute loss and loss backward._
loss, _ = fwd_bwd_fn()
_# gradients are now v(w-n*v(w))_

**with torch.no_grad():**


-----

**for jj_, group in enumerate(self.param_groups):**

**for ii_, p in enumerate(group['params']):**

**if p.grad is None:**

**continue**

d_p = p.grad # v(w-n*v(w))

_# unpacking the first part of the step._
p1 = temp_step[f"{jj_}_{ii_}"]

_# updating p with the stored value from before._
_# this is equivalent to p = p1 = w-n/2 * v(w) (but sligthly faster)_
p.zero_()
p.add_(p1)

_#w= w - n/2*v(w) - n/2 * v(w-n*v(w))_
p.add_(d_p, alpha=-0.5 * group['lr'])


-----

