# TRAINING INVARIANCES AND THE LOW-RANK PHE## NOMENON: BEYOND LINEAR NETWORKS

**Thien Le & Stefanie Jegelka**
Massachusetts Institute of Technology
_{thienle,stefje}@mit.edu_

ABSTRACT

The implicit bias induced by the training of neural networks has become a topic of
rigorous study. In the limit of gradient flow and gradient descent with appropriate
step size, it has been shown that when one trains a deep linear network with logistic or exponential loss on linearly separable data, the weights converge to rank-1
matrices. In this paper, we extend this theoretical result to the last few linear layers
of the much wider class of nonlinear ReLU-activated feedforward networks containing fully-connected layers and skip connections. Similar to the linear case, the
proof relies on specific local training invariances, sometimes referred to as alignment, which we show to hold for submatrices where neurons are stably-activated
in all training examples, and it reflects empirical results in the literature. We also
show this is not true in general for the full matrix of ReLU fully-connected layers. Our proof relies on a specific decomposition of the network into a multilinear
function and another ReLU network whose weights are constant under a certain
parameter directional convergence.

1 INTRODUCTION

Recently, great progress has been made in understanding the trajectory of gradient flow (GF) (Ji &
Telgarsky, 2019; 2020; Lyu & Li, 2020), gradient descent (GD) (Ji & Telgarsky, 2019; Arora et al.,
2018) and stochastic gradient descent (SGD) (Neyshabur et al., 2015; 2017) in the training of neural
networks. While good theory has been developed for deep linear networks (Zhou & Liang, 2018;
Arora et al., 2018), practical architectures such as ReLU fully-connected networks or ResNets are
highly non-linear. This causes the underlying optimization problem (usually empirical risk minimization) to be highly non-smooth (e.g. for ReLUs) and non-convex, necessitating special tools
such as the Clarke subdifferential (Clarke, 1983).

One of the many exciting results of this body of literature is that gradient-based algorithms exhibit
some form of implicit regularization: the optimization algorithm prefers some stationary points to
others. In particular, a wide range of implicit biases has been shown in practice (Huh et al., 2021)
and proven for deep linear networks (Arora et al., 2018; Ji & Telgarsky, 2019), convolutional neural
networks (Gunasekar et al., 2018) and homogeneous networks (Ji & Telgarsky, 2020). One well
known result for linearly separable data is that in various practical settings, linear networks converge
to the solution of the hard SVM problem, i.e., a max-margin classifier (Ji & Telgarsky, 2019), while
a relaxed version is true for CNNs (Gunasekar et al., 2018). This holds even when the margin does
not explicitly appear in the optimization objective - hence the name implicit regularization.

Another strong form of implicit regularization relates to the structure of weight matrices of fully
connected networks. In particular, Ji & Telgarsky (2019) prove that for deep linear networks for
binary classification, weight matrices tend to rank-1 matrices in Frobenius norm as a result of GF/GD
training, and that adjacent layers’ singular vectors align. The max-margin phenomenon follows as
a result. In practice, Huh et al. (2021) empirically document the low rank bias across different
non-linear architectures. However, their results include ReLU fully-connected networks, CNNs and
ResNet, which are not all covered by the existing theory. Beyond linear fully connected networks,
Du et al. (2018) show vertex-wise invariances for fully-connected ReLU networks and invariances
in Frobenius norm differences between layers for CNNs. Yet, despite the evidence in (Huh et al.,
2021), it has been an open theoretical question how more detailed structural relations between layers,


-----

which, e.g., imply the low-rank result, generalize to other, structured or local nonlinear and possibly
non-homogeneous architectures, and how to even characterize these.

Hence, in this work, we take steps to addressing a wider set of architectures and invariances. First,
we show a class of vertex and edge-wise quantities that remain invariant during gradient flow training. Applying these invariances to architectures containing fully connected, convolutional and residual blocks, arranged appropriately, we obtain invariances of the singular values in adjacent (weight)
matrices or submatrices. Second, we argue that a matrix-wise invariance is not always true for general ReLU fully-connected layers. Third, we obtain low-rank results for arbitrary non-homogeneous
networks whose last few layers contain linear fully-connected and linear ResNet blocks. To the
best of our knowledge, this is the first time a low-rank phenomenon is proven rigorously for these
architectures.

Our theoretical results offer explanations for empirical observations on more general architectures,
and apply to the experiments in (Huh et al., 2021) for ResNet and CNNs. They also include the
squared loss used there, in addition to the exponential or logistic loss used in most theoretical
low-rank results. Moreover, our Theorem 2 gives an explanation for the “reduced alignment” phenomenon observed by Ji & Telgarsky (2019), where in experiments on AlexNet over CIFAR-10
the ratio ∥W _∥2/∥W_ _∥F converges to a value strictly less than 1 for some fully-connected layer W_
towards the end of the network.

One challenge in the analysis is the non-smoothness of the networks and ensuring an “operational”
chain rule. To cope with this setting, we use a specific decomposition of an arbitrary ReLU architecture into a multilinear function and a ReLU network with +1/-1 weights. This reduction holds in
the stable sign regime, a certain convergence setting of the parameters. This regime is different from
stable activations, and is implied, e.g., by directional convergence of the parameters to a vector with
non-zero entries (Lemma 1). This construction may be of independent interest.

In short, we make the following contributions to analyzing implicit biases of general architectures:

_• We show vertex and edge-wise weight invariances during training with gradient flow. Via these_
invariances, we prove that for architectures containing fully-connected layers, convolutional layers
and residual blocks, when appropriately organized into matrices, adjacent matrices or submatrices
of neurons with stable activation pattern have bounded singular value (Theorem 1).

_• In the stable sign regime, we show a low-rank bias for arbitrary nonhomogeneous feedforward_
networks whose last few layers are a composition of linear fully-connected and linear ResNet
variant blocks (Theorem 2). In particular, if the Frobenius norms of these layers diverge, then the
ratio between their operator norm and Frobenius norm is bounded non-trivially by an expression
fully specified by the architecture. To the best of our knowledge, this is the first time this type of
bias is shown for nonlinear, nonhomogeneous networks.

_• We prove our results via a decomposition that reduces arbitrarily structured feedforward networks_
with positively-homogeneous activation (e.g., ReLU) to a multilinear structure (Lemma 1).

1.1 RELATED WORKS

Decomposition of fully-connected neural networks into a multilinear part and a 0-1 part has been
used by Choromanska et al. (2015); Kawaguchi (2016), but their formulation does not apply to
trajectory studies. In Section 3, we give a detailed comparison between their approach and ours.
Our decomposition makes use of the construction of a tree network that Khim & Loh (2019) use
to analyze generalization of fully-connected networks. We describe their approach in Section 2
and show how we extend their construction to arbitrary feedforward networks. This construction
makes up the first part of the proof of our decomposition lemma. The paper also makes use of path
enumeration of neural nets, which overlaps with the path-norm literature of Neyshabur et al. (2015;
2017). The distinction is that we are studying classical gradient flow, as opposed to SGD (Neyshabur
et al., 2015) or its variants (Neyshabur et al., 2017).

For linear networks, low-rank bias is proven for separable data and exponential-tailed loss in Ji &
Telgarsky (2019). Du et al. (2018) give certain vertex-wise invariances for fully-connected ReLU
networks and Frobenius norm difference invariances for CNNs. Compared to their results, ours are
slightly stronger since we prove that our invariances hold for almost every time t on the gradient


-----

flow trajectory, thus allowing for the use of a Fundamental Theorem of Calculus and downstream
analysis. Moreover, the set of invariances we show is strictly larger. Radhakrishnan et al. (2020)
show negative results when generalizing the low-rank bias from (Ji & Telgarsky, 2019) to vectorvalued neural networks. In our work, we only consider scalar-valued neural networks performing
binary classification. For linearly inseparable but rank-1 or whitened data, a recent line of work
from Ergen & Pilanci (2021) gives explicit close form optimal solution, which is both low rank and
aligned, for the regularized objective. This was done for both the linear and ReLU neural networks.
In our work, we focus on the properties of the network along the gradient flow trajectory.

2 PRELIMINARIES AND NOTATION

For an integer k ∈ N, we write the set [k] := {1, 2, . . ., k}. For vectors, we extend the sign function
sgn : R 1, 1 coordinate-wise as sgn : (xi)i [d] (sgn(xi))i [d]. For some (usually the
_→{−_ _}_ _∈_ _7→_ _∈_
canonical) basis (ei)i [n] in some vector space R[n], for all x R[n] we use the notation [x]i = _x, ei_
_∈_ _∈_ _⟨_ _⟩_
to denote the i-th coordinate of x.

**Clarke subdifferential, definability and nonsmooth analysis. The analysis of non-smooth func-**
tions is central to our results. For a locally Lipschitz function f : D → R with open domain D,
there exists a set D[c] _⊆_ _D of full Lebesgue measure on which the derivative ∇f exists everywhere_
by Rademacher’s theorem. As a result, calculus can usually be done over the Clarke subdifferential
_∂f_ (x) := CONV limi→∞ _∇f_ (xi) | xi ∈ _D[c], xi →_ _x_ where CONV denotes the convex hull.

The Clarke subdifferential generalizes both the smooth derivative when _f_ _C_ [1] (continuously dif_∈_
ferentiable) and the convex subdifferential when f is convex. However, it only admits a chain rule
with an inclusion and not equality, which is though necessary for backpropagation in deep learning. We do not delve in too much depth into Clarke subdifferentials in this paper, but use it when
we extend previous results that also use this framework. We refer to e.g. (Davis et al., 2020; Ji &
Telgarsky, 2020; Bolte & Pauwels, 2020) for more details.

**Neural networks. Consider a neural network ν : R[d]** _→_ R. The computation graph of ν is a
weighted directed graph G = (V, E, w) with weight function w : E → R. For each neuron v ∈ _V,_
let INv := {u ∈ _V : uv ∈_ _E} and OUTv := {w ∈_ _V : vw ∈_ _E} be the input and output neurons of_
_v. Let_ _i1, i2, . . ., id_ =: I _V and O :=_ _o_ _V be the set of input and output neurons defined_
_{_ _}_ _⊂_ _{_ _} ⊂_
as IN(i) = ∅ = OUT(o), ∀i ∈ _I. Each neuron v ∈_ _V \I is equipped with a positively 1-homogeneous_
activation function σv (such as the ReLU x 7→ max(x, 0), leaky ReLU x 7→ max(x, αx) for some
small positive α, or the linear activation).

To avoid unnecessary brackets, we write we := w(e) for some e ∈ _E. We will also write w ∈_ R[E],
where E is the set of learnable weights, as the vector of learnable parameters. Let P be a path in G,
i.e., a set of edges in E that forms a path. We write v ∈ _P for some v ∈_ _V if there exists u ∈_ _V_
such that uv ∈ _P or vu ∈_ _P_ . Let ρ be the number of distinct paths from any i ∈ _I to o ∈_ _O. Let_
:= _p1, . . ., pρ_ be the enumeration of these paths. For a path p and an input x to the neural
_P_ _∈P_
network, denote by xp the coordinate of x used in p.


Given a binary classification dataset (xi, yi) _i_ [n] [with][ x][i][ ∈] [R][d][,][ ∥][x][i][∥≤] [1][ and][ y][i][ ∈{−][1][,][ 1][}][, we]

minimize the empirical risk (w) =n[1] _ni=1_ _[ℓ][(]∈[y][i][ν][(][x][i][)) =][ 1]n_ _ni=1_ _[ℓ][(][ν][(][y][i][x][i][))][ with loss][ ℓ]_ [:][ R][ →]
_R_

R, using gradient flow [d][w]d[(]t[t][)] _∈−∂R(w(Pt))._ P

As we detail the architectures used in this paper, we recall that the activation of each neuron is still
positively-homogeneous. The networks considered here are assumed to be bias-free.

**Definition 1 (Feedforward networks). A neural net ν with graph G is a feedforward network if G**
_is a directed acyclic graph (DAG)._

**Definition 2 (Fully-connected networks). A feedforward network ν with graph G is a fully-**
connected network if there exists a partition of V into V = (I _V1)_ _V2_ _. . ._ (VL+1 _O) such_
_that for all u, v ∈_ _V, uv ∈_ _E iff there exists i ∈_ [L] such that u ≡ ∈ _Vi and ⊔_ _v ⊔ ∈_ _Vi ⊔+1._ _≡_

**Definition 3 (Tree networks). A feedforward network ν with graph G is a tree network if the under-**
_lying undirected graph G is a tree (undirected acyclic graph)._


-----

Examples of feedforwards networks include ResNet (He et al., 2016), DenseNet (Huang et al.,
2017), CNNs (Fukushima, 1980; LeCun et al., 2015) and other fully-connected ReLU architectures.

of (hidden) layers, letFor a fully-connected network ni := | νVi with layer partition| be the number of neurons in the V =: V1 ⊔ _. . . i ⊔-th layer and enumerateVL+1 where L is the number Vi =_
_{vi,j}j∈[ni]. Weights in this architecture can be organized into matrices W_ [[1]], W [[2]], . . ., W [[][L][]] where
R[n][i][+1][×][n][i] _∋_ _W_ [[][i][]] = ((wvi,j _vi+1,k_ ))j∈[ni],k∈[ni+1], for all i ∈ [L].

**Tree** **networks.** Most practical architectures are not tree networks, but trees have
been used to prove generalization bounds for adversarial risk. In particular, for fullyconnected neural networks _f_ whose activations are monotonically increasing and 1Lipschitz, Khim & Loh (2019) define the tree transform as the tree network Tf (x; w) =
_npLL=1_ _[W][ []1[L],p[]]L_ _[σ]_ _. . ._ _p2=1_ _[W][ [2]]p3,p2_ _[σ]_ _wp2..pL +_ _p1=1_ _[W][ [1]]p2,p1_ _[x]p1_ for vectors _w_ with
_L_   []
Pj=2 _[n][j][ entries, indexed by an][ L][-tuple][ (][p][2][, . . ., p][L][)][. We extend this idea in the next section.]_

[P][n][2] [P][n][1]
Q

3 STRUCTURAL LEMMA: DECOMPOSITION OF DEEP NETWORKS

We begin with a decomposition of a neural network into a multilinear and a non-weighted nonlinear
part, which will greatly facilitate the chain rule that we need to apply in the analysis. Before stating
the decomposition, we need the following definition of a path enumeration function, which computes
the product of all weights and inputs on each path of a neural network.
**Definition 4 (Path enumeration function). Let ν be a feedforward neural network with graph G**
_and paths_ = _p1, . . ., pρ_ _. The path enumeration function h is defined for this network as_
_P_

_h : (x1, x2, . . ., xd)_ _xp_ _e_ _p_ _[w][e]_
_7→_ _∈_ _p_ _[where][ x][p][ :=][ x][k][ such that][ i][k][ ∈]_ _[p][.]_

_∈P_

 
Q

We first state the main result of this section, proven in Appendix B.
**Lemma 1 (Decomposition). Let ν : R[d]** _→_ R be a feedforward network with computation graph G,
_and ρ the number of distinct maximal paths in G. Then there exists a tree network µ : R[ρ]_ _→_ R such
_that ν = µ ◦_ _h where h : R[d]_ _→_ R[ρ] _is the path enumeration function of G. Furthermore, all weights_
_in µ are either −1 or +1 and fully determined by the signs of the weights in ν._

**Path activation of ReLU networks in the literature. The viewpoint that for every feedforward**
network ν there exists a tree network µ such that ν = µ ◦ _h is not new and our emphasis here is on_
the fact that the description of µ : R[ρ] _→_ R is fully determined by the signs of the weights. Indeed,
in analyses of the loss landscape (Choromanska et al., 2015; Kawaguchi, 2016), ReLU networks are
described as a sum over paths:


(1)
R[ρ][,]


_ν(x; w) =_


_Zp(x; w)_
_pX∈P_


_we =_ (Zp(x; w))p _, h(x; w)_
_∈P_
_eY∈p_


where Zp(x; w) = 1 iff all ReLUs on path p are active (have nonnegative preactivation) and 0
otherwise. One can then take µ as a tree network with no hidden layer, ρ input neurons all connected
to a single output neuron. However, this formulation complicates analyses of gradient trajectories,
because of the explicit dependence of Zp on numerical values of w. In our lemma, µ is a tree network
whose description depends only on the signs of the weights. If the weight signs (not necessarily the
ReLU activation pattern!) are constant, µ is fixed, allowing for a chain rule to differentiate through
it. That weight signs are constant is realistic, in the sense that it is implied by directional parameter
convergence (Section 4.1). To see this, compare the partial derivative with respect to some we (when
it exists) between the two approaches, in the limit where weight signs are constant:


(using Lemma 1) _∂ν/∂we =_

(using Zp in Eqn. 1) _∂ν/∂we =_


_wf_ _,_ (2)
_f_ _∈Yp,f_ ≠ _e_



[ _wµ(x; w)]p_ _xp_
_∇_ _·_
_p∈P|Xe∈p_


(using Zp in Eqn. 1) _∂ν/∂we =_ _we∂Zp(x; w)/∂we + Zp(x; w)_ _wf_ _._ (3)

_p∈P|Xe∈p_    _f_ _∈Yp,f_ ≠ _e_

In particular, the dependence of Equation 2 on we is extremely simple. The utility of this fact will
be made precise in the next section when we study invariances.


-----

**Proof sketch. The proof contains two main steps. First, we “unroll” the feedforward network into**
a tree network that computes the same function by adding extra vertices, edges and enable weight
sharing. This step is part of the tree transform in Khim & Loh (2019) if the neural network is a
fully-connected network; we generalize it to work with arbitrary feedforward networks. Second,
we “pull back” the weights towards the input nodes using positive homogeneity of the activations:
_a · σ(x) = sgn(a) · σ(x|a|). This operation is first done on vertices closest to the output vertex_
(in number of edges on the unique path between any two vertices in a tree) and continues until all
vertices have been processed. Finally, all the residual signs can be subsumed into µ by subdividing
edges incident to input neurons. We give a quick illustration of the two steps described above for a
fully-connected ReLU-activated network with 1 hidden layer in Appendix A.

4 MAIN THEOREM: TRAINING INVARIANCES

In this section, we put the previous decomposition lemma to use in proving an implicit regularization
property of gradient flow when training deep neural networks.

4.1 STABLE SIGN REGIME: A CONSEQUENCE OF DIRECTIONAL CONVERGENCE

dw(t)
Recall the gradient flow curve {w(t)}t∈[0,∞) defined by the differential inclusion dt _∈_

_−∂R(w(t)). We first state the main assumption in this section._
**Assumption 1[t0, tN** ), sgn(w (Stable sign regime)(t)) = sgn(w(t0)). If this holds, we say that gradient flow is in a. For some t0 < tN ∈ [0, ∞], we assume that for all stable sign regime t ∈.
_Without loss of generality, when using this assumption, we identify t0 with 0 and write ”for some_
_t_ 0” to mean ”for some t [t0, tN )”.
_≥_ _∈_

In fact, the following assumption - the existence and finiteness part of which has been proven in (Ji
& Telgarsky, 2020) for homogeneous networks, is sufficient.
**Assumption 2 (Directional convergence to non-vanishing limit in each entry). We assume that**

_ww((tt))_ 2 _t→∞_ _w exists, is finite in each entry and furthermore, for all e_ _E, we_ = 0.
_∥_ _∥_ _−−−→_ _∈_ _̸_

**Motivation and justification.** It is straightforward to see that Assumption 1 follows from Assumption 2 but we provide a proof in the Appendix (Claim 1). Directional convergence was proven
by Ji & Telgarsky (2020) for the exponential/logistic loss and the class of homogeneous networks,
under additional mild assumptions. This fact justifies the first part of Assumption 2 for these architectures. The second part of Assumption 2 is pathological for our case, in the sense that directional
convergence alone does not imply stable signs (for example, a weight that converges to 0 can change
sign an infinite number of times).

**Pointwise convergence is too strong in general.** Note also that assuming pointwise convergence
of the weights (i.e. limt→∞ _w(t) exists and is finite) is a much stronger statement, which is not true_
for the case of exponential/logistic loss and homogeneous networks (since ∥w(t)∥2 diverges, see for
example Lyu & Li (2020), Ji & Telgarsky (2020), Ji & Telgarsky (2019)). Even when pointwise
convergence holds, it would immediately reduce statements on asymptotic properties of gradient
flow on ReLU activated architectures to that of linearly activated architectures. One may want
to assume that gradient flow starts in the final affine piece prior to its pointwise convergence and
thus activation patterns are fixed throughout training and the behavior is (multi)linear. In contrast,
directional convergence of the weights does not imply such a reduction from the ReLU-activation
to the linear case. Similarly, with stable signs, the parts of the input where the network is linear are
not convex, as opposed to the linearized case (Hanin & Rolnick, 2019) (see also Claim 2).

**Stable sign implication.** The motivation for Assumption 1 is that weights in the tree network µ
in Lemma 1 are fully determined by the signs of the weights in the original feedforward network ν.
Thus, under Assumption 1, one can completely fix the weights of µ - it has no learnable parameters.
Since we have the decomposition ν = µ ◦ _h where h is the path enumeration function, dynamics of_
_µ are fully determined by dynamics of h in the stable sign regime. To complete the picture, observe_
that h is highly multilinear in structure: the degree of a particular edge weight we in each entry of h
is at most 1 by definition of a path; and if ν is fully-connected, then h is a R[n][1][×][n][2][×][...][×][n][L] tensor.


-----

4.2 TRAINING INVARIANCES

First, we state an assumption on the loss function that holds for most losses used in practice, such as
the logistic, exponential or squared loss.
**Assumption 3 (Differentiable loss). The loss function ℓ** : R → R is differentiable everywhere.
**Lemma 2 (Vertex-wise invariance). Under Assumptions 1, and 3, for all v ∈** _V \{I ∪_ _O} such that_
_all edges incident to v have learnable weights, for a.e. time t ≥_ 0,

_wuv[2]_ [(][t][)][ −] _wvb[2]_ [(][t][) =] _wuv[2]_ [(0)][ −] _wvb[2]_ [(0)][.] (4)
_u_ INv _b_ OUTv _u_ INv _b_ OUTv

X∈ _∈X_ X∈ _∈X_

_If we also have INu = INv = IN and OUTu = OUTv = OUT and u and v have the same activation_
_pattern (preactivation has the same sign) for each training example and for a.e time t ≥_ 0, then for
_a.e. time t ≥_ 0,

_wau(t)wav(t)_ _wub(t)wvb(t) =_ _wau(0)wav(0)_ _wub(0)wvb(0)._ (5)
_−_ _−_
_aX∈IN_ _bX∈OUT_ _aX∈IN_ _bX∈OUT_

**Comparison to Du et al. (2018)** A closely related form of Equation 4 in Lemma 2 has appeared
in Du et al. (2018) (Theorem 2.1) for fully-connected ReLU/leaky-ReLU networks. In particular,
the authors showed that the difference between incoming and outgoing weights does not change. Invoking the Fundamental Theorem of Calculus (FTC) over this statement will return ours. However,
their proof may not hold on a nonnegligible set of time t due to the use of the operational chain rule
that holds only for almost all we. Thus the FTC can fail. The stronger form we showed here is useful
in proving downstream algorithmic consequences, such as the low rank phenomenon. Furthermore,
our result also holds for arbitrary feedforward architectures and not just the fully-connected case.

Before we put Lemma 2 to use, we list definitions of some ResNet variants.
**Definition 5. Denote ResNetIdentity, ResNetDiagonal and ResNetFree to be the version of ResNet**
_described in He et al. (2016) where the residual block is defined respectively as_

_1. r(x; U, Y ) = σ(Uσ(Y x) + Ix) where x ∈_ R[a], Y, U _[⊤]_ _∈_ R[b][×][a], and I is the identity,

_2. r(x; U, Y, D) = σ(Uσ(Y x) + Dx) where x ∈_ R[a], Y, U _[⊤]_ _∈_ R[b][×][a], and D is diagonal,

_3. r(x; U, Y, Z) = σ(Uσ(Y x) + Zx) where x ∈_ R[a], Y ∈ R[b][×][a], U ∈ R[c][×][b] _and Z ∈_ R[c][×][a].

ResNetIdentity is the most common version of ResNet in practice. ResNetIdentity is a special
case of ResNetDiagonal, which is a special case of ResNetFree. Yet, theorems for ResNetFree do
not generalize trivially to the remaining variants, due to the restriction of Lemma 2 and Lemma 3 to
vertices adjacent to all learnable weights and layers containing all learnable weights. For readability,
we introduce the following notation:
**Definition 6 (Submatrices of active neurons). Fix some time t, let W ∈** _R[a][×][b]_ _be a weight matrix_
_from some set of a neurons to another set of b neurons. Let Iactive ⊆_ [b] be the set of b neurons that
_are active (linear or ReLU with nonnegative preactivation). We writefor the submatrix of W_ _W with rows and columns from Iactive. Similarly, if [W_ _[⊤]W W]active ∈b_ R c[|][I] is another[active][|×|][I][active][|]

_[⊤]_ _[′]_ _∈_ _×_
_weight matrix from the same set of b neurons to another set of c neurons then [W_ _[′]W_ _[′⊤]]active is_
_defined as the submatrix with rows and columns from Iactive._

When applying Lemma 2 to specific architectures, we obtain the following:
**Theorem 1 (Matrix-wise invariances). Recall that a convolutional layer with number of input ker-**
_nels a, kernel size b and number of output kernels c and is a tensor in R[a][×][b][×][c]. Under Assumptions_
_1 and 3, we have the following matrix-wise invariance for a.e. time t ≥_ 0:

d

_W2(t)[⊤]W2(t)_ _W1(t)W1(t)[⊤][i]_ = 0, for: (6)

dt _active_ _active_

h i _[−]_ h 

_1. (Fully-connected layers) W1 ∈_ R[b][×][a] _and W2 ∈_ R[c][×][b] _consecutive fully-connected layers,_

_2. (Convolutional layers) W1 is convolutional, viewed as a flattening to a matrix R[c][×][(][a][×][b][)], and_
_W2 adjacent convolutional, viewed as a flattening to a matrix R[(][d][×][e][)][×][c],_


-----

_3. (Within residual block of ResNet) W1 = Y and W2 = U where r(x; U, Y, Z) is a residual block_
_of ResNetIdentity, ResNetDiagonal or ResNetFree,_

_4. (Between residual blocks of ResNet) W1 =_ _U1_ _Z1_ _, W2 =_ _Y2_ _where r(x; Uj, Yj, Zj),_
_Z2_
 

_j_ 1, 2 _are consecutive ResNetFree blocks,_ 
_∈{_ _}_

_5. (Convolutional-fully-connected layers) W1 convolutional, viewed as a flattening to a matrix_
R[c][×][(][a][×][b][)] _and W2 adjacent fully-connected layer, viewed as an rearrangement to R[d][×][c],_

_6. (Convolutional-ResNetFree block) W1 convolutional, viewed as a flattening to a matrix_
R[c][×][(][a][×][b][)] _and W2 is a rearrangement of_ _U_ _Z[]_ _into an element of R[d][×][c], where r(x; U, Y, Z)_
_is an adjacent ResNetFree block,_


_7. (ResNetFree block-fully-connected layers and vice versa) W1 =_ _U_ _Z[]_ _∈_ R[b][×][a] _and W2 ∈_

_Y_
R[c][×][b] _adjacent fully-connected or W1_ R[b][×][a] _fully-connected and W_ 2 = R[c][×][b] _adjacent_
_∈_ _Z_ _∈_
 

_ResNet block where r(x; U, Y, Z) is the ResNetFree block._

We emphasize that the above theorem only makes local requirements on the neural network, to
have local parts that are either fully-connected, convolutional or a residual block. The only global
architecture requirement is feedforward-ness. The first point of Theorem 1 admits an extremely
simple proof for the linear fully-connected network case in Arora et al. (2018) (Theorem 1).

**Significance of Theorem 1.** If we have a set of neurons that is active throughout training (which
is vacuously true for linear layers), we can invoke an FTC and get W2(t)[⊤]W2(t) _W1(t)W1(t)[⊤]_ =
_−_
_W2(0)[⊤]W2(0)_ _W1(0)W1(0)[⊤]_ for the submatrix restricted to these neurons. Assume for simplicity
_−_
that the right hand side is 0, then the singular values of W1 and W2 are the same for each of the
cases listed in Theorem 1. If we can form a chain of matrices whose singular values are the same
by iteratively invoking Theorem 1, then all matrices considered have the same singular values as the
final fully-connected layer that connects to the output. Recall that our networks are scalar-valued, so
the final layer is a row vector, which is rank 1 and thus all layers considered in the chain have rank
1, which is useful in the next section.

**Proof sketch of Theorem 1.** Given Lemma 2, we demonstrate the proof for the first point. The
remaining points admit the exact same proof technique but on different matrices, which require some
_a, b, cbookkeeping. Let ∈_ N number of vertices in these layers. Applying Equation 4 of Lemma 2 to each of the W1 ∈ R[b][×][a] and W2 ∈ R[c][×][b] be two consecutive fully-connected layers for some
_b shared neurons between these two layers, one obtains the diagonal entries of Equation 6 of the_
Theorem. Now, apply Equation 5 to each pair among the b shared neurons between these two layers
to get the off-diagonal entries of Equation 6.

Next, we define layers for architectures where weights are not necessarily organized into matrices,
e.g., ResNet or DenseNet.
**Definition 7 (Layer). Let F ⊂** _E be such that 1) for all e ̸= f ∈_ _F_ _, there is no path that contains_
_both e and f_ _; and 2) the graph (V, E\F, w) is disconnected. Then F is called a layer of G._

For this definition, we have the following invariance:
**Lemma 3 (Edge-wise invariance). Under Assumptions 1 and 3, for all layers F and F** _[′]_ _that contain_
_all learnable weights, it holds that for a.e. time t ≥_ 0,

_we[2][(][t][)][ −]_ _wf[2][(][t][) =]_ _we[2][(0)][ −]_ _wf[2][(0)][.]_ (7)
_eX∈F_ _fX∈F_ _[′]_ _eX∈F_ _fX∈F_ _[′]_

**Significance of Lemma 3.** A flattening of a convolutional parameter tensor and a stacking of
matrices in a ResNetDiagonal and ResNetFree block forms a layer. This lemma implies that the
squared Frobenius norm of these matrices in the same network differs by a value that is fixed at
initialization. The lemma also gives a direct implicit regularization for networks with biases, by
treating neurons with bias as having an extra in-edge whose weight is the bias, from an extra invertex which is an input vertex with fixed input value 1.


-----

4.3 PROOF SKETCH OF LEMMA 2 AND LEMMA 3

The proofs of Lemma 2, Lemma 3 and Theorem 1 share the technique of double counting paths,
which we explain next. For simplicity, we assume here that we are working with a network that is
differentiable everywhere in some domain that we are considering – we give a full general proof in
the Appendix. The main proof idea was used in (Arora et al., 2018) and involves simply writing
down the partial derivative of the risk. We have, for some particular weight we, e _E, via the_
_∈_
smooth chain rule


_∂R(w)_ = [1]

_∂we_ _n_

= [1]


_n_

_l[′](yiν(xi; w))_ _yi_ _[∂ν][(][w][)]_ (8)
_i=1_ _·_ _·_ _∂we_

X


= n[1] Xi=1 _l[′](yiν(xi; w)) · yi ·_ _p∈PX,p∋e_ µ[′](xi; w)p _[·][ (][x][i][)][p]_ _f_ _∈Yp,f_ ≠ _e_ _wf_ _,_ (9)

where in the second line, we invoke the decomposition Lemma 1 and emphasize that µ has no
learnable parameters in the stable sign regime. Now multiply we to the above expression to get

_n_

_∂_ (w) 1
_we_ _R_ = _Ai,p(w),_ (10)

_∂we_ _n_

_p∈PX,p∋e_ Xi=1

where Ai,p(w) = l[′](yiν(xi; w)) _yi_ _µ[′](xi; w)_ _p_ _f_ _p_

depend explicitly on the edge e, with respect to which we are differentiating (only through · _·_ _[·][ (][x][i][)][p]_ _∈_ _[|][w][f]_ _[|][. Notice that][ A][i,p][(][w] w[)][ does not]). Thus,_

  Q

we sum over in-edges and out-edges of a particular v satisfying the assumption of Lemma 2 to get

_n_

_∂_ (w) 1 _∂_ (w)
_wuv_ _R_ = _Ai,p(w) =_ _wvb_ _R_ _._ (11)
_u_ INv _∂wuv_ _p_ _,p_ _v_ _n_ _i=1_ _b_ OUTv _∂wvb_

X∈ _∈PX∋_ X _∈X_

Note that the only difference between Equations 10 and 11 is the set of paths that we are summing over, and we double count this set of paths. We use the definition of gradient flow to obtain
_∂R(w)/∂we = dwe(t)/dt and integrate with respect to time using a FTC to get the first part of_
Lemma 2. More work is needed to get the second part of Lemma 2, which is detailed in Appendix
C. Finally, to get Lemma 3, we double count the set of all paths P.


_l[′](yiν(xi; w))_ _yi_
_i=1_ _·_ _·_

X


_∂_ (w)
_we_ _R_

_∂we_


4.4 NONINVARIANCE OF GENERAL RELU LAYERS

The restriction of Theorem 1 to submatrices of active neurons may appear limiting, but does not
extend to the general case. With the same technique as above, we can write down the gradient for
the Gram matrix W1[⊤][W][1] [for ReLU layers and show that it is not equal to its counterpart][ W][2][W][ ⊤]2 [,]
thus giving a negative result:
**Lemma 4 (Noninvariance in ReLU layers). Even under Assumptions 3 and 1, for a.e. time t ≥** 0,
d

_W2(t)[⊤]W2(t)_ _W1(t)W1(t)[⊤][]_ = 0, (12)

dt _−_ _̸_



_for the different pairs of W1, W2 detailed in Theorem 1._

Despite the negative result, the closed form of the gradient for the Gram matrices can be shown to
be low rank with another subgradient model. Details may be found in Appendix C.

5 CONSEQUENCES: LOW RANK PHENOMENON FOR NONLINEAR
NONHOMOGENEOUS DEEP FEEDFORWARD NET

We apply the results from previous parts to prove a low-rank bias result for a large class of feedforward networks. To the best of our knowledge, this is the first time such a result is shown for this
class of deep networks, although the linear fully-connected network analogue has been known for
some time. In light of Theorem 1, we define a matrix representation of a layer:
**Definition 8 (Matrix representation of a layer). The matrix representation for a ResNetFree block**
_r(x; U, Y, Z) is_ _U_ _Z[]; for a T_ _[a,b,c]_ _∈_ R[a][×][b][×][c] _convolutional tensor it is the flattening to an_
_element of R[a][×][(][b][×][c][)]; and for a fully-connected layer it is the weight matrix itself._



-----

**Theorem 2 (Reduced alignment for non-homogeneous networks). Under Assumptions 1 and 3, let ν**
_consist of an arbitrary feedforward neural network η, followed by K ≥_ 0 linear convolutional layers
(T _[a][k][,b][k][,c][k]_ )k∈[K], followed by M ≥ 0 layers that are either linear ResNetFree blocks or linear fully_connected layers; and finally ending with a linear fully-connected layer Fin. For j ∈_ [K + M ],
_denote by W_ [[][j][]] _the matrix representation of the j-th layer after η, Nr(j) the number of ResNetFree_
_blocks between j and Fin exclusively and Vc(j) := max dim W_ [[][M] [+1]] _·_ _j<k≤M_ [min(][a][k][, b][k][)][ if]

_j ≤_ _M and 1 otherwise. Then there exists a constant D ≥_ 0 fixed at initialization such that for a.e.
_time t > 0,_

[Q]

1

8[N][r][(][j][)]Vc(j) _[∥][W][ [][j][]][(][t][)][∥]F[2]_ _[−∥][W][ [][j][]][(][t][)][∥]2[2]_ _[≤]_ _[D,][ ∀][j][ ∈]_ [[][K][ +][ M] []][.] (13)

_Furthermore, assume that_ _W_ [[][j][]] _F_ _for some j_ [K + M ], then we have, as t _:_
_∥_ _∥_ _→∞_ _∈_ _→∞_

1/ min rank(W [[][k][]]), 8[N][r][(][j][)]Vc(j) _W_ [[][k][]](t) 2[/][∥][W][ [][k][]][(][t][)][∥][2]F (14)
_≤∥_ _∥[2]_ _[≤]_ [1][,]

_In particular, for the last few fully-connected layers_  _j with Nr(j) = 0 and Vc(j) = 1, we have:_
_W_ [[][j][]](t) _uj(t)vj[⊤][(][t][)]_ _t→∞_ 0, _k_ [M ], _vj+1, uj_ _t→∞_ 1, (15)

_W_ [[][j][]](t) _F_ _−_ _−−−→_ _∀_ _∈_ _|_ _|_ _−−−→_
_∥_ _∥_ _F_

_where uj and vj are the left and right principal singular vectors of W_ [[][j][]].
**Corollary 1. For fully-connected networks with ReLU activations where the last K layers are lin-**
_ear layers, trained with linearly separable data under logistic loss ℓ, under the assumptions that_
_R(w(0)) < ℓ(0) and the limiting direction of weight vector (which exists (Ji & Telgarsky, 2020))_
_has no 0 entries, Equation 15 holds for the last K layers._

**Significance** Equation 13 and its limiting counterpart Equation 14 quantify a low-rank phenomenon by providing a lower bound on the ratio of the largest squared singular value (the operator
norm) and the sum of all squared singular values (the Frobenius norm). This lower bound depends
on the number (not dimensions) of ResNetFree layers (Nr) and certain dimensions of convolutional
layers (Vc). When the dimensions of ResNetFree layers are large, max dim W [[][M] [+1]] is small and the
number of input channels of convolutional layers are large, this lower bound is strictly better than the
trivial lower bound of 1/rank(W [[][k][]]). This is a quantification of the reduced alignment observed in
(Ji & Telgarsky, 2019). In particular, for the last few fully connected layers (Equation 15, Corollary
1), the lower bound matches the upper bound of 1 in the limit of Frobenius norm tending to infinity
and the limiting weight matrices have rank 1 and adjacent layers align.

6 CONCLUDING REMARKS AND FUTURE DIRECTIONS

In this paper, we extend the proof of the low rank phenomenon, which has been widely observed in
practice, beyond the linear network case. In particular, we address a variety of nonlinear architectural
structures, homogeneous and non-homogeneous, which in this context have not been addressed
theoretically before. To this end, we decomposed a feedforward ReLU/linear activated network into
a composition of a multilinear function with a tree network. If the weights converge in direction to a
vector with non-zero entries, the tree net is eventually fixed, allowing for chain rules to differentiate
through. This leads to various matrix-wise invariances between fully-connected, convolution layers
and ResNet blocks, enabling us to control the singular values of consecutive layers. In the end, we
obtain a low-rank theorem for said local architectures.

Proving convergence to the stable sign regime for a wider set of architectures will strengthen Theorem 2. Another direction is to connect our low-rank bias results to the max-margin implicit
regularization literature, which has been shown for linear networks and, more recently, certain 2homogeneous architectures (Ji & Telgarsky, 2020).

ACKNOWLEDGMENTS

This work was partially funded by NSF CAREER award 1553284 and NSF award 2134108. The
authors thank the anonymous reviewers for their insightful feedback. We would also like to thank
Matus Telgarsky for fruitful discussions on their related papers and on the Clarke subdifferential,
and Kaifeng Lyu for pointing out an error in an earlier version of this paper.


-----

REFERENCES

Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In Jennifer G. Dy and Andreas Krause (eds.), ICML,
volume 80 of Proceedings of Machine Learning Research, pp. 244–253. PMLR, 2018. URL
[http://dblp.uni-trier.de/db/conf/icml/icml2018.html#AroraCH18.](http://dblp.uni-trier.de/db/conf/icml/icml2018.html#AroraCH18)

J´erˆome Bolte and Edouard Pauwels. A mathematical model for automatic differentiation in machine learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad_vances in Neural Information Processing Systems, volume 33, pp. 10809–10819. Curran Asso-_
[ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/](https://proceedings.neurips.cc/paper/2020/file/7a674153c63cff1ad7f0e261c369ab2c-Paper.pdf)
[7a674153c63cff1ad7f0e261c369ab2c-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/7a674153c63cff1ad7f0e261c369ab2c-Paper.pdf)

Anna Choromanska, MIkael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
Loss Surfaces of Multilayer Networks. In Guy Lebanon and S. V. N. Vishwanathan (eds.), Pro_ceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,_
volume 38 of Proceedings of Machine Learning Research, pp. 192–204, San Diego, Califor[nia, USA, 09–12 May 2015. PMLR. URL https://proceedings.mlr.press/v38/](https://proceedings.mlr.press/v38/choromanska15.html)
[choromanska15.html.](https://proceedings.mlr.press/v38/choromanska15.html)

F.H. Clarke. Optimization and Nonsmooth Analysis. Wiley New York, 1983.

Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D. Lee. Stochastic subgradient
method converges on tame functions. Foundations of Computational Mathematics, 20(1):119–
[154, 2020. doi: 10.1007/s10208-018-09409-5. URL https://doi.org/10.1007/](https://doi.org/10.1007/s10208-018-09409-5)
[s10208-018-09409-5.](https://doi.org/10.1007/s10208-018-09409-5)

Simon S. Du, Wei Hu, and Jason D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced, 2018.

Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality.
In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on
_Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of_
_[Machine Learning Research, pp. 3004–3014. PMLR, 2021. URL http://proceedings.](http://proceedings.mlr.press/v139/ergen21b.html)_
[mlr.press/v139/ergen21b.html.](http://proceedings.mlr.press/v139/ergen21b.html)

Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of
pattern recognition unaffected by shift in position. Biological Cybernetics, 36(4):193–202, 1980.
[doi: 10.1007/BF00344251. URL https://doi.org/10.1007/BF00344251.](https://doi.org/10.1007/BF00344251)

Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
[volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2018/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf)
[paper/2018/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf.](https://proceedings.neurips.cc/paper/2018/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf)

Boris Hanin and David Rolnick. Deep relu networks have surprisingly few activation patterns. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alche Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso[ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf)
[9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf)

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 770–778, 2016. doi: 10.1109/CVPR.2016.90.

Christopher Heil. Absolute continuity and the fundamental theorem of calculus, 2019. URL
[https://doi.org/10.1007/978-3-030-26903-6_6.](https://doi.org/10.1007/978-3-030-26903-6_6)

Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pat_tern Recognition, 2017._


-----

Minyoung Huh, Hossein Mobahi, Richard Zhang, Pulkit Agrawal, and Phillip Isola. The low-rank
simplicity bias in deep networks. arXiv, 2021.

Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In In_[ternational Conference on Learning Representations, 2019. URL https://openreview.](https://openreview.net/forum?id=HJflg30qKX)_
[net/forum?id=HJflg30qKX.](https://openreview.net/forum?id=HJflg30qKX)

Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neu_ral Information Processing Systems, volume 33, pp. 17176–17186. Curran Associates,_
Inc., 2020. [URL https://proceedings.neurips.cc/paper/2020/file/](https://proceedings.neurips.cc/paper/2020/file/c76e4b2fa54f8506719a5c0dc14c2eb9-Paper.pdf)
[c76e4b2fa54f8506719a5c0dc14c2eb9-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/c76e4b2fa54f8506719a5c0dc14c2eb9-Paper.pdf)

Kenji Kawaguchi. Deep learning without poor local minima. In D. Lee, M. Sugiyama, U. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol[ume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2016/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf)
[paper/2016/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf.](https://proceedings.neurips.cc/paper/2016/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf)

Justin Khim and Po-Ling Loh. Adversarial risk bounds via function transformation, 2019.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444,
[2015. doi: 10.1038/nature14539. URL https://doi.org/10.1038/nature14539.](https://doi.org/10.1038/nature14539)

Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In International Conference on Learning Representations, 2020. [URL https:](https://openreview.net/forum?id=SJeLIgBKPS)
[//openreview.net/forum?id=SJeLIgBKPS.](https://openreview.net/forum?id=SJeLIgBKPS)

Behnam Neyshabur, Russ R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in deep neural networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Asso[ciates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/](https://proceedings.neurips.cc/paper/2015/file/eaa32c96f620053cf442ad32258076b9-Paper.pdf)
[eaa32c96f620053cf442ad32258076b9-Paper.pdf.](https://proceedings.neurips.cc/paper/2015/file/eaa32c96f620053cf442ad32258076b9-Paper.pdf)

Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Geometry of
optimization and implicit regularization in deep learning, 2017.

Adityanarayanan Radhakrishnan, Eshaan Nichani, Daniel Bernstein, and Caroline Uhler. On alignment in deep linear neural networks, 2020.

Miaoyan Wang, Khanh Dao Duc, Jonathan Fischer, and Yun S. Song. Operator norm inequalities
[between tensor unfoldings on the partition lattice, May 2017. ISSN 0024-3795. URL http:](http://dx.doi.org/10.1016/j.laa.2017.01.017)
[//dx.doi.org/10.1016/j.laa.2017.01.017.](http://dx.doi.org/10.1016/j.laa.2017.01.017)

Yi Zhou and Yingbin Liang. Critical points of linear neural networks: Analytical forms and
landscape properties. In International Conference on Learning Representations, 2018. URL
[https://openreview.net/forum?id=SysEexbRb.](https://openreview.net/forum?id=SysEexbRb)

A ILLUSTRATION OF LEMMA 1

We give a quick illustration of the two steps described in Section 3 for a fully-connected ReLUactivated network with 1 hidden layer. Figure 1 describes the unrolling of the neural network (Figure
1a) into a tree network (Figure 1b). Figure 2a describes the weight pull-back in the hidden layer and
Figure 2b describes the weight pull-back in the input layer. It is clear that the inputs of the tree net
in Figure 2b are coordinates of the path enumeration function h(x; w) in this example. Furthermore,
weights in the tree net depend entirely on the signs of the original weights. The rest of the proof
argues this intuition for general feed-forward neural nets. As a remark, in general, ρ is a very large
number - exponential in the number of layers for a fully-connected net with fixed width.


-----

_o_

[2]
_w1_ _w2[2]_

_v1_ _v2_

_w[1]11_ _w21[1]_ _w[1]12_ _w22[1]_

_i11_ _i12_ _i21_ _i22_

1 2 1 2
_x_ _x_ _x_ _x_


_o_

[2]
1 _w_
_w_ 2[2]

_v1_ _v2_

_w11_ _w[1]12_ _w21[1]_ 22w

_i1_ _i2_

1 2
_x_ _x_


(a)


(b)


Figure 1: Transformation of a feedforward network into a tree net. All nodes apart from the input
nodes use ReLU activation. The two neural nets drawn here compute the same function. This idea
has been used in Khim & Loh (2019) to prove generalization bounds for adversarial risk.


_o_

[2] )
(w1 sgn(

_w_

sgn 2[2]

)

_v1_ _v2_

_w_

[1]w[2]|w11 |1 _| |21w1w[1][2]_ [1]w[2]|w12 |2 _| |222w[1][2]_

_i11_ _i12_ _i21_ _i22_

1 2 1 2
_x_ _x_ _x_ _x_


_o_

[2] )
(w1 sgn(

_w_

sgn 2[2]

)

_v1_ _v2_

([1]w)11 sgn(w ([1]w)12 sgn(22w[1]
sgn 21)[1] sgn )

_i11_ _i12_ _i21_ _i22_

_|_ _|_ _|_ _|_
1 [2] 1 [2] 2 [2] 2
_w_ _w_ _w_ _w_
11 [1] 21 [1] 12 [1] 22
_|w_ _|w_ _|w_ _|w_
1 2 1 2
_x_ _x_ _x_ _x_


(a)


(b)


Figure 2: Pulling back of weights in a tree net. All nodes apart from the input nodes use the
ReLU activation. The original net was drawn in Figure 1a and 1b. This is possible due to the
positive-homogeneity of the activation function. From Figure 2b, one can recover the final tree net
in Theorem 1 with weights from {−1, +1} and input from the path enumeration function h of the
neural net by subdividing the edge incident to the input neurons and assign weights corresponding
to sgn(h(x)).

B PROOF OF LEMMA 1


We first prove an absolute-valued version of Lemma 1 and show that the extension to Lemma 1 is
straightforward. In other words, we first prove

**Lemma 5 (Absolute-valued decomposition). For an arbitrary feed-forward neural network ν :**
R[d] _→_ R, there exists a tree network µ[′] : R[ρ] _→_ R such that ν = µ[′] _◦_ _h[′]_ _where h[′]_ _is the_


-----

_absolute-valued path enumeration function defined as h[′](x) =_ _xp_ _e_ _p_

_∈_ _[|][w][e][|]_ _p∈P_ _[. Furthermore,]_

_the weights of µ[′]_ _is in_ 1, 1 _and only depends on the sign of the weights of the original network_ Q 
_{−_ _}_
_ν._

We first give some extra notation for this section. In general, we define:

_• ℘(S) to be the finite power set of some finite set S._
_p_ N 0 to be the cardinality of a set p. When p is a path then it is viewed as the number

_• |_ _| ∈_ _≥_
of edges.

To make it clear which graph we are referring to, define for an arbitrary feedforward neural network
_ν : R[d]_ _→_ R with computation graph G = (G[V ], G[E], G[w]):

_• Isingle output node ofG = {i[G]1_ _[, . . ., i]d[G][}][ to be the set of input node of] G._ _[ G][ and][ O][G][ =][ {][o][G][}][ to be the set of the]_



_• PG to be the enumeration of all paths from any input node in IG to the output node oG._

_• HG to be the enumeration of all paths from any node v ∈_ _G[V ] to the output node oG._
Note that if G has more than 2 vertices then _G_ _G._
_H_ _⊃P_

_• Each v ∈_ _G[V ] to be equipped with a fix activation σv that is positively-1-homogeneous._
To be precise, G is enforced to be a DAG, is connected and is simple (no self-loop, at most
1 edge between any pair of vertices). Each node v of G[V ] is equipped with: an activation
function G[σ](v) that is positively-1-homogeneous; a pre-activation function defined as:

_xj_ if v _i[G]j_ [for some][ j][ ∈] [[][d][]]

(G[PRE](v))(x) = _≡_ (16)

( _u∈INv_ [POST][v][(][x][)][w][uv] otherwise;

and a post-activation function defined asP


(G[POST](v))(x) = σv((G[PRE](v))(x)) (17)

Note that POSToG = ν.

_vp[G][, e]p[G]_ [to be the vertex and edge, respectively, furthest from][ o][G] [on some path][ p][ of][ G][.]

_•_

_x[G]p_ [to be the unique][ x][j] [such that][ i][G]j

_•_ _[∈]_ _[p][.]_

**Definition 9 (Hasse diagram of inclusion). Let S be a finite set and a set S ⊂** _℘(S) of elements_
_in S. A Hasse diagram of S is a directed unweighted graph HASSE(S) = (S, E) where for any_
_p, q ∈S, pq ∈_ _E iff p ⊆_ _q and |q| −|p| = 1._
**Definition 10 (Unrolling of feedforward neural networks). The unrolled tree neural network τG of**
_G is a tree neural network with computation graph TG where_

_• Unweighted graph (TG[V ], TG[E]) = HASSE(HG). In particular TG[V ] = HG and we_
_identify vertices of TG with paths in G._

_Weight function TG[w] : TG[E]_ _pq_ _G[w](ep)._

_•_ _∋_ _7→_

_Activation TG[σ](v) := G[σ](vp)._

_•_

**Lemma 6 (Unrolled network computes the same function). Fix an arbitrary feedforward neural**
_network ν : R[d]_ _→_ R with computation graph G. Let τG be the unrolled tree network of G with
_computation graph TG. Then ν = τG._

_Proof. We proceed with induction on αG := maxp_ _p_ the longest path between any input node
and the output node of G. In the base case, set αG = 0∈P | . Then| _VG = {oG} is a singleton and the_
neural network ν computes the activation of the input and return it. HG = VT = {p0} then is
a singleton containing just the trivial path that has just the output vertex of G and no edges. The
activation function attached to p0 in T, by construction, is the activation of oG. Thus, τ also simply
returns the activation of the input.

constructed as described. We will show that the proposition is also true whenAssume that τG = ν for any ν with graph G such that αG ≤ _t −_ 1 for some t ν ≥ has graph1; and for the G with τ
_αG = t. Fix such a ν and G that αG = t. We prove this induction step by:_


-----

1. First constructing G[′] from G such that ν[′] = ν where ν[′] is the neural network computed by
_G[′]._

2. Then showing that TG = G[′] by constructing an isomorphism π : G[′][V ] → _TG[V ] that_
preserves E, w and σ.


**The construction of G[′]** An illustration of the following steps can be found in Figure 3. Recall
that INoG = {v1, . . ., vm} is the set of in-vertices of oG.

1. Create m distinct, identical copies of G: G1, . . ., Gm.


2. For each j ∈ [m], remove from Gj all vertices u (and their adjacent edges) such that there
are no directed path from u to vj.

3. We now note that αGj _t_ 1 (to be argued) and invoke inductive hypothesis over Gj
to get an unrolled tree network ≤ _−_ _τj with graph Tj such that τj = νj where νj is the neural_
network computed by Gj.

4. Finally, construct G[′] by creating a new output vertex oG′ and connect it to the output
vertices oTj for all j ∈ [m]. As a sanity check, since each Tj is a tree network, so is G[′].
More precisely,

(a) G[′][V ] = {oG′ _} ∪_ [S]j[m]=1 _[T][j][[][V][ ]][;]_

(b) G[′][E] = {oGj _oG′ | j ∈_ [m]} ∪ [S]j[m]=1 _[T][j][[][E][]]_

(c) G[′][w](e) = G[w](vjoG) if e _oGj_ _oG′ for some j_ [m] and Tj[w](e), where
_≡_ _∈_
_e_ _Tj[E], otherwise._
_∈_

(d) G[′][σ](v) = G[σ](oG) if v ≡ _oG and Tj[σ](v), where v ∈_ _Tj[V ], otherwise._


_v1_

[1]11 _w_
_w_ 21[1]

_i1_ _i2_

1 2
_x_ _x_


_v2_

[1]12 _w_
_w_ 22[1]

_i1_ _i2_

1 2
_x_ _x_


_o_

[2]
1 _w_
_w_ 2[2]

_v1_ _v2_

_w11_ _w[1]12_ _w21[1]_ 22w

_i1_ _i2_

1 2
_x_ _x_


(a) G


(b) G1 _TG1_
_≡_


(c) G2 = TG2


Figure 3: Construction of G[′]. G1 and G2 are the modified copies of G in step 2. In step 3, the
transformation TGi happens to coincide with Gi for i = 1, 2 in this case. G[′] is created in step 4 by
adding an extra vertex oG′ and connecting it to v1 and v2 with the appropriate weights and activation
and can be seen in Figure 1b.
.

_G[′]_ **is well-defined** We verify each steps in the above construction:


1. This step is well-defined.

2. For anystep; since Gj v, as long asj is an in-vertex of αG ≥ 2 (by definition), we always remove oG and G is a DAG. Otherwise, this step is well-defined. oG from each Gj in each


-----

3. Fix a j ∈ [m]. By construction (and since we always remove oG from Gj in the previous
step), OGj = {vj}. If there is a path p[∗] with length at least t in Gj, then since Gj[E] ⊆
_Gvalid path with length[E] and oG ∈_ _G[V ] t \ + 1 Gj. This violates the assumption that[V ], the path p[∗]_ _∪{oG} created by appending αG = t and we conclude, oG to p[∗]_ is a
by contradiction, that αGj _t_ 1. This justifies the invocation of inductive hypothesis for
_νj to get a tree neural net τ ≤j._ _−_

4. The final step is well-defined.

_ν[′]_ **computes the same function as ν** Recall that ν[′] is the neural network with graph G[′]. We have
for any input x ∈ R[d]

_ν[′](x) = (G[′][POST](oG′_ ))(x) (18)


= G[′][σ](oG[′] )

= G[σ](oG)

= G[σ](oG)


_G[′][w](vjoG′_ ) (G[′][POST](vj))(x)

 _·_

_j=1_

X
m

_G[w](vjoG)_ (Tj[POST](vj))(x)
_·_ 
_j=1_

X




(19)

(20)


= G[σ](oG) _G[w](vjoG)_ (G[POST](vj))(x) = ν(x) (21)

 _·_ 

_j=1_

X

where we invoked the inductive hypothesis in the last line to get 
(Tj[POST](vj))(x) = τj(x) = νj(x) = (G[POST](vj))(x), (22)
and the rest are definitions.

_G[′]_ **is isomorphic to TG** Although this should be straightforward from the construction, we give a
formal proof. Consider the isomorphism π : G[′][V ] _TG[V ] given as_
_→_

_oTG =_ _oG_ _G_ if v _oG′_

_π(v) =_ _{_ _} ∈H_ _≡_ (23)

(p _oG_ if Tj[V ] _v_ _p_ _Gj for some j_ [m],
_∪{_ _}_ _∋_ _≡_ _∈H_ _∈_

where paths are viewed as set of vertices. The second case is well-defined since p ∪{oG} ∈HG for
any p ∈HGj for any j ∈ [m] by construction of Gj.

We can now verify that π is an isomorphism between the two structures. Fix pq ∈ _G[′][E]. Consider_
two separate cases: pq _Tj[E] for some j and pq_ _Tj[E] for any j. In the first case, by definition_
_∈_ _̸∈_
of Tj as a Hasse diagram, p = {vp} ∪ _q are paths in HGj_ . Thus, π(p)π(q) = (p ∪{oG})(q ∪
_oG_ ) satisfying π(p) = _vp_ _π(q). Thus, by definition of Hasse diagram, π(p)π(q)_ _TG[E]._
_{_ _}_ _{_ _} ∪_ _∈_
Furthermore, G[′][w](pq) = G[w](ep) = TG[w](π(p)π(q)) In the second case, we have pq = vjoG
for some j [m]. Thus, π(p)π(q) = ( _vj, oG_ )( _oG_ ) _TG[E] also by definition of Hasse_
_∈_ _{_ _}_ _{_ _}_ _∈_
diagram. At the same time, G[′][w](pq) = G[w](vjoG) = TG[w](π(p)π(q)) by definition.

Fix v _G[′][V ]. If v_ _oG′ then G[′][σ](v) = G[σ](oG). At the same time, TG[σ](π(v)) =_
_∈_ _≡_
_G[σ](v_ _oG_ = G[σ](oG) = G[′][σ](v). If v _oG′ then there is a j_ [m] and a p _Gj such_
_{_ _}_ _̸≡_ _∈_ _∈H_
that v = p _Tj[V ]. Then by definition,_
_∈_
_G[′][σ](v) = Tj[σ](v) = G[σ](vp) = G[σ](vp_ _oG′_ ) = TG[σ](π(v)). (24)
_∪{_ _}_

This completes the inductive proof that shows τG = ν[′] = ν for G with αG = t. By mathematical
induction, the claim holds for all neural network ν.

**Lemma 7 (Pull-back of numerical weights). Fix an arbitrary feedforward neural network ν : R[d]** _→_
Re1 with computation graph, . . ., ek _G[E] and a single out-edge G. Let v ∈_ _G f[V ] \ (GI[GE ∪]. Then the networkOG) be an inner vertex of ν[′]_ _with computation graph G with k in-edges_
_G[′]_ = (G[V ∈ ], G[E], G[′][w]) defined as _∈_


_G[w](e)|G[w](f_ )| _if e ≡_ _ej for some j ∈_ [k]
sgn(G[w](f )) _if e ≡_ _f_ (25)
_G[w](e)_ _otherwise._


_G[′][w] : e 7→_


-----

_computes the same function as ν. In other words, we can pull the numerical values of G[w](f_ )
_through v, into its in-edges; leaving behind only its sign._

_When fixing inputsgn(G[w](f_ )) and update x to G, one can extend this operation to xj to xj|G[w](f )|. _ij ∈_ _IG for j ∈_ [d]. By setting G[′][w](f ) =

_Proof. Let the single out-vertex of v be b. If suffices to show that G[PRE](b) = G[′][PRE](b). Fix_
an input x to ν. Let the k in-edges of v be a1, . . ., ak. Since we only change edges incident to v,
_G[POST]aj = G[′][POST]aj for all j ∈_ [k]. We have:

_G[′][PRE](b) = sgn(G[w](vb)) · (G[σ](v))(G[′][PRE](v))_ (26)


= sgn(G[w](vb)) · (G[σ](v))


(27)

(28)


_G[w](vb)_ _G[w](ajv)_ _G[POST](aj)_
_|_ _|_ _·_
_j=1_

X


= sgn(G[w](vb))|G[w](vb)| · (G[σ](v))


_G[w](ajv)_ _G[POST](aj)_
_·_
_j=1_

X


= G[w](vb) (G[σ](v)) _G[w](ajv)_ _G[POST](aj)_ = G[PRE](b), (29)
_·_  _·_ 

_j=1_

X
 

where equation 28 is due to positive homogeneity of σv and the rest are just definitions.

We can now give the proof of Lemma 5.

_Proof of Lemma 5. Given an arbitrary feedforward neural network ν with computation graph_
_G_ = (G[V ], G[E], G[w]), we use Lemma 6 and get the unrolled tree network TG =
(TG[V ], TG[E], TG[w]) such that ν = τG.

Let π = π1, π2, . . ., πξ be an ordering of TG[V ] \ OTG (so ξ = |H| − 1) by a breadth first search
on TG starting from oTG. In other words, if dtopo(u, oTG ) > dtopo(v, oTG) then u appears after v in
_π, where dtopo(a, b) is the number of edges on the unique path from a to b for some a, b ∈_ _TG[V ]._
Iteratively apply Lemma 7 to vertex π1, . . ., πξ in TG while maintaining the same function. After
_ξ such applications, we arrive at a network µ[′]G_ [with graph][ M][G][ such that][ µ]G[′] [=][ τ][G][ =][ ν][. Recall]
that the pull-back operation of Lemma 7 only changes the tree weights. Furthermore, the π ordering
is chosen so that subsequent weight pull-back does not affect edges closer to oTG. Therefore, at
iteration j,

1. MG[w](πkq) = sgn(G[w](eπk )) for all k _j, for some q_ _MG[V ] such that (πk)q_
_≤_ _∈_ _∈_
_MG[E]._

2. if πj _IMG then MG[w](r(πj)) = G[w](er)_ _f_ _πj_

such that ̸∈ _r(πj) ∈_ _MG[E]; otherwise, πj is an input vertex corresponding to input∈_ _[|][G][[][w][](][f]_ [)][|][, for some][ r][ ∈] _[M] x[G][[]π[V]j[ ]],_
then xπj is modified to xπj _f_ _πj_ [Q]G[(][x][π]j [)][ where][ h][′]G [is the absolute-valued]

_∈_
path enumeration function. _[|][G][[][w][](][f]_ [)][|][ =][ h][′]

Q

This completes the proof.

Now we present the extension to Lemma 1:

_Proof of Lemma 1. Invoke Lemma 5 to get a tree network µ[′]_ such that ν = µ[′] _◦_ _h[′]. Then one_
can subdivide each input edges (edges that are incident to some input neuron ij) into two edges
connected by a neuron with linear activation. One of the resulting egde takes the weight of the old
input edge; and the other is used to remove the absolute value in the definition of the (basic) path
enumeration function.

More formally, for all p ∈P, recall that ip is a particular input neuron of µ[′] in the decomposition
lemma (Lemma 1). Since µ[′] is a tree neural network, we there exists a distinct node up in µ[′] that is


-----

adjacent to ip. Remove the edge ipup, add a neuron u[′]p[, connect][ i][p][u][′]p [and][ u]p[′] _[u][p][, where the weight]_

of the former neuron is set to sgn _e∈p_ _[w][e]_ and the latter to w[µ[′]](ipup). It is straightforward to

see that with µ constructed from above,Q _ν = µ_ _◦_ _h where h is the path enumeration function._

With slight modifications to the proof technique, one can show all the results for Theorem 1, Theorem 2 and Corollary 1 to the same matrix representation as presented in the paper but with the
absolute signs around them.

C PROOF OF TRAINING INVARIANCES

**Claim 1 (Directional convergence to non-vanishing point implies stable sign). Assumption 2 implies**
_Assumption 1._

_Proof. Let v be the direction that_ _∥ww((tt))∥2_ [converges to. Let][ O][ be the orthant that][ v][ lies in. Since][ v]

does not have a 0 entry, the ball with radius mini _vi_ _/2 and center v is a subset of the interior of_
_w(t)_ _B_ _|_ _|_ _w(s)_
_O. Since_ _∥w(t)∥2_ [converges to][ v][, there exists a time][ T][ such that for all][ s > T,] _∥w(s)∥2_

eventually, w(s) ∈B where its signs stay constant. _[∈B][. Thus,]_

**Claim 2 (Directional convergence does not imply stable activation). There exists a function w : R →**
R[d] _such that w(t) converges in direction to some vector v but for all u ∈_ _w(R), w[−][1](u) has infinitely_
_many elements. This means that for some ReLU network empirical risk function R(w) whose set_
_of nondifferentiable points D has nonempty intersection with w(R), the trajectory_ _w(t)_ _t_ 0 _[can]_

_≥_
_cross a boundary from one activation pattern to another an infinite number of times._



_Proof. Fix a vector v ∈_ R[d]. Consider the function t 7→ _v|t sin(t)|._

**Lemma 8 (Clarke partial subderivatives of inputs with the same activation pattern is the same).**
_Assume stable sign regime (Assumption 1). Let p1, p2_ _be paths of the same length L. Let_
_p1 = {v1, . . ., vL} ∈_ _V_ _[L]_ _and p2 = {u1, . . ., uL} ∈_ _V_ _[L] ∈P. Assume that for each i ∈_ [L], we have vi
_and ui having the same activation pattern for each input training example, where the activation of a_
_neuron is 0 if it is ReLU activated and has negative preactivation; and is 1 if it is linearly activated_
_or has nonnegative preactivation. Then ∂p1_ _µ(h) = ∂p2_ _µ(h) where ∂_ _is the Clarke subdifferential._

_Proof. In this proof, we use the absolute-valued version of the decomposition lemma. Fix a training_
example j and some weight w0 and let the output of the path enumeration function be h := (hp =
_h(xj; w0))p_ . Denote R[2] the input space of all possible pairs of values of (p1, p2) such that
_∈P_ _X ⊆_
Assumption 1 and the extra assumption that both paths have the same activation pattern on each
neuron hold. Let m : R[2] _→_ R be the same function as the tree network µ but with all but the two
inputs at p1, p2 frozen. We will show that m is symmetric in its input. Once this is establish, it is
trivial to invoke the definition of the Clarke subdifferential to obtain the conclusion of the lemma.

Let (a, b) ∈X ⊆ R[2]. We thus show that if (b, a) ∈X then m(a, b) = m(b, a). Recall that µ is itself
a ReLU-activated (in places where the corresponding original neurons are ReLU-activated) neural
network. The fact that µ has a tree architecture means that for each input node, there is a unique
path going to the output node. Thus, the set of paths from some input node in µ to its output node
can be identified with the set of inputs itself: P. Now let us considered the product of weights on
some arbitrary path p. It is not hard to see that for each such path, the product is just Pp = _e∈p_ _[w][e]_

since the input to p is |P _| and going along the path collects all signs of we for all e ∈_ _p._

We now invoke the 0-1 form of ReLU neural network to get m(a, b) = _µ[Q](h)_ =

_p_ _[Z][p][(][a, b][)][P][p][(][a, b][)][ where][ Z][p][ is][ 1][ iff all neurons on path][ p][ is active in the][ µ][ network (recall that]_
_∈P_
we identify paths in µ to input nodes). Consider that what changes between m(a, b) and m(b, a):

P

since µ is a tree network, exchanging two inputs can only have effect on the activation pattern of
neurons along the two paths from these inputs. However, we restricted X to be the space where
activation pattern of each neuron in the two paths is identical to one another. Since both (a, b) and
(b, a) is in X, swapping one for another does not affect the activation pattern of each neuron in the
two paths! These neurons activation pattern is then identical to those in network µ by construction


-----

and hence Zp(a, b) = Zp(b, a) for all p since activation pattern of each node of the µ network
_∈P_
stays the same. Thus we conclude that m(a, b) = m(b, a). This completes the proof.

_Proof of Lemma 2. This proof uses the absolute-value-free version of the decomposition lemma._
This is just to declutter notations, as the same conclusion can also be reach using the other version,
with some keeping track of weight signs. Recall that our real weight vector w(t) is an arc Ji &
Telgarsky (2020) which, by definition, is absolutely continuous. It then follows from real analysis
that its component we(t) is also absolutely continuous for any e _E. For absolutely continuous_
_∈_
functions we(t) for some e _E, invoke the Fundamental Theorem of Calculus (FTC) (Chapter 6,_
_∈_
(Heil, 2019)), to get:


_wuv(s)_ [d][w][uv] (30)

[0,t] dt [(][s][)][d][s]


_wuv[2]_ [(][t][)][ −]
_uX∈INv_


_wuv[2]_ [(0) = 2]
_uX∈INv_


_u∈INv_


We now proceed to compute [d]d[w]t [(][s][)][. Since][ w][(][t][)][ is absolutely continuous and we are taking the]

integral via FTC, we only need to compute [d]d[w]t [(][s][)][ for a.e. time][ s][. By chain rule (see for example,]

the first line of the proof of Lemma C.8 in Lyu & Li (2020)), there exists functions (gj)[n]j=1 [such that]
_gj ∈_ _∂νxj_ (w) ⊆ R[|][E][|] for all j ∈ [n] where νxj (w) = ν(xj; w), and for a.e. time s ≥ 0,

_n_

dw

_l[′](yjν(xj; w(s)))_ _yj_ _gj_ (31)

dt [(][s][) = 1]n _j=1_ _·_ _·_

X

Fix j ∈ [n], by the inclusion chain rule, since νxj = µ ◦ _hxj_, with h and µ also locally Lipschitz,
we have by Theorem I.1 of Lyu & Li (2020),

_∂νxj_ (w) = ∂(µ _hxj_ )(w) CONV [α]pβj,p _α_ _∂µ(h(w)), βj,p_ _∂[hxj_ ]p(w) (32)
_◦_ _⊆_ p _ρ_ _|_ _∈_ _∈_ 

X∈ 

  _[.]_

that:Thus there exists (γa)[A]a=1 _[≥]_ [0][,][ P]a[A]=1 _[γ][a][ = 1][ and][ (][α][a][ ∈]_ _[∂µ][(][h][(][w][))][, β]j,p[a]_ _[∈]_ _[∂][[][h][x]j_ []][p][(][w][))][A]a=1 [such]

_A_

_gj =_ _γa_ [α[a]]pβj,p[a] _[.]_

_aX=1_ Xp∈ρ

Here we use Assumption 1 to deduce that eventually, all weights are non-zero in gradient flow
trajectory to compute:


d[hxj ]p(w)

d(w)


1e∈p · (xj)p


_∂[hxj_ ]p(w) =

Plug this back into gj to get:

_gj =_


_wf_
_f_ _∈Yp,f_ ≠ _e_


_e∈E_



[α][a]p
_p∈Xρ|e∈p_ _[·][ (][x][j][)][p]_


_γa_
_a=1_

X


_wf_
_f_ _∈Yp,f_ ≠ _e_


_e∈E_


Plug this back into [d]d[w]t [(][s][)][ and to get, coordinate-wise, for a.e.][ s][ ≥] [0][,]


dwe

dt [(][s][) = 1]n


_l[′](yiν(xi; w(s)))_ _yi_
_j=1_ _·_ _·_

X



[α[a]]p (xj)p
_·_
_p∈Xρ|e∈p_


_wf_ (s). (33)
_f_ _∈Yp,f_ ≠ _e_


_γa_
_a=1_

X


Multiply both sides with we gives:

_we(s)_ [d][w][e]

dt [(][s][) =]


_dj,p(w(s)),_ (34)
_j=1_

X


_p∈P|e∈p_


-----

where


_dj,p(w) = ℓ[′](yiν(xi; w))_ _yi_ _γa[α][a]p_
_·_ _·_ _a=1_ _[·][ (][x][j][)][p]_ _[·]_

X


_wf_ (s) _._ (35)
_|_ _|_
_fY∈p_


Note that d does not depend on the specific edge e used in Equation 33 and also that the term given
by β does not depend on a and we can simply write αp for _a=1_ _[γ][a][[][α][]]p[a][.]_

Plugging back into the FTC to get:

[P][A]


_wuv[2]_ [(][t][)][ −]
_uX∈INv_


_wuv[2]_ [(0) = 2]
_uX∈INv_

= 2


_dj,p(w(s))ds_ (36)
_j=1_

X



[0,t]


_u∈INv_

[0,t]

Z


_p∈P|uv∈p_


_dj,p(w(s))ds._ (37)
_j=1_

X


_p∈P|v∈p_


Finally, by an identical argument but applied to the set of edges vb for some b OUTv, we have:
_∈_


_wvb[2]_ [(][t][)][ −]
_b∈XOUTv_


_wvb[2]_ [(0) = 2]
_b∈XOUTv_


1

_dj,p(w(s))ds_ (38)

_n_

_j=1_

X

_wuv[2]_ [(0)][,] (39)
_uX∈INv_



[0,t]


_p∈P|v∈p_


= _wuv[2]_ [(][t][)][ −]

_uX∈INv_

which completes the proof of the first part of the lemma.


For the second part, recall that we have 2 vertices u, v such that INv = INu = IN and OUTv =
OUTu = OUT with stable activation pattern for each training example. To make it more readable, we
drop the explicit dependence on t in our notation and introduce some new ones: for some a ∈ IN, let
_I_ _a be the set of all paths from some input node in I to node a and for some b_ OUT, let _b_ _o_
_P_ _→_ _∈_ _P_ _→_
be the set of all paths from b to the output node o. Then one can decompose the sum as


_ℓ[′](yν(xj; w))_ _y_
_−_ _·_ _·_
_j=1_

X


(xj)p1 _wub_
_p2∈PXb→o_ _·_ _·_


_f_ _∈Yp1∪p2_ _wf · αp1∪{u}∪p2_ _,_

(40)


_dt_ _[w][au][ =]_


_p1∈PI→a_


_b∈OUT_


where αp is the partial Clarke subdifferential at input p.

Recall that the end goal is to derive


_d_

(41)
_dt_ _[w][au][.]_


_dt_ _[w][au][w][av][ =][ w][au]_ _dt_ _[w][av][ +][ w][av]_


Using equation equation 40, the second term on the right hand side becomes


_ℓ[′](yν(xj; w))_ _y_
_−_ _·_ _·_
_j=1_

X


(42)
_p2∈PXb→o_


_wav_


_dt_ _[w][au][ =]_


_p1∈PI→a_ _b∈OUT_


 _· wub_ _· αp1∪{u}∪p2_ (xj; w) (43)



 


(xj)p1 _wav_
_·_ _·_




_wf_
_f_ _∈Yp1∪p2_


-----

where the product wav · _f_ _∈p1∪p2_ _[·][w][f]_ _wub is a jagged path. Then one continues with the deriva-_

tion to get Q 

_d_ _d_

_wauwav_ = (44)

_dt_   _dt_ [(][w][au][w][av][)]

_a∈IN_ _aX∈IN_

_n_ [X] 

= _ℓ[′](yν(xj; w))_ _y_ (45)

_−_ _·_ _·_

Xj=1 _aX∈IN_ _bX∈OUT_ _p1∈PXI→a_ _p2∈PXb→o_


(xj)p1 _wf_

 _·_ _f_ _∈Yp1∪p2_




_wav · wub · αp1∪{u}∪p2_ (xj; w) + wau · wvb · αp1∪{v}∪p2 (xj; w)


(46)


On the other hand

_d_ _d_

_wubwvb_ = (47)

_dt_   _dt_ [(][w][ub][w][vb][)]

_b∈OUT_ _bX∈OUT_

_n_  [X] 

= _ℓ[′](yν(xj; w))_ _y_ (48)

_−_ _·_ _·_

Xj=1 _aX∈IN_ _bX∈OUT_ _p1∈PXI→a_ _p2∈PXb→o_


(xj)p1 _wf_

 _·_ _f_ _∈Yp1∪p2_




_wav · wub · αp1∪{v}∪p2_ (xj, w) + wau · wvb · αp1∪{u}∪p2 (xj; w)


(49)


This is where the more restrictive assumption that for each training example, u and v have the
same activation pattern as each other (but the same activation pattern between u and v of, say
_x1, may differs from that on, say x2). Under this assumption, we can invoke Lemma 8 to get_
_αp1∪{u}∪p2_ (xj; w) = αp1∪{v}∪p2 (xj; w) as a set. This identifies 46 with 49 and gives us:

_d_

_wau(t)wav(t)_ _wub(t)wvb(t)_ = 0. (50)

_dt_  _−_ 

_a∈IN_ _bX∈OUT_

[X] 

This holds for any time t where u and v has the same activation pattern. If further, they have the
same activation pattern throughout the training phase being considered, then one can invoke FTC to
get the second conclusion of Lemma 2. Note, however, that we will be using this differential version
in the proof of Theorem 1.

_Proof of Lemma 3. The proof is identical to that of Lemma 2, with the only difference being the set_
_A that we double count. Here, set A to be P. Then by the definition of layer (see Definition 7), one_
can double count P by counting paths that goes through any element of a particular layer. The proof
completes by considering (using the same notation as the proof of Lemma 2) for a.e. time s ≥ 0,

_n_

1

_we(s)_ [d][w][e] _dj,p(w(s))_ (xj)p _wf_ (s), (51)
_eX∈F_ dt [(][s][) =] _pX∈P_ _n_ Xj=1 _·_ _·_ _fY∈p_

for any layer F .

Before continuing with the proof of Theorem 1, we state a classification of neurons in a convolutional
layer. Recall that all convolutional layers in this paper is linear. Due to massive weight sharing within
a convolutional layer, there are a lot more neurons and edges than the number of free parameters in
this layer. Here, we formalize some concepts:


-----

_u_

_i_ _a_ _b_ _o_

_v_


Figure 4: Jagged path. Here the straight arrow denote a single edge in the graph while the snaked
arrow denote a path with possibly more than one edge. u and v are nodes in the statement of the
second part of Lemma 2, a ∈ IN = INv = INu, b ∈ OUT = OUTv = OUTu.

**Definition 11 (Convolutional tensors). Free parameters in a (bias-free) convolutional layer can be**
_organized into a tensor T ∈_ _R[a][×][b][×][c]_ _where a is the number of input channels, b is the size of each_
2D filter and c is the number of output channels.

For example, in the first convolutional layer of AlexNet, the input is an image with 3 image channels
(red, blue and green), so a = 3 in this layer; filters are of size 11 × 11 so b = 121 is the size of
each 2D filter; and finally, there are 96 output channels, so c = 96. Note that changing the stride
and padding does not affect the number of free parameters, but does change the number of neurons
in the computation graph of this layer. Thus, we need the following definition:
**Definition 12 (Convolutional neuron organization). Fix a convolutional layer with free parame-**
_ters T ∈_ _R[c][×][b][×][a]. Then neurons in this layer can be organized into a two dimensional array_
_vj,k_ _j_ [l],k [c] for some l N such that
_{_ _}_ _∈_ _∈_ _∈_

_1. For all j, j[′]_ [l] and for all k [c], vj,k and vj′,k share all its input weights. More
_∈_ _∈_
_formally, there exists a bijection φ : INvj,k_ INvj′ _,k such that:_
_→_

_wuvj,k_ _wφ(u)vj′_ _,k_ _,_ (52)
_≡_

_for all u ∈_ INvjk .

_2. For all j_ [l] and for all k, k[′] [c], vj,k and vj,k′ has the same in-vertices and out-vertices.
_∈_ _∈_
_In other words,_

INvjk = INvj,k′ =: INvj and OUTvjk = OUTvj,k′ =: OUTvj _._ (53)


For example, in the first convolutional of AlexNet in R[96][×][121][×][3], the input image dimension is
224 224 3 pixels and the stride of the 11 11 3 filters in the layer is 4, with no padding.
_×_ _×_ _×_ _×_ 2

Thus a single 2D filter traverse the image 224−(114 _−4)_ =: l times, each corresponds to a different

neuron in the layer. This process is then repeated _c times for each output channel, for a total of_ _c_ _l_
_×_
neurons in the convolutional layer.
**Lemma 9 (Extension of Lemma 2 to weight sharing). Let** := _vj,k_ _j_ [l],k 1,2 _be a convolu-_
_V_ _{_ _}_ _∈_ _∈{_ _}_
_tional neuron organization. Recall that by definition 12, for all j ∈_ [l], INvj,1 = INvj,2 =: INj and
OUTvj,1 = OUTvj,2 =: OUTj. We have for a.e. time t ≥ 0,


_|w(t)w[′](t)| −|w(0)w[′](0)| =_
(w,w[′]X)∈wIN(V)


_|z(t)z[′](t)| −|z(0)z[′](0)|,_ (54)
(z,z[′])X∈wOUT(V)


_where_

_wIN(V) := {(w1, w2) | ∀j ∈_ [l], ∃aj ∈ INj, waj _vj,1 ≡_ _w1 and waj_ _vj,2 ≡_ _w2},_

_and similarly,_


_wOUT(V) := {(w1, w2) | ∀j ∈_ [l], ∃bj ∈ OUTj, wvj,1bj ≡ _w1 and wvj,2bj ≡_ _w2}._


-----

Before we start the proof, some remarks are in order. Letlet = _vj,k_ _j_ [l],k [2] be its convolutional neuron organization. Then T1 ∈ R[2][×][b][×][a] be a convolutional layer and
_V_ _{_ _}_ _∈_ _∈_

_wIN(_ ) = ([T1]1,j2,j1 _, [T1]2,j2,j1_ ) _j1_ [a], j2 [b] _._ (55)
_V_ _{_ _|_ _∈_ _∈_ _}_

If furthermore T2 _R[e][×][d][×][2]_ is a convolutional layer immediately after T1, then
_∈_

_wOUT(_ ) = ([T2]k1,k2,1, [T2]k1,k2,2) _k1_ [e], k2 [d] _._ (56)
_V_ _{_ _|_ _∈_ _∈_ _}_

If instead of T2, the subsequent layer to T1 is a fully-connected layer W ∈ R[d][×][(][l][×][2)] (recall that
there are 2l neurons in T1 layer), then

_wOUT(V) = {(Wl1,l2×1, [T2]l1,l2×2) | l1 ∈_ [d], l2 ∈ [l]}. (57)

_Proof of Lemma 9. As before, the proof is identical to that of Lemma 2 with the exception being the_
set of paths that we are double counting over. For all j ∈ [l], let

_Aj := {p1 ∪_ _p2 | p1 is a path from I to vj,1, p2 is a path from vj,2 to o},_ (58)

and
_j_ [:=][ {][p][1] [is a path from][ I][ to][ v][j,][2] _[, p][2]_ [is a path from][ v][j,][1] [to][ o][}][.] (59)
_A[′]_ _[∪]_ _[p][2]_ _[|][ p][1]_

Let := _j_ [l] _j[. Then by an identical argument as that of Lemma 2, we can show that]_
_A_ _∈_ _[A][j][ ∪A][′]_

_n_

1

[S]

_w(t)w[′](t)_ _w(0)w[′](0) =_ _dj,p(w(s))ds_ (60)
(w,w[′]X)∈wIN(V) _−_ Z[0,t] _pX∈A_ _n_ Xj=1

= _z(t)z[′](t) −_ _z(0)z[′](0)._ (61)

(z,z[′])X∈wOUT(V)

Here the notation di,j is well-defined since we do not have to specify a path for the subgradient
_αp. This is because we are working with linear convolutional layers and thus all subgradients are_
gradients and is evaluated to 1.

_Proof of Theorem 1. All points in this theorem admit the same proof technique: First, form the_
matrices as instructed. Let X = _v1, v2, . . ., vm_ _V be the active neurons shared between the_
_{_ _} ⊆_
two layers. Check the conditions of the second part of Lemma 2 and invoke the lemma for each pair
_u, v ∈_ _X. We now apply this to each points:_

1. LetR[c][×] W[b] be a fully-connected layer from1 ∈ R[b][×][a] be a fully-connected layer from neurons in V2 to V3. Then we have V1 to neurons in INu = V and V OUT2 andu = W W2 ∈
for all u ∈ _U_ . Furthermore, all weights around any u are learnable for all u in U . Invoke
the second part of Lemma 2 to get the conclusion.

2. letorganization of T1 ∈ R[c][×][b][×] T[a] and1 (Definition 12) being T2 ∈ R[e][×][d][×][c] be the convolutional tensors with convolutional neuronvj,k _j_ [l1],k [c]. Form the matrix representation
_{_ _}_ _∈_ _∈_
_Wk, k1 ∈[′]_ R[c[c]], for all[×][(][ab][)] and j _W[l2] ∈, INvRj,k[(][de] =[)][×] IN[c]_ as per the theorem statement. By Definition 12, forvj,k′ and OUTvj,k = OUTvj,k′ . Invoke Lemma 9 to get
_∈_ _∈_
the conclusion.

3. Let r(x; U, Y, Z) be a residual block of either ResNetIdentity, ResNetDiagonal or ResNetFree. In all variants, skip connection affects neither the edges in U ∈ R[b][×][a] and Y ∈ R[c][×][b].
Let Y be fully-connected from neurons V1 to neurons V2 and U be fully-connected from
neurons V2 to neurons V3. Then for each u ∈ _V2, INu = V1 and OUTu = V3. Furthermore,_
all weights around any vertices in V2 are learnable. Invoke the second part of Lemma 2 to
get the conclusion.


-----

4. Let ri(x; Ui, Yi, Zi), i ∈{1, 2} be consecutive ResNetFree block. Let Y1 be fully connected from neurons V1 to neurons V2, U1 be fully-connected from V2 to neurons V3, Y2 be
fully-connected from neurons V3 to V4 and U2 be fully-connected from V4 to neurons V5.

Then _U1_ _Z1_ is fully-connected from V1 _V2 to V3 and_ _Y2_ is fully-connected from
_∪_ _Z2_
 

_V3 to V_ 4 ∪ _V5. Invoke the first point to get the conclusion._

5. Let the convolutional tensor be T ∈ R[c][×][b][×][a] with convolutional neuron organization
_vj,k_ _j_ [l],k [c] for some l N. Let the adjacent fully-connected layer be W R[d][×][(][l][×][c][)].
_{_ _}_ _∈_ _∈_ _∈_ _∈_
Form the matrix representationment. Then we have for any k, k W[′] 1 ∈[cR] and for all[c][×][ab] and W j2 ∈ [Rl],[dl] IN[×][c]vj,kas per the theorem state- = INvj,k′ =: INj and
_∈_ _∈_

OUTvj,k = OUTvj,k′ =: OUTj. Invoke Lemma 9 to get the conclusion.

6. Let r(x; U, Y, Z) be a residual block of either ResNetIdentity, ResNetDiagonal or ResNetFree. Let Y be fully-connected from neurons V1 to neurons V2, U be fully-connected from
neurons V2 to neurons V3. Thus, Z is fully-connected from V1 to V3. Then W2 = _U_ _Z[]_

is fully connected from V1 to V2 _V3.We invoke the fifth point to get the conclusion._
_∪_ 

7. first consider the case where the ResNetFree block is followed by the fully-connected layer.
Let r(x; U, Y, Z) be the first ResNetFree block with input neurons where Y fully-connects
neurons V1 to V2 and U fully connects V2 to V3. Then we have _U_ _Z[]_ fully-connects
_Vget the conclusion; otherwise if the subsequent layer is a ResNetFree block1_ _∪V2 to V3. If the subsequent layer is a fully-connected layer then invoke the first point to_ _r(x; U_ _[′], Y_ _[′], Z_ _[′])_

_Y_
with Y _[′]_ fully-connects V3 to V4 and U _[′]_ fully-connects V4 to V5. Then _Z_ fully-connects
 

_V3 to V4 ∪_ _V5 and we one again invoke the first point to get the conclusion._

_Proof of Lemma 4. This is the continuation of the proof of Lemma 2. To obtain noninvariance, one_
only needs to show that when u is active and v inactive, the expression in 46 and 49 are not equal in
general. For the sake of notation, we pick the case where the preactivation of u is strictly positive,
that of v is strictly negative, and further assume that the whole network is differentiable at the current
weights w for all training examples.

In this case, it is not hard to see that the Clarke subdifferential ∂wµ(h) is a singleton and contains
the gradient of the µ network. Furthermore, for any path p = (v1, . . ., vL), the partial derivative _[∂µ]∂p_

is 1 if all neurons on p are active and 0 otherwise. Thus, we have


_wauwav_



_a∈IN_

[X]


(62)

_wav_ _wub_

 _·_ _·_

 (63)


_dt_


_ℓ[′](yν(xj; w))_ _y_
_−_ _·_ _·_
_j=1_

X


(xj)p1

 _·_


_wf_
_f_ _∈Yp1∪p2_


active p1∈PI→a


active p2∈Pb→o


_a∈IN_


_b∈OUT_


We can actually factorize this even further by noticing that the term wav does not depend on b and
_wub does not depend on a. Rearranging the sum and factorizes give:_


_wauwav_



_a∈IN_

[X]


(64)

_._ (65)






_dt_


_ℓ[′](yν(xj; w))_ _y_
_−_ _·_ _·_
_j=1_

X


(xj)p1
active pX1∈PI→v _·_


_wf_
_fY∈p1_


_wf_
_fY∈p2_


active p2∈Pu→o


-----

On the other hand


_wubwvb_



_b∈OUT_

 [X]


(66)

_._ (67)






_dt_


_ℓ[′](yν(xj; w))_ _y_
_−_ _·_ _·_
_j=1_

X


(xj)p1
active pX1∈PI→u _·_


_wf_
_fY∈p1_


_wf_
_fY∈p2_


active p2∈Pv→o


Take, for example, an asymmetric case where the in-edges of v has much larger weights than that of
_u while out-edges of v has much smaller weights than that of u, then 65 is much larger than 67 and_
therefore the two expressions are not equal in the general case. A symmetric initialization scheme
that prevents the above asymmetry may prevent this from happens, but this requires additional assumptions and is opened to future work.

**Remark 1. Using the automatic differentiation framework while setting the gradient of ReLU to be**
1 if the preactivation is nonnegative while 0 otherwise, the same derivation of 65 can be achieved.
_Interestingly, if one only has a single example, then the final expression of 65 implies that the matrix_

_dtd_ _Wk[⊤][W][k]_ _has rank at most 2, where Wk is a weight matrix in, for example, ReLU fully-connected_
_neural network. Controlling the eigenvalues under low-rank updates may allow us to bound singular_
_values of the full weight matrices _  _Wk. The resulting bound would not be uniform over time, but_
_improves with training and is thus a different kind of low rank result. This, however, is outside of the_
_scope of this paper._

D PROOF OF THEOREM 2

First we state a helper lemma

**Lemma 10 (Largest singular value of different flattening of the same tensor is close). Let T ∈** R[a,b,c]
_be an order 3 tensor (say a convolutional weight tensor). Let T1, T2 be the standard flattening of_
_this tensor into an element in R[c][×][(][a][×][b][)]_ _and R[(][b][×][c][)][×][a]_ _respectively. Then,_

1

min(a, b) _[∥][T][1][∥]2[2]_ _[≤∥][T][2][∥]2[2][.]_ (68)

_Proof. Invoke Theorem 4.8 of Wang et al. (2017) and using the same notation in the same paper, we_
have for π1 = {{c}, {a, b}} and π2 = {{b, c}, {a}},

dimT (π1, π2)

dim(T ) _∥T1∥2[2]_ _[≤∥][T][2][∥]2[2][.]_ (69)

All that is left is to compute the left hand side in term of a, b, c. By definition, dim(T ) = abc and

dimT (π1, π2) = dimT ({{c}, {a, b}}, {{b, c}, {a}}) (70)

= max _DT ({c}, {b, c}), DT ({c}, {a}_ (71)

_·_ maxh _D _ _T ({a, b}, {b, c}), DT ({a, b}, {a[i]})_ (72)

=h max(  _c, 0)_ _·_ max(a, b) = c max(a, b).[i] (73)


Plug this in equation 69 to get the final result.

**Lemma 11 (Shuffling layer in linear ResNetFree block preserves largest singular value up to multi-**
ple by 8). Recall that for a ResNetFree block r(U, Y, Z) with Y ∈ R[b][×][a], U ∈ R[c][×][b] _and Z ∈_ R[c][×][a],

_Y_
_there are two possible rearrangement of the weights A =_ _U_ _Z[]_ _and B =_ _. We have_
_Z_
 

_∥B∥2[2]_ _[≥]_ 8[1] _[∥][A][∥]2[2]_ _[−]_ _[D][′][ where][ D][′][ ≥]_ [0][ is fixed at inialization.] 


-----

_Proof. Recall that by point three of Theorem 1, we have matrix invariance_

_U_ _[⊤](t)U_ (t) − _Y (t)Y_ _[⊤](t) = U_ _[⊤](0)U_ (0) − _Y (0)Y_ _[⊤](0)._

Note that we can obtain this form since we are only considering linear ResNetFree blocks, so all
neurons are active at all time for all training examples. Thus, we can invoke a FTC to get this form
from the differential from in theorem 1.

By line B.2 in Ji & Telgarsky (2019),

_Y_ 2 2 (74)
_∥_ _∥[2]_ _[≥∥][U]_ _[∥][2]_ _[−]_ _[D,]_

where D = ∥U _[⊤](0)U_ (0) − _Y (0)Y_ _[⊤](0)∥2[2]_ [is fixed at initialization.]

For positive semidefinite matrix X, denote λ(X) to be the function that returns the maximum eigenvalue of X. We have

_∥B∥2[2]_ [= (][λ][(][B][⊤][B][))][2] (75)

= _λ_ _Y_ _[⊤]Y + Z_ _[⊤]Z_ (76)
  [][2]

_≥_ [1]4 _λ_ _Y_ _[⊤]Y_ + λ _Z_ _[⊤]Z_ (77)

    [][2]

_≥_ 4[1] _λ_ _Y_ _[⊤]Y_ + [1]4 _λ_ _Z_ _[⊤]Z_ (78)

  [][2]   [][2]

_≥_ 4[1] _λ_ _UU_ _[⊤][][2]_ + 4[1] _λ_ _ZZ_ _[⊤][][2]_ _−_ _[D]4_ (79)

   

_≥_ [1]8 _λ_ _UU_ _[⊤][]_ + λ _ZZ_ _[⊤][][2]_ _−_ _[D]4_ (80)

  

_≥_ 8[1] _λ_ _UU_ _[⊤]_ + ZZ _[⊤][][2]_ _−_ _[D]4_ (81)

 

= 8[1] _λ_ _AA[⊤][][2]_ _−_ _[D]4 [= 1]8_ _[∥][A][∥]2[2]_ _[−]_ _[D]2_ _[,]_ (82)

 

where 77 is by an application of Weyl’s inequality for Hermitian matrices which states that λ(C +
_D) ≥_ _λ(C) + t where t is is the smallest eigenvalue of D, which is nonnegative since matrices here_
are all positive semidefinite; 79 is a consequence of 74 and 81 is the application of the inequality
_λ(C + D) ≤_ _λ(C) + λ(D) which is another of Weyl’s inequality for Hermitian matrices._

_Proof of Theorem 2. Fix j ∈_ [K + M ], we first invoke Lemma 3 to bound the Frobenius norm of
each of the final K + M + 1 layers (counting the last layer Fin) via the last layer. Let Wj be the
matrix representation of the j-th layer among the last M + 1 layer as described in Theorem 1. Note
that even if a layer has more than one matrix representation in Theorem 1, their Frobenius norm is
still the same because different representation merely re-organize the weights. Thus, we can pick
an arbitrary representation in this step. However, the same is not true for the operator norm and we
have to be a lot more careful in the next step. For each j ∈ [K + M ], we have

_∥Wj(t)∥F[2]_ _[−∥][Fin][(][t][)][∥]F[2]_ [=][ D][0][,] (83)

where D0 = ∥Wj(0)∥F[2] _[−∥][Fin][(0)][∥]F[2]_ [fixed at initialization.]

Now, we bound the difference between the operator norm of Wj(t) and Fin(t) by a telescoping
argument. By Lemma 11, switching from one matrix representation to the other for ResNet incurs
at most a multiplicative factor of 8 and an additive factor cost that depends only on the initialization.
In each adjacent layer, the maximum number of switch between representation is one (so that it fits
the form prescribed in Theorem 1). By matrix invariance between adjacent layers of the K + M
pairs of adjacent layers,
_Wl(t)_ 2 2 (84)
_∥_ _∥[2]_ _[≥]_ _[C][∥][W][l][+1][(][t][)][∥][2]_ _[−]_ _[D][l][,]_


-----

for C = 1/8 if the k + 1 layer is a ResNetFree block (Lemma 11), C = 1/ min(ak+1, bk+1) if the
_k + 1 layer is a convolutional layer, C = max dim W_ [[][M] [+1]] if k = M (Lemma 10) and C = 1
otherwise; for Dl = ∥Wl[⊤]+1[(0)][W][l][+1][(0)][ −] _[W][l][(0)][W][ ⊤]l_ [(0)][∥]2[2][.]

Telescope the sum and subtract from 83 to get the statement of the theorem. When Frobenius norm
diverges, divide both sides by the Frobenius norm to get the ratio statement. Note that the bound
_∥W_ _∥F[2]_ _[/][rank][(][W]_ [)][ ≤∥][W] _[∥]2[2]_ [is trivial since the largest singular value squared is at least the average]
singular value squared.

When the lower bound of 14 is 1, it matches the upper bound and thus the largest singular value
dominates all other singular values. Alignment follows from the proof of Lemma 2.6 second point
in Ji & Telgarsky (2019).

E PROOF OF COROLLARY 1

_Proof. Let L be the number of layers in the network. Under the conditions stated in Corollary 1,_
Lyu & Li (2020) and Ji & Telgarsky (2020) showed that ∥w(t)∥2 diverges. Invoke Lemma 3 for all
_k ∈_ [L − 1] and sum up the results, we have L∥WL(t)∥F[2] [=][ D][′′][ +][ P]j[L]=1 _[∥][W][j][(][t][)][∥]F[2]_ _[∥][W][k][∥]F[2]_ [=]
_D[′′]_ + ∥w(t)∥2[2] [where][ D][′′][ is constant in][ t][. Thus][ ∥][W][j][(][t][)][∥]F[2] [diverges for all][ j][ ∈] [[][L][]][. Since the sum of]
all but the largest singular value is bounded, but the sum of all singular values diverge, we conclude
that the largest singular value eventually dominates the remaining singular values. Together with
convergence in direction for these architecture Ji & Telgarsky (2020), we have each matrix converges
to its rank-1 approximation.

That all the feedforward neural networks with ReLU/leaky ReLU/linear activations can be definable
in the same o-minimal structure that contains the exponential function follows from the work of Ji
& Telgarsky (2020) and that definability is closed under function composition.


-----

