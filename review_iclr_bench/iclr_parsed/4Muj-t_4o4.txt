# LEARNING A SUBSPACE OF POLICIES FOR ONLINE ADAPTATION IN REINFORCEMENT LEARNING

**Jean-Baptiste Gaya[∗]†** **Laure Soulier[†]** **Ludovic Denoyer[∗]‡**

ABSTRACT

Deep Reinforcement Learning (RL) is mainly studied in a setting where the training and the testing environments are similar. But in many practical applications,
these environments may differ. For instance, in control systems, the robot(s) on
which a policy is learned might differ from the robot(s) on which a policy will run.
It can be caused by different internal factors (e.g., calibration issues, system attrition, defective modules) or also by external changes (e.g., weather conditions).
There is a need to develop RL methods that generalize well to variations of the
training conditions. In this article, we consider the simplest yet hard to tackle
generalization setting where the test environment is unknown at train time, forcing
the agent to adapt to the system’s new dynamics. This online adaptation process
can be computationally expensive (e.g., fine-tuning) and cannot rely on meta-RL
techniques since there is just a single train environment. To do so, we propose
an approach where we learn a subspace of policies within the parameter space.
This subspace contains an infinite number of policies that are trained to solve the
training environment while having different parameter values. As a consequence,
two policies in that subspace process information differently and exhibit different
behaviors when facing variations of the train environment. Our experiments[1] carried out over a large variety of benchmarks compare our approach with baselines,
including diversity-based methods. In comparison, our approach is simple to tune,
does not need any extra component (e.g., discriminator) and learns policies able
to gather a high reward on unseen environments.

1 INTRODUCTION

In recent years, Deep Reinforcement Learning (RL) has succeeded at solving complex tasks, from
defeating humans in board games (Silver et al., 2017) to complex control problems (Peng et al.,
2017; Schulman et al., 2017). It relies on different learning algorithms (e.g., A2C - (Mnih et al.,
2016), PPO - (Schulman et al., 2017)). These methods aim at discovering a policy that maximizes
the expected (discounted) cumulative reward received by an agent given a particular environment. If
existing techniques work quite well in the classical setting, considering that the environment at train
time and the environment at test time are similar is unrealistic in many practical applications. As
an example, when learning to drive a car, a student learns to drive using a particular car, and under
specific weather conditions. But at test time, we expect the driver to be able to generalize to any new
car, new roads, and new weather conditions. It is critical to consider the generalization issue where
one of the challenges is to learn a policy that generalizes and adapts itself to unseen environments.

Different techniques have been proposed in the literature (Section 6) to automatically adapt the
learned policy to the test environment. In the very large majority of works, the model has access
to multiple training environments (meta-RL setting). Therefore, the training algorithm can identify
which variations (or invariants) may occur at test time and how to adapt quickly to similar variations.
But this setting may still be unrealistic for concrete applications: for instance, it supposes that the
student will learn to drive on multiple cars before getting their driving license.

_∗Facebook AI Research, correspondence to jbgaya@fb.com_
_†CNRS-ISIR, Sorbonne University, Paris, France_
_‡Now at Ubisoft_
1Code available at [https://github.com/facebookresearch/salina/tree/main/](https://github.com/facebookresearch/salina/tree/main/salina_examples/rl/subspace_of_policies)
[salina_examples/rl/subspace_of_policies](https://github.com/facebookresearch/salina/tree/main/salina_examples/rl/subspace_of_policies)


-----

(a) The figure represents the parameter space. The red
(resp. blue) region is the space of good policies over the
training (resp. testing) environment. A single learned
policy (red point) may be inefficient for the test environment and has to be adapted (e.g., fine-tuning) to become good at test-time (blue point). Instead of learning a single policy, we learn a convex sub-space (the
pentagon) delimited by anchor policies (red stars) that
aims at capturing a large set of good policies. Then the
adaptation is just made by sampling policies in this subspace, keeping the best one (blue star).


(b) Qualitative example of k-shot adaptation on a modified Ant environment (20% of observations masked).
5 policies (i.e 5 values of z) are tested on one episode.
In this case, for z = 0., the Ant is able to adapt to this
new environment. More example of LoP trajectories in
[Figures 5 and 6. See https://sites.google.](https://sites.google.com/view/subspace-of-policies/home)
[com/view/subspace-of-policies/home for](https://sites.google.com/view/subspace-of-policies/home)
videos of the learned behaviors.


Figure 1: (Left) An illustration of the process of learning a subspace of policies. (Right) Comparison
between PPO and our model in a test environment.

In this paper, we address a simpler yet harder to tackle generalization setting in which the learning
algorithm is trained over one single environment and has to perform well on test environments;
preventing us from using meta-RL approaches. A natural way to attack this setting is to start by
learning a single policy using any RL algorithm, and to fine-tune this training policy at test time,
over the test environment (See red/blue points in Figure 1a), but this process may be costly in terms
of environment interactions.

Very recently, the idea of learning a set of diverse yet effective policies (Kumar et al., 2020b; Osa
et al., 2021) has emerged as a way to deal with this adaptation setting. The intuition is that, if instead of learning one single policy, one learns a set of ’diverse’ policies, then there is a chance that
at least one of these policies will perform well over a new dynamics. The adaptation in that case just
consists in selecting the best policy in that set by evaluating each policy over few episodes (K-shot
adaptation). But the way this set of policies is built and the notion of diversity proposed in these
methods have a few drawbacks: these models increase diversity by using an additional intrinsic
reward which encourages the different policies to generate different distributions of states. This objective potentially favors the learning of policies that are sub-optimal at train time. Moreover, these
approaches make use of an additional component in the policy architecture (e.g., a discriminator)
that may be difficult to tune, particularly considering that, at train time, we do not have access to any
test environment and thus cannot rely on validation techniques to tune the extra architecture.

Inspired by recent research on mode connectivity (Benton et al., 2021; Kuditipudi et al., 2019) and
by (Wortsman et al., 2021) which aims to learn a subspace of models in the supervised learning
setting, we propose to learn a subspace of policies in the parameter space as a solution to the online
adaptation in the RL setting (see Figure 1a). Each particular point in this subspace corresponds
to specific parameter values, and thus to a particular policy. This subspace is learned by adapting
a classical RL algorithm (PPO and A2C in our case, see Section 3.3) such that an infinite continuum of policies is learned, each policy having different parameters. The policies thus capture and
process information differently, and react differently to variations of the training environment (see
Figure 1b). We validate our approach (Section 5) over a large set of reinforcement learning environments and compare it with other existing approaches. These experiments show that our method
is competitive, achieves good results and does not require the use of any additional component of
hyper-parameters tuning contrarily to baselines.


-----

2 SETTING
**Reinforcement Learning:** Let us define a state space S and an action space A. In the RL setting,
one has access to a training Markov Decision Process (MDP) denoted M defined by a transition
distribution P (s[′]|s, a) : S × A × S → R+, an initial state distribution P [(][i][)](s) : S → R+ and a
reward function r(s, a) : S × A → R.

A policy is defined as πθ(a|s) : S × A → R+, where θ denotes the parameters of the policy. A
trajectory sampled by a policy πθ given a MDP M is denoted τ ∼ _πθ(M). The objective of an RL_
algorithm is to find a policy that maximizes the expected cumulative (discounted) reward:
_θ[∗]_ = arg max Eτ _πθ(_ )[R(τ )] (1)
_θ_ _∼_ _M_
where R(τ ) is the discounted cumulative reward over trajectory τ .

**Online adaptation:** We consider the setting where the policy trained over M will be used over
another MDP (denoteddynamics and/or initial state distribution and/or reward functionM[¯] ) that shares the same state and action space as[2]. Importantly, M, but with a differentM¯ is unknown at
train time, and cannot be used for model selection, making the tuning of hyper-parameters difficult.

Given a trained model, we consider the K-shot adaptation setting where the test phase is decomposed into two stages: a first phase in which the model adapts itself to the new test environment
over K episodes, and a second phase in which the adapted model is used to collect the reward. We
thus expect the first phase to be as short as possible (few episodes), corresponding to a fast adaptation to the new environment. Let us consider that a model πθ generates a sequence of trajectories
_τ¯1, ¯τ2, ...., ¯τ+∞_ over _M[¯]_, the performance of such a model, is defined as:

_T_

1
_Perf_ (πθ, [¯] _, K) = lim_ _R(¯τK+t)_ (2)
_M_ _T_ _T_
_→∞_ _t=1_

which corresponds to the average performance of the policyX _πθ over_ _M¯_ after K episodes used
for adapting the policy. Note that we are interested in methods that adapt quickly to new a test
environment and we will consider small values of K in our experiments. In the following, for sake
of simplicity, K will refer to the number of policies evaluated during adaptation since each policy
may be evaluated over more than a single episode when facing stochastic environments.

3 LEARNING SUBSPACES OF POLICIES
**Motivation and Idea:** To illustrate our idea, let us consider a toy example where the train environment contains states with correlated and redundant features, in such a way that multiple subsets
of state features can be used to compute good actions to execute. Traditional RL algorithms will
discover one policy πθ[∗] that is optimal w.r.t the environment. This policy will typically use the state
features in a particular way to decide the optimal action at each step. If some features become noisy
(at test time) while, unluckily, πθ∗ particularly relies on these noisy features, the performance of the
policy will drastically drop. Now, let us consider that, instead of learning just one optimal policy, we
also learn a second optimal policy πθ∗[′], but enforcing θ[∗][′] to be different than θ[∗]. This second policy
may tend to make use of various features to compute actions. We thus obtain two policies instead of
one, and we have more chances that at least one of these policies is efficient at test time. Identifying
which of these two policies is the best for the test environment (i.e., adaptation) can simply be done
by evaluating each policy over few episodes, keeping the best one. Our model is built on top of this
intuition, extending this example to an infinite set of policies and to variable environment dynamics.

Inspired by Wortsman et al. (2021) proposing to learn a subspace of models for supervised learning,
we study the approach of learning a subspace of policies in the parameter space, and the use of
such a model for online adaptation in reinforcement learning. Studying the structure of the parameter space has seen a recent surge of interest through the mode connectivity concept (Benton et al.,
2021; Kuditipudi et al., 2019; Wortsman et al., 2021) and obtain good results in generalization, but
it has never been involved in the RL setting. As motivated in the previous paragraph, we expect that,
given a variation of the training environment, having access to a subspace of policies that process information differently instead of a single policy will facilitate the adaptation. As a result, our method
is very simple, does not need any extra hyper-parameter tuning and achieves good performance.

2In the experimental study, one training environment is associated to multiple test environments to analyze
the ability to adapt to different variations.


-----

3.1 SUBSPACES OF POLICIES
Given Θ the space of all possible parameters, a subspace of policies is a subset Θ[¯] _⊂_ Θ that defines
a set of corresponding policies Π =[¯] _πθ_ _θ_ Θ¯ [.]
_{_ _}_ _∈_

Since our objective is to learn such a subspace, we have to rely on a parametric definition of such a
subspace and consider Θ[¯] as a simplex in Θ. Let us define N anchor parameter values _θ[¯]1, ....θ[¯]N_
Θ. We define the Z-space as the set of possible weighted sum of the anchor parameters: Z = ∈
_z = (z1, ...zN_ ) ∈ [0, 1][N] _|_ _zi = 1_ . The subspace we aim to learn is defined by:

_N_



[P] Θ =¯ _zkθ[¯]k,_ _z_ (3)

_{_ _∀_ _∈Z}_

_k=1_

X

In other words, we aim to learn a convex hull of N vertices in Θ. Note that policies in this subspace
can be obtained by sampling z ∼ _p(z) uniformly over Z._

The advantages of this approach are: a) the number of parameters of the model can be controlled
by choosing the number N of anchor parameters, b) since policies are sharing parameters (instead
of learning a set of independent policies), we can expect that the learning will be sample efficient.
Such a subspace is illustrated in Figure 1a through the "pentagon" (i.e., N = 5) in which angles
correspond to the anchor parameters and the surface corresponds to all the policies in the built
subspace.

**K-shot adaptation:** Given a subspace of policies Θ[¯], different methods can be achieved to find the
best policy over the test environment. For instance, it could be done by optimizing the distribution
_p(z) at test time. In this article, we use the same yet effective K-shot adaptation technique than_
Kumar et al. (2020b) and Osa et al. (2021): we sample K episodes using different policies defined
by different values of z that are uniformly spread over Z. In our example, it means that we evaluate
policies uniformly distributed within the pentagon to identify a good test policy (blue star). Note
that, when the environment is deterministic, only one episode per value of z needs to be executed to
find the best policy, which leads to a very fast adaptation.

3.2 LEARNING ALGORITHM
Learning a subspace of policies can be done by considering the RL learning problem as maximizing:

(Θ) =[¯] Eτ _πθ_ [R(τ )]dθ (4)
_L_ Zθ∈Θ[¯] _∼_

Considering that Θ[¯] is a convex hull as defined in Equation 3, and using the uniform distribution p(z)
over Z, the loss function of Equation 4 can be rewritten as:

_N_

(θ[¯]1, ....θ[¯]N ) = Ez _p(z) [Eτ_ _πθ_ [R(τ )]] with θ = _zkθ[¯]k_ (5)
_L_ _∼_ _∼_

_k=1_

X

Maximizing such an objective function over _θ[¯]1, ....θ[¯]N outputs a (uniform) distribution of policies_
trained to maximize the reward, all these policies sharing common parameters.

**Avoiding subspace collapse:** One possible effect when optimizing L(θ[¯]1, ....θ[¯]N ) is to reach a
solution where all θk values are similar. In that case, all the policies would have the same parameters
value, and will thus all achieve the same performance at test-time. Since we want to encourage the
policies to process information differently, and following Wortsman et al. (2021), we encourage the
anchor policies to have different parameters. This is implemented through the use of a regularization
term denoted C(θ[¯]1, ....θ[¯]N ) that measures how much anchor policies are similar in the parameter
space. This auxiliary loss is defined as a pairwise loss between pairs of anchor parameters:


_L(Θ) =[¯]_


(θ[¯]1, ....θ[¯]N ) = Ez _p(z) [Eτ_ _πθ_ [R(τ )]] with θ =
_L_ _∼_ _∼_


_C(θ[¯]1, ....θ[¯]N_ ) =

The final optimization loss is then:


_cosine[2](θi, θj)_ (6)

Xi≠ _j_


(θ[¯]1, ....θ[¯]N ) = Ez _p(z) [Eτ_ _πθ_ [R(τ )]] _β_ _cosine[2]( θ[¯]i,_ _θ[¯]j) with θ =_ _zkθ[¯]k_
_L_ _∼_ _∼_ _−_
Xi≠ _j_ _kX=1_

where β is an hyper-parameter (see Section 5 for a discussion abot the tuning of this term) that
weights the auxiliary term.


-----

**Initialize:** _θ[¯]1,_ _θ[¯]2, ϕ (Critic), n batch size_

**1 for k = 0, 1, 2... do**

**2** Sample z1, ..., zn [0,1]
_∼U_

**43** Sample trajectoriesDefine θzi ← _ziθ[¯]1 + (1τi_ _−1_ [using]zi)θ[¯]2[ {][π][θ]zi

**5** Update _θ[¯]1 and_ _θ[¯]2 to maximize: {_ _}[n]_ _n[1]_ _ni=1[}]_ _P P O (θzi_ ) _β cosine[2][  ]θ[¯]1,_ _θ[¯]2_

**6** Up. ϕ to minimize: _n[1]_ _ni=1_ _LMSEP (ϕ, ziL)_ _−_ 

**7 end** [b]

P

Figure 2: The adaptation of the PPO Algorithm with the LoP model. The different with the standard[b]
PPO algorithm is that: a) trajectories are sampled using multiple policies θzi b) The policy loss is
augmented with the auxiliary loss, and c) The critic takes the values zi as an input.

3.3 LINE OF POLICIES (LOP)
In the case of N = 2, the subspace of policies corresponds to a simple segment in the parameter
space defined by _θ[¯]1 and_ _θ[¯]2 as extremities._ _θ[¯]1 and_ _θ[¯]2 are combined through a scalar value z ∈_ [0; 1]:

_θ = zθ[¯]1 + (1 −_ _z)θ[¯]2_ (7)
Computationally, learning a line of policies[3] is similar to learning a single policy for which the
number of parameters is doubled, making this particular case a good trade-off between expressivity
and training speed. It corresponds to the following objective function:

_L(θ[¯]1,_ _θ[¯]2) = Ez∼U_ [0;1] Eτ _∼πz ¯θ1+(1−z)θ[¯]2_ [[][R][(][τ] [)]] _−_ _cosine[2]( θ[¯]1,_ _θ[¯]2)_ (8)
h i

We provide in Algorithm 7 the adapted version of the clipped PPO algorithm (Schulman et al.,
2017) for learning a subspace of policies. In comparison to the classical approach, the batch of
trajectories is first acquired by multiple policies sampled following p(z) (line 2-3). Then the PPO
objective is optimized taking into account the policies used when sampling trajectories (line 4). At
last, the critic is updated (line 5), taking as an input the z value so that it can make robust estimations
of the expected reward for all the policies in the subspace. Adapting off-policy algorithms would
be similar. Additional details are provided in appendix. Note that, for environments with discrete
actions, we have made the same adaptation based on the A2C algorithm since A2C has less hyperparameters than PPO and is easier to tune, with similar results.

4 EXPERIMENTS

We perform experiments in 6 different environments. Implementations based on the SaLinA (Denoyer et al., 2021) library together with train and test environments will be released upon acceptance.
For each environment, we consider one train environment on which we trained the different methods,
and multiple variations of the training environment for evaluation resulting in 50 test environments
in total. The details of all the environment configurations and detailed performance are given in Appendix B. Note that the complete experiments correspond to hundred of trained policies, and dozens
of thousands of policy evaluations. For simple control environments (i.e., CartPole, Pendulum and
AcroBot), we introduce few variations of the physics constant at test-time, for instance by varying
the mass of the cart, the length of the pole. For complex control environments (i.e., HalfCheetah
and Ant using the BRAX library (Freeman et al., 2021), we both use variations of the physics (e.g.,
gravity), variations of the agent shape (e.g., changing the size of the leg, or of the foot) and sensor
alterations. At last, in MiniGrid and ProcGen we perform experiments where the agent is trained in
one particular levels, but is evaluated in other levels (single levels on MiniGrid, and set of 10 levels
in ProcGen). Note that ProcGen is a pixel-based environment where the architecture of the policy is
much more complex than in control environments. Toy experiments on a simple Maze 2d are given
in Appendix B.8 to show the nature of the policies learned by the different methods.

We compare our approach LoP[4] with different state-of-the-art methods: a) The Single approach is
just a single policy learned on the train environment, and evaluated on the test ones. b) The DI
3Other ways to control the shape of the subspace can be used and we investigate some of them in Section 4
4We consider the LoP-A2C and the LoP-PPO models for environments with respectively discrete and continuous actions. LoP-PPO could be also used in the discrete case but requires more hyper-parameter tuning.


-----

**AYN+R(reward) method is an extension of DIAYN (Eysenbach et al., 2018) where a set of discrete**
policies is learned using a weighted sum between the DIAYN reward and the task reward:
_RDIAY N_ +R(s, a) = r(s, a) + β log p(z _s)_ (9)
_|_
Critically, this model requires to choose a discriminator architecture to compute log p(z|s) and modifies the train reward by defining an intrinsic reward that may drastically change the behavior of the
policies at train time. c) At last, we also compare with the model proposed in (Osa et al., 2021)
denoted Lc (Latent-conditioned) that works only for continuous actions. This model is also based
on a continuous z variable sampled uniformly at train time, but only uses an auxiliary loss without
changing the reward. This auxiliary loss is defined through the joint learning of a density estimation model log P (z|s, a) where back-propagation is made over the action a. As in DIAYN+R, this
model needs to carefully define a good neural network architecture for density estimation. Since
Lc cannot be used with environment that have discrete actions, we have adapted DIAYN+R (called
**DIAYN+R Cont.) using a continuous z variable (instead of a discrete one) and a density estimation**
model log P (z|s) as in Osa et al. (2021). Note that we do not compare to (Kumar et al., 2020a)
for the exact same reason as the one identified in (Osa et al., 2021): SMERL assumes that the reward is known over the complete trajectories which results in unnatural adaptation of on-policy RL
algorithms like PPO. Moreover, preliminary experiments with SMERL does not demonstrate any
advantage against DIAYN+R correctly tuned. We also provide (see Table 4 in Appendix B.1) results
where K independent policies are learned, the best one being selected over each test environment.
This approach obtains lower performance than the proposed baseline and needs K more training
samples making it unrealistic in most of the environments.

As network architectures, we use multi-layer perceptrons (MLP) with ReLU units for both the policy
and the critic (detailed neural network architectures are described in Appendix). For DIAYN+R
log P (z|s, ...) is also modeled by a MLP with a soft-max output. For Lc and DIAYN+R Cont.,
log P (z|s, ...) is modeled by a MLP that computes the mean of a Gaussian distribution with a fixed
variance. For these baselines, z is concatenated with the environment observation as an input for the
policy and the critic models.

To choose the hyper-parameters of the different methods, let us remind that test environments cannot
be used at train time for doing hyper-parameters search and/or model selection which makes this
setting particularly difficult. Therefore, we rely on the following procedure: a grid-search over
hyper-parameters is made, learning a single policy over the train environment. The best value of
the hyper-parameters is then selected as the one that provides the best policy at train time. These
hyper-parameters are then used for all the different baselines. Concerning the β value, for LoP, we
report test results for β = 1.0 while, for Lc and DIAYN+R, we use the best value of β on test
environments. This corresponds to an optimistic evaluation of the baseline performances; aiming at
showing that our method is much more efficient since it does not need such a beta-tuning (β = 1.0
giving good performance in the majority of cases). Said otherwise, we compare our model in the
less favorable case where baselines have been unrealistically tuned.

For the adaptation step, each policy is evaluated over 10 episodes for stochastic environments or 1
single episode for deterministic environments. We repeat this procedure over 10 different training
seeds, and report the reward over the different test environments together with standard deviation.
All detailed results are available in Appendix.

5 ANALYSIS
We report the test performance of the models on different environments in Table 1. In all the environments, the adaptive models perform better than learning a single policy over the train environment
which is not surprising. In most of the cases, LoP is able to achieve a better performance than other
methods. For instance, on HalfCheetah where we evaluate the different methods over 16 variations
of the train environments, LoP achieves an average reward of 10589 while Lc and DIAYN+R obtain
respectively 9547 and 9680 (standard deviations are reported in Appendix B). Some examples of the
discovered that behaviors in Ant and HalfCheetah[5] for the different methods, and for different values
of z are illustrated in Figures 1b, 5 and 6. This outlines that learning models that are optimal on the
train task reward, but with different parameter values, allows us to discover policies react differently
to variations of the training environment. It seems to be a better approach than encouraging policies to have a different behaviors (i.e., generating different state distributions) at train time. Same

[5Videos available at https://sites.google.com/view/subspace-of-policies/home](https://sites.google.com/view/subspace-of-policies/home)


-----

|Nb. Test Env. Type of actions|CartPole Acrobot Pendulum Minigrid 6 6 3 6 Discr. Discr. Discr. Discr.|Brax HalfCheetah Brax Ant 16 15 Cont. Cont.|ProcGen 3 Discr.|toy maze 4 Discr.|
|---|---|---|---|---|
|Single Policy LoP DIAYN+R DIAYN+R L2 Lc|143.4 -99.7 -52.7 0.169 149.9 -93.2 -28.9 0.447 168.1 -97.0 -47.1 0.248 156.1 -93.6 -44.0 0.443 - - - -|7697 3338 10589 4031 9680 3759 - - 9547 4020|11.09 16.38 11.45 - -|-83.2 -33.6 -42.2 - -|


Table 1: Average cumulated reward of the different models over multiple testing environments averaged over 10 training seeds (higher is better). For DIAYN and Lc, we report the results and tested 10
policies for LoP, Lc and DIAYN+R using 10 episodes per policy for stochastic environments, and 1
episode per policy on deterministic ones. Performance is evaluated using the deterministic policy.
Standard deviation is reported for each single test environment in Appendix B.



|K =|5 10 20|Train Perf.|
|---|---|---|
|β = 0.1 β = 1. LoP β = 10|3905 3991 4164 4035 4031 4174 3998 4012 4145|7659 7630 7670|
|β = 0.1 DIAYN β = 1. β = 10|3558 3833 3949 3451 3759 2878 3356 3400 3109|7739 5388 4430|
|β = 0.1 3909 4020 4150 7767 β = 1. 3820 3947 4126 7650 Lc β = 10 3870 3945 4108 7710|||

|Col1|SmallFeet TinyFriction BigGravity|
|---|---|
|LoP K=5 K=10 K=20|8283 10425 10464 8805 10662 10578 8794 10734 10807|
|DIAYN+R K=5 K=10 K=20|7580 9132 8989 7580 9132 8989 8255 10003 9766|
|Lc K=5 K=10 K=20|8186 9521 9360 8186 9661 9488 8107 9661 9506|
|BoP K=5 (N=3) K=10 K=20|6775 7867 7878 6660 7840 8026 6996 7963 8015|
|CoP K=5 (N=3) K=10 K=20|8996 9468 9287 9210 9523 9568 9155 9979 9695|


Figure 3: (left) Performance of the models at train time that shows that for LoP β is not hurting
train performance while it is DIAYN+R. Standard error deviation is reported in Table B.2 for each
environment. We also report the performace at train time that shows that a too high value of β
hurts DIAYN+R performance while is less critical in LoP. (right) Ablation study on the number of
policies K used at test time on 3 HalfCheetah environment variations (see Appendix B.1 for further
details and additional results) together with the performance of the BoP and CoP variants. Standard
deviation is given in appendix, Table 4.

conclusions can be drawn in most of the environments, including MiniGrid where LoP is able to explore large mazes while being trained only on small ones. On the ProcGen environment, where the
observation is an image processed through a complex ConvNet architecture (See Appendix B.7), enforcing functional diversity (DIAYN+R) does not allow to learn good policies while the LoP model
is able to better generalize to unseen levels. Note that the performance at train time is the same for
all the different approaches reported (see the Table 3 for instance) but quickly decreases in DIAYN
for larger values of β while it stays stable for LoP where the best results are obtained for β = 1.0.

Interestingly, in CartPole, DIAYN+R performs quite well. Indeed, when analyzing the learned
policies, it seems to be a specific case where it is possible to obtain optimal policies that are diverse
w.r.t the states they are sampling (by moving the cart more or less on the right/left while maintaining
the pole vertical).

We have also performed experiments where test environments have the same dynamics as the training environment, but with defective sensors (i.e., some features at test time have a null value – see
Appendix B.2 Table 7 on the Ant environment). The fact that LoP behaves also well confirms the
effectiveness of our approach to different types of variations, including noisy features on which
baselines methods were not applied in previous publications.

**Sensitivity to hyper-parameters: One important characteristic of LoP is that it can be used with**
_β = 1.0 and does not need to define any classifier architecture as opposed to DIAYN+R and Lc._
Indeed, as shown in Figure 3 (left), the training performance of DIAYN drastically depends on a
good tuning of β. Lc, which is less sensible, needs to use a correct classifier architecture as in
DIAYN. LoP is simple to tune since the cosine term is usually easy to satisfy and our approach, at
convergence, always reaches a 0 value on this term when β > 0.0. As illustrated in Appendix B.1
and B2, it is also interesting to note than, on the BRAX environments, the number of environment
interactions needed to train LoP is similar than the one needed to train a single policy and LoP comes
with a very small overhead in comparison to classical methods.

**Online adaptation: One interesting property is the number of policies (and thus of episodes) to**
test over a new environment to get a good performance. For LoP and Lc, given a trained model,


-----

Figure 4: (left) Trajectories generated by K = 10 policies (one rows) on an unseen maze (objective
is to go from left to right, see details in Appendix B.8) for DIAYN+R (left column with best β
value) and LoP (right column with β = 1.0). It illustrates the diversity obtained with DIAYN+R
and LoP. (right) Number of times (y-axis) each policy (x-axis) (over K = 5 tested policies) is chosen
over the 16 test environments of the HalfCheetah setting for each of the 10 seeds. Blue is LoP and
orange is DIAYN+R. Different policies are used for different test environments showing the interest
of learning a subspace of policies. Note that in LoP, the anchor policies are rarely chosen. Results
for K = 10 and K = 20 in Appendix (Figure 7).

one can evaluate as many policies (i.e., different values of z) as desired. For DIAYN+R, testing
more policies also means training more policies which is expensive and less flexible. Table 3 (right)
provides the reward of the different methods when testing K policies on different HalfCheetah
settings: as expected, the performance of DIAYN+R tends to decrease when K is large since the
model has difficulties to learn too many diverse policies. For LoP and Lc, spending more episodes
to evaluate more policies naturally leads to a better performance: these two models provide a better
way to deal with the exploration-exploitation trade-off at test time. Again, please consider that Lc
also needs to define an additional neural network architecture to model log P (z|s, a) while LoP does
not, making our approach simpler.

**Beyond a Line of Policies: While LoP is based on the learning of N = 2 anchor parameters, it is**
possible to combine more than two anchor parameters. We study two approaches combining N = 3
anchor parameters (that can be extended to N = 3): a) the first approach is a convex combination of
policies (CoP) where z is sampled following a Dirichlet distribution. (b) The second approach is a
Bézier combination (BoP) as explained in Appendix A.2. The results are presented in Table 3 (right)
over multiple HalfCheetah environments. It can be seen that these two strategies are not so efficient.
LoP is thus a good trade-off between the number of parameters to train and the performance (Note
that BoP and CoP need more samples to converge), at least given the particular neural network
architectures we have used in this paper. We also performed an in-depth analysis of the evolution
of the reward when K is increasing for LoP and CoP in Halfcheetah test environment (Figure 9 in
Annex). While we expected CoP to outperform LoP when K is high, the best reward becomes stable
when K=20 for both methods, and in most test environments, CoP is not able to reach the same best
reward as LoP.

**Analysis of the learned policies:** To better understand the nature of the policies discovered by
the different approaches, we have made a qualitative study in which we analyze i) the robustness of
the methods to corrupted observations, ii) the functional diversity induced by the different models,
and iii) the specificity of the different learned policies to particular test environments. First, LoP is
more robust to input feature corruption (see Table B.2 for the results, and Table 5 for the setting in
Appendix) and we conjecture that it is because the diversity in the parameter space allows this model
to learn policies that does not take into account the same input features equally. We also measure
the functional diversity induced by the different models by training a posteriori a classifier that


-----

aims at recovering which policy (i.e which value of z) has generated particular trajectories (Exact
protocol in Figure 8 in Appendix, with the training curves). On LoP with K = 5, such a classifier
obtains a 82% accuracy at validation time showing that the 5 policies are quite diverse, but less than
the DIAYN+R policies where the classifier reaches a 100% accuracy which is logical knowing the
auxiliary loss introduced by DIAYN which enforces this type of diversity. It is interesting to note
that with the trajectories generated in the test environments with LoP policies, the accuracy of the
classifier is reaching 87 %: when LoP is facing new environments, it tends to generate more diverse
policies. We think that it is due to the fact that, since the policies have different parameter values,
they react differently to states that have not been encountered at train time. At last, Figure 4 (right)
(and Figure 7 in appendix for K=10,20) illustrates which upon K = 5 policies is used for different
test environments. It shows that both LoP and DIAYN+R use different policies over different test
environments, showing that these methods are able to solve new environments by learning various
policies and not a single but robust one. Examples of policies on a simple maze2d are given in
Figure 4 (left) and Appendix which illustrate the diversity of the discovered policies.

6 RELATED WORK
Our contribution shares connections with different families of approaches. First of all, it focuses on
the problem of online adaptation in Reinforcement Learning which has been studied under different
terminologies: Multi-task Reinforcement Learning (Wilson et al., 2007; Teh et al., 2017), Transfer
_Learning (Taylor and Stone, 2009; Lazaric, 2012) and Meta-Reinforcement Learning (Finn et al.,_
2017; Hausman et al., 2018; Humplik et al., 2019). Many different methods have been proposed,
but the best majority considers that the agent is trained over multiple environments such that it can
identify variations (or invariant) at train time. For instance, Duan et al. (2016) assume that the agent
can sample multiple episodes over the same environments and methods like (Kamienny et al., 2020;
Liu et al., 2021) consider that the agent has access to a task identifier at train time.
More recently, diversity-based approaches have been adapted to focus on the setting where only
one training environment is available. They share with our model the idea of learning multiple
policies instead of a single one. For instance, DIAYN (Eysenbach et al., 2018) learns a discrete set
of policies that can be reused and fine-tuned over new environments. It has been adapted to online
adaptation in (Kumar et al., 2020a) where the authors propose to combine the intrinsic diversity
reward together with the training task reward. This trade-off is obtained through a threshold-based
method (instead of a simple weighted sum) with good results. But this method suffers from a major
drawback identified in (Osa et al., 2021): it necessitates to sample complete episodes at each epoch
which is painful and not adapted to all the RL learning algorithms. Osa et al. (2021) also proposed
an alternative based on learning a continuous set of policies instead of a discrete one without using
any intrinsic reward.
The method we propose is highly connected to recent researches on mode connectivity with neural
networks. Mode connectivity is a set of approaches and analyses that focus on the shape of the
parameter space. It has been used as a tool to study generalization in the supervised learning setting
(Garipov et al., 2018), but also as a way to propose new algorithms in different settings (Mirzadeh
et al., 2021). Obviously, the work that is the most connected to our approach is the model proposed
in (Wortsman et al., 2021) that provides a way to learn a subspace of models in the supervised
learning setting. Our contribution adapts this approach to RL for learning policies in a completely
different setting which is online adaptation.
At last, our work is sharing similarities with robust RL which aims at discovering policies robust
to variations of the training environment (Oikarinen et al., 2020; Zhang et al., 2021). The main
difference is that robust RL techniques learn policies efficient ‘in the worst case’ and are not focused
on the online adaptation to test environments (the objective is usually to learn a single policy efficient
on different variations while we are learning multiple policies, just selecting one at test time).

7 CONCLUSION AND PERSPECTIVES
We investigate the idea of learning a subspace of policies in the reinforcement learning setting, and
describe how this approach can be used for online adaptation. While simple, our method allows
to obtain policies that are robust to variations of the training environments. Contrarily to other
techniques, LoP does not need any particular tuning or definition of additional architectures to handle
diversity, which is a critical aspect in the online adaptation setting where hyper-parameters tuning is
impossible or at least very difficult. Future work includes the extension of this family of approaches
in the continual reinforcement learning setting, the deeper understanding of the the built subspace
and the investigation of different auxiliary losses to better control the shape of such a subspace.


-----

8 REPRODUCIBILITY STATEMENT

We have made several efforts to ensure that the results provided in the paper are fully reproducible.
In Appendix, we provide a full list of all hyperparameters and extra information needed to reproduce our experiments. The source code is available on SaLinA repository such that everyone can
reproduce the experiments[6].

REFERENCES

G. W. Benton, W. Maddox, S. Lotfi, and A. G. Wilson. Loss surface simplexes for mode connecting volumes and fast ensembling. In M. Meila and T. Zhang, editors, Proceedings of the 38th
_International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, vol-_
ume 139 of Proceedings of Machine Learning Research, pages 769–779. PMLR, 2021. URL
[http://proceedings.mlr.press/v139/benton21a.html.](http://proceedings.mlr.press/v139/benton21a.html)

G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai
gym, 2016.

M. Chevalier-Boisvert, L. Willems, and S. Pal. Minimalistic gridworld environment for openai gym.
[https://github.com/maximecb/gym-minigrid, 2018.](https://github.com/maximecb/gym-minigrid)

L. Denoyer, A. de la Fuente, S. Duong, J.-B. Gaya, P.-A. Kamienny, and D. H. Thompson. Salina:
Sequential learning of agents. [https://gitHub.com/facebookresearch/salina,](https://gitHub.com/facebookresearch/salina)
2021.

Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. Rl$ˆ2$: Fast reinforcement learning via slow reinforcement learning. _CoRR, abs/1611.02779, 2016._ URL
[http://arxiv.org/abs/1611.02779.](http://arxiv.org/abs/1611.02779)

L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley,
I. Dunning, S. Legg, and K. Kavukcuoglu. Impala: Scalable distributed deep-rl with importance
weighted actor-learner architectures, 2018.

B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills without
[a reward function. CoRR, abs/1802.06070, 2018. URL http://arxiv.org/abs/1802.](http://arxiv.org/abs/1802.06070)
[06070.](http://arxiv.org/abs/1802.06070)

C. Finn, P. Abbeel, and S. Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep
Networks. arXiv e-prints, art. arXiv:1703.03400, Mar 2017.

C. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and O. Bachem. Brax - a differ[entiable physics engine for large scale rigid body simulation, 2021. URL http://github.](http://github.com/google/brax)
[com/google/brax.](http://github.com/google/brax)

T. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. In S. Bengio, H. M. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neu_ral Information Processing Systems 31:_ _Annual Conference on Neural Information Pro-_
_cessing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages_
[8803–8812, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/](https://proceedings.neurips.cc/paper/2018/hash/be3087e74e9100d4bc4c6268cdbe8456-Abstract.html)
[be3087e74e9100d4bc4c6268cdbe8456-Abstract.html.](https://proceedings.neurips.cc/paper/2018/hash/be3087e74e9100d4bc4c6268cdbe8456-Abstract.html)

K. Hausman, J. T. Springenberg, Z. Wang, N. Heess, and M. A. Riedmiller. Learning an embedding
space for transferable robot skills. In ICLR (Poster). OpenReview.net, 2018.

P. Henderson, W.-D. Chang, F. Shkurti, J. Hansen, D. Meger, and G. Dudek. Benchmark environments for multitask learning in continuous domains. ICML Lifelong Learning: A Reinforcement
_Learning Approach Workshop, 2017._

[6https://github.com/facebookresearch/salina/tree/main/salina_examples/](https://github.com/facebookresearch/salina/tree/main/salina_examples/rl/subspace_of_policies)
[rl/subspace_of_policies](https://github.com/facebookresearch/salina/tree/main/salina_examples/rl/subspace_of_policies)


-----

J. Humplik, A. Galashov, L. Hasenclever, P. A. Ortega, Y. Whye Teh, and N. Heess. Meta reinforcement learning as task inference. arXiv e-prints, art. arXiv:1905.06424, May 2019.

P. Kamienny, M. Pirotta, A. Lazaric, T. Lavril, N. Usunier, and L. Denoyer. Learning adaptive
exploration strategies in dynamic environments through informed policy regularization. CoRR,
[abs/2005.02934, 2020. URL https://arxiv.org/abs/2005.02934.](https://arxiv.org/abs/2005.02934)

R. Kuditipudi, X. Wang, H. Lee, Y. Zhang, Z. Li, W. Hu, R. Ge, and S. Arora. Explaining landscape connectivity of low-cost solutions for multilayer nets. In H. M. Wallach,
H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. B. Fox, and R. Garnett, editors, Ad_vances in Neural Information Processing Systems 32: Annual Conference on Neural Informa-_
_tion Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,_
[pages 14574–14583, 2019. URL https://proceedings.neurips.cc/paper/2019/](https://proceedings.neurips.cc/paper/2019/hash/46a4378f835dc8040c8057beb6a2da52-Abstract.html)
[hash/46a4378f835dc8040c8057beb6a2da52-Abstract.html.](https://proceedings.neurips.cc/paper/2019/hash/46a4378f835dc8040c8057beb6a2da52-Abstract.html)

S. Kumar, A. Kumar, S. Levine, and C. Finn. One solution is not all you need: Few-shot extrapolation via structured maxent RL. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems 33: Annual
_Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-_
_[12, 2020, virtual, 2020a. URL https://proceedings.neurips.cc/paper/2020/](https://proceedings.neurips.cc/paper/2020/hash/5d151d1059a6281335a10732fc49620e-Abstract.html)_
[hash/5d151d1059a6281335a10732fc49620e-Abstract.html.](https://proceedings.neurips.cc/paper/2020/hash/5d151d1059a6281335a10732fc49620e-Abstract.html)

S. Kumar, A. Kumar, S. Levine, and C. Finn. One solution is not all you need: Few-shot extrapola[tion via structured maxent RL. CoRR, abs/2010.14484, 2020b. URL https://arxiv.org/](https://arxiv.org/abs/2010.14484)
[abs/2010.14484.](https://arxiv.org/abs/2010.14484)

A. Lazaric. Transfer in reinforcement learning: a framework and a survey. In Reinforcement Learn_ing, pages 143–173. Springer, 2012._

E. Z. Liu, A. Raghunathan, P. Liang, and C. Finn. Decoupling exploration and exploitation for
meta-reinforcement learning without sacrifices. In M. Meila and T. Zhang, editors, Proceedings
_of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual_
_Event, volume 139 of Proceedings of Machine Learning Research, pages 6925–6935. PMLR,_
[2021. URL http://proceedings.mlr.press/v139/liu21s.html.](http://proceedings.mlr.press/v139/liu21s.html)

S. Mirzadeh, M. Farajtabar, D. Görür, R. Pascanu, and H. Ghasemzadeh. Linear mode connectivity
in multitask and continual learning. In 9th International Conference on Learning Representations,
_[ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://](https://openreview.net/forum?id=Fmg_fQYUejf)_
[openreview.net/forum?id=Fmg_fQYUejf.](https://openreview.net/forum?id=Fmg_fQYUejf)

V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Harley, T. P. Lillicrap, D. Silver, and K. Kavukcuoglu.
Asynchronous methods for deep reinforcement learning. In Proceedings of the 33rd Interna_tional Conference on International Conference on Machine Learning - Volume 48, ICML’16,_
page 1928–1937. JMLR.org, 2016.

T. P. Oikarinen, T. Weng, and L. Daniel. Robust deep reinforcement learning through adversarial
[loss. CoRR, abs/2008.01976, 2020. URL https://arxiv.org/abs/2008.01976.](https://arxiv.org/abs/2008.01976)

T. Osa, V. Tangkaratt, and M. Sugiyama. Discovering diverse solutions in deep reinforcement learning, 2021.

C. Packer, K. Gao, J. Kos, P. Krähenbühl, V. Koltun, and D. Song. Assessing generalization in deep
reinforcement learning, 2018.

X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-Real Transfer of Robotic Control
with Dynamics Randomization. arXiv e-prints, art. arXiv:1710.06537, Oct 2017.

J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
[algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347.](http://arxiv.org/abs/1707.06347)

J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control
using generalized advantage estimation, 2018.


-----

D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and shogi by
self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017. URL
[http://arxiv.org/abs/1712.01815.](http://arxiv.org/abs/1712.01815)

M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A survey. J. Mach.
_Learn. Res., 10:1633–1685, Dec. 2009. ISSN 1532-4435._

Y. W. Teh, V. Bapst, W. M. Czarnecki, J. Quan, J. Kirkpatrick, R. Hadsell, N. Heess, and R. Pascanu.
[Distral: Robust multitask reinforcement learning. CoRR, abs/1707.04175, 2017. URL http:](http://arxiv.org/abs/1707.04175)
[//arxiv.org/abs/1707.04175.](http://arxiv.org/abs/1707.04175)

P. N. Ward, A. Smofsky, and A. J. Bose. Improving exploration in soft-actor-critic with normaliz[ing flows policies. CoRR, abs/1906.02771, 2019. URL http://arxiv.org/abs/1906.](http://arxiv.org/abs/1906.02771)
[02771.](http://arxiv.org/abs/1906.02771)

A. Wilson, A. Fern, S. Ray, and P. Tadepalli. Multi-task reinforcement learning: A hierarchical
bayesian approach. In Proceedings of the 24th International Conference on Machine Learning,
ICML ’07, page 1015–1022, New York, NY, USA, 2007. Association for Computing Machin[ery. ISBN 9781595937933. doi: 10.1145/1273496.1273624. URL https://doi.org/10.](https://doi.org/10.1145/1273496.1273624)
[1145/1273496.1273624.](https://doi.org/10.1145/1273496.1273624)

M. Wortsman, M. Horton, C. Guestrin, A. Farhadi, and M. Rastegari. Learning neural network
subspaces. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Con_ference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of_
_[Proceedings of Machine Learning Research, pages 11217–11227. PMLR, 2021. URL http:](http://proceedings.mlr.press/v139/wortsman21a.html)_
[//proceedings.mlr.press/v139/wortsman21a.html.](http://proceedings.mlr.press/v139/wortsman21a.html)

H. Zhang, H. Chen, D. S. Boning, and C. Hsieh. Robust reinforcement learning on state observations
with learned optimal adversary. In 9th International Conference on Learning Representations,
_[ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://](https://openreview.net/forum?id=sCZbhBvqQaU)_
[openreview.net/forum?id=sCZbhBvqQaU.](https://openreview.net/forum?id=sCZbhBvqQaU)


-----

A IMPLEMENTATION DETAILS

In this section, we provide details about the implementations of the baselines and our models. Appendix B provides details and additional results for each environment we used.

A.1 LOP-PPO

We detail the losses used in algorithm 7. First, we recall the clipped-PPO surrogate objective described in Schulman et al. (2017). For a given trajectory τ = {(st, at, rt)}0[T] [collected by the policy]
_πθold_, and denoting ρt (θ) = _[π][θ]π[(]θold[a][t][|][s][t][)]_, the goal is to maximize:


_P P O (θ) := [1]_
_Lb_ _T_


min [ρt (θ) A[π][θold] (st, at), clip (ρt (θ), 1 + ϵ, 1 − _ϵ) A[π][θold]_ (st, at)] (10)
_t=0_

X


Where function A[π][θold] is computed thanks to a value function Vϕ by using Generalized Advantage
Estimation (Schulman et al. (2018)). This function is simultaneously updated by regression on
mean-squared error over the rewards-to-go _Rt. In our case, this function not only takes st as an_
input, but also the value z:

[b]

_T_

2

_MSE (ϕ, z) := [1]_ _Vϕ (st, z)_ _Rt_ (11)
_L_ _T_ _−_ [b]

_t=0_

X  

b

In HalfCheetah and Ant experiments, we sampled actions from a reparametrized Gaussian distribution using a squashing function (Ward et al. (2019)), but we set the standard deviation fixed (so
it is an hyper-parameter encouraging exploration, called action std in Tables 3 and 6): the policy
network only learns the mean of this distribution.

A.2 BOP AND COP

The only change between LoP and these models resides in the way we combine the N anchor
policies. For CoP, it is just the generalization of LoP for N > 2 (see 3.1). BoP, makes use of a
Bezier parametric curve that uses Bernstein polynomials (the anchor parameters being the control
_points). For N = 3, it is defined by:_

Θ =¯ (1 − _z)[2][ ¯]θ1 + 2 (1 −_ _z) z_ _θ[¯]2 + z[2][ ¯]θ3,_ _∀z ∈_ [0, 1] (12)

Concerning the policies z evaluated at test time, BoP uses the same strategy as LoP by testing values
that are uniformly distributed in [0; 1]. For CoP, we opted for sampling K policies using a Dirichlet
distribution over [0, 1][3].

A.3 DIAYN+R AND LC

In order to find the best trade-off between maximizing environment rewards and intrinsic rewards in
DIAYN+R algorithm, we add the hyper-parameter β :

_RDIAY N_ +R(s, a) = r(s, a) + β log p(z _s)_ (13)
_·_ _|_

As an alternative to DIAYN+R Osa et al. (2021) proposes an algorithm where the discriminator
takes not only observations as an input but also the policy output, updating both discriminator qϕ
and policy πθ when back propagating the gradient. In this case, it is not necessary to add an intrinsic
reward. While Osa et al. (2021) illustrate their methods with TD3 and SAC, we adapted it to PPO.
The surrogate loss is given by:

_LLC :=_ _L[b]P P O + β · log qϕ (z | s, πθ (.|s, z))_ (14)


-----

B EXPERIMENTS DETAILS AND ADDITIONAL RESULTS

B.1 HALFCHEETAH

Task originally coming from OpenAI Gym (Brockman et al., 2016). Instead of using MuJoCo
engine, we decided to use Brax (Freeman et al., 2021) as it enables the possibility to acquire episodes
on GPU. We use the vanilla environment for training.The policy and the critic are encoded by two
different multi-layer perceptrons with ReLU activations. The base learning algorithm is PPO.

**Test environments: we operated modifications similar as the ones proposed in (Henderson et al.,**
2017). Morphological variations: we changed the radius and mass of specific body parts (torso, thig,
shin, foot). Variations in physics: we changed the gravity and friction coefficients.
Table 2 precisely indicates the nature of the changes for each environment.

|Env name|Modifications|
|---|---|


|BigFeet BigFriction BigGravity BigShins BigThighs BigTorso SmallFeet SmallFriction SmallGravity SmallShins SmallThighs SmallTorso HugeFriction HugeGravity TinyFriction TinyGravity|Feet mass and radius 1.25 × Friction coefficient 1.25 × Gravity coefficient 1.25 × Shins mass and radius 1.25 × Thighs mass and radius 1.25 × Torso mass and radius 1.25 × Feet mass and radius 0.75 × Friction coefficient 0.75 × Gravity coefficient 0.75 × Shins mass and radius 0.75 × Thighs mass and radius 0.75 × Torso mass and radius 0.75 × Friction coefficient 1.5 × Gravity coefficient 1.5 × Friction coefficient 0.5 × Gravity coefficient 0.5 ×|
|---|---|



Table 2: Modified HalfCheetah environments used for testing. Morphological modifications include
a variation on the mass and the radius of a specific part of the body (torso, thighs, shins, or feet). We
also modified the dynamics (gravity and friction). Environment names are exhaustive: Big refers to
a increase of 25% of radius and mass, Small refers to a decrease of 25%. For example, "BigFoot"
refers to an HalfCheetah agent where feet have been increased in mass and radius by 25%. For
gravity and friction, we also tried an increase/decrease by 50% (respectively tagged "Huge" and
"Tiny").


-----

|Hyper-parameter|Value|
|---|---|
|lr policy: lr critic: n parallel environments: n acquisition steps per epoch: batch size: num minibatches: update epochs: discount factor: clip ration: action std: gae coefficient: reward scaling: gradient clipping: n layers (policy): n neurons per layer (policy): n layers (critic): n neurons per layer (critic):|0.0003 0.0003 2048 20 512 32 8 0.99 0.3 0.5 0.96 1. 10. 4 64 5 256|


LoP, BoP, CoP


|β:|1|
|---|---|


DIAYN

|β: lr discriminator: n layers (discriminator): n neurons per layer (discriminator):|0.1 0.0001 2 64|
|---|---|



Lc

|β: dimensions of z: lr discriminator: n layers (discriminator): n neurons per layer (discriminator):|10 1 0.001 2 64|
|---|---|



Table 3: Hyper-parameters for PPO over HalfCheetah


-----

|Col1|Single Policy (K=1)|Ensemble (5 policies)|LoP|DIAYN + R|Lc|BoP|CoP|
|---|---|---|---|---|---|---|---|


|Col1|(K=1)|(5 policies)|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|K=5 BigFeet BigFriction BigGravity BigShins BigThig BigTorso SmallFeet SmallFriction SmallGravity SmallShin SmallThig SmallTorso HugeFriction HugeGravity TinyFriction TinyGravity|7433 ± 1988 8579 ± 2224 7508 ± 2086 7274 ± 898 7963 ± 1466 7091 ± 2221 5973 ± 2490 8652 ± 1717 9004 ± 1665 7492 ± 2999 8914 ± 1837 8885 ± 1522 6999 ± 3441 6133 ± 2147 7953 ± 1843 7304 ± 3484|8115 ± 344 10757 ± 319 9131 ± 260 7992 ± 294 9331 ± 310 8942 ± 94 10012 ± 1921 7299 ± 464 8004 ± 595 10379 ± 257 11133 ± 384 9669 ± 213 10006 ± 560 10452 ± 371 9444 ± 293 9887 ± 1822|8895 ± 289 11635 ± 355 10464 ± 798 8879 ± 98 10054 ± 724 9834 ± 310 8283 ± 648 11391 ± 348 11840 ± 406 10840 ± 196 11294 ± 46 11433 ± 360 11537 ± 613 8425 ± 554 10425 ± 211 11385 ± 169|8454 ± 655 9962 ± 2113 8989 ± 1723 8099 ± 806 8940 ± 1558 8701 ± 1472 7580 ± 1411 9813 ± 2074 10132 ± 1903 9540 ± 1592 10078 ± 1410 9850 ± 1761 9483 ± 2228 7700 ± 1216 9132 ± 1862 9363 ± 1734|8353 ± 804 10649 ± 2085 9360 ± 1531 8206 ± 770 9360 ± 1669 9198 ± 1342 8186 ± 1512 10181 ± 1617 10434 ± 1951 9837 ± 1343 10603 ± 1160 10010 ± 1856 10305 ± 2104 7621 ± 1137 9521 ± 1462 9620 ± 2041|7993 ± 702 8846 ± 2279 7878 ± 1593 7726 ± 903 8105 ± 1622 7790 ± 1713 6775 ± 2308 8652 ± 2075 9428 ± 2180 8451 ± 1889 9340 ± 1751 9182 ± 1779 8387 ± 2515 6532 ± 1133 7867 ± 2003 9118 ± 2360|8087 ± 496 10596 ± 1738 9287 ± 1200 8226 ± 1144 9525 ± 832 8927 ± 1128 8996 ± 1150 10459 ± 1387 10594 ± 1438 9967 ± 1079 10603 ± 1540 10092 ± 1541 10749 ± 1591 7632 ± 903 9468 ± 1468 9020 ± 2028|
|Average|7697|9410|10413|9114|9465|8254|9514|


|Col1|Single Policy (K=1)|LoP|DIAYN + R|Lc|BoP|CoP|
|---|---|---|---|---|---|---|


|Col1|(K=1)|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|K=10 BigFeet BigFriction BigGravity BigShins BigThig BigTorso SmallFeet SmallFriction SmallGravity SmallShin SmallThig SmallTorso HugeFriction HugeGravity TinyFriction TinyGravity|7433 ± 1988 8579 ± 2224 7508 ± 2086 7274 ± 898 7963 ± 1466 7091 ± 2221 5973 ± 2490 8652 ± 1717 9004 ± 1665 7492 ± 2999 8914 ± 1837 8885 ± 1522 6999 ± 3441 6133 ± 2147 7953 ± 1843 7304 ± 3484|8903 ± 246 11982 ± 330 10578 ± 648 8854 ± 153 10335 ± 1001 10023 ± 427 8805 ± 495 11434 ± 358 11969 ± 341 10764 ± 147 11524 ± 298 11567 ± 381 11659 ± 307 8793 ± 791 10662 ± 385 11578 ± 460|8472 ± 535 10781 ± 1870 9766 ± 1424 8276 ± 607 9644 ± 1323 9295 ± 1106 8255 ± 504 10637 ± 1505 10568 ± 1906 9990 ± 1107 10689 ± 1137 10328 ± 1736 10335 ± 1955 8124 ± 1086 10003 ± 1226 9723 ± 2167|8340 ± 888 10705 ± 2092 9488 ± 1616 8340 ± 986 9427 ± 1532 9271 ± 1317 8186 ± 1458 10204 ± 1663 10444 ± 1984 9933 ± 1315 10632 ± 1170 10273 ± 1917 10379 ± 2302 7811 ± 1115 9661 ± 1497 9650 ± 2228|8147 ± 835 9093 ± 2436 8026 ± 1809 7831 ± 978 8080 ± 1676 7928 ± 1772 6660 ± 1842 8708 ± 2007 9569 ± 2418 8441 ± 1801 9430 ± 1881 9228 ± 1736 8526 ± 2710 6573 ± 1138 7840 ± 2019 9221 ± 2224|8553 ± 1033 10850 ± 1526 9568 ± 803 8753 ± 866 9591 ± 884 9131 ± 928 9210 ± 1063 10459 ± 1387 11334 ± 1429 9898 ± 982 10600 ± 1473 10541 ± 1135 10899 ± 1541 8071 ± 352 9523 ± 1537 5107 ± 6910|
|Average|7697|10589|9680|9547|8331|9506|


|Col1|Single Policy (K=1)|LoP|DIAYN + R|Lc|BoP|CoP|
|---|---|---|---|---|---|---|


**K=20**
BigFeet 7433 ± 1988 **9096 ± 257** 8363 ± 800 8431 ± 803 8405 ± 961 8754 ± 1041
BigFriction 8579 ± 2224 **12001 ± 276** 9655 ± 2304 10779 ± 2158 9108 ± 2437 11124 ± 1397
BigGravity 7508 ± 2086 **10807 ± 592** 8695 ± 1550 9506 ± 1609 8015 ± 1834 9695 ± 879
BigShins 7274 ± 898 **9036 ± 192** 7922 ± 670 8393 ± 959 7994 ± 1046 8796 ± 415
BigThig 7963 ± 1466 **10521 ± 1016** 8639 ± 1315 9537 ± 1633 8159 ± 1602 9652 ± 1012
BigTorso 7091 ± 2221 **10084 ± 449** 8494 ± 1376 9383 ± 1363 7873 ± 1612 9462 ± 299
SmallFeet 5973 ± 2490 8794 ± 614 7540 ± 1155 8107 ± 1506 6996 ± 2483 **9155 ± 497**
SmallFriction 8652 ± 1717 **11429 ± 319** 9603 ± 2022 10271 ± 1680 8844 ± 2162 10498 ± 1331
SmallGravity 9004 ± 1665 **12004 ± 259** 9934 ± 2370 10521 ± 1986 9666 ± 2280 11382 ± 1377
SmallShin 7492 ± 2999 **11041 ± 420** 9135 ± 1715 10057 ± 1281 8709 ± 2127 10025 ± 803
SmallThig 8914 ± 1837 **11571 ± 127** 9970 ± 1847 10682 ± 1092 9563 ± 1820 10921 ± 1018
SmallTorso 8885 ± 1522 **11673 ± 434** 9620 ± 2009 10315 ± 1880 9301 ± 1814 10602 ± 1297
HugeFriction 6999 ± 3441 **11724 ± 685** 9438 ± 2471 10505 ± 2355 8553 ± 2603 11047 ± 1167
HugeGravity 6133 ± 2147 **8930 ± 714** 7498 ± 837 7897 ± 1147 6772 ± 1413 7925 ± 740
TinyFriction 7953 ± 1843 **10734 ± 381** 9001 ± 1908 9661 ± 1519 7963 ± 2234 9979 ± 878
TinyGravity 7304 ± 3484 **11604 ± 388** 9419 ± 2317 9822 ± 2200 9329 ± 2424 10822 ± 1045

**Average** 7697 **10691** 8933 9617 8453 9990

Table 4: Mean and standard deviation of cumulative reward achieved on HalfCheetah test sets per
model (see Table 2 for environment details). Results are averaged over 10 training seeds (i.e., 10
models are trained with the same hyper-parameters and evaluated on the 16 test environments). K is
the number of policies tested at adaptation time, using 1 episode per policy since this environment is
deterministic. Ensembling with K = 5 models takes 5 times more iterations to converge and testing
values of K > 5 is very costly in terms of GPU consumption.


-----

Figure 5: Qualitative example of LoP trajectories on HalfCheetah "BigShins" test environment (5shot setting). The best reward is obtained for z = 0.75.

Figure 6: Extreme case: when torso radius and mass are increased by 50%. Only one policy is able
to adapt without falling down (z = 0.5).


-----

Figure 7: Number of times (y-axis) each policy (x-axis) is chosen by k-shot evaluation over the 16
test environments of the HalfCheetah for each of the 10 seeds (one table per seed). In blue, LoP, in
orange, DIAYN+R. Please note that the 10 same LoP models are used for K=5, K=10, K=20 which
is not the case for DIAYN+R.


-----

Figure 8: We trained small discriminators over a dataset (100,000 environment interactions) of trajectories obtained with the learned policies of LoP and DIAYN+R when K=5. For each environment,
for each seed, we trained a single discriminator and averaged the results. While the discriminators
trained on DIAYN+R reach 100% accuracy rapidly on both train and test environments, they learn
slower for LoP, with a slight advantage for the test environment, validating the fact that the diversity induced by the cosine similarity on the weights is more visible in variations of the environment
rather than the environment on which the model has been trained. We evaluated the discriminator on
a validation dataset (also 100,000 environment interactions) resulting in 100% accuracy for DIAYN
in both train and test environments. For LoP, we obtained 82% accuracy on the training environment,
and 87% on the test environments. The discriminator architecture consists in a neural network of
two hidden layers of size 16, taking the unprocessed states as an input and outputting the predicted
policy used (like in DIAYN).


-----

Figure 9: Evolution of the best reward obtained with respect to K for LoP (N=2) and CoP (N=3) for
each Halfcheetah test environment. We ran the K-shot evaluation for each K from K=1 to K=100
using the method described in Appendix A.2: we simply sample K random coefficients using the
uniform distribution over [0, 1] for LoP and the Dirichlet distribution over [0, 1][3] for CoP. Results
are averaged over 10 run for each K, and over the 10 models we learned for each method.


-----

B.2 ANT

Task originally coming from OpenAI Gym (Brockman et al. (2016)). Instead of using MuJoCo engine, we decided to use Brax (Freeman et al. (2021)) as it enables the possibility to acquire episodes
on GPU. We use the vanilla environment for training. The policy and the critic are encoded by two
different multi-layer perceptrons with ReLU activations. The base learning algorithm is PPO.

**Test environments: As for HalfCheetah, we operated variations in physics (gravity and friction**
coefficients). We also designed environments with a percentage of masked features to simulate
defective sensors (They are sampled randomly and remain the same for each run). Table 5 precisely
indicates the nature of the changes for each environment

|Env name|Modifications|
|---|---|


|BigFriction BigGravity SmallFriction SmallGravity HugeFriction HugeGravity TinyFriction TinyGravity DefectiveSensor 5% DefectiveSensor 10% DefectiveSensor 15% DefectiveSensor 20% DefectiveSensor 25% DefectiveSensor 30% DefectiveSensor 35%|Friction coefficient 1.25 × Gravity coefficient 1.25 × Friction coefficient 0.75 × Gravity coefficient 0.75 × Friction coefficient 1.5 × Gravity coefficient 1.5 × Friction coefficient 0.5 × Gravity coefficient 0.5 × 5% of env obs set to 0 10% of env obs set to 0 15% of env obs set to 0 20% of env obs set to 0 25% of env obs set to 0 30% of env obs set to 0 35% of env obs set to 0|
|---|---|



Table 5: Modified Ant environments used for testing.


-----

|Hyper-parameter|Value|
|---|---|
|lr policy: lr critic: n parallel environments: n acquisition steps per epoch: batch size: num minibatches: update epochs: discount factor: clip ration: action std: gae coefficient: reward scaling: gradient clipping: n layers (policy): n neurons per layer (policy): n layers (critic): n neurons per layer (critic):|0.0003 0.0003 2048 20 1024 16 16 0.99 0.3 0.4 0.96 1. 10. 4 64 5 256|


LoP

|β:|0.1, 1, 10 { }|
|---|---|



DIAYN

Lc

|β: lr discriminator: n layers (discriminator): n neurons per layer (discriminator):|0.1, 1, 10 { } 0.001 2 64|
|---|---|

|β: dimensions of z: lr discriminator: n layers (discriminator): n neurons per layer (discriminator):|0.1, 1, 10 { } 1 0.001 2 64|
|---|---|


Table 6: Hyper-parameters for PPO over Ant


-----

|β =|Single Policy LoP DIAYN + R Lc (K=1) 0.1 1 10 0.1 1 10 0.1 1 10|
|---|---|


|K=5 BigFriction BigGravity SmallFriction SmallGravity HugeFriction HugeGravity TinyFriction TinyGravity DefectiveSensor 5% DefectiveSensor 10% DefectiveSensor 15% DefectiveSensor 20% DefectiveSensor 25% DefectiveSensor 30% DefectiveSensor 35%|7454 ± 166 6905 ± 138 6755 ± 2073 7057 ± 1875 7505 ± 252 380 ± 507 2747 ± 1241 -520 ± 426 4308 ± 59 2770 ± 171 1531 ± 90 1026 ± 77 736 ± 95 472 ± 50 424 ± 48|Col3|7544 ± 140 7470 ± 96 7541 ± 202 7038 ± 211 6937 ± 23 7027 ± 182 7695 ± 156 7652 ± 126 7599 ± 167 7738 ± 134 7639 ± 120 7748 ± 169 7634 ± 152 7616 ± 68 7601 ± 253 1111 ± 538 1407 ± 557 1494 ± 934 3716 ± 996 4584 ± 801 4070 ± 1751 -106 ± 125 -139 ± 274 116 ± 322 5243 ± 322 5630 ± 124 5560 ± 272 3620 ± 238 3660 ± 328 3625 ± 291 2583 ± 247 2738 ± 312 2740 ± 167 1750 ± 249 1965 ± 234 2008 ± 199 1595 ± 317 1757 ± 154 1526 ± 338 695 ± 112 793 ± 166 674 ± 120 606 ± 135 684 ± 182 635 ± 85|7256 ± 1010 6403 ± 726 6267 ± 627 6858 ± 827 6082 ± 583 5865 ± 543 7046 ± 1777 6491 ± 807 6133 ± 706 7533 ± 896 6582 ± 727 6486 ± 769 7220 ± 1331 6387 ± 752 6384 ± 733 846 ± 556 2924 ± 1992 2847 ± 1598 3540 ± 948 2950 ± 1113 2600 ± 572 -479 ± 374 -3 ± 202 224 ± 108 4958 ± 126 4492 ± 211 4360 ± 338 3440 ± 64 3371 ± 57 3223 ± 400 1774 ± 84 1984 ± 383 2055 ± 311 1104 ± 101 1490 ± 387 1450 ± 347 762 ± 34 1107 ± 396 1120 ± 241 577 ± 34 859 ± 290 766 ± 212 449 ± 68 650 ± 245 565 ± 151|Col6|7666 ± 103 7573 ± 154 7538 ± 136 7123 ± 184 7075 ± 168 7051 ± 131 7846 ± 127 7673 ± 183 7734 ± 211 7876 ± 100 7745 ± 105 7772 ± 84 7799 ± 103 7647 ± 128 7668 ± 77 1134 ± 934 915 ± 465 1440 ± 985 3550 ± 843 4140 ± 294 3487 ± 508 -234 ± 790 -401 ± 373 -83 ± 329 5428 ± 424 4988 ± 123 5225 ± 562 3519 ± 76 3406 ± 215 3491 ± 207 2408 ± 340 2226 ± 78 2349 ± 271 1658 ± 245 1546 ± 108 1714 ± 238 1344 ± 262 1271 ± 205 1363 ± 254 673 ± 115 575 ± 49 622 ± 100 618 ± 170 525 ± 92 602 ± 175|
|---|---|---|---|---|---|---|
|Average|3338||3905 4035 3998|3558 3451 3356||3909 3820 3870|
|K=10 BigFriction BigGravity SmallFriction SmallGravity HugeFriction HugeGravity TinyFriction TinyGravity DefectiveSensor 5% DefectiveSensor 10% DefectiveSensor 15% DefectiveSensor 20% DefectiveSensor 25% DefectiveSensor 30% DefectiveSensor 35%|7454 ± 166 7591 ± 134 7487 ± 101 7556 ± 199 7695 ± 113 6771 ± 573 6021 ± 748 7685 ± 103 7580 ± 163 7554 ± 116 6905 ± 138 7107 ± 153 7015 ± 89 7009 ± 166 7250 ± 57 6282 ± 476 5638 ± 600 7137 ± 96 7083 ± 184 7114 ± 98 6755 ± 2073 7681 ± 154 7693 ± 103 7671 ± 193 7823 ± 142 6874 ± 590 5980 ± 716 7864 ± 131 7743 ± 161 7762 ± 196 7057 ± 1875 7788 ± 135 7732 ± 86 7764 ± 185 7886 ± 127 6947 ± 619 6354 ± 818 7896 ± 91 7757 ± 117 7767 ± 75 7505 ± 252 7640 ± 199 7589 ± 72 7613 ± 238 7860 ± 115 6788 ± 630 6160 ± 767 7846 ± 107 7695 ± 115 7668 ± 79 380 ± 507 1329 ± 593 1711 ± 74 1583 ± 442 921 ± 302 4564 ± 2033 3009 ± 747 874 ± 358 1237 ± 662 1156 ± 812 2747 ± 1241 4015 ± 1167 4918 ± 723 4393 ± 1252 4527 ± 560 3001 ± 1261 3676 ± 1382 4545 ± 731 4119 ± 555 3791 ± 476 -520 ± 426 55 ± 184 11 ± 226 154 ± 293 -56 ± 256 6 ± 236 236 ± 336 -149 ± 800 -248 ± 362 227 ± 357 4308 ± 59 5088 ± 224 5323 ± 173 5098 ± 305 5063 ± 101 4498 ± 191 3992 ± 277 5355 ± 262 5016 ± 109 5229 ± 424 2770 ± 171 3974 ± 380 3978 ± 117 3934 ± 297 3486 ± 161 3413 ± 213 3061 ± 184 3960 ± 106 3711 ± 194 3779 ± 196 1531 ± 90 2504 ± 283 2423 ± 154 2496 ± 92 1851 ± 142 2365 ± 426 2226 ± 216 2309 ± 137 2374 ± 120 2352 ± 127 1026 ± 77 1795 ± 174 1629 ± 111 1774 ± 147 1205 ± 63 1777 ± 466 1737 ± 203 1834 ± 326 1987 ± 127 1787 ± 180 736 ± 95 1486 ± 156 1351 ± 220 1372 ± 85 851 ± 36 1422 ± 411 1380 ± 218 1342 ± 121 1368 ± 95 1390 ± 93 472 ± 50 1103 ± 87 968 ± 93 1133 ± 158 635 ± 99 967 ± 297 866 ± 157 1069 ± 144 925 ± 109 962 ± 149 424 ± 48 714 ± 149 640 ± 86 634 ± 75 441 ± 49 714 ± 223 662 ± 109 583 ± 95 609 ± 122 630 ± 107||||||
|Average|3338|3991 4031 4012||3833 3759 3400||4020 3947 3945|
|K=20 BigFriction BigGravity SmallFriction SmallGravity HugeFriction HugeGravity TinyFriction TinyGravity DefectiveSensor 5% DefectiveSensor 10% DefectiveSensor 15% DefectiveSensor 20% DefectiveSensor 25% DefectiveSensor 30% DefectiveSensor 35%|7454 ± 166 7592 ± 147 7510 ± 89 7569 ± 172 7646 ± 118 4328 ± 3086 5301 ± 429 7692 ± 94 7589 ± 129 7573 ± 122 6905 ± 138 7121 ± 162 7028 ± 50 7090 ± 171 7196 ± 139 4172 ± 2949 4867 ± 370 7162 ± 86 7135 ± 147 7132 ± 121 6755 ± 2073 7767 ± 111 7712 ± 98 7703 ± 228 7833 ± 141 4360 ± 3118 5021 ± 322 7862 ± 131 7726 ± 144 7775 ± 170 7057 ± 1875 7807 ± 123 7698 ± 86 7777 ± 203 7863 ± 109 4512 ± 3258 5343 ± 404 7906 ± 78 7774 ± 113 7806 ± 95 7505 ± 252 7674 ± 171 7648 ± 97 7699 ± 198 7747 ± 104 4331 ± 3089 5061 ± 334 7851 ± 119 7714 ± 115 7737 ± 81 380 ± 507 1655 ± 712 2111 ± 1405 1587 ± 343 2005 ± 743 3596 ± 2437 4231 ± 121 1357 ± 197 2122 ± 1006 1514 ± 845 2747 ± 1241 4437 ± 827 5147 ± 595 4625 ± 1145 4393 ± 469 2203 ± 1432 3117 ± 816 4707 ± 490 3979 ± 656 3917 ± 529 -520 ± 426 259 ± 324 188 ± 241 289 ± 316 357 ± 525 291 ± 620 348 ± 152 -121 ± 812 -110 ± 257 177 ± 370 4308 ± 59 5816 ± 313 5996 ± 153 6002 ± 216 5185 ± 121 4570 ± 175 3647 ± 174 6023 ± 257 5780 ± 241 5884 ± 177 2770 ± 171 3979 ± 248 3850 ± 199 4083 ± 313 3500 ± 208 3475 ± 242 2827 ± 261 4173 ± 325 4023 ± 176 4038 ± 391 1531 ± 90 2789 ± 368 2526 ± 132 2790 ± 367 2047 ± 117 2495 ± 52 2125 ± 218 2596 ± 306 3011 ± 202 2810 ± 403 1026 ± 77 1890 ± 107 1794 ± 88 1750 ± 278 1333 ± 86 1875 ± 26 1714 ± 189 1746 ± 123 1789 ± 151 1901 ± 185 736 ± 95 1582 ± 104 1422 ± 187 1416 ± 150 992 ± 51 1351 ± 16 1320 ± 217 1480 ± 128 1286 ± 166 1450 ± 195 472 ± 50 1102 ± 133 1069 ± 128 968 ± 62 657 ± 75 931 ± 136 994 ± 70 909 ± 178 1035 ± 99 992 ± 51 424 ± 48 996 ± 103 908 ± 104 833 ± 77 485 ± 67 678 ± 15 719 ± 59 791 ± 100 927 ± 92 907 ± 113||||||
|Average|3338|4164 4174 4145||3949 2878 3109|4150 4126 4108||


Table 7: Mean and standard deviation of cumulative reward achieved on Ant test sets per model. Results are averaged over 10 training seeds (i.e., 10 models are trained with the same hyper-parameters
and evaluated on the 12 test sets). K is the number of policies tested at adaptation time, using 1
episode per policy since this environment is deterministic. For this environment, we split the results
per β value as it has been used for beta ablation study (see Figure 3)


-----

Figure 10: Evolution of the cumulative reward during training on the generic Ant environment for
LoP, DIAYN+R and Lc for different values of beta. On can see that DIAYN+R struggles to perform
well on the train set for β = 1 and β = 10. Results are averaged over 10 seeds.


-----

B.3 CARTPOLE

We use the openAI gym implementation of CartPole as a training environment. The 6 test environments are provided by Packer et al. (2018) where three different factors may vary: the mass of the
cart, the length of the pole and the force applied to the cart. The length of each episode is 200. The
policy and the critic are encoded by two different multi-layer perceptrons with ReLU activations.
The base learning algorithm is A2C.

|Environment|Characteristics|
|---|---|
|(Train) CartPole|mass = 0.1, length = 0.5, force = 10.0|
|HeavyPole CartPole LightPole CartPole LongPole CartPole ShortPole CartPole StrongPush CartPole WeakPush CartPole|mass = 1.0 mass = 0.001 length = 1.0 length = 0.05 force = 20.0 force = 1.0|



Table 8: CartPole train and test environments

|Hyper-parameter|Value|
|---|---|
|learning rate: n acquisition steps per epoch: n parallel environments: critic coefficient: entropy coefficient: discount factor: gae coefficient: gradient clipping: n neurons per layer: n layers:|0.001 8 32 1.0 0.001 0.99 1.0 2.0 8 2|



LoP

|β:|1.0|
|---|---|


DIAYN

|β: n neurons per layer discriminator: n layers discriminator: learning rate discriminator:|1.0 8 2 0.001|
|---|---|



Table 9: Hyper-parameters for A2C over CartPole

|Col1|Single|LoP|DIAYN+R|DIAYN+R L 2|
|---|---|---|---|---|
|HeavyPole CartPole LightPole CartPole LongPole CartPole ShortPole CartPole StrongPush CartPole WeakPush CartPole|200.0 ± 0.0 200.0 ± 0.0 54.4 ± 81.6 67.0 ± 33.1 200.0 ± 0.0 138.9 ± 43.8|200.0 ± 0.0 200.0 ± 0.0 56.1 ± 72.2 78.9 ± 25.3 200.0 ± 0.0 164.4 ± 18.3|200.0 ± 0.0 200.0 ± 0.0 163.3 ± 73.3 50.7 ± 18.1 200.0 ± 0.0 194.3 ± 10.0|200.0 ± 0.0 200.0 ± 0.0 123.8 ± 86.2 64.8 ± 31.2 199.9 ± 0.2 148.1 ± 64.8|
|Average|143.4|149.9|168.1|156.1|



Table 10: Results over CartPole, using 10 policies, and 10 episodes per policy at adaptation time.


-----

B.4 ACROBOT

We use the openAI gym implementation of Acrobot as a training environment. The 4 test environments are provided by Packer et al. (2018) where two different factor may vary: the intertia factor
and the lengh of the system. We have used A2C as a learning algorithm.

|Environment|Characteristics|
|---|---|
|(Train) Acrobot|mass = 0.1, length = 1.0, inertia = 1.0|
|Heavy Acrobot HighInertia Acrobot Light Acrobot Long Acrobot LowInertia Acrobot Short Acrobot|mass = 1.5 inertia = 1.5 mass = 0.5 length = 1.5 inertia = 0.5 length = 0.5|



Table 11: Acrobot train and test environments

|Hyper-parameter|Value|
|---|---|
|learning rate: n acquisition steps per epoch: n parallel environments: critic coefficient: entropy coefficient: discount factor: gae coefficient: gradient clipping: n neurons per layer: n layers:|0.001 8 32 1.0 0.001 0.99 0.7 2.0 16 2|



LoP

|β:|1.0|
|---|---|



DIAYN

|β: n neurons per layer discriminator: n layers discriminator: learning rate discriminator:|1.0 16 2 0.001|
|---|---|



Table 12: Hyper-parameters for A2C over Acrobot

|Col1|Single|LoP|DIAYN+R|DIAYN+R L 2|
|---|---|---|---|---|
|Heavy Acrobot HighInertia Acrobot Light Acrobot Long Acrobot LowInertia Acrobot Short Acrobot|-108.4 ± 3.2 -108.7 ± 5.6 -120.7 ± 71.3 -124.3 ± 2.7 -71.3 ± 2.3 -65.1 ± 2.8|-105.1 ± 1.0 -99.8 ± 2.8 -107.2 ± 58.8 -115.8 ± 9.6 -70.7 ± 0.7 -60.7 ± 0.6|-108.0 ± 1.8 -106.0 ± 2.7 -115.2 ± 33.1 -117.3 ± 4.1 -71.2 ± 0.7 -64.2 ± 2.6|-108.2 ± 4.4 -106.8 ± 8.9 -93.1 ± 37.3 -117.5 ± 5.3 -71.3 ± 1.9 -64.7 ± 5.6|
|Average|-99.7|-93.2|-97.0|-93.6|



Table 13: Results over Acrobot, using 10 policies, and 10 episodes per policy at adaptation time.


-----

B.5 PENDULUM

We use the openAI gym implementation of Pendulum as a training environment. The 3 test environments are provided by Packer et al. (2018) where two different factor may vary: the mass and the
length of the pendulum. We have considered 5 discrete actions between −1 and +1. We have used
A2C as a learning algorithm.

|Environment|Characteristics|
|---|---|
|(Train) Pendulum|mass = 1.0, length = 1.0|
|Light Pendulum Long Pendulum Short Pendulum|mass = 0.5 length = 1.5 length = 0.5|



Table 14: Pendulum train and test environments

|Hyper-parameter|Value|
|---|---|
|learning rate: n acquisition steps per epoch: n parallel environments: critic coefficient: entropy coefficient: discount factor: gae coefficient: gradient clipping: n neurons per layer: n layers:|0.001 8 32 1.0 0.001 0.99 0.7 2.0 16 2|



LoP

|β:|1.0|
|---|---|



DIAYN

|β: n neurons per layer discriminator: n layers discriminator: learning rate discriminator:|1.0 16 2 0.001|
|---|---|



Table 15: Hyper-parameters for A2C over Pendulum

|Col1|Single|LoP|DIAYN+R|DIAYN+R L 2|
|---|---|---|---|---|
|Light Pendulum Long Pendulum Short Pendulum|-36.5 ± 58.5 -82.1 ± 20.4 -39.6 ± 66.9|-11.4 ± 2.7 -64.5 ± 13.0 -10.7 ± 2.1|-39.3 ± 10.9 -70.6 ± 15.2 -31.3 ± 11.7|-32.1 ± 15.4 -71.9 ± 17.3 -28.2 ± 13.3|
|Average|-52.7|-28.9|-47.1|-44.0|



Table 16: Results over Pendulum, using 10 policies, and 10 episodes per policy at adaptation time.


-----

B.6 MINIGRID

We have use Gym Minigrid to perform experiments on mazes Chevalier-Boisvert et al. (2018). We
have used the MultiRoom-N2-S4 for training considering one single maze. At test time, we have
tested on three different MultiRoom-N2-S4 environments composed of two rooms, but also on three
MultiRoom-N4-S5 composed of four rooms. This allows us to evaluate the generalization power of
the different methods to larger mazes. We have used A2C as a learning algorithm.

|Hyper-parameter|Value|
|---|---|
|learning rate: n acquisition steps per epoch: n parallel environments: critic coefficient: entropy coefficient: discount factor: gae coefficient: gradient clipping: n neurons per layer: n layers:|0.001 8 32 1.0 0.001 0.99 0.7 2.0 16 2|



LoP

|β:|1.0|
|---|---|


DIAYN

|β: n neurons per layer discriminator: n layers discriminator: learning rate discriminator:|1.0 16 2 0.001|
|---|---|



Table 17: Hyper-parameters for A2C over Minigrid

|Col1|Single|LoP|DIAYN+R|DIAYN+R L 2|
|---|---|---|---|---|
|Two Rooms Maze 1 Two Rooms Maze 2 Two Rooms Maze 3 Four Rooms Maze 1 Four Rooms Maze 2 Four Rooms Maze 3|0.387 ± 0.447 0.433 ± 0.499 0.194 ± 0.387 0.0 ± 0.0 0.0 ± 0.0 0.0 ± 0.0|0.619 ± 0.309 0.865 ± 0.0 0.617 ± 0.309 0.294 ± 0.36 0.004 ± 0.007 0.281 ± 0.345|0.348 ± 0.348 0.627 ± 0.363 0.348 ± 0.353 0.0 ± 0.0 0.0 ± 0.0 0.166 ± 0.287|0.656 ± 0.328 0.692 ± 0.346 0.656 ± 0.328 0.24 ± 0.359 0.137 ± 0.274 0.28 ± 0.345|
|Average|0.169|0.447|0.248|0.443|



Table 18: Results over Minigrid, using 10 policies, and 1 episode per policy at adaptation time.


-----

B.7 PROCGEN (FRUITBOT)

We performed experiments on pixel-based environment with ProcGen. We used the FruitBot game
for training considering 10 levels sampled uniformly at each episode. At test time, we selected 3
different environments, each of them composed of 10 uniformly sampled levels, sampled uniformly,
and that were not seen at train time. We used the CNN architecture described in the ProcGen paper
(IMPALA architecture (Espeholt et al. (2018))). For LoP, the two first blocks are fixed and only
the parameters of the last block depend on the value of z. For DIAYN, the z value is provided as a
one-hot vector stacked to the observation. As for Brax environment, we used PPO algorithm.

LoP

|β:|0.01, 0.1, 1.0|
|---|---|


**Hyper-parameter** **Value**

learning rate: 5e − 4
n acquisition steps per rollout: 128
n batches epoch: 8
n epochs per rollout: 5
n parallel environments: 64
critic coefficient: 1.0
entropy coefficient: 0.01
discount factor: 0.99
gae coefficient: 0.0
gradient clipping: 20.0


DIAYN+R

|β:|0.001, 0.01, 0.1, 1.0|
|---|---|



Table 19: Hyper-parameters for PPO over ProcGen

|Col1|Single|LoP|DIAYN+R|
|---|---|---|---|
|Levels 100 to 110 Levels 200 to 210 Levels 300 to 310|11.9 ± 2.6 7.1 ± 1.7 14.3 ± 5.1|20.1 ± 4.4 10.3 ± 0.2 18.7 ± 2.7|12.1 ± 5.2 5.1 ± 1.7 17.1 ± 4.6|



Table 20: Results over Procgen, using K=10 at adaptation time, (averaged over 16 episodes per
shot), averaged over 5 runs.


-----

B.8 MAZE 2D WITH WALLS

To visualize the policies learned by the different methods, we just implemented a simple discrete
maze (4 actions = up,down, left, right) where the objective is to go from the top-middle tile to the
bottom-middle tile by moving through a corridor (size is 21 × 11). Reward is -1 at each step until
goal is reached and the maximum number of steps is −100. The optimal policy in the training
environment achieves −16. At test time, we generate walls in the corridor such that the agent has to
avoid these walls to reach the goal. The observation space is a 5×5 square around the agent. Policies
are learned by using PPO. Illustrations of the trajectories over the train and the 4 test environments
are illustrated in Figures 11,12,13,14

LoP

|β:|0.01, 0.1, 1.0|
|---|---|


**Hyper-parameter** **Value**

learning rate: 0.001
n acquisition steps per rollout: 16
n batches epoch: 4
n epochs per rollout: 3
n parallel environments: 32
critic coefficient: 1.0
entropy coefficient: 0.01
discount factor: 0.99
gae coefficient: 0.0
gradient clipping: 20.0


DIAYN+R

|β:|0.01, 0.1, 1.0|
|---|---|


Table 21: Hyper-parameters for PPO used for Maze 2d

|β =|Single Policy|LoP DIAYN + R 0.01 0.1 1. 0.01 0.1 1.|
|---|---|---|


|Test Env #1 Test Env #2 Test Env #3 Test Env #4 Test Env #4|-16.0 ± 0.0 -100 ± 0.0 -100 ± 0.0 -100 ± 0.0 -100 ± 0.0|-17.6 ± 2.2 -17.2 ± 1.1 -17.6 ± 2.6 -23.6 ± 3.6 -39.4 ± 25.8 -30.2 ± 11.6 -37.8 ± 18.9 -41.6 ± 10.6 -43.6 ± 11.5 -42.2 ± 11.8 -60.8 ± 27.9 -48 ± 22.9 -28.4 ± 10.7 -44.4 ± 30.2 -28.8 ± 8.9|-16 ± 0 -16 ± 0 -16 ± 0 -56.2 ± 40.2 -42.6 ± 33 -44.8 ± 31.5 -54.2 ± 27.1 -55.8 ± 31.4 -60.4 ± 27 -54.8 ± 28.9 -53.4 ± 26.5 -72.8 ± 33.8 -37.4 ± 35.4 -43.2 ± 34.7 -50.4 ± 38.8|
|---|---|---|---|
|Average|-83.2|-29.9 -40.68 -33.64|-43.72 -42.2 -48.88|



Table 22: Results over Maze2d, using K=10, averaged over 5 runs.

Figure 11: Trajectories learned by a Single Policy. First column is the training environment. Other
columns are test environments. The lighter red the tiles are, the longer the agent stays on a particular
location.


-----

Figure 12: Trajectories learned by LoP (β = 1.0). Rows are environments, columns are the K = 10
policies test during online adaptation. For each test environment, at least one policy is able to reach
the goal


-----

Figure 13: Trajectories learned by DIAYN+R (β = 0.01). Rows are environments, columns are the
_K = 10 policies test during online adaptation. For many test environment, at least one policy is able_
to reach the goal


-----

Figure 14: Trajectories learned by DIAYN+R (β = 1). Rows are environments, columns are the
_K = 10 policies test during online adaptation. With a too high value of β, the training policy is_
suboptimal, and does not achieve good performance at train and test times.


-----

