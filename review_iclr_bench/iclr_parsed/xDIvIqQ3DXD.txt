# On the approximation properties of recur- rent encoder-decoder architectures


**Zhong Li[∗]**
School of Mathematical Sciences
Peking University
```
li zhong@pku.edu.cn

```
**Qianxiao Li[†]**
Department of Mathematics
National University of Singapore
```
qianxiao@nus.edu.sg

```

**Haotian Jiang[∗]**
Department of Mathematics
National University of Singapore
```
e0012663@u.nus.edu

```

Abstract

Encoder-decoder architectures have recently gained popularity in sequence
to sequence modelling, featuring in state-of-the-art models such as transformers. However, a mathematical understanding of their working principles still remains limited. In this paper, we study the approximation properties of recurrent encoder-decoder architectures. Prior work established
theoretical results for RNNs in the linear setting, where approximation
capabilities can be related to smoothness and memory of target temporal
relationships. Here, we uncover that the encoder and decoder together form
a particular “temporal product structure” which determines the approximation efficiency. Moreover, the encoder-decoder architecture generalises
RNNs with the capability to learn time-inhomogeneous relationships. Our
results provide the theoretical understanding of approximation properties
of the recurrent encoder-decoder architecture, which precisely characterises,
in the considered setting, the types of temporal relationships that can be
efficiently learned.

1 Introduction

Encoder-decoder is an increasingly popular architecture for sequence to sequence modelling
problems (Sutskever et al., 2014; Chiu et al., 2018; Venugopalan et al., 2015). The core
of this architecture is to first encode the input sequence into a vector using the encoder
and then map the vector into the output sequence through the decoder. In particular,
such architecture forms the main component in the transformer network (Vaswani et al.,
2017), which has become a powerful method for modelling sequence to sequence relationships
(Parmar et al., 2018; Beltagy et al., 2020; Li et al., 2019).

The encoder-decoder family of structures differ significantly from direct application of recurrent neural networks (RNNs, Elman (1990)) and its generalisations (Hochreiter & Schmidhuber, 1997; Cho et al., 2014b) for processing sequences. However, both architectures can
be considered as modelling mappings between sequences, albeit with different underlying
structures. Hence, a natural but unresolved question is: how are these approaches fundamentally different? Answering this question is not only of theoretical importance but also
of practical interest. Currently, architectural selection for different time series modelling
tasks is predominantly empirical. Thus, it is desirable to develop a concrete mathematical framework to understand the key differences between separate architectures in order to
guide practitioners in a principled way.

_∗Equal contribution_
_†Corresponding author_


-----

In this paper, we investigate the approximation properties of encoder-decoder architectures.
Approximation is one of the most basic and important problems for supervised learning. It
considers to what extent the model can fit a target. In particular, we prove a general approximation result in the linear setting, which characterises the types of temporal input-output
relationships that can be efficiently approximated by encoder-decoder architectures. These
results reveal that such architectures essentially generalise RNNs by lifting the requirement
of time-homogeneity (see Remark 3.2) in the target relationships. Hence, it can be used
to tackle a broader class of sequence to sequence problems. Furthermore, of particular interest is the identification of a “temporal product structure” — a precise property of the
target temporal relationship that highlights another intrinsic difference between recurrent
encoder-decoders and RNNs.

Our main contributions can be summarised as follows.

1. We prove a universal approximation result for recurrent encoder-decoder architectures in the linear setting, including the approximation rates.

2. We show that in the considered setting, the recurrent encoder-decoder generalises
the RNNs and can approximate time-inhomogeneous relationships, which further
adapt to additional temporal product structures in the target relationship. This
answers precisely how encoder-decoders are different from RNNs, at least in the
considered setting.

**Organisation.** In Section 2, we review the related work on encoder-decoder architectures
and general approximation theories of sequence modelling. The approximation problem is
formulated in Section 3. Our main results, their consequences and numerical illustrations
are presented in Section 4. All the proofs and numerical details are included in appendices.

**Notations.** For consistency, we adhere to the following notations. Boldfaced letters are
reserved for sequences or paths, which can be understood as functions of time. Lower case
letters can mean vectors or scalars. Matrices are denoted by capital letters. For α ∈ N, C _[α]_
denotes the space of functions with continuous derivatives up to order-α.

2 Related work

We first review some previous works on sequence to sequence modelling. The encoderdecoder architecture first appeared in Kalchbrenner & Blunsom (2013), where they map the
input sequence into a vector using convolutional neural networks (CNNs), and then using
a recurrent structure to map the vector to the output sequence. With the flexibility of
manipulating the underlying structure of encoder and decoder, numerous models based on
this architecture have come out thereafter. For instance, Cho et al. (2014b) used gated RNNs
as both the encoder and decoder, while in the later work (Cho et al., 2014a), they proposed
a CNN-based decoder. In Sutskever et al. (2014), they proposed a deep LSTM for both
the encoder and decoder. Bahdanau et al. (2015) first introduced the attention mechanism,
which was further developed in the well-known transformer networks (Vaswani et al., 2017).
However, most of the research on encoder-decoder architectures focused on applications. A
theoretical understanding is helpful for its further improvement and development.

From the theoretical point of view, Ye & Sung (2019) studied several theoretical properties of
CNN encoder-decoders, including expressiveness, generalisation capability and optimisation
landscape. Of particular relevance to the current work is expressiveness, which considers
the relationships that can be generated from the architecture. However, this is not approximation. Yun et al. (2020) proved the universal approximation property of transformers for
certain classes of functions, for example, permutation equivariant functions, but they did not
consider the actual dynamical properties of target relationships that affect approximation.
Dynamical proprieties such as memory, smoothness and low rank structures are essential,
because they can precisely characterise different temporal relationships and affect the approximation capabilities of models. Assuming the target generated from a hidden dynamical
system is one approach, which is widely applied (Maass et al., 2007; Sch¨afer & Zimmermann,
2007; Doya, 1993; Funahashi & Nakamura, 1993). In contrast, a functional-based approach is


-----

recently introduced, where the target temporal relationships are generated from functionals
satisfying specific properties such as linearity, continuity, regularity and time-homogeneity
(Li et al., 2021). In Li et al. (2021), the approximation properties of linear RNN models
are studied, and the results therein show that the approximation efficiency is related to the
memory structure. In Jiang et al. (2021), similar formulations are applied to investigate
convolutional architectures, where the results suggest that targets with certain spectrum
regularity can be well approximated by dilated CNNs. Under this framework, the target
temporal relationship that can be efficiently approximated is characterised by properties
such as memory, smoothness and sparsity. This enables us to make precise mathematical comparisons between different architectures. Our results in this work reveal that the
encoder-decoders have a special temporal product structure which is intrinsically different
from other sequence modelling architectures.

3 Problem formulation

In this section, we precisely define the input space, output space, concept space and hypothesis space, respectively.

**Functional formulation of temporal modelling.** First, we define the input and output
space precisely. A temporal sequence can be viewed as a function of time t. The input space
is defined by X = C0((−∞, 0], R[d]). This is the space of continuous functions from (−∞, 0]
to R[d] vanishing at infinity, where d N+ is the dimension. Denote the element in by
_∈_ _X_
**_xtake the outputs space as = {xt ∈_** R[d] : t ≤ 0}, we equip Y = C Xb([0 with the supremum norm, ∞), R), the space of bounded continuous functions ∥x∥X := supt≤0 ∥xt∥∞. We
from [0, ∞) to R. We consider real-valued outputs, since each dimension can be handled
individually for vector-valued outputs.

The mapping between input and output sequences can be formulated as a sequence of
_functionals, i.e. yt = Ht(x), t_ 0. The output yt at the time step t depends on the input
_≥_
sequence x. The ground truth relation between inputs and outputs is formulated by the
sequence of functionals H := _Ht : t_ 0 .
_{_ _≥_ _}_

We provide an example to illustrate the above formulation. Given an input x, the output
**_y is a smoothed version of x, resulting from convolving x with the Gaussian kernel g(s) =_**
_√12π_ [exp(][−] _[s]2[2]_ [). This relation can be formulated as][ y][t][ =][ H][t][(][x][) =] _∞0_ _g(t + s)x−sds._

R

**The RNN encoder-decoder model.** For the supervised learning problem, our goal is
to use a model to learn the target relationship H. First, we define the model. Among all
different variants of the encoder-decoder architectures, the RNN encoder-decoder introduced
in Cho et al. (2014b) can be considered as the most simple and representative model, where
the encoder and decoder are both RNNs. We study this particular model as we try to
eliminate other factors and only focus on the encoder-decoder architecture itself.

Under our setting, the simplified model of Cho et al. (2014b) with RNNs as both encoder
and decoder can be formulated as
_hs = σE(WEhs_ 1 + UExs + bE), _v = hτ_ _,_
_−_
_gt = σD(WDgt_ 1 + bD), _g0 = v,_ (1)
_−_
_ot = WOgt + bO,_

where ht, gt are hidden states of the encoder and decoder respectively. Recurrent activation
functions are denoted by σE and σD. Here, τ denotes the terminating time step of the
encoder, and v is the summary of the input sequence, which is called as the coding vector.
The model prediction is denoted asEquation (1) describes the following model dynamics. First, the encoder reads the entire ot ∈ R. All the other notations are model parameters.
input x, and then summarises the input into a fixed size coding vector v, which is also the
last hidden state of the encoder. Next, the coding vector is passed into the decoder as the
initial state, and then the decoder produces an output at each time step. Note that the
encoder has a terminating time, and the decoder has a starting time. This is the reason
why we take the input and output as semi-infinite sequences.


-----

We study a linear, residual and continuous-time idealisation of the model dynamics (1):

_h˙_ _s = Whs + Uxs,_ _v = Qh0,_ _s_ 0
_≤_
_g˙t = V gt,_ _g0 = Pv,_ (2)

_ot = c[⊤]gt,_ _t_ 0,
_≥_

where W ∈ R[m][E] _[×][m][E]_, U ∈ R[m][E] _[×][d], Q ∈_ R[N] _[×][m][E]_, V ∈ R[m][D][×][m][D], P ∈ R[m][D][×][N] and c ∈ R[m][D]
are parameters. mE and mD denote the width of encoder and decoder, respectively. The
coding vector v has dimension N, where we apply linear transformations to control it. We
assume h = 0, which is the usual choice for the initial condition of RNN hidden states.
_−∞_

Since our goal is to investigate approximation problems over large time horizons, we are
supposed to consider the stable RNN encoder-decoders, where

_W_ _mE :=_ _W_ R[m][E] _[×][m][E]_ : eigenvalues of W have negative real parts _,_ (3)
_∈W_ _{_ _∈_ _}_

_V_ _mD :=_ _V_ R[m][D][×][m][D] : eigenvalues of V have negative real parts _._ (4)
_∈V_ _{_ _∈_ _}_

The hypothesis space of RNN encoder-decoder models with arbitrary widths and coding
vector dimension is defined as [ˆ] := _mE_ _,mD,N_ N+ [ˆ]mE _,mD,N_, where
_H_ _∈_ _H_

_∞_

## ˆmE,mD,N := Hˆ := H t : t 0 [S] : [ˆ]H t(x) = c[⊤]e[V t]P Qe[W s]Ux sds, with
_H_ _{[ˆ]_ _≥_ _}_ 0 _−_
 Z (5)

(W, U, Q, V, P, c) ∈WmE × R[m][E] _[×][d]_ _× R[N]_ _[×][m][E]_ _× VmD × R[m][D][×][N]_ _× R[m][D]_ _._


The widths mE, mD and the coding vector dimension N together control the capacity/complexity of the hypothesis space. Note that the assumptions on eigenvalues of W
and V ensure that the parameterized linear functionals are continuous.

Due to the mathematical form (5), not all functionals can be represented by RNN encoderdecoders. To achieve a good approximation, the target functionals must possess certain
structures. We introduce the following definitions to clarify these structures.
**Definition 3.1. Let H =** _Ht : t_ 0 _be a sequence of functionals._
_{_ _≥_ _}_

_1. For any t_ 0, the functional Ht is linear and continuous if for any λ1, λ2 R
_and x1, x2 ≥_ _, we have Ht(λ1x1 + λ2x2) = λ1Ht(x1) + λ2Ht(x2), and_ _H ∈t_ :=
supx _,_ **_x_** _∈X1_ _Ht(x)_ _<_ _, where_ _Ht_ _denotes the induced functional norm. ∥_ _∥_
_∈X_ _∥_ _∥X ≤_ _|_ _|_ _∞_ _∥_ _∥_

_2. For any t ≥_ 0, the functional Ht is regular if for any sequence {x[(][n][)]}n[∞]=1 _[⊂X]_
_such that limn→∞_ _x[(]s[n][)]_ = 0 for almost every s ≤ 0 (Lebesgue measure), we have
limn→∞ _Ht(x[(][n][)]) = 0._

_∞_
_For a sequence of functionals H, we define its norm by_ **_H_** := _Ht_ _dt._
_∥_ _∥_ 0 _∥_ _∥_
Z

**Remark 3.1. The definitions of linear and continuous functionals are standard. One can**
_view regular functionals as those not determined by inputs on arbitrarily small time intervals,_
_e.g. an infinitely thin spike (i.e. δ-functions)._

Given the above definitions, we immediately have the following observation.

**Proposition 3.1. Let** **_H[ˆ]_** _be a sequence of functionals in the RNN encoder-decoder_
_∈_ [ˆ]H
_hypothesis space (see (5)). Then for any t_ 0, [ˆ]H _t_ **_H is a linear, continuous and regular_**
_≥_ _∈_ [ˆ]
_functional. Furthermore,_ _H_ _t_ _decays exponentially as a function of t._
_∥[ˆ]_ _∥_

The proof is found in Appendix A. This proposition characterises properties of the encoderdecoder hypothesis space. In particular, it is different from the RNN hypothesis space
discussed in Li et al. (2021), since the encoder-decoder is not necessarily time-homogeneous.
**Remark 3.2. A sequence of functionals H is time-homogeneous if for any t, τ** 0, Ht(x) =
_≥_
_Ht+τ_ (x(τ )), with x(τ )s = xs−τ for all s ∈ R. That is, if the input is shifted to the right by


-----

_τ_ _, the output is also shifted by τ_ _. Temporal convolution is an example of time-homogeneous_
_operation (recall the Gaussian convolution discussed in Section 3._ _An example of time-_
_inhomogeneous relationship is video captioning: shifts in the sequence of input video frames_
_do not necessarily lead to corresponding shifts in the caption text sequence._

**Relation with RNNs.** Here, we emphasise the differences between the encoder-decoder
hypothesis space and the RNN hypothesis space discussed in Li et al. (2021), where

## ˆH [(RNN)]t (x) = ∞0 c[⊤]e[W][ (][t][+][s][)]Ux−sds. A key difference is that the encoder-decoder has a
structure involving two temporal parameters t and s, while the RNN only has one depending on t + s, due to the time-homogeneity.R Owing to this difference and the fact that
## ˆ, the encoder-decoder hypothesis space (5) is more general, with the extra ca_H[(RNN)]_ _⊂_ [ˆ]H
pability to learn time-inhomogeneous relationships. Furthermore, e[V t] and e[W s] adapt to a
temporal product structure, which is an intrinsic difference between encoder-decoders and
other architectures. We will discuss this in detail in the next section.

4 Approximation results

One of the most fundamental problems for supervised learning is the approximation problem. It basically concerns the capacity of the hypothesis space to fit the concept space. In
general, there are two levels of approximation problems that can be discussed. The first is
known as the universal approximation, which considers the density of the hypothesis space
in the concept space. The second is the approximation rate, which aims to characterise
quantitatively the approximation accuracy concerning the capacity/complexity of the hypothesis space (e.g. the number of trainable parameters). In this section, both of them are
developed for RNN encoder-decoders.

4.1 Universal approximation

We first present the most basic density result, which states that any linear, continuous,
and regular temporal relationship can be approximated by RNN encoder-decoders up to
arbitrary accuracy. The proof is found in Appendix B.
**Theorem 4.1. Let H be a sequence of linear, continuous and regular functionals defined**
_on_ _, and satisfy_ **_H_** _<_ _. Then for any ϵ > 0, there exists_ **_H[ˆ]_** _such that_
_X_ _∥_ _∥_ _∞_ _∈_ [ˆ]H

_∞_
**_H_** **_H_** _Ht_ _H_ _t_ _dt < ϵ._ (6)
_∥_ _−_ [ˆ]∥≡ 0 _∥_ _−_ [ˆ] _∥_
Z

Here, we highlight two important observations while deriving Theorem 4.1. First, one
can show that each sequence of functionals H can be associated with a unique twoparameter “representation” ρ(t, s), such that H ∈Ht(x) = _∞0_ _x[⊤]−s[ρ][(][t, s][)][ds][. Recall the model]_
forming representation. The functional approximation is then reduced to function approximation[ˆ]H _t(x) =_ _∞0_ _x[⊤]−sρ[ˆ](t, s)ds, where ˆρ(t, s) := [c[⊤]e[V t]PQeR_ _[W s]U_ ][⊤] denotes the correspondR
in the sense of representations, i.e. ∥H −H[ˆ]∥≤∥ρ−ρˆ∥L1([0,∞)2). It turns out that ρ directly
affects the rate of approximation and gives rise to intrinsic properties. We will discuss this
in detail in Section 4.3. In addition, we again emphasise the differences between the present
work and Li et al. (2021). In Li et al. (2021), the target relationships are assumed to be
time-homogeneous with the representation Ht(x) = _∞0_ _ρ(t + s)x−sds, which only depends_
on t + _s. However, the setting here does not assume time-homogeneity, hence implies a more_
general representation ρ depending on the two temporal directionsR _t and s simultaneously._

4.2 General approximation rates

While the density result (Theorem 4.1) ensures the universal approximation property of the
RNN encoder-decoder, it does not identify targets that can be efficiently approximated. To
achieve this, we focus on approximation rates next. We characterise the temporal structure
of a target relationship by observing its responses to “constant” input signals. Here, we
consider the approximation rates for the model with “large size” coding vector, where the


-----

dimension N _m¯_ := min _mE, mD_ . This is the scenario where we fix the widths but take
_≥_ _{_ _}_
an oversized coding vector.
**Theorem 4.2. Let H be a sequence of linear, continuous and regular functionals defined**
_on_ _, and satisfy_ **_H_** _<_ _. Consider the output of piece-wise constant signals yi[c][(][t, s][) =]_
_X_ _∥_ _∥_ _∞_
_Ht(ei1(−∞,−s]), t, s ≥_ 0, i = 1, 2, . . ., d, where {ei}i[d]=1 _[denotes the standard basis of][ R][d][.]_
_Assume that there exist α ∈_ N+, β > 0 such that for any i = 1, 2, . . ., d,

_yi[c]_ (7)

_[∈]_ _[C]_ _[α][+1][([0][,][ ∞][)][2][)][,]_

_e[β][(][t][+][s][)][ ∂][k][+][l]_ _i_ [(][t, s][) =][ o][(1)][ as][ ∥][(][t, s][)][∥→∞][,] (k, l) N N+, k + l _α + 1._ (8)

_∂t[k]∂s[l][ y][c]_ _∈_ _×_ _≤_

_Then for any mE, mD, N_ N+, there exists **_H[ˆ]_** _mE_ _,mD,N such that_
_∈_ _∈_ [ˆ]H

1 1

**_H_** **_H_** + _,_ (9)
_∥_ _−_ [ˆ]∥≤ _[C][(]β[α][2][)][γd]_ _m[α]E_ _m[α]D_

 

_where C(α), γ > 0 are both universal constants with dependence only on α and (α, β),_

_respectively, and γ :=_ _i∈Nmax+, i≤d_ _k,l∈Nmax, k+l≤α+1_ _t,s[sup]≥0_ _β[−][(][k][+][l][)]e[β][(][t][+][s][)]_ _∂t[∂][k][k][+]∂s[l]_ _[l][ y]i[c][(][t, s][)]_ _< ∞. Here,_

_the number of trainable parameters is dN_ (mE + mD) with N _m¯_ _._
_≥_

The proof is found in Appendix C. First, note that the error bound does not depend on the
coding vector size N, as long as N _m¯_ . This is because further increasing N beyond ¯m
_≥_
only increases the number of trainable parameters, but does not increase the model capacity
(see Remark C.2). Only the model widths mE, mD affect the approximation capabilities.

Next, we focus on the classes of target relationships that can be well approximated. Here,
_α characterises the smoothness of H, and β characterises the temporal decay rates of the_
output responding to a constant signal under H. This is a notion of memory in the target
relationship. The error bound (9) indicates that a sequence of target functionals can be
efficiently approximated by the encoder-decoder if it is smooth (large α), and has fast
decayed memory (large β).

The characterisation in smoothness and memory decay also appears in the approximation results of RNNs (Li et al., 2021), where the upper bound is _[C]βm[(][α][)][α][γd][ . However, our]_

results for encoder-decoders suggest extra structures, where the bound involves two (instead of one) temporal parameters together with smoothness and decay memories in both.
The two-parameter temporal dependence allows the encoder-decoder to approximate timeinhomogeneous relationships, which generalises the RNN. This two-parameter structure
further leads to adaptation to a specific low rank type of target relationships, resulting in
finer approximation rates as we discuss next.

4.3 Approximation rates via temporal product structure

**Motivation of temporal product structure.** In contrast with Theorem 4.2, we next
consider the model with N < ¯m = min _mE, mD_ . In this situation, the model has fewer
_{_ _}_
parameters, and we aim to characterise the target relationships by further exploiting the
structure of the two-parameter representation ρ(t, s). This leads to a finer approximation
rate by considering mE, mD, N together.

We first motivate how the “temporal product structure” arises, and how it relates to the
approximation. Detailed discussions and proofs are found in Appendix D. For the illustration
purpose, we set the input dimension d = 1. Recall Q ∈ R[N] _[×][m][E]_, P ∈ R[m][D][×][N], then the
representation ˆρ of the encoder-decoder functional can be rewritten as

_N_ _mD_ _mE_

_ρˆ(t, s) = c[⊤]e[V t]P_ _Qe[W s]u =_ _ciPjn_ _e[V t][]ij_ _uiQnj_ _e[W s][]ji_
_·_ _n=1_ _i,j=1_ ! _i,j=1_ !

X X  X 

_N_

= _ϕˆn(t)φ[ˆ]n(s)._ (10)

_n=1_

X


-----

This is a tensor product structure over the (t, s) time domain (determined by the encoder
_φn_ and decoder _ϕ ˆn_ successively). We call it the temporal product structure. As is
_{_ [ˆ] _}_ _{_ _}_
shown later, this structure significantly affects approximation rates. When _φn_ and _ϕ ˆn_
_{_ [ˆ] _}_ _{_ _}_
are selected as the “bases” along s, t direction, respectively, N is considered as the rank of
the temporal product. We also define N as the rank of the model, which is understood as
the maximum rank of temporal products that the encoder-decoder model can represent.

**The rank concept of temporal relationships.** Recall that the given number of trainable parameters is dN (mE + mD). Hence, a low rank model may achieve fewer trainable
parameters. When investigating relationships that can be well approximated by low rank
models, a natural conjecture would be “low rank” targets.

What is the meaning of “low rank” for a temporal relationship? It is well-known that in linear
algebra, an operator is low rank means that its range space is low-dimensional. This idea
can be also applied to temporal relationships. For a “low rank” temporal relationship, the
output sequence is more “regular”, meaning that the output sequences (viewed as functions)
are in a low-dimensional function space. We provide an intuitive numerical illustration for
better understanding.

|Col1|Col2|
|---|---|
|||
|||
|||
|||
|||

|Col1|Col2|
|---|---|
|||
|||
|||
|||
|||
|||
|||

|Col1|Col2|
|---|---|
|||
|||
|||
|||
|||

|Col1|Col2|
|---|---|
|||
|||
|||
|||
|||
|||


_xt_ _Ht(x)_

2.0

2 1.5

1.0

1

0.5

0 0.0

0.5

1 _−_
_−_ _−1.0_

_−2_ _−1.5_

_−3_ 0 5 10 15 20 25 30 _−2.0_ 0 5 10 15 20 25 30

_t_ _t_


_xt_ _Ht(x)_

40

2

30

1

20

0

10

_−1_ 0

_−2_ _−10_

3
_−_ 0 5 10 15 20 25 30 0 5 10 15 20 25 30

_t_ _t_


(a) high rank relationship


(b) low rank relationship


Figure 1: We construct a high rank and a low rank target from the temporal product. For
both (1a) and (1b), we plot the inputs xt together with the corresponding outputs Ht(x).
Detailed settings are found in Appendix E.1.

Figure 1 shows the outputs of a high rank (a) and a low rank (b) target relationship on
the same set of random input sequences. Different colours refer to different instances of
inputs. In the first case (high rank), the temporal structure of the outputs is very complex
and depends sensitively on the inputs. However, in the second case (low rank), the output
sequences are much more regular, and only macroscopic structures (e.g. scale/offset) appear.

**Remark 4.1. In the research of approximation theories for temporal sequences, prior works**
_also related a notion of rank to approximation properties of the dilated convolutional structure_
_(Jiang et al., 2021). Here, we emphasise that the notion of rank considered in our work is_
_very different from that in Jiang et al. (2021), which mainly concerns the tensorisation of_
_a discrete-time sequence according to the width of convolution filters._

**POD as an analogue of SVD.** Now, we characterise low rank and high rank temporal
relationships in a mathematical way. We will introduce the concepts informally, and rigorous
definitions and arguments can be found in Appendix D.

For a matrix, we can assess its rank by performing the singular value decomposition
(SVD). This method can be extended to the temporal relationships using proper orthogonal decomposition (POD; Liang et al. (2002), Berkooz et al. (1993), Chatterjee (2000)).
The basic insight is that the function ρ can be decomposed into the following form:
_ρ(t, s) =_ _n=1_ _[σ][n][ϕ][n][(][t][)][φ][n][(][s][), where][ N][0][ ≤∞][,][ {][ϕ][n][}][ and][ {][φ][n][}][ are orthonormal bases, and]_
ing SVD to an infinite-dimensional space (whenσ1 ≥ _σ2 ≥· · · ≥_ 0 denote the singular values. This procedure can be viewed as apply- N0 = ). An analogue of Eckart–Young

[P][N][0] _∞_
theorem (Eckart & Young, 1936), which characterises the best low rank approximation,
also holds for POD. It roughly states that inf rank(ˆρ)=N _[∥][ρ][ −]_ _ρ[ˆ]_ _L[2][ =][ P]n[N]=[0]_ _N_ +1 _[σ]n[2]_ [. That is,]
_∥[2]_


-----

any target ρ has a rank-N best approximation, with error equalling to the tail sum of the
squared singular values. In other words, a target with fast decayed σn (low “effective rank”)
has smaller approximation errors. This forms the basis of our next result, which states that
if the target relationship possesses an effective low rank structure in terms of the decay
of singular values, then one can achieve an efficient approximation using encoder-decoder
structures by limiting the size of coding vectors. Detailed definitions for _σn_ and proofs
_{_ _}_
are found in Appendix D.

**Theorem 4.3. Assume the same conditions as in Theorem 4.2. Then for any mE, mD, N**
_∈_
N+ with N _m¯_ _, there exists_ **_H[ˆ]_** _mE_ _,mD,N such that_
_≤_ _∈_ [ˆ]H

1 1 _m¯_ 1/2

**_H_** **_H_** ≲ _[C][(][α][)][γd]_ 1 + _√m¯_ _N_ + + _σn[2]_
_∥_ _−_ [ˆ]∥ _β[2]_ (  _−_  _·_  _m[α]E_ _m[α]D_  _n=XN_ +1 !

_m¯_ 1/2 1 1

+ _σn_ + _,_ (11)

_n=N_ +1 ! _·_ _m[α/]E_ [2] _m[α/]D_ [2] ! )

X

_where ≲_ _hides universal positive constants, and ¯m = min{mE, mD}. Here, the number of_
_trainable parameters is dN_ (mE + mD) with N _m¯_ _._
_≤_

This is a finer approximation rate compared to Theorem 4.2, where both the widths mE, mD
and the coding vector size N affect the model capacity for approximation. Besides the
smoothness and memory decay, we have the additional rank structure of the target relationship, which is characterised by its singular values _σn_ . We again focus on the class of
_{_ _}_
functionals that can be well approximated. Smoothness α and decay rate β is the same as
Theorem 4.2. The difference lies in the rank structure indicated by _σn_ : the error bound is
_m_ _{_ _}_
small if _σn_ has a small tail _n=N_ +1 _[σ]n[2]_ [. It suggests that a target with fast decayed][ {][σ][n][}]
_{_ _}_
or low “effective rank” can be well approximated by the RNN encoder-decoder with fewer
parameters. Due to the Eckart–Young-like low rank approximation, we can appropriately

[P][ ¯]
select N based on the decay rate of singular values.

Here, we emphasise that the temporal product is an intrinsic structure arising from the
encoder-decoder architecture. Recall the dynamics of the encoder-decoder: it first encodes
the input sequence into a coding vector, and then decodes an entire output sequence from
it. In this sense, the coding vector is the only interaction between the input and output.
Thus, the coding vector size N is an essential measure of the model capacity concerning
the dependence of outputs on inputs. Here, we show that this concept can be formalised
as a notion of rank, which can pinpoint the precise types of input-output relationships that
encoder-decoder architectures are well adapted to.

**Numerical illustrations.** Here, we utilise numerical examples to illustrate the above
discussions. We observe how the decay rate of singular values, the rank N0 of the target
relationships, and the model rank N affect the approximation error **_H_** **_H_** . However,
_∥_ _−_ [ˆ]∥
it is not always possible to construct the best approximation. Instead, we perform some
training steps to achieve an upper bound of the approximation error, which is consistent
with our theoretical results.

In Figure 2, we train linear encoder-decoder models to learn three relationships of different
ranks determined by various decay patterns of singular values, given in (a), (b) and (c).
Different colours denote targets with different ranks. From Figure 2, we have the following
observations consistent with previous discussions. First, observe that increasing the model
rank N makes approximation errors smaller, as expected. Moreover, note that when increasing N, the speeds of error decrements are different. If the singular values decay fast, the
approximation errors also decay fast. This implies that a target with fast decayed singular
values can be approximated efficiently with fewer parameters (smaller N ). In addition, for
each experiment, we are able to achieve low approximation errors by choosing N _m. The_
_≪_
errors will remain unchanged or decrease much more slowly when further increasing N . This
suggests that in practice, one can choose N such that it covers the major singular values of
the target in order to improve the approximation efficiency.


-----

_×10[−][3]_


2.8

2.3

1.8

1.3

0.8

0.0


1.1

0.9

0.7

0.5

0.3

0.0


1.0

0.8

0.6

0.4

0.2

0.0

|×10−2|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||Target|Rank N0 = 2|
||||||N0 = 4 N0 = 6|
||||||N0 = 8|
|||||||
|||||||
|||||||

|×10−3|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||Target|Rank N0 = 2|
||||||N0 = 4 N0 = 6|
||||||N0 = 8|
|||||||
|||||||
|||||||

|−3|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||Targe|t Rank|
|||||N0 = ∞|
||||||
||||||
||||||
||||||


2 4 6 8 10
_N_

_n[−]_ 8[1], _n_ _N0_
(a) σn = _≤_

0, _n > N0_




10


10 20 30

_N_

(c) σn = n[−][2]


_n[−][1],_ _n_ _N0_
(b) σn = _≤_
0, _n > N0_



Figure 2: In (a), (b), (c) we consider target relationships with different singular values
indicated in the respective caption. For (a), (b) we also consider targets with different rank,
where N0 = 2, 4, 6, 8. We use models with fixed width m = mE = mD = 128 and coding
vector size N . Detailed settings are found in Appendix E.2.

In Figure 3, we perform experiments on the forced Lorentz 96 system (Lorenz, 1996), which
parameterises a high-dimensional and nonlinear relationship between input forcing and
model states. The parameters K, J in the Lorenz 96 system control the overall complexity
of the target (see Appendix E.3 for details). We use the RNN encoder-decoder with tanh
activations to learn this target. Although our theories are developed in the linear regime,
the low rank approximation phenomenon also appears in this nonlinear setting. The error
decrements saturate when increasing the coding vector size N beyond a threshold, suggesting the existence of some implicit notion of “rank” of the target nonlinear functional. This
“rank” increases with the target complexity (mainly K).


_×10[−][4]_


|Col1|Col2|Ta|Col4|rget Parameters|Col6|
|---|---|---|---|---|---|
|||||K = 1, J = 6 K = 5, J = 6||
|||||||
|||||||
|||||K = 10, J = K = 20, J =|6 6|
|||||||
|||||||
|||||||
|||||||
|||||||

|×10−4|Col2|Col3|Col4|
|---|---|---|---|
||Targ||et Parameters|
||||K = 5, J = 5 K = 5, J = 15 K = 5, J = 25|
||||K = 5, J = 100|
|||||
|||||
|||||


10 20 30


10 20 30


Figure 3: K, J are the parameters of the Lorenz 96 system. They describe the number
of independent and coupled variables in the system, which can be viewed as a complexity
measure. Detailed settings are found in Appendix E.3.

5 Conclusion


We theoretically study the approximation properties of the RNN encoder-decoder in a linear setting. We prove a universal approximation result for linear temporal relationships
utilising encoder-decoder architectures, and show that they generalise RNNs to the timeinhomogeneous setting. Moreover, we discover an important temporal product structure
that characterises the types of input-output relationships especially suited for the efficient
approximation using encoder-decoders. This elucidates the key differences between these
novel architectures and classical methods for temporal modelling, and forms a basic step
towards understanding the intricacies of modern deep learning.


-----

**Reproducibility statements.** Detailed proofs for theoretical results, and complete settings of numerical examples are found in the appendix. The source code for numerical tests
can be made available upon request.

Here is a quick reference:

Proposition 3.1 Properties of RNN encoder-decoder functionals Appendix A
Theorem 4.1 Universal approximation theorem Appendix B
Theorem 4.2 General approximation rates Appendix C
Theorem 4.3 Approximation rates concerning temporal product structure Appendix D
Figure 1 Illustration of high/low rank temporal relationships Appendix E.1
Figure 2 Numerical examples on singular values Appendix E.2
Figure 3 Numerical examples on Lorenz 96 systems Appendix E.3

Acknowledgements

ZL is supported by Peking University under BICMR mathematical scholarship. HJ is supported by National University of Singapore under PGF scholarship. QL is supported by the
National Research Foundation, Singapore, under the NRF fellowship (NRF-NRFF13-20210005).


-----

References

Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. Neural machine translation by
jointly learning to align and translate. In International Conference on Learning Repre_sentations, pp. 1–15, 2015._

Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

G. Berkooz, P. Holmes, and J. L. Lumley. The proper orthogonal decomposition in the
analysis of turbulent flows. _Annual Review of Fluid Mechanics, 25(1):539–575, 1993._
doi: 10.1146/annurev.fl.25.010193.002543. [URL https://doi.org/10.1146/annurev.](https://doi.org/10.1146/annurev.fl.25.010193.002543)
```
 fl.25.010193.002543.

```
Vladimir I. Bogachev. Measure theory, volume 1. Springer Science & Business Media, 2007.

Ching-Hua Chang and Chung-Wei Ha. On eigenvalues of differentiable positive definite
kernels. Integral Equations and Operator Theory, 33:1–7, 1999. doi: 10.1007/BF01203078.

Anindya Chatterjee. An introduction to the proper orthogonal decomposition. Current
_[Science, 78(7):808–817, 2000. ISSN 00113891. URL http://www.jstor.org/stable/](http://www.jstor.org/stable/24103957)_
```
 24103957.

```
Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen,
Z. Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Katya Gonina, Navdeep Jaitly,
Bo Li, Jan Chorowski, and Michiel Bacchiani. State-of-the-art speech recognition with
sequence-to-sequence models. In IEEE International Conference on Acoustics, Speech and
_Signal Processing, pp. 4774–4778, 2018._

Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio. On
the properties of neural machine translation: Encoder–decoder approaches. In Eighth
_Workshop on Syntax, Semantics and Structure in Statistical Translation, pp. 103–111._
Association for Computational Linguistics, 2014a. doi: 10.3115/v1/W14-4012. URL
```
 https://aclanthology.org/W14-4012.

```
Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using
RNN encoder-decoder for statistical machine translation. Conference on Empirical Meth_ods in Natural Language Processing, pp. 1724–1734, 2014b. doi: 10.3115/v1/d14-1179._

Kenji Doya. Universality of fully-connected recurrent neural networks. IEEE Transactions
_on Neural Networks, 1993._

Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank.
_Psychometrika, 1(3):211–218, 1936._

Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179–211, 1990.

Ken-ichi Funahashi and Yuichi Nakamura. Approximation of dynamical systems by continuous time recurrent neural networks. Neural Networks, 6(6):801 – 806, 1993. ISSN
0893-6080.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation,
9(8):1735–1780, 1997.

Haotian Jiang, Zhong Li, and Qianxiao Li. Approximation theory of convolutional architectures for time series modelling. In Proceedings of the 38th International Conference on
_Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 4961–_
[4970. PMLR, 2021. URL https://proceedings.mlr.press/v139/jiang21d.html.](https://proceedings.mlr.press/v139/jiang21d.html)

Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In Confer_ence on Empirical Methods in Natural Language Processing, pp. 1700–1709, 2013. ISBN_
9781937284978.


-----

Peter D. Lax. Functional Analysis. John Wiley & Sons, Inc., 2002.

Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis
with transformer network. In AAAI Conference on Artificial Intelligence, volume 33, pp.
6706–6713, 2019.

Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. On the curse of memory in recurrent neural networks: Approximation and optimization analysis. In International Conference on
_[Learning Representations, 2021. URL https://openreview.net/forum?id=8Sqhl-nF50.](https://openreview.net/forum?id=8Sqhl-nF50)_

Y. C. Liang, H. P. Lee, S. P. Lim, W. Z. Lin, K. H. Lee, and C. G. Wu. Proper orthogonal
decomposition and its applications—Part I: Theory. Journal of Sound and Vibration, 252
(3):527–544, 2002. ISSN 0022-460X. doi: https://doi.org/10.1006/jsvi.2001.4041. URL
```
 https://www.sciencedirect.com/science/article/pii/S0022460X01940416.

```
G. G. Lorentz. Approximation of Functions. AMS Chelsea Publishing Series. Holt, Rinehart
[and Winston, 2005. ISBN 9780821840504. URL https://books.google.com.sg/books?](https://books.google.com.sg/books?id=8VMrOmTKSe0C)
```
 id=8VMrOmTKSe0C.

```
Edward N. Lorenz. Predictability: A problem partly solved. In Proc. Seminar on Pre_dictability, volume 1, pp. 40–58, 1996._

Wolfgang Maass, Prashant Joshi, and Eduardo D. Sontag. Computational aspects of feedback in neural circuits. PLOS Computational Biology, 3(1):e165, 2007.

Charles Bradfield Morrey. Multiple Integrals in the Calculus of Variations. Springer-Verlag,
1966.

Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine
_Learning, pp. 4055–4064. PMLR, 2018._

Walter Rudin. Real and Complex Analysis. Higher Mathematics Series. McGraw-Hill Edu[cation, 1987. ISBN 9780070542341. URL https://books.google.com.sg/books?id=Z_](https://books.google.com.sg/books?id=Z_fuAAAAMAAJ)
```
 fuAAAAMAAJ.

```
Anton Maximilian Sch¨afer and Hans-Georg Zimmermann. Recurrent neural networks are
universal approximators. International Journal of Neural Systems, 17(4):253–263, 2007.

Martin H. Schultz. L[∞]-multivariate approximation theory. SIAM Journal on Numerical
_[Analysis, 6(2):161–183, 1969. doi: 10.1137/0706017. URL https://doi.org/10.1137/](https://doi.org/10.1137/0706017)_
```
 0706017.

```
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural
networks. In Advances in Neural Information Processing Systems, volume 4, pp. 3104–
3112, 2014.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances
_in Neural Information Processing Systems, pp. 5999–6009, 2017._

Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor
Darrell, and Kate Saenko. Sequence to sequence-video to text. In Proceedings of the
_IEEE International Conference on Computer Vision, pp. 4534–4542, 2015._

Jong Chul Ye and Woon Kyoung Sung. Understanding geometry of encoder-decoder
CNNs. In International Conference on Machine Learning, pp. 12245–12254, 2019. ISBN
9781510886988.

Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar.
Are transformers universal approximators of sequence-to-sequence functions? In Interna_[tional Conference on Learning Representations, 2020. URL https://openreview.net/](https://openreview.net/forum?id=ByxRM0Ntvr)_
```
 forum?id=ByxRM0Ntvr.

```

-----

A Properties of model functionals

In this section, we prove observations of the hypothesis space reported in Proposition 3.1.

_Proof of Proposition 3.1. Recall that_

_∞_
## ˆH t(x; θ) = 0 c[⊤]e[V t]Me[W s]Ux−sds. (12)
Z

Fix any θ = (W, V, U, M, c). The linearity is obvious. Since both W _mE and V_ _mD_
_∈W_ _∈V_
have eigenvalues with negative real parts, there exist c1, c2, c[′]1[, c][′]2 _[>][ 0, such that][ ∥][e][V t][∥][∞]_ _[≤]_
_c1e[−][c][2][t]_ and _e[W s]_ _c[′]1[e][−][c]2[′]_ _[s]_ for any t, s 0, hence
_∥_ _∥∞_ _≤_ _≥_

_|c[⊤]e[V t]Me[W s]Ux−s| ≤∥c∥1∥e[V t]Me[W s]Ux−s∥∞_
_≤∥c∥1∥e[V t]∥∞∥M_ _∥∞∥e[W s]∥∞∥U_ _∥∞∥x−s∥∞_

≲ _e[−][c][2][t]e[−][c]2[′]_ _[s]_ **_x_** _,_ (13)
_∥_ _∥X_

where ≲ hides universal positive constants depending on parameters θ. Therefore

_H_ _t(x; θ)_ ≲ _e[−][c][2][t]_ **_x_** _H_ _t(_ ; θ) ≲ _e[−][c][2][t]._ (14)
_|[ˆ]_ _|_ _∥_ _∥X ⇒∥[ˆ]_ _·_ _∥_

That is, the functional [ˆ]H _t(_ ; θ) is bounded (i.e. continuous) with an exponentially-decayed

_·_
norm (as a function of t). Finally, by (13) and Lebesgue’s dominated convergence theorem,
the functional [ˆ]H _t(_ ; θ) is also regular. The proof is completed.

_·_

B Universal approximation

In this section, we provide the proof of Theorem 4.1, i.e. the universal approximation
property of RNN encoder-decoders. As is stated in the main text, the key step is to utilise
the classical representation theorem, which helps us to reduce the approximation problem
of functionals to functions.

B.1 Preliminaries

First, we list the background definitions and notations used in the following theorems. Let
(X, ) be a measure space (with as the σ-algebra of subsets of X).
_A_ _A_

-  The space X is called locally compact if for any x _X, x has a compact neighbour-_
_∈_
hood.

-  X is a Hausdorff space if all distinct points in X are pair-wisely separable by neighbourhoods. That is, for any x, y _X, there exists a neighbourhood ∆x of x and a_
_∈_
neighbourhood ∆y of y, such that ∆x ∆y = ∅.
_∩_

-  The measure µ is called a finite measure, if it satisfies µ(X) < . The measure
_∞_
_ν is called a σ-finite measure, if X can be covered with at most countably many_
measurable sets with finite measure. That is, there are measurable sets with ν(An) < for all n N+, such that _n=1_ _[A][n][ =][ X][. Obviously, a finite] {An}n[∞]=1_ _[⊂]_
_A_ _∞_ _∈_
measure is also σ-finite.

-  Let X = ( _, 0]. For a measure µ on the measure space (([S][∞]_ _, 0],_ ), µ is absolutely
_−∞_ _−∞_ _A_
_continuous with respect to the Lebesgue measure ν, if for every measurable set A,_
_ν(A) = 0 implies µ(A) = 0, which is written as µ_ _ν._
_≪_

Denote by C0(X) the linear space of continuous functions defined on X vanishing at infinity.
We have the following classical representation theorem.
**Theorem B.1 (Riesz-Markov-Kakutani representation theorem). Let X be a locally com-**
_pact Hausdorff space. For any continuous linear functional ψ on C0(X), there is a unique,_
_regular, countably additive and signed measure µ on X, such that_


_ψ(f_ ) =


_f_ (x)dµ(x), _∀f ∈_ _C0(X),_ (15)


-----

_with_ _ψ_ = _µ_ (X). Here, _µ_ (X) denotes the total variation of (the signed measure) µ,
_which is defined as ∥_ _∥_ _|_ _|_ _µ_ (X) := sup | _|_ _ki=1_ _i=1_ _[A][i][ is a partition over]_
_X with Ai_ _for all |_ _| i = 1, 2,_ _P, k._ _[|][µ][(][A][i][)][|][, where][ P][ :][ X][ =][ S][k]_
_∈A_ _· · ·_ P

_Proof. Well-known, see e.g. Bogachev (2007) (CH 7.10.4)._

**Remark B.1. It is straightforward to verify that |µ|(X) = supA∈A(|µ(A)| + |µ(A[c])|). Fur-**
_thermore, if µ has a density dµ/dν with respect to a countably additive, nonnegative measure_
_ν, then we have |µ|(X) = ∥dµ/dν∥L1(ν)._

To handle signed measures, the following Jordan decomposition theorem (Bogachev, 2007)
is necessary.

**Theorem B.2. Let µ be a signed measure on the measure space (X,** ). Then, there are two
_A_
_mutually singular (non-negative) measures µ[+]_ _and µ[−]_ _on (X,_ ), such that µ = µ[+] _µ[−]._
_A_ _−_
_Moreover, such a pair (µ[+], µ[−]) is unique._

Based on this, we have the following proposition to characterise absolutely continuous signed
measures.

**Proposition B.1. If µ and ν are signed measures, then we have µ** _ν_ _µ[+]_ _ν and_
_≪_ _⇔_ _≪_
_µ[−]_ _ν._
_≪_

We also need the following Radon-Nikodym theorem (Bogachev, 2007).

**Theorem B.3. Let (X,** _, ν) be a σ-finite measure space, and let µ be a σ-finite signed_
_A_
_measure, such that µ_ _ν. Then there exists a unique measurable function f_ _, such that_
_≪_
_µ(A) =_ _A_ _[fdν][ for every measurable set][ A][.]_
R

B.2 Proofs

Before we prove the universal approximation theorem (Theorem 4.1), we need some lemmas.

**Lemma B.1. Let** _Ht : t_ 0 _be a family of linear, continuous and regular functionals_
_{_ _≥_ _}_
_defined on X_ _. Then there exists a integrable function ρ : [0, ∞)[2]_ _→_ R[d], i.e.


_ρ_ _L1([0,_ )2) :=
_∥_ _∥_ _∞_


_ρi_ _L1([0,_ )2) < _,_ (16)
_∥_ _∥_ _∞_ _∞_
_i=1_

X


_such that_

_∞_
_Ht(x) =_ _x[⊤]s[ρ][(][t, s][)][ds,]_ **_x_** _._ (17)

0 _−_ _∀_ _∈X_

Z

_In particular, we have ∥H∥_ = _∞0_ _∥Ht∥dt = ∥ρ∥L1([0,∞)2)._
R

_Proof. Obviously, (_ _, 0] is a locally compact Hausdorff space._ For any t 0, since
_−∞_ _≥_
_Ht is linear continuous, according to the Riesz-Markov-Kakutani representation theorem_
(Theorem B.1), there exists a unique, regular, countably additive and signed measure µt,
such that

0
_Ht(x) =_ _x[⊤]s_ _[dµ][t][(][s][)][,]_ _∀x ∈X_ _,_ (18)
Z−∞

absolutely continuous with respect towith _i=1_ _[|][µ][t,i][|][((][−∞][,][ 0]) =][ ∥][H][t][∥][.]_ We show that for any ν (the Lebesgue measure), i.e. t ≥ 0, i µ = 1t,i _, 2, · · ·ν. According, d, µt,i is_
to Theorem B.2 and Proposition B.1, one can assume µt,i to be non-negative without loss ≪

[P][d]
of generality. Take a measurable set A ( _, 0] with ν(A) = 0, the aim is to show_
_⊂_ _−∞_
_µt,i(A) = 0._ Let A[′] = ( _, 0]_ _A._ Since both A and A[′] are measurable, there exist
_−∞_ _\_
_Kn ⊂_ _A, Kn[′]_ _[⊂]_ _[A][′][ with][ K][n][, K]n[′]_ [closed, such that][ µ][t,i][(][A][ \][ K][n][)][ ≤] [1][/n][,][ µ][t,i][(][A][′][ \][ K]n[′] [)][ ≤] [1][/n]


-----

and ν(A[′] _Kn[′]_ [)][ ≤] [1][/n][ for any][ n][ ∈] [N][+][. Fix any][ i][ ∈{][1][,][ 2][, . . ., d][}][, we construct the sequence]
_\_
of input signals **_x[(][n][)]_** _n=1_ [as]
_{_ _}[∞]_

0, _s_ 0, j = i,
_≤_ _̸_

_x[(]s,j[n][)]_ [=] 0, _s_ _Kn[′]_ _[, j][ =][ i,]_ _j = 1, 2, . . ., d,_ (19)

 _∈_
1, _s_ _Kn, j = i,_

_∈_

which can then be continuously extended to ( _−∞, 0] by defining x[(]s,i[n][)]_ [:=] _d(s,Kdn(s,K)+dn[′](s,K[)]_ _n[′]_ [)][ ∈]

[0, 1]. [1]

We deduce that limn→∞ _x[(]s,i[n][)]_ [= 0 for][ ν][-a.e.][ s][ ≤] [0. In fact, let][ S][ :=][ {][s][ ≤] [0 : lim][n][→∞] _[x]s,i[(][n][)]_ [=]

0}, we have Kn[′] _[⊂]_ _[S][ since for any][ s][ ∈]_ _[K]n[′]_ [,][ x][(]s,i[n][)] [= 0. Hence, (][−∞][,][ 0]][ \][ S][ ⊂] _[A][ ∪]_ [(][A][′][ \][ K]n[′] [),]
which gives ν(( _, 0]_ _S)_ _ν(A)+_ _ν(A[′]_ _Kn[′]_ [)][ ≤] [1][/n][ →] [0 as][ n][ →∞][. Due to the regularity]
_−∞_ _\_ _≤_ _\_
of Ht, we get limn→∞ _Ht(x[(][n][)]) = 0. By (18) and (19), we have_

_d_ 0 0

_Ht(x[(][n][)]) =_ _x[(]s,j[n][)][dµ][t,j][(][s][) =]_ _x[(]s,i[n][)][dµ][t,i][(][s][)]_

_j=1_ Z−∞ Z−∞

X

= + + _x[(]s,i[n][)][dµ][t,i][(][s][) =][ µ][t,i][(][K][n][) +][ I][1][,n][ +][ I][2][,n][,]_ (20)

_Kn_ _A_ _Kn_ _A[′]_ _Kn[′]_

Z Z _\_ Z _\_

where µt,i(Kn) = _µt,i(A) −_ _µt,i(A \ Kn)_ _∈_ [µt,i(A) − 1/n, µt,i(A)], and |I1,n| +
_|I2,n| ≤_ _A\Kn_ [+] _A[′]\Kn[′]_ [1][dµ][t,i][(][s][) =][ µ][t,i][(][A][ \][ K][n][) +][ µ][t,i][(][A][′][ \][ K]n[′] [)][ ≤] [2][/n][, which gives]

limn→∞ _HRt(x[(][n][)]) =R µt,i(A). Therefore, µt,i(A) = 0._

Notice that _µt,i((_ _, 0])_ _µt,i_ (( _, 0])_ _Ht_ _<_ for a.e. t 0 (since **_H_** =
_|_ _−∞_ _| ≤|_ _|_ _−∞_ _≤∥_ _∥_ _∞_ _≥_ _∥_ _∥_
_∞_
0
Obviously, (([∥][H][t][∥][dt <][ ∞][), we get that ((], 0], _, ν) is a σ[−∞]-finite measure space. According to the Radon-Nikodym[,][ 0]][,][ A][, µ][t,i][) is a finite measure space, and hence][ σ][-finite.]_
R _−∞_ _A_
theorem (Theorem B.3), there exists a unique measurable function ρt,i : ( _, 0]_ R, such
_−∞_ _→_
that µt,i(A) = _A_ _[ρ][t,i][(][s][)][dν][(][s][) for every measurable set][ A][. Hence, we have]_

0

R _∞_

_Ht(x) =_ _x[⊤]s_ _[ρ][t][(][s][)][ds][ =]_ _x[⊤]s[ρ][(][t, s][)][ds,]_ **_x_** _,_ (21)
Z−∞ Z0 _−_ _∀_ _∈X_

with ρ(t, s) := _ρt(_ _s)._ In addition, by Remark B.1, we have _µt,i_ (( _, 0])_ =
0 _−_ _|_ _|_ _−∞_
_−∞_ _[|][ρ][t,i][(][s][)][|][ds][, which gives]_
R _d_ _d_

_∞_ _∞_
**_H_** = _Ht_ _dt =_ _µt,i_ (( _, 0])dt =_ _ρi_ _L1([0,_ )2) = _ρ_ _L1([0,_ )2). (22)
_∥_ _∥_ Z0 _∥_ _∥_ _i=1_ Z0 _|_ _|_ _−∞_ _i=1_ _∥_ _∥_ _∞_ _∥_ _∥_ _∞_

X X

The proof is completed.

Based on this representation theorem, the problem of functional approximation is reduced
as function approximation. That is,

_∞_ _∞_
**_H_** **_H_** = _Ht_ _H_ _t_ _dt =_ sup _Ht(x)_ ˆH _t(x; θ)_ _dt_
_∥_ _−_ [ˆ]∥ 0 _∥_ _−_ [ˆ] _∥_ 0 **_x_** 1 _−_
Z Z _∥_ _∥X ≤_

_∞_ _∞_
= sup _x[⊤]s[(][ρ][(][t, s][)][ −]_ _ρ[ˆ](t, s))ds_

0 **_x_** 1 0 _−_

Z _∥_ _∥X ≤_ Z

_∞_ _∞_
_≤_ 0 **_xsup_** 1 0 _∥x−s∥∞∥ρ(t, s) −_ _ρˆ(t, s)[dt]∥1dsdt_
Z _∥_ _∥X ≤_ Z

_d_ _∞_ _∞_

_ρi(t, s)_ _ρˆi(t, s)_ _dsdt,_ (23)

_≤_ _i=1_ Z0 Z0 _|_ _−_ _|_

X

1
Here, d(s, B) := inf _s_ _a_ : a _B_ is the distance between a point s and a set B.
_{|_ _−_ _|_ _∈_ _}_


-----

i.e.


**_H_** **_H_** _ρ_ _ρˆ_ _L1([0,_ )2) :=
_∥_ _−_ [ˆ]∥≤∥ _−_ _∥_ _∞_


_i=1_ _∥ρi −_ _ρˆi∥L1([0,∞)2)._ (24)

X


**Lemma B.2. Let ρ(t, s) : [0,** )[2] R with _ρ_ _L1([0,_ )2) < _. Then for any ϵ > 0, there_
_∞_ _→_ _∥_ _∥_ _∞_ _∞_
_exists a polynomial p(u, v) =_ _j=1,k=1_ _[c][jk][u][j][v][k][, such that]_
_∞_ _∞_

[P][m] _ρ(t, s)_ _p(e[−][t], e[−][s])_ _dtds < ϵ._ (25)

0 0 _|_ _−_ _|_

Z Z

_Proof. Fix any ϵ > 0. Consider the following transformation_

1

_uv_ _[ρ][(][−]_ [ln][ u,][ −] [ln][ v][)][,] _u, v_ (0, 1],

_R(u, v) =_ _∈_ (26)

0, _uv = 0._



This transformation preserves the norm with _ρ_ _L1([0,_ )2) = _R_ _L1([0,1]2)._
_∥_ _∥_ _∞_ _∥_ _∥_

First, according to the density of continuous functions in L[p] space (Rudin, 1987, Theorem
3.14), there exists R[˜] ∈ _C([0, 1][2]), such that ∥R −_ _R[˜]∥L1([0,1]2) < ϵ/2. Next, by the density of_
polynomials in the space of continuous functions (Lorentz, 2005, Theorem 6), there exists
a polynomial q(u, v) = _j,k=0_ _[c][jk][u][j][v][k][, such that][ ∥]R[˜]_ _q_ _L∞([0,1]2) < ϵ/2._ Finally, let
_−_ _∥_
_p(u, v) = uvq(u, v), we have_

[P][m] 1 1
_∞_ _∞_

_ρ(t, s)_ _p(e[−][t], e[−][s])_ _dtds =_

0 0 _|_ _−_ _|_ 0 0 _uv [p][(][u, v][)]_

Z Z Z Z

= ∥R − _q∥[R]L1[(]([0[u, v],1][)]2[ −])_ [1] _[dudv]_
_R_ _R_ _L1([0,1]2) +_ _R_ _q_ _L∞([0,1]2)_
_≤∥_ _−_ [˜]∥ _∥_ [˜] − _∥_
_< ϵ/2 + ϵ/2 = ϵ,_ (27)

which completes the proof.

Now we are ready to prove the universal approximation theorem.

_Proof of Theorem 4.1. According to the representation theorem (Lemma B.1), we have_

_∞_
_Ht(x) =_ _x[⊤]s[ρ][(][t, s][)][ds,]_ **_x_** _,_ (28)

0 _−_ _∀_ _∈X_

Z

where _ρ_ _L1([0,_ )2) = **_H_** _<_ . Therefore, by Lemma B.2, there exists pi(u, v) =
_m_ _∥_ _∥_ _∞_ _∥_ _∥_ _∞_
_j,k=1_ _[c]jk[(][i][)][u][j][v][k][,][ i][ = 1][,][ 2][, . . ., d][, where][ m][ is the maximal degree of][ {][p][i][}]i[d]=1[, such that]_
P _d_

_∞_ _∞_

_ρi(t, s)_ _pi(e[−][t], e[−][s])_ _dtds < ϵ._ (29)

_i=1_ Z0 Z0 _|_ _−_ _|_

X

Let

_c = u = 1m, V = W[˜]_ = diag (1, 2, . . ., m),
_−_

_W = diag( W,[˜]_ _W,[˜]_ _· · ·,_ _W[˜]_ ) ∈ R[dm][×][dm], U = diag(u, u, · · ·, u) ∈ R[dm][×][d], (30)

_M = PQ = (M1, M2, · · ·, Md) ∈_ R[m][×][dm], [Mi]jk = c[(]jk[i][)][,]

we get

_ρˆ(t, s)[⊤]_ = c[⊤]e[V t]PQe[W s]U = c[⊤]e[V t]Me[W s]U


= c[⊤]e[V t] _· (M1, M2, · · ·, Md) · diag(e[W s], e[W s], · · ·, e[W s]) · diag(u, u, · · ·, u)_

= (c[⊤]e[V t]M1, c[⊤]e[V t]M2, · · ·, c[⊤]e[V t]Md) · diag(e[W s]u, e[W s]u, · · ·, e[W s]u)

= (c[⊤]e[V t]M1e[W s]u, c[⊤]e[V t]M2e[W s]u, · · ·, c[⊤]e[V t]Mde[W s]u), (31)


-----

with

_ρˆi(t, s) = c[⊤]e[V t]Mie[W s]u = pi(e[−][t], e[−][s]),_ _i = 1, 2, . . ., d._ (32)

Therefore, by (24) and (29), we have


_∥H −_ **_H[ˆ]∥≤_**

=

which completes the proof.


_i=1_ _∥ρi −_ _ρˆi∥L1([0,∞)2)_

X

_d_ _∞_ _∞_

_ρi(t, s)_ _pi(e[−][t], e[−][s])_ _dsdt < ϵ,_ (33)

_i=1_ Z0 Z0 _−_

X


C General approximation rates

In this section, the proof of Theorem 4.2 is given. Again, by (24), the aim now is to
investigate the function approximation _ρ_ _ρˆ_ . Since one can handle each spatial dimension
_∥_ _−_ _∥_
separately (similarly with (30) and (31)), we firstly derive the estimates by assuming d = 1,
and then extend the obtained results to the case of multi-dimensional inputs (for general
_d ∈_ N+).

**Conditions on representation.** To characterise the accuracy of using the model
_c[⊤]e[V t]Me[W s]u (with M := PQ) to approximate the target ρ(t, s), the first stuff is to trans-_
late the conditions on the output (of piece-wise constant signals) to the representation.
Recall that y[c](t, s) = Ht(1( _,_ _s]), t, s_ 0, we get ρ(t, s) = _ds_ _[H][t][(][1][(][−∞][,][−][s][]][). Hence, the]_
_−∞_ _−_ _≥_ _−_ _[d]_

assumptions on y[c] in Theorem 4.2 is equivalent to the following smoothness and exponential
decay conditions on ρ. That is, there exist α ∈ N+, β > 0 such that

_ρ_ _C_ _[α]([0, +_ )[2]), (34)
_∈_ _∞_

_e[β][(][t][+][s][)][ ∂][k][+][l]_ _k, l_ N, k + l _α._ (35)

_∂t[k]∂s[l][ ρ][(][t, s][) =][ o][(1) as][ ∥][(][t, s][)][∥→∞][,]_ _∈_ _≤_

Note that the last decay condition implies


_∂[k][+][l]_
sup _β[−][(][k][+][l][)]e[β][(][t][+][s][)]_ _k, l_ N, k + l _α_ (36)
_t,s≥0_ _∂t[k]∂s[l][ ρ][(][t, s][)]_ _[≤]_ _[γ,]_ _∈_ _≤_


for some γ > 0.

C.1 Basics


Let Ω _∈_ R[d] be a bounded set. Define the spaces

_C_ _[α](Ω) := {f ∈_ _C(Ω) :[¯]_ _D[i]f ∈_ _C(Ω) for all[¯]_ _|i| ≤_ _α},_ _α ∈_ N, (37)

_C_ _[α,µ](Ω) := {f ∈_ _C_ _[α](Ω) : |D[i]f_ (x) − _D[i]f_ (y)| ≤ _K∥x −_ _y∥2[µ]_ [for some][ K >][ 0][,]
for all x, y Ωand **_i_** = α _,_ (38)
_∈_ _|_ _|_ _}_

and the “norm”

_D[i]f_ (x) _D[i]f_ (y)

_f_ _α,µ,Ω_ := sup sup _|_ _−_ _|_ _,_ _f_ _C_ _[α,µ](Ω),_ (39)
_|_ _|_ _|i|=α_ _x,y∈Ω_ _∥x −_ _y∥2[µ]_ _∀_ _∈_

with the shorthand ∥· ∥Ω := | · |0,0,Ω.

**Theorem C.1 (Multivariate Jackson’s theorem (Schultz, 1969), Theorem 4.10). Let Ω** _∈_ R[d]

_be a regular,_ [2] _bounded and open set, and f ∈_ _C_ _[α,µ](Ω)[¯]_ _for some α ∈_ N, µ ∈ [0, 1]. Then for
_any n ∈_ N+, we have

inf _f_ _p_ Ω¯ _[≤]_ _[C][(][α, µ][)]_ Ω[,] (40)
_p_ _n_ _∥_ _−_ _∥_ _n[α][+][µ][ |][f]_ _[|][α,µ,]_ [¯]
_∈P_ _[d]_

2It is proved that every bounded, open and convex set is regular. See Morrey (1966) (Lemma
3.4.1).


-----

_where_ _n_ _[denotes the set of all polynomials with the degree of no more than][ n][ in each]_
_P_ _[d]_
_variable, C(α, µ) > 0 is a universal constant only depending on α, µ and Ω._

A commonly used case is when µ = 0. That is, for f _C_ _[α](Ω), we get[¯]_
_∈_

_f_ _α,0,Ω¯_ _[≤]_ [2 max] Ω) _[<][ ∞][.]_ (41)
_|_ _|_ **_i_** =α
_|_ _|_ _[∥][D][i][f]_ _[∥][L][∞][(¯]_

For any x, x0 Ω, let ˜p(x) := p(x) + f (x0) _p(x0), and ˜e(x) := f_ (x) _p˜(x). Then ˜p_ _n[,]_
and _∈_ [¯] _−_ _−_ _∈P_ _[d]_

_|f_ (x) − _p˜(x)| ≤|e˜(x0)| + |e˜(x) −_ _e˜(x0)|_

_≤_ _x,ysup∈Ω[¯]_ _|e˜(x) −_ _e˜(y)| = |e˜|0,0,Ω¯_ [=][ ∥][f][ −] _p[˜]∥Ω¯_ _[,]_ _∀x ∈_ Ω[¯] _._ (42)


This gives the following convenient corollary.

**Corollary C.1. Let Ω** _∈_ R[d] _be a regular, bounded and open set, and f ∈_ _C_ _[α](Ω)[¯]_ _for some_
_α_ N. Then for any n N+, there exists p _n_ _[such that]_
_∈_ _∈_ _∈P_ _[d]_

_f_ _p_ _L∞(¯Ω)_ Ω)[,] (43)
_∥_ _−_ _∥_ _[≤]_ _[C]n[α][α][ max]|i|=α_ _[∥][D][i][f]_ _[∥][L][∞][(¯]_

_where_ _n_ _[denotes the set of all polynomials with the degree of no more than][ n][ in each]_
_P_ _[d]_
_variable, Cα > 0 is a universal constant only depending on α and Ω._

C.2 Proofs

Now we are ready to present the proof.

_Proof of Theorem 4.2. Step 1: domain transform. Consider the transform from the infinite_
domain [0, )[2] to the compact one [0, 1][2]:
_∞_

1

_uv_ _[ρ][(][−][c][0][ ln][ u,][ −][c][0][ ln][ v][)][,]_ _u, v_ (0, 1],

_R(u, v) =_ _∈_ (44)

0, _uv = 0,_



where c0 := (α + 1)/β > 0 is a fixed constant. A straightforward computation by induction
shows that, for any k, l ∈ N, k + l ≤ _α, and any u, v ∈_ (0, 1],


_∂[k][+][l]_

_∂u[k]∂v[l][ R][(][u, v][) = (]u[k][−][+1][1)]v[k][l][+][+1][l]_


_l_

_∂[i][+][j]_
_C(k, i)C_ _[′](l, j)c[i]0[+][j]_ (45)

_∂t[i]∂s[j][ ρ][(][−][c][0][ ln][ u,][ −][c][0][ ln][ v][)][,]_

_j=0_

X


_i=0_


where C(k, i), C _[′](l, j) are some integer constants, and (t, s) = (_ _c0 ln u,_ _c0 ln v) is a one-_
_−_ _−_
to-one mapping between (0, 1][2] and [0, )[2]. By (36), we get
_∞_


_∂[k][+][l]_

_∂u[k]∂v[l][ R][(][u, v][)]_


_∂[i][+][j]_

_∂t[i]∂s[j][ ρ][(][−][c][0][ ln][ u,][ −][c][0][ ln][ v][)]_


_|C(k, i)||C_ _[′](l, j)|c[i]0[+][j]_
_j=0_

X


_u[k][+1]v[l][+1]_


_i=0_


_∂[k][+][l]_

_c0, e[−]_ _c[s]0 )_

_∂u[k]∂v[l][ R][(][e][−]_ _[t]_

_[≤]_ _[e]_


_∂[i][+][j]_

_∂t[i]∂s[j][ ρ][(][t, s][)]_


(k+1) (l+1)

_c0_ _te_ _c0_


_|C(k, i)||C_ _[′](l, j)|c[i]0[+][j]_
_j=0_

X


_i=0_


_l_

_∂[i][+][j]_
_C(k, i)_ _C_ _[′](l, j)_ (α + 1)[i][+][j] _β[−][(][i][+][j][)]e[β][(][t][+][s][)]_
_|_ _||_ _|_ _·_ _∂t[i]∂s[j][ ρ][(][t, s][)]_
_j=0_

X

_l_

_C(k, i)_ _C_ _[′](l, j)_ (α + 1)[i][+][j]γ _C(α)γ,_ (46)
_|_ _||_ _|_ _≤_
_j=0_

X


_i=0_

_k_

_i=0_

X


-----

where C(α) > 0 is a universal constant only depending on α. According to the decay
condition (35), we get


_∂[i][+][j]_

_∂t[i]∂s[j][ ρ][(][−][c][0][ ln][ u,][ −][c][0][ ln][ v][)]_

= lim
_u[k][+1]v[l][+1]_ (t,s) (+ _,+_ ) _[e]_
_→_ _∞_ _∞_


(k+1) (l+1)

_c0_ _te_ _c0_ _s_ _[∂][i][+][j]_

_∂t[i]∂s[j][ ρ][(][t, s][)]_


lim
(u,v)→(0,0)


= lim (kα−+1α) _[βt]e_ (αl−+1α) _[βs]_ _e[β][(][t][+][s][)][ ∂][i][+][j]_
(t,s) (+ _,+_ ) _[e]_ _·_ _∂t[i]∂s[j][ ρ][(][t, s][)]_
_→_ _∞_ _∞_

= 0, (47)


(kα−+1α) _[βt]e_


and similarly for u0, v0 ∈ (0, 1],

_∂[i][+][j]_

_∂t[i]∂s[j][ ρ][(][−][c][0][ ln][ u,][ −][c][0][ ln][ v][)]_

lim = lim
(u,v)→(u0,0) _u[k][+1]v[l][+1]_ (t,s)→(−c0 ln u0,+∞) _[e]_


(l+1)

_c0_ _s_ _[∂][i][+][j]_

_∂t[i]∂s[j][ ρ][(][t, s][)]_


(k+1)

_c0_ _te_


= lim (kα−+1α) _[βt]e_ (αl−+1α) _[βs]_ _e[β][(][t][+][s][)][ ∂][i][+][j]_
(t,s) + _·_ _∂t[i]∂s[j][ ρ][(][t, s][)]_
_∥_ _∥→_ _∞_ _[e]_

= 0, (48)


(kα−+1α) _[βt]e_


_∂[i][+][j]_

_∂t[i]∂s[j][ ρ][(][−][c][0][ ln][ u,][ −][c][0][ ln][ v][)]_

= 0. (49)
_u[k][+1]v[l][+1]_

_∂[k][+][l]_

(u, v) [0, 1] 0 0 [0, 1], (50)
_∂u[k]∂v[l][ R][(][u, v][) = 0][,]_ _∈_ _× {_ _} ∪{_ _} ×_


lim
(u,v)→(0,v0)

This gives

and


_∂[k][+][l]_

_M0 :=_ max max (51)
_k,l_ N, k+l _α_ (u,v) [0,1][2] _∂u[k]∂v[l][ R][(][u, v][)]_
_∈_ _≤_ _∈_ _[≤]_ _[C][(][α][)][γ]_

by (46) and (50). Hence, R(u, v) _C_ _[α]([0, 1][2]) with bounded derivatives._
_∈_

_Step 2: polynomial approximation. According to Corollary C.1 and (51), we obtain that_
there exists R[˜]n _n[, such that]_
_∈P_ [2]

_∂[k][+][l]_

_R_ _Rn_ _L∞([0,1]2)_ max
_∥_ _−_ [˜] _∥_ _≤_ _[C]n[α][α]_ _k,l∈N, k+l=α_ _∂u[k]∂v[l][ R][(][u, v][)]_ _L[∞]([0,1][2])_

_,_ _n_ N+, (52)

_≤_ _[C][(]n[α][α][)][γ]_ _∀_ _∈_

whereR˜n(u, v C) (αR) >n( 0 is a universal constant only related tou, 0) _Rn(0, v) + R[˜]n(0, 0), we get R[ˆ]n_ _n α[with ˆ]. Furthermore, let Rn(u, 0) = R[ˆ]n(0R[ˆ], vn() = 0 foru, v) :=_
any u, v _−[0[˜], 1]. By (50), we have −_ [˜] _R(u, v) = 0 for any ( ∈Pu, v[2]_ ) [0, 1] 0 0 [0, 1], then
_∈_ _∈_ _× {_ _} ∪{_ _} ×_

_R_ _Rn_ _L∞([0,1]2)_ _R_ _Rn_ _L∞([0,1]2) +_ _Rn_ _Rn_ _L∞([0,1]2)_
_∥_ _−_ [ˆ] _∥_ _≤∥_ _−_ [˜] _∥_ _∥_ [˜] _−_ [ˆ] _∥_
_R_ _Rn_ _L∞([0,1]2) +_ _Rn(u, 0)_ _R(u, 0)_ _L∞([0,1]2)_
_≤∥_ _−_ [˜] _∥_ _∥_ [˜] _−_ _∥_
+ _Rn(0, v)_ _R(0, v)_ _L∞([0,1]2) +_ _Rn(0, 0)_ _R(0, 0)_ _L∞([0,1]2)_
_∥_ [˜] _−_ _∥_ _∥_ [˜] _−_ _∥_
≲ _∥R −_ _R[˜]n∥L∞([0,1]2),_ (53)

i.e. we can further require the approximator satisfying the zero half-boundary condition
(i.e. vanishing on (u, v) [0, 1] 0 0 [0, 1]) without effecting the approximation
_∈_ _× {_ _} ∪{_ _} ×_
accuracy. Let ¯m := min _mE, mD_, we get
_{_ _}_

1 1

_R_ _R_ _L∞([0,1]2)_ _C(α)γ_ + _,_ (54)
_∥_ _−_ [˜]∥ _≤_ _[C]m[(]¯[α][α][)][γ]_ _≤_ _m[α]E_ _m[α]D_

 

where


_R˜ := R[˜] ¯m_ _[∈P]m[2]¯_ _[,]_ _R˜(u, v) :=_


_r˜iju[i]v[j]._ (55)
_j=1_

X


_i=1_


-----

Let

_c = 1 ¯m[,]_ _u = 1m ¯_ _[,]_ _M = [˜rij] ∈_ Rm[¯] _×m ¯_ _,_ (56)
_V = −diag(2, 3, · · ·, ¯m + 1)/c0,_ _W = −diag(2, 3, · · ·, ¯m + 1)/c0,_ (57)

then by (24) and (54), we have

_∥H −_ **_H[ˆ]∥≤∥ρ −_** _ρˆ∥L1([0,∞)2)_ (58)

= _ρ(t, s)_ _c[⊤]e[V t]Me[W s]u_ _L[1]([0,_ )[2])
_−_ _∞_

= _ρ(t, s)_ _e[−]_ _c[t]0 e[−]_ _c[s]0 ˜R(e[−]_ _c[t]0, e[−]_ _c[s]0 )_
_−_ _L[1]([0,_ )[2])

_∞_
= c[2]0 _R_ _R˜_ _L[1]([0,1][2])_ (59)
_−_

1 1

_c[2]0_ _R_ _R˜_ _L[∞]([0,1][2])_ + _._ (60)
_≤_ _−_ _[≤]_ _[C]β[2][(][α]m¯[)][α][γ][ ≤]_ _[C][(]β[α][2][)][γ]_  _m[α]E_ _m[α]D_ 

That is, one can achieve an approximation accuracy scaling like (1/ ¯m)[α] with ¯m[2] parameters.
The proof is completed.

**Remark C.1. The extension to multi-dimensional inputs (general d ∈** N+) is found in the
_last paragraph of Appendix D.2._

**Remark C.2. Recall M = PQ ∈** R[m][D][×][m][E] _with P ∈_ R[m][D][×][N] _, Q ∈_ R[N] _[×][m][E]_ _, we only need_
_to investigate the case of N_ min _mE, mD_ = ¯m, since rank(M ) _m¯_ _._
_≤_ _{_ _}_ _≤_


D Approximation rates via temporal product structure

In this section, the proof of Theorem 4.3 is provided.

D.1 Proper Orthogonal Decomposition

Proper orthogonal decomposition (POD; (Liang et al., 2002), (Berkooz et al., 1993), (Chatterjee, 2000)) is a method for model reduction, which is commonly applied to numerical
PDEs and fluids mechanics. It can be viewed as an extension of singular value decomposition (SVD) and principal component analysis (PCA) to infinite-dimensional spaces.

Fix any R _L[∞]([0, 1][2])._ [3] Define the POD operator
_∈_

1 1
: φ(v) _R(u, v)φ(v)dv_ _R(u, v)du,_ _φ(v)_ _L[2][0, 1]._ (61)
_K_ _7→_ 0 0 _·_ _∈_
Z Z

**Proposition D.1. The operator** _is linear, bounded, compact, self-adjoint and non-_
_K_
_negative._


_Proof. (i) The linearity is obvious._

(ii) Let


1
: φ(v)
_R_ _7→_ 0
Z


_R(u, v)φ(v)dv,_ _φ(v)_ _L[2][0, 1],_ (62)
_∈_


then


1
( _φ)(v) =_ _R(u, v)(_ _φ)(u)du,_ _φ(v)_ _L[2][0, 1]._ (63)
_K_ 0 _R_ _∈_
Z

By the Cauchy-Schwartz inequality, we get


_∥Rφ∥L2[0,1] ≤∥R∥L2([0,1]2)∥φ∥L2[0,1],_ (64)

3Here, we use the same notation as (44), since the function defined there is also bounded.


-----

which gives

_∥Kφ∥L2[0,1] ≤∥R∥L2([0,1]2)∥Rφ∥L2[0,1] ≤∥R∥L[2]_ [2]([0,1][2])[∥][φ][∥][L][2][[0][,][1]][.] (65)

Note that R(u, v) _L[∞]([0, 1][2])_ _L[2]([0, 1][2]), hence both_ and are bounded operator
_∈_ _⊂_ _K_ _R_
from L[2][0, 1] to itself.

(iii) It is well-known that the Hilbert–Schmidt integral operator

1
( _ψ)(v) =_ _R(u, v)ψ(u)du,_ _ψ(u)_ _L[2][0, 1]_ (66)
_C_ 0 _∈_
Z

is a compact operator from L[2][0, 1] to itself. Therefore

_φ =_ _φ,_ _φ_ _L[2][0, 1],_ (67)
_K_ _CR_ _∈_

which gives = . Since is bounded and is compact, we get is also compact.
_K_ _CR_ _R_ _C_ _K_

(iv) By Fubini’s theorem, it is straightforward to verify that

1 1 1
_φ, ψ_ _L2[0,1] =_ _R(u, w)_ _R(u, v)φ(v)dv_ _du_ _ψ(w)dw_
_⟨K_ _⟩_ 0 0 0 _·_
Z Z Z 

1 1 1
= _R(u, w)R(u, v)φ(v)ψ(w)dvdudw_

0 0 0

Z Z Z

1 1 1
= _R(u, v)_ _R(u, w)ψ(w)dw_ _du_ _φ(v)dv_

0 0 0 _·_

Z Z Z 

= ⟨φ, Kψ⟩L2[0,1], _φ, ψ ∈_ _L[2][0, 1]._ (68)

(v) By Fubini’s theorem, it is straightforward to verify that

1 1
_φ, φ_ _L2[0,1] =_ ( _φ)(u)R(u, w)du_ _φ(w)dw_
_⟨K_ _⟩_ 0 0 _R_ _·_
Z Z

1 1
= ( _φ)(u)R(u, w)φ(w)dudw_

0 0 _R_

Z Z

1 1
= ( _φ)(u)_ _R(u, w)φ(w)dw_ _du_

0 _R_ 0

Z Z 

1
= ( _φ)[2](u)du_ 0, _φ_ _L[2][0, 1]._ (69)

0 _R_ _≥_ _∈_

Z

The proof is completed.

Combining (i)—(iv) and applying Hilbert–Schmidt’s expansion theorem, we obtain that
_L[2][0, 1] has an orthonormal basis {φn}n∈N_ _{ψξ}ξ∈Ξ, such that_

-  _φn = λnφn, λn_ = 0 for n, andS _ψξ = 0 for ξ_ Ξ, where is a finite or
_Kcountable set. If_ _̸_ is not finite, we have lim ∈N _K_ _n_ _λn = 0; ∈_ _N_
_N_ _→∞_

-  For any ψ _L[2][0, 1], we have_
_∈_

_ψ =_ _ψ, φn_ _L2[0,1]φn +_ _ψ, ψξ_ _L2[0,1]ψξ,_ (70)

_⟨_ _⟩_ _⟨_ _⟩_
_nX∈N_ _ξX∈Ξ_

where the second summation has at most countable non-zero terms, and


_ψ =_
_K_


_λn⟨ψ, φn⟩L2[0,1]φn._ (71)
_nX∈N_


Here, all the series converge under the norm ∥· ∥L2[0,1]. Without loss of generality, N =
1, 2, _, N0_ for N0 N+ or N0 = + (i.e. = N+). By (69), we get
_{_ _· · ·_ _}_ _∈_ _∞_ _N_

0 ≤⟨Kφn, φn⟩L2[0,1] = λn⟨φn, φn⟩L[2] [2][0,1] [=][ λ][n][,] _∀n ∈N_ _,_ (72)


-----

i.e.n all the eigenvalues of. In addition, limn K are non-negative, andλn = 0 implies that one can index all the eigenvalues in a λn ̸= 0 for n ∈N implies λn > 0,
_∀_ _∈N_ _→∞_
non-increasing sequence: λ1 ≥ _λ2 ≥· · · ≥_ _λn ≥· · · ≥_ 0.

Then, we can present the POD estimate.
**Theorem D.1. For any R** _L[∞]([0, 1][2]), we have_
_∈_

1 _N_ 2 _N0_
Z0 _n=1⟨R(u, v), φn(v)⟩L2[0,1]φn(v)_ _L[2][0,1]_ _du =_ _n=N_ +1 _λn,_ _∀N ∈_ N. (73)

X X

_[R][(][u, v][)][ −]_

_Proof. Combining (69) and (72) gives_


1
_λn =_ _φn, φn_ _L2[0,1] =_
_⟨K_ _⟩_ 0
Z


( _φn)[2](u)du,_ _n_ _._ (74)
_R_ _∀_ _∈N_


Similarly,
1

( _ψξ)[2](u)du =_ _ψξ, ψξ_ _L2[0,1] =_
0 _R_ _⟨K_ _⟩_

Z

which gives


_λn⟨ψξ, φn⟩L[2]_ [2][0,1] [= 0][,] _∀ξ ∈_ Ξ, (75)
_nX∈N_


1
( _ψξ)(u) =_ _R(u, v)ψξ(v)dv = 0,_ _a.e. u_ [0, 1]. (76)
_R_ 0 _∈_
Z

Notice that Ru(v) := R(u, v) _C_ _[α][0, 1]_ _L[2][0, 1] for any u_ [0, 1]. By (70) and (76), we
_∈_ _⊂_ _∈_
get

_Ru_ _L[2][0,1]_ [=] _Ru, φn_ _L2[0,1]φn +_ _Ru, ψξ_ _L2[0,1]ψξ,_
_∥_ _∥[2]_ _⟨_ _⟩_ _⟨_ _⟩_

-  Xn∈N _ξX∈Ξ_

_Ru, φn_ _L2[0,1]φn +_ _Ru, ψξ_ _L2[0,1]ψξ_
_nX∈N_ _⟨_ _⟩_ _ξX∈Ξ⟨_ _⟩_ +L[2][0,1]

= _Ru, φn_ _L[2][0,1]_ [+] _Ru, ψξ_ _L[2][0,1]_

_⟨_ _⟩[2]_ _⟨_ _⟩[2]_
_nX∈N_ _ξX∈Ξ_

= ( _φn)[2](u),_ _a.e. u_ [0, 1]. (77)

_R_ _∈_
_nX∈N_

Hence for any N ∈ N, we have

2

_N_ _N_

_n=1⟨Ru, φn⟩L2[0,1]φn_ _L[2][0,1]_ = ∥Ru∥L[2] [2][0,1] _[−]_ _n=1⟨Ru, φn⟩L[2]_ [2][0,1] (78)

X X

_N_

_[R][u][ −]_

= ( _φn)[2](u)_ ( _φn)[2](u)_

_R_ _−_ _R_
_nX∈N_ _nX=1_

_N0_

= ( _φn)[2](u),_ (79)

_R_
_n=N_ +1

X

where the summation is zero by convention if the subscript is larger than the superscript.
This by (74) implies


1

0

Z


_N0_

_λn._ (80)
_n=N_ +1

X


Z0 _n=1⟨R(u, v), φn(v)⟩L2[0,1]φn(v)_ _L[2][0,1]_ _du =_ _n=N_ +1 _λn._ (80)

X X

Here, the equality holds as a consequence of Beppo Levi’s monotone convergence lemma

_[R][(][u, v][)][ −]_

and Lebesgue’s dominated convergence theorem, and one has _n_ _[λ][n][ <][ +][∞][. In fact, for]_
_∈N_

[P]


-----

_N0 = +_, let Sn = _k=1_ _[λ][k][, we get][ S][n][ increasing (since][ λ][k][ ≥]_ [0 for all][ k][ ∈N] [). By (74)]
_∞_
and (78), we have

_n_ 1 1 _n_ 1

[P][n]

_Sn =_ _k=1_ Z0 (Rφk)[2](u)du = Z0 _k=1⟨Ru, φk⟩L[2]_ [2][0,1][du][ ≤] Z0 _∥Ru∥L[2]_ [2][0,1][du][ =][ ∥][R][∥]L[2] [2]([0,1][2])[,]

X X

(81)

which gives that Sn converges as n . The proof is completed.
_→∞_

**Remark D.1. Let ϕn(u) := ⟨R(u, v), φn(v)⟩L2[0,1], then we have the POD estimate**
_R(u, v)_ _n_ _[ϕ][n][(][u][)][φ][n][(][v][)][, where the error is characterised by the tail sum of eigenvalues of]_
_≈_ [P]

_the POD operator._

Recall the POD operator defined in (61). We similarly define

1 1
˜ : φ(v) _R˜(u, v)φ(v)dv_ _R[˜](u, v)du,_ _φ(v)_ _L[2][0, 1],_ (82)
_K_ _7→_ 0 0 _·_ _∈_
Z Z

where R[˜] is defined as (55), i.e. the approximator constructed in the general approximation
theorem before. Obviously, as a polynomial, R[˜] _L[∞]([0, 1][2]). Hence, by Proposition D.1,_
_∈_
˜ is also linear, bounded, compact, self-adjoint and non-negative. In addition, according to
_K_
Theorem D.1, we have the following POD estimate
1 _N_ 2 _N˜0_
Z0 _R˜(u, v) −_ _n=1_ ˜R(u, v), _φ[˜]n(v)_ _L[2][0,1]_ _φ[˜]n(v)_ _L[2][0,1]_ _du =_ _n=N_ +1 _λ˜n,_ (83)

X X

where _λn_ _Nn˜=10_ [are eigenvalues of ˜] satisfying λ[˜]1 _λ2_ _λn_ 0, and _φn_ _Nn˜=10_
_L[2][0, 1] are the corresponding orthonormal eigenfunctions, i.e. {[˜]_ _}_ _K_ _≥_ [˜] _≥· · · ≥_ [˜] _≥· · · ≥˜φn = λ[˜]nφ[˜]n, {λ[˜]n[˜] >} 0 for[⊂]_
_K_ [˜]
_n ∈{1, 2, · · ·,_ _N[˜]0}._

**Lemma D.1.** _K[˜] is a finite-rank operator. That is,_ _N[˜]0 ≤_ _m¯_ = min{mE, mD} < ∞.

_Proof. Let ˜ϕn(u) :=_ ˜R(u, v), _φ[˜]n(v)_ _L[2][0,1][. We first show that both ˜]ϕn(u) and φ[˜]n(v) are_

_m_ _m ¯_
polynomials. In fact, since R[˜](u, v) = _i=1_ _j=1_ _r[˜]iju[i]v[j], we have_

_m¯_ _m¯_ P

_∂[k]_ _i!_

[P][ ¯]

_R(u, v)_ _r˜ij_
_∂u[k][ ˜]_ [=] (i _k)!_ _[u][i][−][k][v][j]_

_i=k_ _j=1_ _−_

X X

_m¯_ _m¯_

_i!_
_r˜ij_ _m),_ _k = 1, 2,_ _,_ (84)

_≤_ _i=k_ _j=1_ _|_ _|_ (i − _k)!_ [≜] _[C][1][(][k,][ ¯]_ _· · ·_

X X

with the convention that the summation is zero if the subscript is larger than the superscript,
i.e. C1(k, ¯m) = 0 for any k > ¯m. Let C1( ¯m) := max{∥R[˜]∥L∞([0,1]2), max1≤k≤m¯ _[C]1[(][k,][ ¯]m)},_
then we have
_∂[k]_

_R(u, v)φ[˜]n(v)_ _m)_ _φn(v)_ _L[2][0, 1]_ _L[1][0, 1],_ _k = 0, 1,_ _._ (85)
_∂u[k][ ˜]_ _[≤]_ _[C][1][( ¯]_ _|_ [˜] _| ∈_ _⊂_ _· · ·_

According to Lebesgue’s dominated convergence theorem, we get by induction that

1 1

_d[k]_ _ϕn(u) =_ _[d][k]_ _R˜(u, v)φ[˜]n(v)dv =_ _∂[k]_ _R(u, v)φ[˜]n(v)dv,_ _k = 0, 1,_ _._ (86)

_du[k][ ˜]_ _du[k]_ 0 0 _∂u[k][ ˜]_ _· · ·_

Z Z

Similarly, we get

_m¯_ _m¯_

_∂[k]_ _j!_

_R(u, v)_ _r˜iju[i]_
_∂v[k][ ˜]_ [=] (j _k)!_ _[v][j][−][k]_

_i=1_ _j=k_ _−_

X X


_m¯_

_j!_
_r˜ij_ _m),_ _k = 1, 2,_ _,_ (87)
_j=k_ _|_ _|_ (j − _k)!_ [≜] _[C][2][(][k,][ ¯]_ _· · ·_

X


_i=1_


-----

and C2(k, ¯m) = 0 for any k > ¯m. Let C2( ¯m) := max{∥R[˜]∥L∞([0,1]2), max1≤k≤m¯ _[C]2[(][k,][ ¯]m)},_
then for k = 0, 1,, we have
_· · ·_

1

_∂[k]_ _R(u, v)_ _R˜(u, v)φ[˜]n(v)dv_ _m)_ _R_ _L∞([0,1]2)_ _φn_ _L1[0,1]_

_∂v[k][ ˜]_ Z0 _[≤]_ _[C][2][( ¯]_ _∥_ [˜]∥ _∥_ [˜] _∥_

_C2[2][( ¯]m)_ _φn_ _L2[0,1] = C2[2][( ¯]m)_ _L[1][0, 1]._ (88)
_≤_ _∥_ [˜] _∥_ _⊂_

According to Lebesgue’s dominated convergence theorem, we get by induction that

1 1

_dvd[k][k][ ˜]φn(v) = λ˜[1]n_ 0 _∂v∂[k][k][ ˜]R(u, v)_ 0 _R˜(u, v)φ[˜]n(v)dvdu,_ _k = 0, 1, · · · ._ (89)

Z Z

That is, ˜ϕn, _φ[˜]n ∈_ _C_ _[∞][0, 1] for any n = 1, 2, · · ·,_ _N[˜]0._ Since R[˜]d([k]u, 0) = R[˜](0d, v[k] ) = 0 for
_u, v_ [0, 1], we get ˜ϕn(0) = φ[˜]n(0) = 0. Furthermore, we have _du[k][ ˜]ϕn(u) =_ _dv[k][ ˜]φn(v) = 0_

forN˜0 k > ≤ ∈m <¯ ¯m ∞, hence ˜. [4] The proof is completed.ϕn, _φ[˜]n ∈Pm ¯_ [. Since][ {]φ[˜]n}Nn˜=10 _[⊂]_ _[L][2][[0][,][ 1] are orthonormal, we must have]_

D.2 Approximation rates


**Perturbation of eigenvalues.** First, we need to bound the gap between the eigenvalues
_λn_ _Nn˜=10_ [and][ {][λ][n][}]n[N]=1[0] [(corresponding to the function][ R][ defined in (44)). The following]
_{[˜]_ _}_
theorem is necessary.
**Theorem D.2 (Courant–Fischer–Weyl min-max principle; Lax (2002) (Chapter 28, The-**
orem 4)). Let _be a compact, self-adjoint operator on a Hilbert space_ _, whose positive_
_B_ _H_
_eigenvalues are listed in a decreasing order µ1 ≥_ _µ2 ≥· · · ≥_ _µk ≥· · · > 0. Then_

max min (90)
_Sk_ _x∈Sk, ∥x∥H=1[⟨B][x, x][⟩][H][ =][ µ][k][,]_

_where_ _k_ _is any k-dimensional linear subspace._
_S_ _⊂H_

Based on it, we have the following lemma to characterise the perturbation of singular values.
**Lemma D.2. For any R1, R2 ∈** _L[∞]([0, 1][2]), we have the estimate_

_λ[R]k_ [1] _λ[R]k_ [2] (91)
_−_ _[≤∥][R][1][ −]_ _[R][2][∥][L][2][([0][,][1]][2][)][.]_

q q

_Proof. According to Theorem D.2 and by (69), we have_

_λ[R]k_ [= max] min min _L[2][0,1][.]_ (92)
_Sk_ _φ∈Sk, ∥φ∥L2[0,1]=1[⟨K][R][φ, φ][⟩][L][2][[0][,][1]][ = max]Sk_ _φ∈Sk, ∥φ∥L2[0,1]=1_ _[∥R][R][φ][∥][2]_

Note that RR1 −RR2 = RR1−R2 and by (64), we have

_λ[R]k_ [1] = max min

q _Sk_ _φ∈Sk, ∥φ∥L2[0,1]=1_ _[∥R][R][1][φ][∥][L][2][[0][,][1]]_

max min ( _R1_ _R2)φ_ _L2[0,1] +_ _R2φ_ _L2[0,1]_
_≤_ _Sk_ _φ∈Sk, ∥φ∥L2[0,1]=1_ _∥_ _R_ _−R_ _∥_ _∥R_ _∥_

  

_≤_ maxSk _φ∈Sk, ∥minφ∥L2[0,1]=1_ _∥RR1−R2∥_ + ∥RR2φ∥L2[0,1]

  

max min
_≤_ _Sk_ _φ∈Sk, ∥φ∥L2[0,1]=1_ _[∥R][R][2][φ][∥][L][2][[0][,][1]][ +][ ∥][R][1][ −]_ _[R][2][∥][L][2][([0][,][1]][2][)]_

= _λ[R]k_ [2] + _R1_ _R2_ _L2([0,1]2),_ (93)

_∥_ _−_ _∥_

4 q _m_
In fact, for any p _m ¯_ [with][ p][(0) = 0, we have][ p][ ∈] [span][{][v, v][2][,][ · · ·][, v][ ¯] . Through a standard
Schmidt-orthogonalization, we can get ∈P _ek(v)_ _k, k = 1, 2,_ _, ¯m, such that}_ _ei, ej_ _L2[0,1] =_
_∈P_ _· · ·_ _⟨_ _⟩_
_δpij(0) = 0, and p, q ∈(0) = 0, then their coordinates under the basisspan{e1(v), e2(v), · · ·, em ¯_ [(][v][)][}][. That is, if][ ⟨][p, q]e[⟩]kL[2]mk[0[¯]=1,1] [are orthogonal. Hence, the ˜][= 0 for some][ p, q][ ∈P]m¯ [with]N0
_{_ _}_
orthogonal ¯m-dimensional coordinates here leads to at most ¯m non-zeros.


-----

and similarly,

_λ[R]k_ [2] = max min

q _Sk_ _φ∈Sk, ∥φ∥L2[0,1]=1_ _[∥R][R][2][φ][∥][L][2][[0][,][1]]_

max min ( _R2_ _R1)φ_ _L2[0,1] +_ _R1φ_ _L2[0,1]_
_≤_ _Sk_ _φ∈Sk, ∥φ∥L2[0,1]=1_ _∥_ _R_ _−R_ _∥_ _∥R_ _∥_

 

_≤_ maxSk _φ∈Sk, ∥minφ∥L2[0,1]=1_ _∥RR2−R1∥_ + ∥RR1φ∥L2[0,1]

  

max min
_≤_ _Sk_ _φ∈Sk, ∥φ∥L2[0,1]=1_ _[∥R][R][1][φ][∥][L][2][[0][,][1]][ +][ ∥][R][2][ −]_ _[R][1][∥][L][2][([0][,][1]][2][)]_


= _λ[R]k_ [1] + _R1_ _R2_ _L2([0,1]2),_ (94)

_∥_ _−_ _∥_

q

which completes the proof.

**Proofs.** Now we are ready to derive the final estimate.

_Proof of Theorem 4.3. By Lemma D.2 and (54), we get_


_λk_ _λ˜k_ _R_ _L2([0,1]2)_ _R_ _R_ _L∞([0,1]2)_ _._ (95)
_−_ _[≤∥][R][ −]_ [˜]∥ _≤∥_ _−_ [˜]∥ _≤_ _[C]m[(]¯[α][α][)][γ]_
q

p

Combining (54), (83) and (95) gives that

1 _N_ _e[−]_ _c[t]0 ˜ϕn(e[−]_ _c[t]0 )_ _e[−]_ _c[s]0 ˜φn(e[−]_ _c[s]0 )_

_c[2]0_ _nX=1_ _·_ _L[1]([0,∞)[2])_

_N_

_[ρ][(][t, s][)][ −]_

= _ϕ˜n(u)φ[˜]n(v)_

_n=1_ _L[1]([0,1][2])_

X

_N_

_≤_ _R[R]([(]u, v[u, v])[)] −[ −]_ _R˜(u, v)_ _L[1]([0,1][2])_ [+] _R˜(u, v) −_ _ϕ˜n(u)φ[˜]n(v)_

_n=1_ _L[1]([0,1][2])_

X

_N_

_≤_ _R(u, v) −_ _R˜(u, v)_ _L[∞]([0,1][2])_ [+] _R˜(u, v) −_ _ϕ˜n(u)φ[˜]n(v)_

_n=1_ _L[2]([0,1][2])_

X

_N˜0_ _N˜0_ _N˜0_

+ _λ˜n_ + _λn_ _λn_ + _λn_

_≤_ _[C]m[(]¯[α][α][)][γ]_ vun=N +1 _≤_ _[C]m[(]¯[α][α][)][γ]_ vun=N +1 _|[˜]_ _−_ _|_ _n=N_ +1

u X u X X
t t


_≤_ _[C]m[(]¯[α][α][)][γ]_

_≤_ _[C]m[(]¯[α][α][)][γ]_

≲ _C(α)γ_


_N˜0_

_λn +_
_n=N_ +1

X

_N˜0_

_λn +_
_n=N_ +1

X


_N˜0_

_n=N_ +1

X

_N˜0_

_n=N_ +1

X

1

_·_ _m¯_ _[α][ +]_


_λ˜n +_

_√2_

v
u
u

_N˜t0_


_λ˜n_
_−_

_λ˜n_
_−_


_λn_

_λn_


_λn_

p

_N˜0_

_n=N_ +1

X


_λn_

1

_m¯_ _[α/][2]_


_λ˜n_
_−_


_λn_


_N˜0_

_λn +_
_n=N_ +1

X


1 1

≲ _C(α)γ_ 1 + _N˜0_ _N_ _λn +_ _λn_ (96)

 q _−_  _·_ _m¯_ _[α][ +]_ vuun=XN +1 vuun=XN +1 p _·_ _m¯_ _[α/][2]_ 

t t

where ≲ hides universal positive constants.  _[,]_

For the corresponding parameters, recall that ˜ϕn, _φ[˜]n_ _m ¯_ [with ˜]ϕn(0) = φ[˜]n(0) = 0, we can
write _∈P_


1 +


_ϕ˜n(u) =_


_Pinu[i],_ _φ˜n(v) =_
_i=1_

X


_Qnjv[j]_ (97)
_j=1_

X


-----

for any n = 1, 2, · · ·, _N[˜]0. Let_

_c = 1 ¯m[,]_ _u = 1m ¯_ _[,]_ (98)
_V = −diag(2, 3, · · ·, ¯m + 1)/c0,_ _W = −diag(2, 3, · · ·, ¯m + 1)/c0,_ (99)

_M = PQ with P = [Pin] ∈_ Rm[¯] _×N_ _, Q = [Qnj] ∈_ RN _×m ¯_ _,_ (100)

then we have


_e[−]_ _[i]c[+1]0_ _[t]Pin_
_i=1_ _·_

X


_Qnje[−]_ _[j]c[+1]0_ _[s]_
_j=1_

X


_c[⊤]e[V t]Me[W s]u = c[⊤]e[V t]P_ _Qe[W s]u =_
_·_


_n=1_


_N_

= _e[−]_ _c[t]0 ˜ϕn(e[−]_ _c[t]0 )_ _e[−]_ _c[s]0 ˜φn(e[−]_ _c[s]0 )._ (101)

_·_

_n=1_

X

Plugging this into (96) gives


_∥H −_ **_H[ˆ]∥≤∥ρ −_** _ρˆ∥L1([0,∞)2)_ (102)

= _ρ(t, s)_ _c[⊤]e[V t]Me[W s]u_ _L[1]([0,_ )[2])
_−_ _∞_

1 _N˜0_ _N˜0_ 1

≲ _[C][(][α][)][γ]_ 1 + _N˜0_ _N_ _λn +_ _λn_

_β[2]_  q _−_  _·_ _m¯_ _[α][ +]_ vuun=XN +1 vuun=XN +1 p _·_ _m¯_ _[α/][2]_ 

t t (103)

  _[,]_

and the number of trainable parameters is 2N ¯m. Together with Lemma D.1, the proof is
completed.

**Extension to multi-dimensional inputs.** The above results can be naturally extended
to the general case where a d-dimensional input is given (∀d ∈ N+). In fact, let
_ρi(t, s) −_ _c[⊤]e[V t]Mie[W s]u_ _L[1]([0,∞)[2])_ [≲] _[ϵ,]_ _i = 1, 2, · · ·, d,_ (104)

for some 0 < ϵ 1, then we take
_≪_

_M = (M1, M2, · · ·, Md) ∈_ Rm[¯] _×dm ¯_ _,_ (105)

_m_ _dm ¯_ _m_ _d_
_W = diag(W, W,_ _, W_ ) R[d][ ¯] _×_ _,_ _U = diag(u, u,_ _, u)_ R[d][ ¯] _×_ _,_ (106)
_· · ·_ _∈_ _· · ·_ _∈_

and have

_c[⊤]e[V t]Me[W s]U = c[⊤]e[V t]_ _· (M1, M2, · · ·, Md) · diag(e[W s], e[W s], · · ·, e[W s]) · diag(u, u, · · ·, u)_

= (c[⊤]e[V t]M1, c[⊤]e[V t]M2, · · ·, c[⊤]e[V t]Md) · diag(e[W s]u, e[W s]u, · · ·, e[W s]u)

= (c[⊤]e[V t]M1e[W s]u, c[⊤]e[V t]M2e[W s]u, · · ·, c[⊤]e[V t]Mde[W s]u). (107)

If the form PQ(= M ) is required, it is sufficient to take Mi = PiQi, i = 1, 2, _, d, and_
_· · ·_

_P = (P1, P2, · · ·, Pd) ∈_ Rm[¯] _×dN_ _, Q = diag(Q1, Q2, · · ·, Qd) ∈_ RdN _×dm ¯_ _._ (108)

Therefore, we obtain

_d_

_i=1_ _ρi(t, s) −_ _c[⊤]e[V t]Me[W s]U_ _i_ _L[1]([0,∞)[2])_ [≲] _[dϵ,]_ (109)

X  

with the number of parameters increased by d-times compared to the corresponding onedimensional setting.

D.3 Case analysis

**Bounds under different cases.** Now we make the comparison between (102) and (58).
Recall that {λn}n[N]=1[0] [is a positive decreased sequence (with lim][n][→∞] _[λ][n][ = 0 and][ P]n[∞]=1_ _[λ][n][ ≤]_
_R_ _L[2]([0,1][2])_ [by (81), if][ N][0][ = +][∞][), and ˜]N0 _m¯_, we have the following cases.
_∥_ _∥[2]_ _≤_


-----

-  if N[˜]0 = o( ¯m), we set N = N[˜]0 in (102) and get the same bound as (58), but the
number of parameters is only ( N[˜]0 ¯m) = o( ¯m[2]);
_O_

-  if N[˜]0 = ( ¯m), then (102) implies that
_O_
_ρ(t, s)_ _c[⊤]e[V t]Me[W s]u_ _L[1]([0,_ )[2])
_−_ _∞_

≲ _[C][(][α][)][γ]_ 1 + _√m¯_ _N_ 1 _m¯_ _λn +_ _m¯_ _λn_ 1

_β[2]_  _−_ _·_ _m¯_ _[α][ +]_ vun=N +1 vun=N +1 _·_ _m¯_ _[α/][2]_ 

  u X u X p 

t t


≲ _[C][(][α][)][γ]_

_β[2]_


1

_m¯_ _[α][−]_ 2[1] [+]


_λn +_
_n=N_ +1

X


(110)




 _[.]_

1

_m¯_ _[α/][2]_ 




_λn_
_·_


_m¯_ _[α/][2]_

_λn_
_·_

p


_n=N_ +1


We are supposed to require that



1

_m¯_ _[α][−]_ [1]2 [≳] [max] 




_λn,_
_n=N_ +1

X


_n=N_ +1


1

(111)
_m¯_ _[α][−][1][ .]_


_λn ≲_
_n=N_ +1

X


_λn ≲_


_m¯_ [2][α][−][1][,]


_n=N_ +1


We give the following typical examples to illustrate sufficient conditions to guarantee
(111).

**– If λn =** (n[−][r]) with r > 2α + 1 3 (α N+), since
_O_ _≥_ _∈_


_n[−][r]_ _≤_
_n=N_ +1 Z

X

we get by (111) that


_x[−][r]dx =_


_r > 1,_ (112)
_∀_

2rα−−11, (113)


_N_ _[r][−][1][ −]_


_r_ 1
_−_


_m¯_ _[r][−][1]_


_m_

_n[−][r]_ ≲
_n=N_ +1

X


1

_m_
_m¯_ [2][α][−][1][ ⇔] _[N][ ≳]_ [¯]


_λn ≲_
_n=N_ +1

X


1

_N_ _[r][−][1][ ≲]_


_m¯_ _m¯_ 1 1 _αr_ _−1_

_n=N_ +1 _λn ≲_ _n=N_ +1 _n[−]_ _[r]2 ≲_ _N_ _r2_ _[−][1][ ≲]_ _m¯_ _[α][−][1][ ⇔]_ _[N][ ≳]_ _m[¯]_ 2 _[−][1]_ _._ (114)

X p X

Assume that N _m¯_ _[δ]_ with δ [0, 1), then by (113) and (114), we require
_∼_ _∈_
_δ ≥_ max{ [2]r[α]−[−]1[1] _[,][ α]r2_ _[−][−][1][1]_ _[}][, i.e.]_

2α 1 _α_ 1
_r_ max _−_ + 1, 2 _−_ + 1 = [2][α][ −] [1] + 1. (115)
_≥_ _δ_ _δ_ _δ_
  

Meanwhile, the POD-estimate (110) achieves an accuracy scaling like (1/ ¯m)[α][−] 2[1]

with ( ¯m[1+][δ]) parameters, while under the same capacity, the accuracy of
_O_ _α(1+δ)_
(58) scales like (1/ ¯m) 2 . The former beats the latter if α 2 _[>][ α][(1+]2_ _[δ][)]_,

_−_ [1]

i.e. _δ < 1_ 1/α (α 2). When δ = 1 1/α, (115) becomes r
_−_ _≥_ _−_ _≥_
max [2]α[α][2][−]1[1] _[,][ 2(][α][ + 1)][}][ =][ 2]α[α][2][−]1[1]_ [. That is to say,][ r][∗] [:=][ 2]α[α][2][−]1[1] can be viewed

as an upper bound of the critical point where the two estimates are compa-{ _−_ _−_ _−_
rable. When r > r[∗], the POD-estimate outperforms the non-POD-estimate,
and this effect gets more notable with r increasing. In fact, when r > r[∗], we

take N = ¯m 2rα−−11 > ¯m _αr2_ _−[−]1[1]_ (hence satisfying (113) and (114)), which gives an

((1/ ¯m)[α][−] [1]2 ) approximation error with ( ¯m[1+][ 2]r[α]−[−]1[1] ) parameters for the POD_O_ _O_

estimate, while ( ¯m[2][−] _α[1] ) trainable parameters are needed to achieve the same_
_O_

accuracy using the non-POD-estimate. Since [2]r[α][−]1[1] _[<][ 1][ −]_ _α[1]_ [, we get that the]

_−_

POD-estimate outperforms the non-POD-estimate. When r +, the num_→_ _∞_
ber of trainable parameters for the POD-estimate is ( ¯m), much better than
_O_
the non-POD-estimate.


-----

**Remark D.2. Let**

1
_K(v, w) :=_

0

Z

_we get_


_R(u, v)R(u, w)du,_ _v, w_ [0, 1], (116)
_∈_


1
( _φ)(w) =_ _K(v, w)φ(v)dv,_ _φ_ _L[2][0, 1]._ (117)
_K_ 0 _∈_
Z

_Recall that R_ _C_ _[α]([0, 1][2])_ _L[∞]([0, 1][2]), we get K_ _L[∞]([0, 1][2])_ _L[2]([0, 1][2]),_
_∈_ _⊂_ _∈_ _⊂_
_and hence_ _can be also viewed as a Hilbert-Schmidt integral operator with the_
_K_
_kernel K, where K is obviously symmetric as K(v, w) = K(w, v), and positive_
1 1
_definite since 0_ _φ, φ_ _L2[0,1] =_ 0 0 _[K][(][v, w][)][φ][(][v][)][φ][(][w][)][dvdw][,][ φ][ ∈]_ _[L][2][[0][,][ 1]]_
_≤⟨K_ _⟩_
_by (69). Since the derivatives of R (up to α-order) are continuous on [0, 1][2]_
R R
_(hence bounded and integrable), we get K_ _C_ _[α,α]([0, 1][2]) (α-differentiable for_
_∈_
_both arguments). According to Chang & Ha (1999) (Theorem 1), we have_


_∞_

_n=N_ +2α+1 _λn ≲_ _N_ _[−][α],_ _∀N ∈_ N+ ⇒ _λn = O(n[−][(][α][+1)]),_ _n →∞._ (118)

X

_That is to say, this general setting (only assume smoothness of the kernel)_
_can not guarantee the sufficient condition provided here (λn =_ (n[−][r]) with
_O_
_r >_ [2]α[α][2][−]1[1] _[), i.e. the point that the POD-estimate outperforms the non-POD-]_

_estimate requires a faster decay of the eigenvalues.−_

**– If λn =** (e[−][ωn]) with ω > 0, we get by (111) that
_O_

_m¯_ _∞_ 1 ¯m[2][α][−][1]

_λn ≲_ _e[−][ωn]_ = _[e][−][ω][(][N]_ [+1)] 1,

1 _e[−][ω][ ≲]_ _m¯_ [2][α][−][1][ ⇔] _[N][ ≳]_ _ω[1]_ [ln] 1 _e[−][ω]_ _−_

_n=N_ +1 _n=N_ +1 _−_  _−_ 

X X

(119)

_m¯_

_∞_ 2 [(][N] [+1)] 1 _m¯_ _[α][−][1]_

_λn ≲_ _e[−]_ _[ω]2_ _[n]_ = _[e]1[−]_ _[ω]_ _e[−]_ _[ω]2_ ≲ _m¯_ _[α][−][1][ ⇔]_ _[N][ ≳]_ _ω[2]_ [ln] 1 _e[−]_ _[ω]2_ _−_ 1.

_n=N_ +1 _n=N_ +1 _−_  _−_ 

X p X

(120)


That is to say, for any α ∈ N+, ω > 0, we have N ∼ (2α − 1) ln ¯m. This
implies an ((1/ ¯m)[α][−] 2[1] ) approximation error with ( ¯m ln ¯m) parameters for
_O_ _O_

the POD-estimate, while ( ¯m[2][−] _α[1] ) parameters are needed to achieve the same_
_O_

accuracy using the non-POD-estimate.

**– If N0 < +∞** (i.e. K is a finite rank operator by (71)), one can just take N = N0
and get by (110) an ((1/m ¯ )[α][−] 2[1] ) approximation error. For ¯m N+ sufficiently
_O_ _∈_

large such that ¯m _N0[κ]_ [for some][ κ][ ≫] [1, the number of trainable parameters for]

the POD-estimate is ∼ _O(N ¯m) = O(N0[κ][+1]), while O_ _N0κ(2−_ _α[1]_ [)] parameters are

needed to achieve the same accuracy using the non-POD-estimate. Obviously, 
if α 2, we get κ(2 _α_ [)][ ≥] [3]2 _[κ][ ≫]_ _[κ][ + 1.]_
_≥_ _−_ [1]

**Eigenvalue approximation.** One can apply (91) in Lemma D.2 to estimate the eigenvalues _λn_ _n=1[, where][ R][1][ =][ R][ and][ R][2][ is taken as some approximator of][ R][, say ˆ]R. Let_
_λˆ := λ {R[ˆ], we recall (91) as}[N][0]_

_λk −_ qλˆk _[≤∥][R][ −]_ _R[ˆ]∥L2([0,1]2)._ (121)

p

We provide a naive method here. That is, one can take R[ˆ] as a piece-wise constant approximation of R. Fix any n ∈ N+. Let I0 := {0}, Ii := ( _[i][−]n[1]_ _[,][ i]n_ [] for][ i][ = 1][,][ 2][,][ · · ·][, n][, then]

_i_ _j_ _i,j=0_ [is the uniform partition over [0][,][ 1]][2][. Let ˆ]R(u, v) := _i,j=0_ _[R][(][ i]n_ _[,][ j]n_ [)][1][I][i][×I][j] [(][u, v][).]
_{I_ _×I_ _}[n]_

[P][n]


-----

Then for any φ _L[2][0, 1], if w_ _k, k = 0, 1,_ _, n, we have_
_∈_ _∈I_ _· · ·_

1 1
( _R ˆ[φ][)(][w][) =]_ _Rˆ(u, v)φ(v)dv_ _R[ˆ](u, w)du_
_K_ 0 0 _·_
Zn Zn _n_ _n_ _i_ _i′_ 1 1

= _i=0_ _j=0_ _i[′]=0_ _j[′]=0_ _R_  _n_ _[, j]n_  _R_  _n_ _[, j]n[′]_  Z0 Z0 **1Ii×Ij** (u, v)φ(v)dv · 1Ii′ _×Ij′_ (u, w)du

X X X X

_n_ _n_

_i_ _i_

= _i=0_ _j=0_ _R_  _n_ _[, j]n_  _R_  _n_ _[, k]n_  ZIi ZIj **1Ii×Ij** (u, v)φ(v)dv · 1Ii×Ik (u, w)du

X X

_n_ _n_

_i_ _i_

= _R_ _R_ [1] _φ(v)dv,_ (122)

_i=0_ _j=0_  _n_ _[, j]n_   _n_ _[, k]n_  _·_ _n_ ZIj

X X

which is a constant only related to k. That is, _R ˆ[φ][ is a piece-wise constant func-]_
_K_
tion, i.e. _KR ˆ[φ][ ∈]_ [span][ {][1][I][k] _[}]k[n]=0[, or range(][K]R[ ˆ][)][ ⊂]_ [span][ {][1][I][k] _[}]k[n]=0[.]_ Obviously, {1Ik _}k[n]=0_
is an orthogonal set, which implies that the operator _R ˆ_ [is of finite rank at most]
1 _K_
_n + 1. Let R[ˆ]ij :=_ _n_ _[R][(][ i][−]n[1]_ _[,][ j][−]n[1]_ [),][ i, j][ = 1][,][ 2][,][ · · ·][, n][ + 1,][ {][σ][k][}]k[n]=1[+1] [be the singular value of]

_Rˆ := [ R[ˆ]ij]_ R[(][n][+1)][×][(][n][+1)] and V R[(][n][+1)][×][(][n][+1)] with columns as the corresponding right
_∈_ _∈_
singular vectors. Set [e1, e2, · · ·, en+1] := [1I0, 1I1, · · ·, 1In] V, then by (122), we get for
_l = 1, 2,_ _, n + 1,_
_· · ·_ _n_ _n_

_i_ _i_

( _R ˆ[e][l][)(][w][) =]_ _R_ _R_ [1]
_K_ _n_ _[, j]n_ _n_ _[, k]n_ _·_ _n[2][ V][j][+1][,l]_

_i=0_ _j=0_    

X X

_n+1_ _n+1_

= _RˆijR[ˆ]i,k+1Vjl =_ _Rˆ[⊤]RV[ˆ]_ :,l (123)

_k+1_ _[,]_

_i=1_ _j=1_

X X h i

which gives


_Rˆ[⊤]RV[ˆ]_ :,l


_KR ˆ[e][l][ =]_ _Rˆ[⊤]RV[ˆ]_ :,l _k+1_ **[1][I][k][ =][ σ]l[2]** _Vk+1,l1Ik = σl[2][e][l][,]_ (124)

_k=0_ _k=0_

X h i X

i.e. the set {σk[2][}][n]k=1[+1] [collects the all the eigenvalues of][ K]R[ ˆ][, which can be obtained by the]
SVD of R[ˆ].

For the error estimate, it is straightforward to have
2
_R(u, v)_ _R(u, v)_
_−_ [ˆ] _L[2]([0,1][2])_

_n_ _n_ _ni_ _nj_ 2

_i_

= _dudv_

_i=1_ _j=1_ Z _i−n1_ Z _j−n1_  _n_ _[, j]n_ 

Xn Xn _ni_ _nj_ 2

_[R][(][u, v][)][ −]_ _[R]_

_≤_ _i=1_ _j=1_ Z _i−n1_ Z _j−n1_ (u,vmax)∈[0,1][2][ ∥∇][R][(][u, v][)][∥]2[2] _[·]_ u − _n[i]_ _[, v][ −]_ _n[j]_ 2 _dudv_

Xn Xn _i_ _j_

_n_ _n_

2C [2](α)γ[2] [2] _,_ (125)

_≤_ _i=1_ _j=1_ Z _i−n1_ Z _j−n1_ _·_ _n[2][ dudv][ = 4][C]_ [2]n[(][α][2] [)][γ][2]

X X

hence by (121), _λk_ _σk_ _n_ for any n N+.
_−_ _≤_ [2][C][(][α][)][γ] _∈_

_[√]_

E Numerical settings

According to Lemma B.1, the target input-output temporal relationship has a Riesz representation form. Under the discrete-time regime, we are supposed to set


_l_
_k+1_ **[1][I][k][ =][ σ][2]**


_R ˆ[e][l][ =]_
_K_


_Ht(x) =_


_ρ(t, s)xs,_ (126)
_s=1_

X


where T N+ is the path length.
_∈_


-----

E.1 Settings of Figure 1

For the input, we generate 6 sequences using Gaussian i.i.d. random variables with the path
length T = 30. Hence, the output (target) is Ht(x) = _s=1_ _[ρ][(][t, s][)][x][s][.]_

The high rank target has the representation

[P][30]

cos(t), _t = s,_
_ρ[high](t, s) =_ (127)
0, _t_ = s,
 _̸_

while the low rank target has the representation


99

_n=0_

X


_ρ[low](t, s) =_

E.2 Settings of Figure 2


1

(128)
_n + 1 [cos(][nπt][) cos(][nπs][)][.]_


Consider the target with the following representation

_ρ(t, s) = e[−]_ _c[t]0 e[−]_ _c[s]0 R(e[−]_ _c[t]0, e[−]_ _c[s]0 ),_ (129)


where


_R(u, v) =_


_∞_

_σnϕn(u)φn(v),_ (130)
_n=1_

X


with both _ϕk_ and _φk_ as orthonormal bases. In this way, we construct a target with
_{_ _}_ _{_ _}_
singular values _σk_ . Under the discrete-time setting, we are supposed to use the following
_{_ _}_
linear, width-m RNN encoder-decoder

_τ_

## ˆH t(x) = c[⊤]V [t]PQW [s][−][1]Ux(τ s), (131)

_−_
_s=1_

X

where P R[m][×][N], Q R[N] _[×][m]_ with m = mD = mE.
_∈_ _∈_

Recall that the approximation error is derived as

_∥H −_ **_H[ˆ]∥_** ≲ _∥R −_ _R[ˆ]∥L1([0,1]2) ≤∥R −_ _R[ˆ]∥L2([0,1]2),_ (132)

where R[ˆ](u, v) := _i=1_ _ϕ[˜]i(u)φ[˜]i(v)_ _m_ [with ˜]ϕn, _φ[˜]n_ _m. In the numerical experiments,_
_∈P_ [2] _∈P_
we first construct an R(u, v), and then fit it with the polynomial R[ˆ](u, v) using the method
of least squares. The norm[P][N][0] _R_ _R_ _L2([0,1]2) is used to evaluate the approximation error._
_∥_ _−_ [ˆ]∥

In the particular example reported in Figure 2, we set m = 128, ϕn(u) = _√2 sin(nπu)_

1
_n−_ 8, _n_ _N0_

and φn(v) = _√2 sin(nπv). The singular values are taken as: (a) σn =_ _≤_ ;

0, _n > N0_



1
_n−_ _,_ _n_ _N0_
(b) σn = _≤_ ; (c) σn = n[−][2], with N0 = 2, 4, 6, 8. As an infinite sum, R is
0, _n > N0_


constructed under a finite truncation with the first 50 terms.

E.3 Settings of Figure 3

We perform experiments on nonlinear targets to show that the insight of low rank approximation also holds in the nonlinear case.

**Nonlinear target.** We consider the forced Lorenz 96 system (Lorenz, 1996), which is an
important example of reduced order modelling for convection dynamics, with applications
in weather forecasting.

Mathematically, the system has K output variables _yk_ and JK hidden variables _zj,k_
_{_ _}_ _{_ _}_
with k = 1, 2, . . ., K and j = 1, 2, . . ., J. The parameters K, J control the number of


-----

variables in the system, and can be viewed as a complexity measure. The input _xk_ is an
_{_ _}_
external temporal forcing. The system satisfies the following dynamics


_dyk_

_dt_ [=][ −][y][k][−][1][(][y][k][−][2][ −] _[y][k][+1][)][ −]_ _[y][k][ +][ x][k][ −]_ _J[1]_


_zj,k,_ (133)
_j=1_

X


_dzj,k_

= _zj+1,k(zj+2,k_ _zj_ 1,k) _zj,k + yk,_ (134)
_dt_ _−_ _−_ _−_ _−_

with cyclic indices yk+K = yk, zj,k+K = zj,k and zj+J,k = zj,k. Here, we take _xk_ as
_{_ _}_
randomly generated input sequences with the path length 64. We have tested for several
cases with different parameters: i) J = 6 with K = 1, 5, 10, 20; ii) K = 5 with J =
5, 15, 25, 100.

Note that the forced Lorenz 96 system parameterizes a highly nonlinear functional.

**Nonlinear model.** We learn the above system using RNN encoder-decoders with nonlinear activations, i.e.

_hs = σ(WEhs_ 1 + UExs + bE), _v = σ(Qhτ + b1),_
_−_
_gt = σ(WDgt_ 1 + bD), _g0 = σ(Pv + b2),_ (135)
_−_
_ot = WOgt + bO,_

where σ is the element-wise tanh activation. Let m = 128 be the hidden dimension, N =
1Wthe model with a fixed hidden dimension, 2E, . . .,, WD 32 be the size of the coding vector ∈ R[m][×][m], b1 ∈ R[N], WO ∈ R[m][×][K] m, Q v but different ∈, we haveR[m][×][N], x Ps ∈, o NtR, b, thus only sizes of[N]O[×] ∈[m]. Note that we constructR[K], hs, bE, bD Q, P, b, b2 ∈ 1R, b[m]2,
are varying, while sizes of other parameters remain unchanged.

**Training and initialisation.** We denote the model with the coding vector size N as
EncDec[(][N] [)]. We utilise the Adam optimiser and train from EncDec[(1)] to EncDec[(32)]. For
EncDec[(1)], we use a normal random initialisation, and train for 3000 epoches until a stable
error. For EncDec[(][N] [)] with N > 1, we use the parameters trained from EncDec[(][N] _[−][1)]_ as the
initialisation. For the parameters Q, P, b1, b2, we pad them to match the size of EncDec[(][N] [)]
with normal distributions as initialisations.

It is shown that the low rank approximation phenomena discovered in the linear setting also
appears in this nonlinear case.


-----

