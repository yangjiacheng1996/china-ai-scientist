# FEDDISCRETE: A SECURE FEDERATED LEARNING ALGORITHM AGAINST WEIGHT POISONING

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Federated learning (FL) is a privacy-aware collaborative learning paradigm that
allows multiple parties to jointly train a machine learning model without sharing their private data. However, recent studies have shown that FL is vulnerable
to availability poisoning attacks, integrity backdoor attacks and inference attacks
via weight poisoning and inference. In this paper, we propose a probabilistic
discretization mechanism on the client side, which transforms the client’s model
weight into a vector that can only have two different values but still guarantees
that the server obtains an unbiased estimation of the client’s model weight. We
theoretically analyze the utility, robustness, and convergence of our proposed discretization mechanism and empirically verify its superior robustness against various weight-based attacks under the cross-device FL setting.

1 INTRODUCTION

Federated learning (FL) is an emerging privacy-aware framework that trains a machine learning
model across multiple parties without accessing their local private data (Koneˇcn`y et al., 2016;

McMahan et al., 2017). In FL, each client first trains the local model using its private data and
then sends the model gradients to an honest central server. The central server aggregates all local
model gradients to form a global model, which is sent back to the clients for the next round of training. However, sharing model gradients might still leads to security concerns and privacy leakage.

Recent studies have shown that FL is vulnerable to various model weight poisoning attacks. The adversarial client(s) can stealthily manipulate the global model via modifying the local model updates
to achieve attack goals like, preventing model convergence, implanting backdoors into the global
model, and inferring the privacy information of clients’ private data (Gu et al., 2017; Blanchard
et al., 2017; Pyrgelis et al., 2017; Xie et al., 2019; Bagdasaryan et al., 2020; Wang et al., 2020; Tang
et al., 2020). To address such an issue, recent works start to explore different defense techniques
to robustify FL against model weight poisoning attacks (Blanchard et al., 2017; Bagdasaryan et al.,
2020; Shen et al., 2016; Geyer et al., 2017; Fung et al., 2018). However, most existing works apply
the defense or robust techniques at the server side, which changes the role of the central server from
being honest to trusted. And prior approaches mostly only focus on one type of attacks.

**Our contributions. In this paper, we propose the FEDDISCRETE, a flexible FL framework that can**
combine any popular FL algorithms, for example, FedAvg(McMahan et al., 2017) and FedProx(Li
et al., 2020), with our probabilistic discretization mechanism. Theoretical analyses on the utility, robustness, and convergence are performed to show that FEDDISCRETE is inherently robust to
availability poisoning attacks (Kurita et al., 2020), integrity backdoor attacks (Gu et al., 2017) and
server inference attacks (Shokri et al., 2017). FEDDISCRETE is evaluated on four popular image
classification datasets under both data are i.i.d and non-i.i.d settings. The numerical results indicate
that FEDDISCRETE is robust to various weight-based attacks.

2 BACKGROUNDS AND RELATED WORKS

In this section, we provide necessary background information on concepts thorough the paper and
formalize the problem to be solved.


-----

**Federated learning (FL). FL is a privacy-aware collaborative learning, where N clients and one**
trusted server work together to learn a global model (McMahan et al., 2017). Depending on the
application scenarios, the number of clients n can range from as small as two to several hundreds in
the cross-silo setting or can easily go beyond millions in the cross-device setting. A classical way of
formulating FL into an optimization problem is (Wang et al., 2021a)


1

min _piFi(w), where Fi(w) =_ _fi(w; ξ) and_ _pi = 1._ (1)
**_w∈R[d][ F]_** [(][w][) :=] Xi=1 _|Di|_ _ξX∈Di_ Xi=1

In Eq. 1, w is the global model weight; Fi is the i-client local objective function; the local loss
functions fi(w; ξ) are often assumed to be the same across all clients; the local data Di can have
different distribution. Following the seminal work of (McMahan et al., 2017), extensive researches
have been conducted to address various challenges in FL. For instance, (Li et al., 2020; Reddi et al.,
2020) aim to design efficient optimization algorithms; (Koneˇcn`y et al., 2016; Luping et al., 2019)
try to improve the communication efficiency; (Bagdasaryan et al., 2020; Xie et al., 2021) study the
security issues on both attacks and defenses.

**Adversaries. For any FL attack, the server is assumed to be honest, which faithfully follow the**
training protocols and any client cannot directly get the model weight of other clients. Under these
assumptions, there are two adversaries: 1) the malicious clients can manipulate the weights for
various attack purposes, such as interfering the global model training or implanting backdoors into
the global model; 2) the curious server can explore the privacy information from local clients via
inference attacks.

**Availability poisoning attacks. The goal of availability poisoning attacks (APAs) is for malicious**
local client(s) to destroy or reduce the global model’s utility (Shen et al., 2016; Sun et al., 2018). In
APAs, the attacker can either control the global model’s utility on target tasks or decrease the most
of the model’s utility as the optimal attack strategy. In practice, adding a large random noise can
successfully worsen the global model’s utility but can be easily discovered by anomaly detection
techniques. In this case, any advanced attack needs to know and tries to bypass the defense techniques used in training. In this work, we assume the attacker has full knowledge of training process
and all possible defense techniques.

**Integrity backdoor attacks. The goals of integrity backdoor attack (IBAs) are two-fold: 1) the**
attacker aims to implant backdoors into the global model, through which the attacker can control the
prediction results by injecting the trigger to any clean examples; 2) at the same time, the attacker
wants the global model can still perform as good as the non-attacked model on all clean inputs
(Bagdasaryan et al., 2020). Note that, IBAs are also conducted by the malicious clients in FL.

**Inference attacks. Many studies show that the attacker could explore the private information from**
the model weights, such as membership and attribute information (Pyrgelis et al., 2017; Shokri
et al., 2017; Ganju et al., 2018; Melis et al., 2019) or even recover the training samples used by
clients Zhu et al. (2019). When the honest-but-curious server becomes malicious, it can explore
privacy information of each client’s local dataset through inference attacks (IAs) since the server
has full access to clients’ model weight.

Both APAs and IBAs can be achieved via weights poisoning that modifies the local model weights.
For example, as the global model converges, the deviations of local models start to cancel out,
i.e.,the communication round and client index respectively,i=1 [(][w]i[t] _[−]_ **_[w][t][−][1][)][ ≈]_** [0][ given as in Eq. 2 of][ Bagdasaryan et al.] w[t] and wi[t] [are the global and local model][ (][2020][), where][ t][ and][ i]
respectively, and nt is the number of participating clients. If the adversary aims to replace global

[P][n][t][−][1]
model w[t] with a target model X, then it could propose to upload a local model weight wA[t] [=]
_nBagdasaryan et al.tX −_ (nt − 1)w ([t][−]2020[1] _−). Prior works ([P]i[n]=1[t][−][1][(][w]i[t]_ _[−]_ **_[w]Bagdasaryan et al.[t][−][1][)][ ≈]_** _[n][t][(][X][ −]_,[w] 2020[t][−][1][) +]; Wang et al.[ w][t][−][1][ as given in Eq. 3 in], 2020; Xie et al.,

2019) demonstrate the effectiveness of both APAs and IBAs when the training algorithm is FedAvg.
IAs can be easily achieved by many existing advanced attack methods once they have the full access
to the client model Shokri et al. (2017); Hu et al. (2021).

**Related works. To address the various attacks, many prior defense works have been proposed and**
achieved good performance against the attacks. However, prior defense works only address one
aspects of aforementioned three attacks. For availability poisoning attacks, Steinhardt et al. (2017)
proposed a defending framework, which can be applied to defenders that remove outliers and then


_piFi(w), where Fi(w) =_

_i=1_

X


min
**_w∈R[d][ F]_** [(][w][) :=]


_Di_
_|_ _|_


-----

Figure 1: (a): An illustration on how adversarial clients can do APAs and IPAs via weight modification. If there is no adversaries, then FL training algorithm will give a clean model. When there are
attackers, the clean model becomes poisoned. (b): An illustration on how discretization mechanism
helps to defend the APAs and IPAs, which brings the poisoned model to the robust area. The robust
area is a parameter space where the model would have the same performance as the clean model.
(c) An illustration on how discretization mechanism helps to defend the IAs as the discrete model
updates provide limited information compared to the continuous counterparts.

minimize a margin-based loss on the remaining data. Under assumptions, this framework provides
the approximate upper bounds on the efficacy of any data poisoning attack. However, this framework
does not fit in the FL setting, as it needs the access to all private data. Blanchard et al. (2017) studied
the presences of Byzantine adversaries and proposed Krum aggregation rule to defend under the
assumption that the participants’ training data are i.i.d, which is not necessarily true in the FL setting.
And indeed as argued in (Bagdasaryan et al., 2020), Krum can be used by the adversaries to make the
attack more effective. FoolsGold (Fung et al., 2018) can mitigate sybils data poisoning attacks under
the assumption that the honest clients can be separated from sybils by the diversity of their gradient
updates. As discussed by the authors, FoolsGold is not successful at mitigating attacks initiated by a
single adversary. For integrity backdoor attacks, Xie et al. (2021) provided a general framework that
is certifiably robust to the backdoor attacks under the FL setting via the model weight smoothing.
However, the convexity assumption is imposed on the loss function, which limits certified robustness
to more challenging and widely used deep neural network. Ozdayi et al. (2021) proposed a defense
approach based on adjusting server’s learning rate with the guidance of sign information of agents’
updates. For inference attacks, differential privacy is widely adopted approach to defend, where are
judiciously random noise added on either the clients’ model update or the global model, e.g., Geyer
et al. (2017), but it suffers from the trade-off between privacy and accuracy. Compared to this work,
neither of these works can address three types of attacks simultaneously.

3 METHOD

We introduce FEDDISCRETE, a federated learning framework with a simply yet effective discrete
mechanism and the flexibility to accommodate various FL optimization algorithms. A complete
description is given in Algorithm 1. FEDDISCRETEdifferences FEDDISCRETE consists of three
stages, namely, local training, discretization and aggregation, which are discussed in details.

**Local Training. Since we mainly focus on the cross-device setting, in each communication round**
_t, the server first selects a subset of_ _St_ = K clients to participate the current round of training and
_|_ _|_
broadcasts the global model w[t] to the selected clients. The clients who receives the global model
then performs local training using any appropriate algorithm to generate a new local model, i.e.
**_wi[t][+1]_** for all i _St. The ith client can 1) use the local stochastic gradient descent(SGD) method as_
_∈_
is considered in FedAVG to perform a fixed number of SGD steps to improve communication efficiency; 2) use adaptive method (Wang et al., 2021b) to improve the convergence; 3) approximately
minimize Fi(w)+ _[µ]2_

and data heterogeneity ([∥][w]Li et al.[−][w][t][∥][2][ for some], 2020). _[ µ >][ 0][ as proposed in FedProx to accommodate the system]_

**Discretization. For all i ∈** _St, the ith local client computes the maximum and minimum values of_
**_wi[t]_** [and sends noise-perturbed maximum value][ u]i[t] [and minimal value][ l]i[t] [back to the server, where the]


-----

noise is added to protect the privacy of the local training data and the noise is sampled and added
in a way that to guarantee u[t]i [and][ l]i[t] [are the valid upper bound and lower bound of][ w]i[t][+1] elementwise. Once the server received the client-wise upper-bounds and lower-bounds, it computes the
minimum of the lower bounds lmin[t] [and the maximum of the upper bounds][ u]max[t] [to prepare the]
inputs for the discretization mechanism (see Definition 3.1), which outputs an unbiased estimator of
the input (see Lemma 4.1). For all i ∈ _St, the ith client discretizes its continuous model wi[t]_ [into]
_M(wi[t][;][ l]min[t]_ _[, u]max[t]_ [)][ and uploads][ M][(][w]i[t][;][ l]min[t] _[, u]max[t]_ [)][ to the server for aggregation.]

**Aggregation. Rather than simply aggregating {M(wi[t][;][ l]min[t]** _[, u]max[t]_ [)][}][i][∈][S]t [, the sever first inspects]
whether each coordinate of wi[t] [is either][ l]min[t] [or][ u]max[t] [to exclude the any potential malicious adver-]
saries that attempt to bypass the discretization process. And the server only aggregates the local
model weights that pass the sanity check. Also when the server computes lmin[t] [and][ u]max[t] [, it can be]
more conservative by discarding the extreme values in {li[t][}][i][∈][S]t [and][ {][u]i[t][}][i][∈][S]t [by setting thresholds]
on the lower quantile of {li[t][}][i][∈][S]t [and upper quantile of][ {][u]i[t][}][i][∈][S]t [to prevent adversaries proposing]
extremely large upper bounds and/or small lower bounds. For the ease of presentation, we do not
include this choice in the description of Algorithm 1.
**Definition 3.1 (Discretization Mechanism). For any (w, l, u) ∈** R × R × R with l ≤ _w ≤_ _u, define_
the discretization mechanism as

(w; l, u) = _u,_ w.p. _[w]u−[−]l[l]_ (2)
_M_ (l, w.p. _[u]u[−][w]l_ _[,]_

_−_

where w.p. is the shorthand notation for “with probability”.

**Algorithm 1 FEDDISCRETE**

1: Input: Initial model weight w[0] _∈_ R[d], total training rounds T, the participants size K, and the
learning rate {η[t]}t[T]=0[ −][1] [and a positive sequence][ {][σ][i][}]i[N]=1[.]

2: for t = 0, 1, 2, . . ., T − 1 do
3: The sever randomly selects an index set St with |St| = K and broadcasts w[t] to the client i
if i _St._
_∈_

4: For the ith client, where i _St, it performs the local training to obtain wi[t][+1]_ and samples
_∈_
two random variables ξi[t] [and][ ζ]i[t] [from the truncated Gaussian][ TN][(0][, σ][i][; 0][,][ 1)][. Then it com-]
putes and uploads li[t] [= min][j][∈][[][d][]][{][[][w]i[t][]][j][ −] **_[w][t][} −]_** _[ξ]i[t]_ [and][ u]i[t] [= max][j][∈][[][d][]][{][[][w]i[t][]][j][ −] **_[w][t][}][ +][ ζ]i[t]_**
to the server.

5: The sever computes l[t] = mini _St_ _li[t][}][ and][ u][t][ = max][i][∈][S]t_ _[{][u][t]i[}][ then broadcasts][ l][t][, u][t][ to]_
_∈_ _{_
clients in St.

6: For the ith client, where i _St, it applies the discretization mechanism_ is an element_∈_ _M_
wise fashion and uploads the discrete model weight (wi[t][+1] **_w[t]; l[t], u[t]) to the server._**
_M_ _−_

7: The server conducts the sanity check for {M(wi[t][+1] _−_ **_w[t]; l[t], u[t])}i∈St_**, i.e., validates
whether all elements of wi[t][+1] are either l[t] or u[t]. Form the set St[′] [to collect all the]
clients’ index passing the sanity check and compute w[t][+1] = w[t] +η[t] _|[⊆]S1t[′]_ _[|][S][t]_ _i∈St[′]_ _i_ _−_

**_w[t]; l[t], u[t])._** _[M][(][w][t][+1]_

P


8: end for

We close this section by making following comments.

-  Discretization is a widely applied technique, for example, in influence maximization

Kempe et al. (2003), the algorithm needs to simulate the propagation from a probabilistic graph that is consists of a set of edges with activation probabilities in [0, 1]. It is also can
be regarded as a special form of quantization that is widely used in the distributed optimization (Alistarh et al., 2017; Reisizadeh et al., 2020) to improve communication efficiency.
Compared with prior discretization works, this is the first work to adopt discretization to
protect FL from weight poisoning attack.

-  Compared with the FedAvg, although an additional round of communication is required in
FEDDISCRETE to perform the discretization mechanism. We argue that FEDDISCRETE is
more communication-efficient than FedAvg. Assume each scalar takes 64bits, then for the


-----

_t-th round, the total bits communicated are 128dK for FedAvg, while for FEDDISCRETE_
are 64dK + 128K + dK. For large d, FEDDISCRETE communicates less bits. [1]

-  Although the discretization mechanism outputs an unbiased estimation of wi[t][+1], the variance of the estimator (wi[t][+1]) can be large. To mitigate this issue, one can compute more
_M_
finely-grained upper bounds and lower bounds. For example, assume each client wants to
train a neural network. Then, instead of computing the maximum and minimum values
over the entire w, the client could compute layer-wise maximum and minimum values.
And the discretization mechanism could be applied layer-wise with tighter upper bounds
and lower bounds. This leads to smaller variance in the estimator at the cost of increasing
the communicated bits.

-  Intuitively, the discretization mechanism is effective in defending weight poisoning attacks
as the judiciously crafted adversary model is discretized. For example, consider the model
replacement attack in Eq. 3 of Bagdasaryan et al. (2020), once the malicious model wA[t] [is]
discretized into M(wA[t] [)][, the model replacement becomes ineffective.]

4 THEORETICAL ANALYSIS

In this section, we analyze the utility of the discretization mechanism and the convergence property
of the FEDDISCRETE. Due to limited space, complete proofs are deferred to the appendix.

We first introduce the notations are used throughout the paper. [w]j denotes the jth coordinate of
the vector w. Unless specified otherwise, ∥·∥ and ∥·∥∞ are the Euclidean norm and infinity norm
respectively. To avoid the cluttered notation, we use M(·) instead of M(·; ·, ·) when there is no
confusion. And if M is applied over a vector, we mean to apply it in a element-wise fashion. Here,

[d] represents {1, · · ·, d}.

4.1 UTILITY ANALYSIS

The first lemma shows that the discretization mechanism produces an unbiased estimator with
bounded variance.
**Lemma 4.1. For any w** R[d] _and (l, u)_ R R such that l minj [d] [w]j < maxj [d] [w]j _u,_
_then_ _∈_ _∈_ _×_ _≤_ _∈_ _∈_ _≤_

-  E[M(w; l, u)] = w;

-  E[ (w; l, u) **_w_** ] 4 _and E[_ (w; l, u) **_w_** ] 4 _._
_∥M_ _−_ _∥[2]_ _≤_ _[d][(][u][−][l][)][2]_ _∥M_ _−_ _∥[2]_ _≤_ _[∥][w][∥][2]_

A natural question to ask is that how far is the averaged local model weights ¯w[t] := _i∈St−1_ **_[w]i[t][−][1]_**

from its discretized counterpart ¯w[t] [:=][ P]i _St_ 1 _i_ ). The next theorem characterizes the
_M_ _∈_ _−_

distance between ¯w[t] and ¯w[t] _[M][(][w][t][−][1]_ [P]
_M[.]_

**Theorem 4.2. For any communication round t ≤** _T and any β ∈_ (0, 1), there exists ϵ[t] =

(u[t]−l[t]) log [2]β[d]

_O_ _√q_ ! _such that_


¯w[t] **_w¯_** _[t]_ 1 _β._
_−_ _M_ _∞_ _[≤]_ _[ϵ][t][]_ _≥_ _−_


4.2 ROBUSTNESS ANALYSIS


Next, we show the discrete mechanism is robust to random weight poisoning and can recover the
original weight from local models against a limited number of independent targeted weight poisonings. To maximize the attack impact, the attacker choose to inverse the discrete local updates to

1Each model weight takes 64d bits. For FedAvg, server broadcasts the global model weight and clients
upload the local model weights, so the total bits are 128dK. For FEDDISCRETE, 64dK is the cost for the
server broadcasting the global model to K server. 128K is the cost for uploading and broadcasting the upper
bounds and lower bounds. Since the discretized model weight (wi[t][+1]) can only taking two values, the clients
_M_
can indeed just upload d bits string and the server can recover the (wi[t][+1]).
_M_


-----

attack the global model. If the intend update of the jth weight [w]j is u, the attacker uploads l to the
server, and vice versa.

**Theorem 4.3. Suppose there are F attackers out of total N clients and denote ¯wM =** _N[1]_ _Ni=1_ **_[w][i][,]_**

_then for any j ∈_ [d], P

_N_ _−F_ _N_

[E[ ¯w ]]j = [1] [wi]j [wi]j + (h + l) _[F]_ (3)
_M_ _N_ Xi=1 _−_ _i=NX−F +1_ ! _N [.]_

More importantly, FEDDISCRETE has an asymptotically non-bias estimation when the number of
clients approaches to +∞ with a fixed number of attackers.
**Corollary 4.3.1. Let N be +∞, for some fixed F attackers, the expectation of the averaged discrete**
_weights is E[ ¯w_ ] = w.
_M_

**Discussion.** The above theorem and corollary have shown the robustness against the weights poisoning attacks, such as IBAs and APAs. Besides, another important reason is the discretization
mechanism limits the modification ability of the adversaries as shown in Figure 1. For example, if
the clean aggregated model’s weight is [0.1, −2.3, 3.8], the IBA or APA could work while modifying the weight from [0.1, −2.3, 3.8] to [1, 0.2, 0.4], where the weight difference between clean and
poisoned model is [0.9, −2.5, −3.4]. However, due to the discretization mechanism, while we only
have 1 attacker among 100 clients, its modification could only be 0 or ±0.01 with the upper bound
_u = 1 and the lower bound l = 0. In this case, the adversary can not achieve its goal in any manner._

4.3 CONVERGENCE ANALYSIS

In this section, we prove the convergence of Algorithm 1 by instantiating the optimization algorithm
for generating local model weights as the FedProx(Li et al., 2020). Specifically, at the tth round,
in line 4 of Algorithm 1, the ith selected client produce the local model weight wi[t][+1] _Fi(w) +_
_µ2_ _Fi(wi[t][+1]) + µ(wi[t][+1]_ **_w[t])_** _γ_ _Fi(w[t])_ for some properly≈
chosen[∥][w][ −] µ >[w][t][∥] 0[2][ in the sense that] and γ > 0. The Assumption∇ 4.1 is made throughout the whole section.− _≤_ _∥∇_ _∥_
**Assumption 4.1.**

1. (Smoothness) For all i [N ], Fi(w) is L-smooth, i.e., _Fi(w)_ _Fi(w[′])_
_∈_ _∥∇_ _−∇_ _∥≤_
_L ∥w −_ **_w[′]∥_** for some L > 0 and all (w, w[′]) ∈ R[d] _× R[d]._

2. (Lower-bounded eigenvalue) The minimal eigenvalue of the Hessian of the client loss function ∇[2]Fi(w) is uniformly bounded below by a constant λmin ∈ R.

3. (Bounded dissimilarity) For all i _∈_ [N ] and any w _∈_ R[d], Ei[∥∇Fi(w)∥[2]] _≤_
_B[2]_ _∥∇f_ (w)∥[2] _, where the expectation is taken with respect to the client index i._

4. (Algorithmic choices) In Algorithm 1, in line 3, the ith client is picked with the probability
_pi; line 7, the step size η[t]_ = 1.

**Theorem 4.4. Consider the Algorithm 1 instantiated with FedProx.** _If µ is chosen to satisfy_
_µ > λmin, and K, µ, γ is chosen properly such that κ =_ 1−µγB _µµ¯_ 2¯µ[2]

_−_ _[LB][(1+][γ][)]_ _−_ _[L][(1+][γ][)][2][B][2]_ _−_

_LB[2](1+γ)[2]_

_K ¯µ[2]_ (2√K + 1) _µ¯√K_ 8K[2]pmin ¯µ[2] _> 0, then after T rounds,_

_−_ _[B][(1+][γ][)]_ _−_ _[LNB][2][(1+][γ][)][2]_


min _f_ (w[t]) ]
_t∈[T −1]_ [E][[] _∇_ _≤_ _[f]_ [(][w][0][)]κT[ −] _[f]_ [(][w][∗][)]


**Remark**



-  The rate of convergence of FEDDISCRETE is the same as that of FedProx but with worse
constant due to the discretization mechanism.

-  One can also prove the convergence of the FEDDISCRETE when the local clients perform
the fixed number of stochastic gradient descent steps as FedAVG. Then the convergence
result is a special case of the FedPAQ (Reisizadeh et al., 2020) when q is set to 1/4.


-----

100

98

96

94

92

90

88

86

84

82

80

100

95

90

85

80

75

70

65

60

55

50


Continous Discrete


100

90 Continous Discrete

80

70

60

50

40

Accuracy (%) 30

20

10

0 50 100 150 200 250 300 350

Number of clients


(d) CIFAR-10

80

70 Continous Discrete

60

50

40

30

Accuracy (%) 20

10

0 50 100 150 200 250 300 350

Number of clients


(d) CIFAR-10


Figure 2: Effect of number of clients N with i.i.d setting.


100

95 Continous Discrete

90

85

80

75

70

Accuracy (%) 65

60

55

50 50 100 150 200 250 300 350

Number of clients

(b) FMNIST

Figure 2: Effect of number of clients

100

90 Continous Discrete

80

70

60

50

40

Accuracy (%) 30

20

10

0 50 100 150 200 250 300 350

Number of clients


100

95 Continous Discrete

90

85

80

75

70

Accuracy (%) 65

60

55

50 50 100 150 200 250 300 350

Number of clients

(c) SVHN

_N with i.i.d setting._

100

90 Continous Discrete

80

70

60

50

40

Accuracy (%) 30

20

10

0 50 100 150 200 250 300 350

Number of clients


(b) FMNIST


(c) SVHN


Figure 3: Effect of number of clients N with non-i.i.d setting.


In this section, we examine the effect of FEDDISCRETE on the four image benchmark datasets:
MNIST (LeCun et al., 1998), Fashion-MNIST (FMNIST) (Xiao et al., 2017), SVHN (Netzer et al.,
2011), and CIFAR-10 (Krizhevsky et al., 2009). For MNIST and FMNIST, we adopt a two-layer
CNN for image classification; for SVHN and CIFAR-10, the small network from the Pytorch li
|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|||||||||||||||||||||
|||||||||||||||||||||
|||||||||||||||||||||
|||||||||||||||||||||
|||||||||||||||||||||
||50||1 (a|00 ) C|N o|1 um nti|50 b M no|e u|2 r N s|00 of I|c S|2 li|50 en T|t D|3 s is|00 cre|3 te|50 F|i|
|||||||||||||||||||||
|||||||||||||||||||||
|||||||||||||||||||||
|||||||||||||||||||||
|||||||||||||||||||||
|||||||||||||||||||||
|h 1|50|E s T,|1 (a|00 )|N P e n|1 um|50 b M|e i|2 r N|00 of I|c S, F|2 li|50 en T|t E w e|3 s|00 F|3 i T e al 1|50 g|u A a|
||i IS )||X a|s (||c L d|E t e||R o C C|I n u I||M n|A||e t R|N -||x ., 0||

brary[2] only achieves around 50% accuracy, so that we re-design a small VGG (Simonyan & Zisserman, 2014) for them. The training data and the testing data are fed into the network directly
in each client, and for each client, the size of the training data is the total number of the training
samples divided by the number of the clients for i.i.d setting. For non-i.i.d setting, we sort the data
by digit labels first, then divide the datasets into 2N shards of size _[|]2[D]N[|]_ [, and assign each of][ N][ clients]

2 shards, which is the same setting as (McMahan et al., 2017). We use the local SGD as the optimization algorithm for each client, where the local learning rate is set as 0.03 for MNIST/FMNIST
and 0.015 for SVHN/CIFAR-10. And the global learning rate ηt is set to 1 for all t. Considering
the randomness from the discrete mechanism, we run the test experiments five times independently
to obtain an averaged value. To evaluate the performance of different approaches, we use different
metrics including accuracy/utility for model performance, attack success rate (ASR)[3] for attack performance and the number of communication rounds (CRs), for communication cost. Any approach
with a high accuracy and a low ASR indicates a good and practical defense solution. The proposed
models are implemented using Pytorch, and all experiments are done with a single GPU NVIDIA
Tesla V100 on the local server. Experiments on MNIST and FMNIST can be finished within an hour
with 10 CRs, and experiments on SVHN and CIFAR-10 need about 2 hours with 15 CRs.

**Evaluation on Discrete Mechanism.** Here, we first test the effectiveness of the discrete mechanism in FL. In Figure 2 and 3, the results demonstrate three observations: 1) for the discrete model,
in i.i.d setting, it performs closer to continuous model aggregation than that in the non-i.i.d setting;
2) the more complex and difficult tasks, i.e., CIFAR-10 > SVHN > FMNIST > MNIST, are more
sensitive to the discrete mechanism; 3) when increasing the number of total clients, the model utility
loss between continuous and discrete models are diminishing. For example, while having 350 clients
in FL, the difference between continuous and discrete models is less than 1% and 2% for all tasks
in the i.i.d and non-i.i.d settings, respectively. These results verify the previous analysis in Theorem

2https://pytorch.org/tutorials/beginner/blitz/cifar10 tutorial.html. Note that, we use the default setting provided from Pytorch for MNIST and Fashion-MNIST. For SVHN and CIFAR-10, we use VGG9 that is modified
from VGG11 for balancing the memory usage and the model utility.
3The precise definition of ASR varies under different attacks. We will give the precise definition in the
subsequent sections.


-----

40

36

32

28

24

20

16

12


50

45

40

35

30

25

20

15

10


80

72

64

56

48

40

32

24

16


60

54

48

42

36

30

24

18

12


|IID Non-IID|Col2|
|---|---|
|||
|||
|||
|||
|||
|||
|||
|||


IID
Non-IID

1 2 3 4 5 6 7 8 9 101112131415

Number of attackers

(a) MNIST


|IID Non-IID|Col2|
|---|---|
|||
|||
|||
|||
|||
|||
|||
|||


IID
Non-IID

1 2 3 4 5 6 7 8 9 101112131415

Number of attackers

(b) FMNIST


|IID Non-IID|Col2|
|---|---|
|||
|||
|||
|||
|||
|||
|||
|||


IID
Non-IID

10


|IID Non-IID|Col2|
|---|---|
|||
|||
|||
|||
|||
|||
|||
|||


IID
Non-IID

10


Number of attackers

(c) SVHN


Number of attackers

(d) CIFAR-10


Figure 4: Effect of number of attackers F with 100 clients in i.i.d and non-i.i.d settings.

4.3. In summary, the discrete mechanism is more practical for customer-related applications, e.g.,
smartphones or IoT devices, since it usually involves millions of clients during training.

**Evaluation on Availability Poisoning Attacks.** The integrity poisoning attacks are evaluated
from multi-angles. In Figure 4, we fix the number of clients as 100, and evaluate the increasing
number of attackers in both i.i.d and non-i.i.d settings. The attack success rate (ASR) in APAs is
the utility loss between before and after attacks. The results show that one or a few attackers, i.e., a
small fraction of attackers of all clients, cannot successfully poison the model. For example, when
there are more than 100 clients, any single adversarial client only gets the 0% ASR for all tasks in
both i.i.d and non-i.i.d settings. Figure 5 and 6 show the results of model utility with a fixed number
of attackers in both i.i.d and non-i.i.d settings. The results consistently show that for a fixed number
of attackers, increasing the number of the total clients will improve the global model’s utility.


Compared with prior defense against IBAs (Blanchard et al., 2017; Bagdasaryan et al., 2020; Shen
et al., 2016; Fung et al., 2018), based on our current knowledge, not many existing works have well
studied the availability poisoning attacks in FL.
Due to the adversary have more flexible attack options of the training process in APAs, most APAs
could easily break the defense solutions for IBAs.


Table 1: FEDDISCRETE against IBAs

|Col1|No Defense|Col3|FEDDISCRETE|Col5|
|---|---|---|---|---|
|Attack|ASR|UL|ASR|UL|
|CAS|81.90%|2.40%|0.00%|0.00%|
|DBA|81.30%|4.70%|0.00%|0.00%|


Table 2: Compare with other defense baselines

**Evaluation on Integrity Backdoor Attacks.**
Next, we evaluate the FEDDISCRETE against Defenses ASR UL
the state-of-the-art backdoor attacks, such as No defense 81.90% 2.40%
constrain-and-scale (CAS) (Bagdasaryan et al., Krum 100% 35.50%
2020) and DBA (Xie et al., 2019) in Table 1, where FoolsGold 100% 39.90%
utility loss (UL) is the difference of model utility Auror 100% 66.10%
between clean and poisoned models, and the at- FedDP 0% 13.30%
tack success rate (ASR) in IBAs is the percentage FEDDISCRETE 0% 0%

|Defenses|ASR|UL|
|---|---|---|
|No defense|81.90%|2.40%|
|Krum|100%|35.50%|
|FoolsGold|100%|39.90%|
|Auror|100%|66.10%|
|FedDP|0%|13.30%|
|FEDDISCRETE|0%|0%|

of poisoned samples that are classified as the target
class by the backdoored model. Due to the variants of studied tasks in prior works, we evaluate the
most common task, i.e., CIFAR-10, for all backdoor attacks in the i.i.d setting. We use the default
attack settings same as the original paper: one attacker for CAS and four attackers for DBA. Here,
ASR is the backdoor attack success rate, and UL is the utility loss compared with none-attack FL.
The results show that FEDDISCRETE can easily defend the backdoor attacks against CAS and DBA.
By mitigating the impacts of the attackers, in addition to the successful IBAs defense, FEDDISCRETE can even improve the model utility.

We also compare FEDDISCRETE with other backdoor defense systems against CAS in FL: Krum
(Blanchard et al., 2017), FoolsGold (Fung et al., 2018), Auror (Shen et al., 2016), and FedDP (Geyer
et al., 2017) in Table 2. The results show that Krum, FoolsGold, and Auror can not well defend the
backdoor attacks in FL. DP can achieve a good backdoor defense, but scarifies the model utility.
FEDDISCRETE can achieve the best ASR and UL in all defense methods.

**Discussion on Client Inference Attacks.** Prior inference attacks (Blanchard et al., 2017; Pyrgelis
et al., 2017; Ganju et al., 2018) require to get the original or similar local client model, i.e., a continuous local model. Then, the malicious server could explore the privacy of the training data from its


-----

70

63

56

49

42

35

28

21

14


|F=1 F=5|Col2|
|---|---|
|F=5 F=10||
|||
|||
|||


F=1
F=5
F=10

10 20 30 40 50 60 70 80 90 100

Number of clients

(d) CIFAR-10


100

90 F=1

80 F=5

70 F=10

60

50

40

Accuracy (%) 30

20

10

0 10 20 30 40 50 60 70 80 90 100

Number of clients


100

90 F=1

80 F=5

70 F=10

60

50

40

Accuracy (%) 30

20

10

0 10 20 30 40 50 60 70 80 90 100

Number of clients


100

90 F=1

80 F=5

70 F=10

60

50

40

Accuracy (%) 30

20

10

0 10 20 30 40 50 60 70 80 90 100

Number of clients


(a) MNIST


(b) FMNIST


(c) SVHN


Figure 5: Effect of number of clients N with fixed attackers (F = 1, 5, 10) in i.i.d settings.



100

90 F=1

80 F=5

70 F=10

60

50

40

Accuracy (%) 30

20

10

0 10 20 30 40 50 60 70 80 90 100

Number of clients


70

63 F=1

56 F=5

49 F=10

42

35

28

Accuracy (%) 21

14

7

0 10 20 30 40 50 60 70 80 90 100

Number of clients


70

63 F=1

56 F=5

49 F=10

42

35

28

Accuracy (%) 21

14

7

0 10 20 30 40 50 60 70 80 90 100

Number of clients


50

45 F=1

F=5

40 F=10

35

30

25

Accuracy (%) 20

15

10

5 10 20 30 40 50 60 70 80 90 100

Number of clients


(a) MNIST


(b) FMNIST


(c) SVHN


(d) CIFAR-10


Figure 6: Effect of number of clients N with fixed attackers (F = 1, 5, 10) in non-i.i.d settings.


continuous local model. However, a discrete local model cannot be used to infer any valuable information. The malicious server could compute the continuous global model, but it would not be useful
to explore the privacy information of each client. Another popular inference defense technique is the
homomorphic encryption (HE) (Gentry et al., 2009). Although HE can perfectly defend the inference attacks from the malicious server, it requires high computation cost and cannot defend against
other weight-based poisoning attacks. Similarly, differential privacy (DP) (Geyer et al., 2017) could
also be used for defending inference attacks. However, Hu et al. (2021) have shown that DP will
lose too much utility of the global model while providing strong privacy protection. FEDDISCRETE
is a concise but practical approach that can naturally defend the client inference attacks from the
malicious server with a more efficient communication protocol.

6 LIMITATIONS AND DISCUSSIONS


FEDDISCRETE can defend both data poisoning attacks and weight poisoning attacks in FL, since
data poisoning attacks are equal to change the clean model to the poisoned model. However, FEDDISCRETE also has two limitations: (1) It could not work in the cross-silos federated learning setting. If we want to mitigate the adversarial impact and achieve a non-biased aggregated model, it
requires there are many clients could join in training in each round. However, it would not be an issue
in the cross-devices setting involving thousands or even more clients in each round of training; (2)
It could not defend the byzantine attacks through our discretization aggregation mechanism, since
the aggregated model represents the majority of interests in FEDDISCRETE. However, the byzantine defense via robust local training could still work in our framework, where the local client can
recognize the poisoned model or only leverage the valuable information from the poisoned model to
train the local model via knowledge distillation (Lee et al., 2021). Our next step is to further explore
a novel defense mechanism that can be more scalable for various adversarial environments.

7 CONCLUSION


In this paper, we proposed a new FL approach that applies the discrete mechanism with adaptive
weight range for protecting FL. To our best knowledge, it is the first work that don’t require the server
to do excessive computation, but successfully defend against various attacks, including availability
poisoning attacks, integrity backdoor attacks, and inference attacks. We also theoretically analyze
the utility, robustness, and convergence of our proposed discrete mechanism in FL. One limitation
of this work is the proposed system cannot defend the attacks well with a small number of clients or
face a large fraction of the attackers, which becomes the next step of our future research.


-----

REFERENCES

Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd:
Communication-efficient sgd via gradient quantization and encoding. Advances in Neural In_formation Processing Systems, 30:1709–1720, 2017._

Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to
backdoor federated learning. In International Conference on Artificial Intelligence and Statistics,
pp. 2938–2948. PMLR, 2020.

Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning
with adversaries: Byzantine tolerant gradient descent. In Proceedings of the 31st International
_Conference on Neural Information Processing Systems, pp. 118–128, 2017._

Clement Fung, Chris JM Yoon, and Ivan Beschastnikh. Mitigating sybils in federated learning
poisoning. arXiv preprint arXiv:1808.04866, 2018.

Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and Nikita Borisov. Property inference attacks
on fully connected neural networks using permutation invariant representations. In Proceedings
_of the 2018 ACM SIGSAC Conference on Computer and Communications Security, pp. 619–633,_
2018.

Craig Gentry et al. A fully homomorphic encryption scheme, volume 20. Stanford university Stanford, 2009.

Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client
level perspective. arXiv preprint arXiv:1712.07557, 2017.

Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the
machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.

Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, and Xuyun Zhang. Source inference
attacks in federated learning. arXiv preprint arXiv:2109.05659, 2021.

David Kempe, Jon Kleinberg, and Eva Tardos. Maximizing the spread of influence through a social[´]
network. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge
_discovery and data mining, pp. 137–146, 2003._

Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richt´arik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. _arXiv_
_preprint arXiv:1610.05492, 2016._

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.

Keita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pre-trained models.
_arXiv preprint arXiv:2004.06660, 2020._

Yann LeCun, Le´on Bottou, Yoshua Bengio, and Geoffrey Hinton. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Gihun Lee, Yongjin Shin, Minchan Jeong, and Se-Young Yun. Preservation of the global knowledge
by not-true self knowledge distillation in federated learning. arXiv preprint arXiv:2106.03097,
2021.

Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. In MLSys, 2020.

WANG Luping, WANG Wei, and LI Bo. Cmfl: Mitigating communication overhead for federated learning. In 2019 IEEE 39th International Conference on Distributed Computing Systems
_(ICDCS), pp. 954–964. IEEE, 2019._

Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli_gence and Statistics, pp. 1273–1282. PMLR, 2017._


-----

Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Exploiting unintended
feature leakage in collaborative learning. In 2019 IEEE Symposium on Security and Privacy (SP),
pp. 691–706. IEEE, 2019.

Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.

Mustafa Safa Ozdayi, Murat Kantarcioglu, and Yulia R Gel. Defending against backdoors in federated learning with robust learning rate. In Proceedings of the AAAI Conference on Artificial
_Intelligence, volume 35, pp. 9268–9276, 2021._

Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De Cristofaro. Knock knock, who’s there?
membership inference on aggregate location data. arXiv preprint arXiv:1708.06145, 2017.

Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcn`y,
Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. _arXiv preprint_
_arXiv:2003.00295, 2020._

Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani.
Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization. In AIStats, pp. 2021–2031. PMLR, 2020.

Shiqi Shen, Shruti Tople, and Prateek Saxena. Auror: Defending against poisoning attacks in collaborative deep learning systems. In Proceedings of the 32nd Annual Conference on Computer
_Security Applications, pp. 508–519, 2016._

Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP),
pp. 3–18. IEEE, 2017.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv, 2014.

Jacob Steinhardt, Pang Wei Koh, and Percy Liang. Certified defenses for data poisoning attacks. In
_Proceedings of the 31st International Conference on Neural Information Processing Systems, pp._
3520–3532, 2017.

Lichao Sun, Yingtong Dou, Carl Yang, Ji Wang, Philip S Yu, Lifang He, and Bo Li. Adversarial
attack and defense on graph data: A survey. arXiv preprint arXiv:1812.10528, 2018.

Ruixiang Tang, Mengnan Du, Ninghao Liu, Fan Yang, and Xia Hu. An embarrassingly simple
approach for trojan attack in deep neural networks. In Proceedings of the 26th ACM SIGKDD
_International Conference on Knowledge Discovery & Data Mining, pp. 218–228, 2020._

Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jyyong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you really can
backdoor federated learning. arXiv preprint arXiv:2007.05084, 2020.

Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H. Brendan McMahan, Blaise Aguera y Arcas, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data,
Suhas Diggavi, Hubert Eichner, Advait Gadhikar, Zachary Garrett, Antonious M. Girgis, Filip
Hanzely, Andrew Hard, Chaoyang He, Samuel Horvath, Zhouyuan Huo, Alex Ingerman, Martin Jaggi, Tara Javidi, Peter Kairouz, Satyen Kale, Sai Praneeth Karimireddy, Jakub Konecny,
Sanmi Koyejo, Tian Li, Luyang Liu, Mehryar Mohri, Hang Qi, Sashank J. Reddi, Peter Richtarik,
Karan Singhal, Virginia Smith, Mahdi Soltanolkotabi, Weikang Song, Ananda Theertha Suresh,
Sebastian U. Stich, Ameet Talwalkar, Hongyi Wang, Blake Woodworth, Shanshan Wu, Felix X.
Yu, Honglin Yuan, Manzil Zaheer, Mi Zhang, Tong Zhang, Chunxiang Zheng, Chen Zhu, and
Wennan Zhu. A field guide to federated optimization, 2021a.

Jianyu Wang, Zheng Xu, Zachary Garrett, Zachary Charles, Luyang Liu, and Gauri Joshi. Local
adaptivity in federated learning: Convergence and consistency, 2021b.

Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv, 2017.


-----

Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. Dba: Distributed backdoor attacks against federated learning. In International Conference on Learning Representations, 2019.

Chulin Xie, Minghao Chen, Pin-Yu Chen, and Bo Li. Crfl: Certifiably robust federated learning
against backdoor attacks. arXiv preprint arXiv:2106.08283, 2021.

Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information
_Processing Systems, volume 32. Curran Associates, Inc., 2019._


-----

A APPENDIX

A.1 PROOF OF LEMMA 4.2.

_Proof. Notice that for any scalar w ∈_ R[n] and l ≤ _w ≤_ _u, by the definition of M(.; l, u), one can_
verify that E[M(w; l, u)] = w, therefore E[M(w; l, u)] = w. Furthermore,


E[∥M(w; l, u) − **_w∥[2]] =_**


_j=1_ E[([M(w; l, u)]j − [w]j)[2]]

X


E[[ (w; l, u)][2]j []][ −] [[][w][]][2]j
_M_

(u + l)[w]j _lu_ [w][2]j
_−_ _−_


_j=1_

_d_

_j=1_

X

_d_

_j=1_

X


_d_

_u_ _l_

= _−_

2

_j=1_ "

X

2

_u_ _l_
= d _−_

2

 


2 2[#]
_u + l_

[w]j
_−_ _−_ 2
   

_d_ 2

_u + l_

[w]j

_−_ _j=1_  _−_  2  _≤_ _[d][(][u][ −]4_ _[l][)][2]_

X


Moreover,


_u −_ _l_

2




2



_u + l_

[w]j
_−_ 2



2



_j=1_


A.2 PROOF OF THEOREM 4.2.

Before proving the Theorem 4.2, a standard probability bound is required.

**Lemma A.1. Let {Xi}i[n]=1** _[be a sequence of i.i.d continuous random variables, whose support is on]_
R. Then for any b ∈ R

P max _nP(Xi_ _b)._
i∈[n] _[X][i][ ≥]_ _[b]_ _≤_ _≥_

_Proof._

P max = 1 P(max
_i_ [n] _[X][i][ ≥]_ _[b]_ _−_ _i_ [n] _[X][i][ ≤]_ _[b][) = 1][ −]_ [P][(][X][1][ ≤] _[b,][ · · ·][, X][n][ ≤]_ _[b][)]_
 _∈_  _∈_


_j=1_ P(Xj ≤ _b) = 1 −_ (1 − P(Xj ≥ _b))[n]_ for any j

Y


= 1 −


Let f (t) := 1 − _nt −_ (1 − _t)[n]_ for t ∈ [0, 1]. As f _[′](t) ≤_ 0 and f (0) = 0, f (t) ≤ 0 for all t ∈ [0, 1].
Therefore, 1 − (1 − _t)[n]_ _≤_ _nt. Take t = P(Xj ≥_ _b), we arrive_

P max _nP(Xi_ _b)._
i∈[n] _[X][i][ ≥]_ _[b]_ _≤_ _≥_

Now we are ready to prove Theorem 4.2.


-----

_Proof.to Lemma For any 4.1. By Bernstein inequality, for any j ∈_ [d], |[ ¯w[t]]j − [ ¯wM[t] []][j][| ≤] _[u][t][ −] j ∈[l][t][ and][d] and[ Var] ϵ >[ ([ ¯]w 0[t]]j −_ [ ¯wM[t] []][j][)][ ≤] [(][u][t][ −] _[l][t][)][2][/][4][ due]_


_Kϵ[2]_

_i_ _St_ **[Var][ ([ ¯]w[t]]j** [ ¯w[t] 3 _[ϵ][(][u][t][ −]_ _[l][t][)]_
_∈_ _−_ _M[]][j][) +][ 2]_

_Kϵ[2]_

!


P [ ¯w[t]]j [ ¯w[t] _ϵ_ 2 exp
_−_ _M[]][j]_ _≥_ _≤_
  

_≤_ 2 exp

By Lemma A.1,


2 _K[1]_


(u[t]−l[t])[2]


+ [2]3 _[ϵ][(][u][t][ −]_ _[l][t][)]_


_Kϵ[2]_

+ [2]3 _[ϵ][(][u][t][ −]_ _[l][t][)]_



[ ¯w[t]]j − [ ¯wM[t] []][j] _≥_ _ϵ_ _≤_ 2d exp



max
_j∈[d]_


(u[t]−l[t])[2]


(u[t]−l[t])


log [2][d]


Therefore, for any _β_ _>_ 0, there exists


such that


P maxj [d] [ ¯w[t]]j [ ¯w[t] holds with probability at least 1 _β._
_∈_ _|_ _−_ _M[]][j][| ≤]_ _[ϵ]_ _−_
  

A.3 PROOF OF THEOREM 4.3

_Proof. Due to the weight discretization mechanism, the adversary can only return u or l for each_
coordinate of the model weight. In order to not return the correct information, the adversary could
choose to return the opposite feedback to attack the model, i.e., return u if the original return is l,
and vice versa. Therefore, we denote, for any l ≤ _w ≤_ _u,_


_l,_ w.p. _[w]h−[−]l[l]_

_h,_ w.p. _[h]h[−]−[w]l_


_Madv(w) =_


Under the scenario that there are F attackers, for any j ∈ [d],

1 _N_ _−F_

[E[ ¯w ]]j = E [ (wi)]j + [ adv(wi)]j
_M_ " _N_ Xi=1 _M_ _i=NX−F +1_ _M_ !#

_N_ _−F_ _N_

= [1] [wi]j + [1] ((h + l) [wi]j) (4)

_N_ _N_ _−_

Xi=1 _i=NX−F +1_

_N_ _−F_ _N_

= [1] [wi]j [wi]j + (h + l) _[F]_ (5)

_N_ Xi=1 _−_ _i=NX−F +1_ ! _N [.]_

A.4 PROOF OF THEOREM 4.4

_Proof. The proof is inspired by the analysis in Theorem 4 of (Li et al., 2020)._


To proceed with the analysis, we first introduce some notations. At the tth round, for the all i
1 _∈_
_St[′][, define][ ˜]w[t][+1]_ = w[t] + _|St[′][|]_ _i∈St[′]_ **_wi[t][+1]_** _−_ **_w[t][], ¯w[t][+1]_** = w[t] + _i∈[N_ ] _[p][i][(][w]i[t][+1]_ _−_ **_w[t]), and_**

**_wˆi[t][+1]_** = arg minw hi(w; w[t]) :=P Fi(w ) + _[µ]2_ **_w[t][+1]_** is the ghost global model as if the

discretization mechanism is not applied to the local model weights;[∥][w][ −] **_[w][t][∥][2][.][ ˜]_** ¯w[P][t][+1] is another ghost global
model as if all clients participate in the tth round training and no discretization mechanism is applied
; ˆwi[t][+1] is the exact minimizer of the strongly convex function hi(w). These points reference points
are crucial for the analysis. Define the gradient residual e[t]i[+1] = _Fi(wi[t][+1]) + µ(wi[t][+1]_ **_w[t]), then_**
_∇_ _−_
**_wi[t][+1]_** _−_ **_w[t]_** = − _µ[1]_ _[∇][F][i][(][w]i[t][+1]) +_ _µ[1]_ _[e]i[t][+1]. Therefore,_


_pi(wi[t][+1]_ _−_ **_w[t]) = −_** _µ[1]_ _pi∇Fi(wi[t][+1]) + µ[1]_
_iX∈[n]_ _iX∈[n]_


**_w¯_** _[t][+1]_ _−_ **_w[t]_** =


_pie[t]i[+1]._ (6)
_iX∈[n]_


-----

Since µ is chosen to satisfy µ > λmin, then hi(w; w[t]) is ¯µ-strongly convex. By the strong convexity
of hi,
**_wi[t][+1]_** _−_ **_wˆi[t][+1]_** _≤_ _µ[1]¯_ [(][w]i[t][+1] _−_ **_wˆi[t][+1])[⊤](∇hi(wi[t][+1]) −∇hi( ˆwi[t][+1]))_**

[2] _≤_ _µ[1]¯_ **_wi[k][+1]_** _−_ **_wˆi[t][+1]_** _∇hi(wi[t][+1]) −∇hi( ˆwi[t][+1])_ _,_

which, together with the fact that ˆwi[t][+1] is the minimizer of hi(w), implies


**_wi[t][+1]_** _−_ **_wˆi[t][+1]_** _≤_ _µ[1]¯_ _∇hi(wi[t][+1]) −∇hi( ˆwi[t][+1]))_

= µ[1]¯ _∇hi( ˆwi[t][+1]))_ = µ[1]¯ _∇Fi(wi[t][+1]) + µ(wi[t][+1]_ _−_ **_w[t])_**

_Fi(w[t])_ (7)

_≤_ _µ[γ]¯_ _∇_

Again use the same analysis, one has ˆwi[t][+1] _−_ **_w[t]_** _≤_ _µ[1]¯_ _[∇][F][i][(][w][t][)][. Therefore, together with Eq.][ 7][,]_

**_wi[t][+1]_** _−_ **_w[t]_** _≤_ **_wi[t][+1]_** _−_ **_wˆi[t][+1]_** + ˆwi[t][+1] _−_ **_w[t]_** _≤_ [1 +]µ¯[ γ] _∇Fi(w[t])_ _._ (8)

Therefore, one can bound the distance from the ghost global model ¯w[t][+1] to the current global
weight as


¯w[t][+1] _−_ **_w[t]_**


= _pi(wi[t][+1]_ **_w[t])_** _pi_ **_wi[t][+1]_** **_w[t]_**

_−_ _≤_ _−_
_iX∈[N_ ] _iX∈[N_ ]

_pi_ _Fi(w[t])_ (by Eq. 8)

_≤_ [1 +]µ¯[ γ] _∇_

_iX∈[N_ ]

_pi_ _Fi(w[t])_ (Jensen’ Inequality)

_≤_ [1 +]µ¯[ γ] _∥∇_ _∥[2]_

s Xi∈[N ]

= [1 +][ γ] Ei[ _Fi(w[t])_ ]

_µ¯_ _∥∇_ _∥[2]_

q


_≤_ _[B][(1 +]µ¯_ _[ γ][)]_


_∇f_ (w[t]) (by Assumption 4.1 (3)) (9)


Note that


_pi_ _Fi(wi[t][+1])_ _e[t]i[+1]_ _Fi(w[t])_
_∇_ _−_ _−∇_
_iX∈[n]_  


_pi_ _Fi(wi[t][+1])_ _Fi(w[t])_ + _e[t]i[+1]_

_≤_ _∇_ _−∇_

_iX∈[n]_   []

_pi_ _L_ **_wi[t][+1]_** **_w[t]_** + _e[t]i[+1]_

_≤_ _−_

_iX∈[n]_  

_Eq. 8_ _L(1 + γ)_ []

+ γ _pi_ _Fi(w[t])_

_≤_ _µ¯_ _∇_
 _i_ [n]

 X∈

_L(1 + γ)_
= + γ Ei[ _Fi(w[t])]_

_µ¯_ _∇_

 

_Eq. 8_ _L(1 + γ)_

_B_ + γ _f_ (w[t]) _._ (10)
_≤_ _µ¯_ _∇_
 


-----

By Assumption 4.1 (1), one has

_f_ ( ¯w[t][+1]) _f_ (w[t]) + _f_ (w[t])[⊤]( ¯w[t][+1] **_w[t]) +_** _[L]_
_≤_ _∇_ _−_ 2


**_w[t]) +_** _[L]_ ¯w[t][+1] **_w[t]_**

2 _−_

_pi∇Fi(wi[t][+1]) + µ[1]_
_iX∈[n]_


¯w[t][+1] _−_ **_w[t]_**


_Eq. 6_


_Eq. 6_

_≤_ _f_ (w[t]) + ∇f (w[t])[⊤] − _µ[1]_ _pi∇Fi(wi[t][+1]) + µ[1]_ _pie[t]i[+1]_ + _[L]2_ ¯w[t][+1] _−_ **_w[t]_**

_iX∈[n]_ _iX∈[n]_

  [2]

= f (w[t]) + ∇f (w[t])[⊤] − _µ[1]_ _pi_ _∇Fi(wi[t][+1]) −_ _e[t]i[+1]_ _−∇Fi(w[t])_ _−_ _µ[1]_ _[∇][f]_ [(][w][t][)]

_iX∈[n]_   

 

+ _[L]_ ¯w[t][+1] **_w[t]_**

2 _−_

[2]

_≤_ _f_ (w[t]) − _µ[1]_ _f_ (w[t]) _−_ _µ[1]_ _[f]_ [(][w][t][)][⊤]  _pi_ _∇Fi(wi[t][+1]) −_ _e[t]i[+1]_ _−∇Fi(w[t])_ 

_i∈[n]_   

[2]  [X] 

+ _[L]_ ¯w[t][+1] **_w[t]_**

2 _−_

[2]

_≤_ _f_ (w[t]) − _µ[1]_ _f_ (w[t]) + µ[1] _f_ (w[t]) _pi_ _∇Fi(wi[t][+1]) −_ _e[t]i[+1]_ _−∇Fi(w[t])_

_iX∈[n]_   

[2]

+ _[L]_ ¯w[t][+1] **_w[t]_**

2 _−_

_Eq. 10,Eq. 9_ _L(1 + γ)_

_f_ (w[t]) _f_ (w[2][t]) + _[B]_ + γ _f_ (w[t]) + _[L]_ )2 _f_ (w[t])
_≤_ _−_ _µ[1]_ _µ_ _µ¯_ _∇_ 2 [(] _[B][(1 +]µ¯_ _[ γ][)]_ _∇_



 (11)

[2] [2]

1 _γB_
= f (w[t]) _−_ _f_ (w[t]) (12)
_−_ _µ_ _−_ _[LB][(1 +]µµ¯_ _[ γ][)]_ _−_ _[L][(1 +]2¯µ[ γ][2][)][2][B][2]_ _∇_
 

[2]


By mean-value theorem and triangular inequality, for some α ∈ [0, 1]

_f_ ( ˜w[t][+1]) ≤ _f_ ( ¯w[t][+1]) + _∇f_ (α ˜w[t][+1] + (1 − _α) ¯w[t][+1])_ ˜w[t][+1] _−_ **_w¯_** _[t][+1]_

_≤_ _f_ ( ¯w[t][+1]) + _∇f_ (α ˜w[t][+1] + (1 − _α) ¯w[t][+1]) −∇f_ (w[t]) + _∇f_ (w[t]) ˜w[t][+1] _−_ **_w¯_** _[t][+1]_

_f_ ( ¯w[t][+1]) +  L _α ˜w[t][+1]_ + (1 _α) ¯w[t][+1]_ **_w[t]_** + _f_ (w[t]) ˜w[t][+1] **_w¯_** _[t][+1]_
_≤_ _−_ _−_ _∇_ [ ]−

_f_ ( ¯w[t][+1]) +  L( ˜w[t][+1] **_w[t]_** + ¯w[t][+1] **_w[t]_** ) + _f_ (w[t]) ˜w[t][+1] **_w¯_** _[t][+1]_
_≤_ _−_ _−_ _∇_ [ ] _−_

(13)

  [ ]


Taking expectation with respect to the random index set St[′][, one gets]

E _t_ [[][f] [( ˜]w[t][+1])] _f_ ( ¯w[t][+1]) + _L_ ¯w[t][+1] **_w[t]_** + _f_ (w[t]) E _t_ ˜w[t][+1] **_w¯_** _[t][+1]_
_S_ _[′]_ _≤_ _−_ _∇_ _S_ _[′]_ _−_

+ LE _t_ [[]   ˜w[t][+1] **_w[t]_** ˜w[t][+1] **_w¯_** _[t][+1]_ ]
_S_ _[′]_ _−_ _−_ []

_f_ ( ¯w[t][+1]) + _L_ ¯w[t][+1] **_w[t]_** + _f_ (w[t]) E _t_ ˜w[t][+1] **_w¯_** _[t][+1]_
_≤_ _−_ _∇_ _S_ _[′]_ _−_

+ LE _t_ [[]   ˜w[t][+1] **_w¯_** _[t][+1]_ + ¯w[t][+1] **_w[t]_** ˜w[t][+1] **_w¯_** _[t][+1]_ ]
_S_ _[′]_ _−_ _−_ [] _−_

= f ( ¯w[t][+1]) + 2L ¯w[t][+1] **_w[t]_** + _f_ (w[t]) E _t_ ˜w[t][+1] **_w¯_** _[t][+1]_
_−_ _∇_ [ ]S _[′]_ _−_

+ LE _t_ [[]   ˜w[t][+1] **_w¯_** _[t][+1]_ ] (14)
_S_ _[′]_ _−_ []

[2]


-----

By the sampling scheme, one has


1
E _t_ ˜w[t][+1] **_w¯_** _[t][+1]_ **_wi[t][+1]_** **_w¯_** _[t][+1]_ ]
_S_ _[′]_ _−_ _≤_ _t[|]_ [E][i][[] _−_

_|S_ _[′]_

1

[2] **_wi[t][+1]_** **_w[t]_** +[2] **_w[t]_** **_w¯_** _[t][+1]_ + 2(wi[t][+1] **_w[t])[⊤](w[t]_** **_w¯_** _[t][+1])]_

_≤_ _t[|]_ [E][i][[] _−_ _−_ _−_ _−_

_|S_ _[′]_

1

**_wi[t][+1]_** **_w[t]_** [2] (by Ei(wi[t][+1][2]) = ¯w[t][+1])

_≤_ _t[|]_ [E][i][[] _−_

_|S_ _[′]_

_Eq. 8_ 1 (1 + γ)[2]

[2]

Ei[ _Fi(w[t])_ ]

_≤_ _|St[′][|]_ _µ¯[2]_ _∇_

(1 + γ)[2]

[2]

_f_ (w[t]) (by Assumption 4.1 (3)). (15)

_≤_ _|S[B]t[2][′][|]_ _µ¯[2]_ _∇_

[2]


Combining Eq. 14, Eq. 15, and Eq. 9, together with the fact that E _t_ ˜w[t][+1] **_w¯_** _[t][+1]_
_S_ _[′]_ _−_

E _t_ **_w[t][+1]_** **_w¯_** _[t][+1]_ as a result of the Jesen’s inequality, one reaches to
_S_ _[′]_ _[∥]_ [˜] _−_ _∥[2]_

q


2L [B][2]

_|St[′][|]_

p


(1 + γ)[2]

_µ¯[2]_


(1 + γ)[2]

+ L [B][2]

_|St[′][|]_ _µ¯[2]_


(1 + γ)



_[B]_ (1 + γ) _f_ (w[t])

_t[|]_ _µ¯[2]_ ! _∇_

_[′]_

_∇f_ (w[t]) _._ (16)

[2]


E _t_ [[][f] [( ˜]w[t][+1])] _f_ ( ¯w[t][+1]) +
_S_ _[′]_ _≤_

= f ( ¯w[t][+1]) +


_|St[′][|]_


_LB[2](1 + γ)[2]_

(2
_|St[′][|]µ[¯][2]_


_t[|][ + 1) +][ B][(1 +][ γ][)]_
_|S_ _[′]_ _µ¯_ _|St[′][|]_

p


Combine Eq. 16 and Eq. 11, one reaches to

1 _γB_
E _t_ [[][f] [( ˜]w[t][+1])] _f_ (w[t]) _−_
_S_ _[′]_ _≤_ _−_ _µ_ _−_ _[LB][(1 +]µµ¯_ _[ γ][)]_ _−_ _[L][(1 +]2¯µ[ γ][2][)][2][B][2]_ _−_


_LB[2](1 + γ)[2]_

(2 _t[|][ + 1) +][ B][(1 +][ γ][)]_ _f_ (w[t]) (17)
_t[|]µ[¯][2]_ _|S_ _[′]_ _µ¯_ _t[|]_ !! _∇_
_|S_ _[′]_ p _|S_ _[′]_

p [2]


Taking the expectation with respect to the discretization mechanism, E [w[t][+1]] = **_w˜_** _[t][+1]_ by
_M_
Lemma 4.1. Since f is L-smooth,


-----

E [f (w[t][+1])] _f_ ( ˜w[t][+1]) + _[L]_ **_w[t][+1]_** **_w˜_** _[t][+1]_ ]
_M_ _≤_ 2 [E][M][[] _−_

1 [2]

= f ( ˜w[t][+1]) + _[L]2_ _t[|][2][ E][M]_  _M(wi[t][+1]_ _−_ **_w) −_** (wi[t][+1] _−_ **_w)_**

_|S_ _[′]_  _iX∈St[′]_  




2[]




_≤_ _f_ ( ˜w[t][+1]) +

_≤_ _f_ ( ˜w[t][+1]) +

_≤_ _f_ ( ˜w[t][+1]) +


(wi[t][+1] **_w)_**
_−_
_i_ _t_

X∈S _[′]_


(by Lemma 4.1)

(wi[t][+1] **_w)[⊤](wj[t][+1]_** **_w)_**
_−_ _−_

Xi≠ _j_


8|St[′][|][2]

_L_

8|St[′][|][2]

_L_

8|St[′][|][2]


_L_
_≤_ _f_ ( ˜w[t][+1]) + 8|St[′][|][2] i∈[n] **_wi[t][+1]_** _−_ **_w_** + Xi≠ _j_ (wi[t][+1] _−_ **_w)[⊤](wj[t][+1]_** _−_ **_w)_**

 [X] [2] 

_L_
_≤_ _f_ ( ˜w[t][+1]) + 8|St[′][|][2] i∈[n] **_wi[t][+1]_** _−_ **_w_** + Xi≠ _j_ **_wi[t][+1]_** _−_ **_w_** **_wj[t][+1]_** _−_ **_w_** 

 [X] [2] 

_≤_ _f_ ( ˜w[t][+1]) + 8[LN]t[|][2]  **_wi[t][+1]_** _−_ **_w_**  (by 2 ∥a∥∥b∥≤∥a∥[2] + ∥b∥[2])

_|S_ _[′]_ _i∈[n]_

 [X] [2]

_LN_
_≤_ _f_ ( ˜w[t][+1]) + 8 _t[|][2][p][min]_  _pmin_ **_wi[t][+1]_** _−_ **_w_** 

_|S_ _[′]_ _i∈[n]_

 [X] [2]

_LN_
_≤_ _f_ ( ˜w[t][+1]) + 8 _t[|][2][p][min]_  _pi_ **_wi[t][+1]_** _−_ **_w_** 

_|S_ _[′]_ _i∈[n]_

 [X] [2]

_LN_ (1 + γ)[2]
_f_ ( ˜w[t][+1]) + _pi_ _Fi(w[t])_
_≤_ 8 _t[|][2][p][min]_  _µ¯[2]_ _∇_ 

_|S_ _[′]_ _i∈[n]_

_LN_ _B[2][X](1 + γ)[2]_ [2]
_f_ ( ˜w[t][+1]) + _f_ (w[t]) (18)
_≤_ 8 _t[|][2][p][min]_ _µ¯[2]_ _∇_

_|S_ _[′]_

[2]

Put Eq. 18 and Eq. 17 together, we reach to

E _,_ _t_ [[][f] [(][w][t][+1][)]][ ≤] _[f]_ [(][w][t][)][ −] 1 − _γB_
_M_ _S_ _[′]_ _µ_ _−_ _[LB][(1 +]µµ¯_ _[ γ][)]_ _−_ _[L][(1 +]2¯µ[ γ][2][)][2][B][2]_ _−_


_LB[2](1 + γ)[2]_

(2 _t[|][ + 1) +][ B][(1 +][ γ][)]_ _f_ (w[t])
_t[|]µ[¯][2]_ _|S_ _[′]_ _µ¯_ _t[|]_ ! _−_ _[LNB]8_ _t[|][2][2][(1 +][p][min][ γ]µ[¯][2][)][2]_ ! _∇_
_|S_ _[′]_ p _|S_ _[′]_ _|S_ _[′]_

(19)

p [2]

When there is no adversary, then |S _[′]| = K, then_

E _,_ _t_ [[][f] [(][w][t][+1][)]][ ≤] _[f]_ [(][w][t][)][ −] _[κ]_ _f_ (w[t])
_M_ _S_ _[′]_ _∇_

Finally, taking the total expectation with respect to all randomness and by telescoping, one reaches[2]


_T −1_

E[∥∇f (wt)∥][2] _≤_ _f_ (w0) − _f_ (w[∗]).
_t=0_

X


Divide T on both sides, then

min
_t∈[T −1]_ [E][[][∥∇][f] [(][w][t][)][∥][]][ ≤] _T[1]_


_T −1_

E[ _f_ (wt) ][2]
_∥∇_ _∥_ _≤_ _[f]_ [(][w][0][)]κT[ −] _[f]_ [(][w][∗][)]
_t=0_

X


-----

