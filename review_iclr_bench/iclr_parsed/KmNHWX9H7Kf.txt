# UNIFORM GENERALIZATION BOUNDS FOR OVERPA## RAMETERIZED NEURAL NETWORKS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

An interesting observation in artificial neural networks is their favorable generalization error despite typically being extremely overparameterized. It is well
known that the classical statistical learning methods often result in vacuous generalization errors in the case of overparameterized neural networks. Adopting the
recently developed Neural Tangent (NT) kernel theory, we prove uniform generalization bounds for overparameterized neural networks in kernel regimes, when
the true data generating model belongs to the reproducing kernel Hilbert space
(RKHS) corresponding to the NT kernel. Importantly, our bounds capture the exact error rates depending on the differentiability of the activation functions. In
order to establish these bounds, we propose the information gain of the NT kernel
as a measure of complexity of the learning problem. Our analysis uses a Mercer
decomposition of the NT kernel in the basis of spherical harmonics and the decay
rate of the corresponding eigenvalues. As a byproduct of our results, we show
the equivalence between the RKHS corresponding to the NT kernel and its counterpart corresponding to the Mat´ern family of kernels, showing the NT kernels
induce a very general class of models. We further discuss the implications of our
analysis for some recent results on the regret bounds for reinforcement learning
and bandit algorithms, which use overparameterized neural networks.

1 INTRODUCTION

Neural networks have shown an unprecedented success in the hands of practitioners in several fields.
Examples of successful applications include classification (Krizhevsky et al., 2012; LeCun et al.,
2015) and generative modeling (Goodfellow et al., 2014; Brown et al., 2020). This practical efficiency has accelerated developing analytical tools for understanding the properties of neural networks. A long standing dilemma in the analysis of neural networks is their favorable generalization error despite typically being extremely overparameterized. It is well known that the classical
statistical learning methods for bounding the generalization error, such as Vapnik–Chervonenkis
(VC) dimension (Bartlett, 1998; Anthony & Bartlett, 2009) and Rademacher complexity (Bartlett
& Mendelson, 2002; Neyshabur et al., 2015), often result in vacuous bounds in the case of neural
networks (Dziugaite & Roy, 2017).

Motivated with the problem mentioned above, we investigate the generalization error in overparameterized neural networks. In particular, let Dn = {(xi, yi)}i[n]=1 [be a possibly noisy dataset collected]
from a true model f : yi = f (xi) + ϵi, where ϵi are well behaved zero mean noise terms. Let _f[ˆ]n_
be the learnt model using a gradient based training of an overparameterized neural network on _n._
_D_
We are interested in bounding _f_ (x) _fn(x)_ uniformly at all test points x belonging to a particular
_|_ _−_ [ˆ] _|_
domain X .

A recent line of work has shown that gradient based learning of certain overparameterized neural
networks can be analyzed using a projection on the tangent space at random initialization. This leads
to the equivalence of the solution of the neural network model to a kernel ridge regression, that is a
linear regression in the possibly infinite dimensional feature space of the corresponding kernel. The
particular kernel arising from this approach is called the neural tangent (NT) kernel (Jacot et al.,
2018). This equivalence is based on the observation that, during training, the parameters of the
neural network remain within a close vicinity of the random initialization. This regime is sometimes
referred to as lazy training (Chizat et al., 2019).


-----

1.1 CONTRIBUTIONS

Here, we summarize our contributions. We propose the maximal information gain (MIG), denoted
by γk(n), of the NT kernel k, as a measure of complexity of the learning problem for overparameterized neural networks in kernel regimes. The MIG is closely related to the effective dimension
of the kernel (for the formal definition of the MIG and its connection to the effective dimension of
the kernel, see Section 4). We provide kernel specific bounds on the MIG of the NT kernel. Using
these bounds, for networks with infinite number of parameters, we analytically prove novel generalization error bounds decaying with respect to the dataset size n. Furthermore, our results capture
the dependence of the decay rate of the error bound on the smoothness properties of the activation
functions.

Our analysis crucially uses Mercer’s theorem and the spectral decomposition of the NT kernel in
the basis of spherical harmonics (which are special functions on the surface of the hypersphere; see
Section 3). We derive bounds on the decay rate of the Mercer eigenvalues (referred to hereafter
as eigendecay) based on the differentiability of the activation functions. These eigendecays are
obtained by careful characterization of a recent result relating the eigendecay to the asymptotic
endpoint expansions of the kernel, given in (Bietti & Bach, 2020). In particular, we consider
_s −_ 1 times differentiable activation functions as(u) = (max(0, u))[s], s ∈ N. We show that, with
these activation functions and a d dimensional hyperspherical support, the eigenvalues λi of the
corresponding NT kernel decay at the rate λi _i[−]_ _[d][+2]d−[s]1[−][2]_ 1 (up to certain parity constraints on i in

special cases; see Section 2 for details). _∼_

We then use the eigendecay of the NT kernel to establish novel bounds on its MIG, which are of the
form γk(n) = [˜](n _d+2d−s1−2 ). Then, we show this implies high probability error bounds of the form_

supHilbert space (RKHS), andx∈X |f (x) −Of[ˆ]n(x)| = DO[˜](nn is distributed over2d−+42ss+1−4 ), when f belongs to the corresponding reproducing kernel X in a sufficiently informative way. In particular,

we consider a Dn that is collected sequentially in a way that maximally reduces the uncertainty in
the model. That results in a quasi-uniform dataset over the entire domain X (see Section 5.2 for
details). A summary of our results on MIG and error bounds is reported in Table 1.

As a byproduct of our results, we show that the RKHS of the NT kernel with activation function
_as(.) is equivalent to the RKHS of a Mat´ern kernel with smoothness parameter ν = s −_ 2[1] [. This]

result was already known for a special case. In particular, Geifman et al. (2020); Chen & Xu (2021)
recently showed the equivalence between the RKHS of the NT kernel with ReLU activation function, a1(.), and that of the Mat´ern kernel with ν = 12 [(also known as the Laplace kernel). Our]
contribution includes generalizing this result to the equivalence between the RKHS of the NT kernel
under various activation functions and that of a Mat´ern kernel with the corresponding smoothness
(see Section 3.2). We note that the members of the RKHS of Mat´ern family of kernels can uniformly
approximate almost all continuous functions (Srinivas et al., 2010). Therefore, our results suggest
that the NT kernels induce a very general class of models.

As an intermediate step, we also consider the simpler random feature (RF) model which is the result
of partial training of the neural network. Also referred to as conjugate kernel or neural networks
Gaussian process (NNGP) (Cho & Saul, 2009; Daniely et al., 2016; Lee et al., 2018; de G. Matthews
et al., 2018), this model corresponds to the case where only the last layer of the neural network is
trained (see Section 2 for details).

Table 1: The bounds on MIG and generalization error for RF and NT kernels, with as(.) activation functions,
when the true model f belongs to the RKHS of the corresponding kernel and is supported on the hypersphere
S[d][−][1] _⊂_ R[d].


Kernel MIG, γk(n) Error bound, _f_ (x) _fn(x)_
_|_ _−_ [ˆ] _|_

NT _O_ _n_ _d+2d−s1−2 (log(n))_ _d+22s−s−1_ 2 _O_ _n_ 2d−+42ss+1−4 (log(n)) 2dd+4+4ss−−34
  


RF _O_ _n_ _dd+2−1s (log(n))_ _d2s+2+1s_ _O_ _n_ _−2d2+4s−s1 (log(n))_ _d2+4d+4s+1s_
  

1The general notations used in this paper are defined in Appendix A.


-----

We note that the decay rate of the error bound is faster in the case of RF kernel with the same
activation function. Also, the decay rate of the error bound is faster when s is larger, for both NT
and RF kernels. In interpreting these results, we should however take into account the RKHS of the
corresponding kernels. The RKHS of the RF kernel is smaller than the RKHS of the NT kernel.
Also, when s is larger, the RKHS is smaller for both NT and RF kernels.

In Section 6, we provide experimental results on the error rate for synthetic functions belonging to
the RKHS of the NT kernel, which are consistent with the analytically predicted behaviors.

Our bounds on MIG may be of independent interest. In Section 7, we comment on the implications
of our results for the regret bounds in reinforcement learning (RL) and bandit problems, which use
overparameterized neural network models (e.g., Yang et al., 2020; Gu et al., 2021).

2 OVERPARAMETERIZED NEURAL NETWORKS IN KERNEL REGIMES

In this section, we briefly overview the equivalence of overparameterized neural network models
to kernel methods. It has been recently shown that, in overparameterized regimes, gradient based
training of a neural network reaches a global minimum, where the weights remain within a close
vicinity of the random initialization (Jacot et al., 2018). In particular, let f (x, W ) denote a neural
network with a large width m parameterized by W . The model can be approximated with its linear
projection on the tangent space at random initialization W0, as

_f_ (x, W ) ≈ _f_ (x, W0) + ⟨W − _W0, ∇W f_ (x, W0)⟩.

The approximation error can be bounded by the second order term ξm _W_ _W0_, and shown to be
_∥_ _−_ _∥[2]_
diminishing as m grows, where ξm is the spectral norm of the Hessian matrix of f (Liu et al., 2020).
The approximation becomes an equality when the width of the hidden layers approaches infinity.
Known as lazy training regime (Chizat et al., 2019), this model is equivalent to the NT kernel (Jacot
et al., 2018), given by

_k(x, x[′]) = lim_
_m→∞[⟨∇][W][ f]_ [(][x, W][0][)][,][ ∇][W][ f] [(][x][′][, W][0][)][⟩][.]

2.1 NEURAL KERNELS FOR A 2 LAYER NETWORK

Consider the following 2 layer neural network with width m and a proper normalization


_vja(wj[⊤][x][)][,]_ (1)
_j=1_

X


_f_ (x, W ) = _√m_


where a(.) is the activation function, c is a constant, x ∈X is a d dimensional input to the network,The weights wj ∈ _wR[d]j are initialized randomly according toare the weights in the first layer, and v Nj ∈(0dR, are the weights in the second layer. Id) and vj are initialized randomly_
according to N (0, 1). Then, the NT kernel of this network can be expressed as (Jacot et al., 2018)

_kNT(x, x[′]) = c[2](x[⊤]x[′])Ew_ (0d,Id)[a[′](w[⊤]x)a[′](w[⊤]x[′])] + c[2]Ew (0d,Id)[a(w[⊤]x)a(w[⊤]x[′])]. (2)
_∼N_ _∼N_

When wj are fixed at initialization, and only vj are trained with ℓ[2] regularization, the model is
equivalent to a certain Gaussian process (GP) (Neal, 2012) that is often referred to as a random
feature (RF) model. The RF kernel is given as (Rahimi et al., 2007)

_k(x, x[′]) = c[2]Ew_ (0d,Id)[a(wj[⊤][x][)][a][(][w][⊤][x][′][)]][.] (3)
_∼N_

Note that the RF kernel is the same as the second term in the expression of the NT kernel given
in equation 2.

2.2 ROTATIONAL INVARIANCE

When the input domain X is the hypersphere S[d][−][1], by the spherical symmetry of the Gaussian
distribution, the neural kernels are invariant to unitary transformations[2] and can be expressed as

2Rotationally invariant kernels are also sometimes referred to as dot product or zonal kernels.


-----

a function of x[⊤]x[′]. We thus use the notations κNT,s(x[⊤]x[′]) and κs(x[⊤]x[′]) to refer to the NT and
RF kernels, as defined in equation 2 and equation 3, respectively. In this notation, s specifies the
smoothness of the activation function, as(u) = (max(0, u))[s].

The particular form of the neural kernels depends on s. For example, for the special case of s = 1,
which corresponds to the ReLU activation function, the following closed form expressions can be
derived from the expressions given in equation 2 and equation 3.

_u_
_κNT,1(u)_ =
_π_ [(][π][ −] [arccos(][u][)) +][ κ][1][(][u][)][,]


1 − _u[2]_


_κ1(u)_


_u(π −_ arccos(u)) +


In deriving the expressions above, the constant c in equations 2 and 3 is set to _√2. In general, we_

choose c[2] = (2s 2 1)!! [, that normalizes the maximum value of the RF kernel to][ 1][:][ κ][s][(1) = 1][,][ ∀][s][ ≥] [1][.]

_−_

2.3 NEURAL KERNELS FOR MULTILAYER NETWORKS

For an l > 2 layer network, the NT kernel is recursively given as (see, Jacot et al., 2018, Theorem 1)

_κ[l]NT,s[(][u][)]_ = _c[2]κNT[l][−][1],s[(][u][)][κ]s[′]_ [(][κ][l][−][1][(][u][)) +][ κ]s[l] [(][u][)][,]

_κ[l]s[(][u][)]_ = _κs(κs[l][−][1](u))._ (4)

Here, κ[l]s[(][.][)][ corresponds to the RF kernel of an][ l][ layer network, as shown in][ Cho & Saul][ (][2009][);]

Daniely et al. (2016); Lee et al. (2018); de G. Matthews et al. (2018). In our notation, when l = 2,
we drop the layer index l from the superscript, as in the previous subsection. An expression for the
derivative κ[′]s[(][.][)][ of the RF kernel in a][ 2][ layer network, based on][ κ][s][−][1][, is given in Lemma][ 1][. That is]
needed to see the equivalence of the expressions in equation 4 to the one given in Theorem 1 of Jacot
et al. (2018).

3 THE RKHS OF THE NEURAL KERNELS

When the domain is S[d][−][1], as discussed earlier, the neural kernels have a rotationally invariant (dot product) form. The rotationally invariant kernels can be diagonalized in the basis
of spherical harmonics, which are special functions on the surface of the hypersphere. Specifically, the Mercer decomposition of the kernels
(for all l ≥ 2 and s ≥ 1) can be given as

_Nd,i_


_λ˜iφ[˜]i,j(x)φ[˜]i,j(x[′]),_
_j=1_

X


_κ(x[⊤]x[′]) =_


_i=1_


where the eigenfunction _φ[˜]i,j is the j’th spher-_
ical harmonic of degree i, _λ[˜]i is the corre-_
2i+d 2 _i+d_ 3 Figure 1: The spherical harmonics of degrees i =
sponding eigenvalue, Nd,i = _i_ _−_ _d−−2_ 1, 2, 3, 4, on S[2]. The function value is given by color.

is the multiplicity of _λ[˜]i (meaning the number _ 
of spherical harmonics of degree i, which all
share the same eigenvalue _λ[˜]i), and the conver-_
gence of the series holds uniformly on X ×X . As a consequence of Mercer’s representation theorem,
the corresponding RKHS can be written as

_Nd,i_ _Nd,i_

_∞_ 1 _∞_

_κ =_ _wi,jλ[˜]i2_ _φ[˜]i,j(_ ) : wi,j R, _f_ _κ_ [=] _wi,j[2]_ _[<][ ∞]_
_H_  _i=1_ _j=1_ _·_ _∈_ _||_ _||H[2]_ _i=1_ _j=1_ 

 X X X X 

Formal statements of Mercer’s theorem and Mercer’s representation theorem, as well as a formal[f] [(][·][) =]  _[.]_
definition of Mercer eigenvalues and eigenfunctions are given in Appendix B.


_Nd,i_

1

_wi,jλ[˜]i2_ _φ[˜]i,j(_ ) : wi,j R, _f_ _κ_ [=]
_j=1_ _·_ _∈_ _||_ _||H[2]_

X


_Hκ =_


-----

We use the notation λi (in contrast to _λ[˜]i) to denote the decreasingly ordered eigenvalues, when their_
multiplicity is taken into account (for all i : _i[′′]=1_ _[N][d,i][′′][ < i][ ≤]_ [P]i[i][′′][′] =1 _[N][d,i][′′]_ [, we define][ λ][i][ = ˜]λi′ ).

3.1 THE DECAY RATE OF THE EIGENVALUES[P][i][′][−][1]

As discussed, both RF and NT kernels can be represented in the basis of spherical harmonics. Our
analysis of the MIG relies on the decay rate of the corresponding eigenvalues. In this section, we
derive the eigendecay depending on the particular activation functions.

In the special case of ReLU activation function, several recent works (Geifman et al., 2020; Chen
& Xu, 2021; Bietti & Bach, 2020) have proven a _λ[˜]i_ _i[−][d]_ eigendecay for the NT kernel on the
hypersphere S[d][−][1]. In this work, we use a recent result from ∼ Bietti & Bach (2020) to derive the
eigendecay based on the differentiability properties of the activation functions.

**Proposition 1 Consider the neural kernels on the hypersphere S[d][−][1], and their Mercer decomposi-**
_tion in the basis of spherical harmonics. Then, the eigendecays are as reported in Table 2._

Table 2: The eigendecays of the neural kernels.

_l = 2_ For i of the same parity of s: _λ[˜]i_ _i[−][d][−][2][s][+2], λi_ _i[−]_ _[d][+2]d−[s]1[−][2]_
NT For i of the opposite parity of s: _λ[˜]i = ∼ o(i[−][d][−][2][s][+2]), λ ∼i = o(i[−]_ _[d][+2]d−[s]1[−] )[2]_


_l > 2_ _λ˜i_ _i[−][d][−][2][s][+2], λi_ _i[−]_ _[d][+2]d−[s]1[−][2]_
_∼_ _∼_

_l = 2_ For i of the opposite parity of s: _λ[˜]i_ _i[−][d][−][2][s], λi_ _i[−]_ _[d]d[+2]−1[s]_
RF For i of the same parity of s: _λ[˜]i = o( ∼i[−][d][−][2][s]), λi = ∼ o(i[−]_ _[d]d[+2]−1[s] )_

_l > 2_ _λ˜i_ _i[−][d][−][2][s], λi_ _i[−]_ _[d]d[+2]−1[s]_
_∼_ _∼_


_Proof Sketch. The proof follows from Theorem 1 of Bietti & Bach (2020), which proved that the_
eigendecay of a rotationally invariant kernel κ(.) : [−1, 1] → R can be determined based on its
asymptotic expansions around the endpoints, ±1. In particular, when

_κ(1_ _t)_ = _p+1(t) + c+1t[θ]_ + o(t[θ]),
_−_

_κ(−1 + t)_ = _p−1(t) + c−1t[θ]_ + o(t[θ]),

for polynomial p 1 and non-integer θ, the eigenvalues decay at the rate _λ[˜]i_ (c+1 + c 1)i[−][d][−][2][θ][+1],
_±_ _∼_ _−_
for evenin Appendix i, and E. They derived these expansions for the case of the ReLU activation function. Forλ[˜]i ∼ (c+1 − _c−1)i[−][d][−][2][θ][+1], for odd i. A formal statement of this result is given_
completeness, we derive the asymptotic endpoint expansions for the neural kernels with s − 1 times
differentiable activation functions, in the case of l = 2 and l > 2 layer networks. Our derivations is
based on the following Lemma.

**Lemma 1 For the RF kernel defined in equation 3, with activation function as(.), and c =** (2s 2 1)!! _[,]_

_−_
_we have_

_s[2]_
_κ[′]s[(][u][)]_ =

2s 1 _[κ][s][−][1][(][u][)][.]_
_−_


The proof is given in Appendix D. Lemma 1 allows us to recursively derive the endpoint expansions
of κs from those of κs 1, through integration. We thus obtain θ and c 1 for s > 1, when l = 2,
_−_ _±_
using a recursive relation over s. We then use the compositional forms given in equation 4 to obtain
the same, when l > 2, resulting in the eigendecays reported in Table 2.
□

**Remark 1 The parity constraints in Table 2 may imply undesirable behavior for 2 layer infinitely**
_wide networks. Ronen et al. (2019) studied this behavior and showed that adding a bias term_
_removes these constraints, for the NT kernel. Although the parity constrained eigendecay affects the_


-----

_representation power of the neural kernels, it does not affect our analysis of MIG and error rates,_
_since for that analysis we only use_ _λ[˜]i = O(i[−][d][−][2][s][+2])._

**Remark 2 We note that our results do not necessarily apply to deep neural networks in the sense**
_that the implied constants in Table 2 grow exponentially in l, for s > 1 (see Appendix E for the_
_expressions of the constants). When s = 1, however, the constants grow at most as fast as l[2]_ _(as_
_shown in Bietti & Bach, 2020)._

3.2 EQUIVALENCE OF THE RKHSS OF THE NEURAL AND MATERN KERNELS´

Our bounds on the eigendecay of the neural kernels show the connection between the RKHS of the
neural kernels, and that of Met´ern family of kernels. Let Hkν denote the RKHS of the Mat´ern kernel
with smoothness parameter ν.

**Theorem 1 On a hyperspherical domain, when l > 2, HκlNT,s** _[(][H][κ]s[l]_ _[) is equivalent to][ H][k][ν]_ _[, and, when]_

_Tablel = 2, 3 H._ _κlNT,s_ _[⊂H][k][ν][ (][H][κ]s[l]_ _[⊂H][k][ν]_ _[), where][ ν][ =][ s][ −]_ 2[1] _[(][ν][ =][ s][ +][ 1]2_ _[). The statement is summarized in]_

Table 3: The relation between the RKHSs corresponding to the neural and Mat´ern kernels.

Kernel _l = 2_ _l > 2_


NT _HκNT,s ⊂Hks−_ 12

RF _Hκs ⊂Hks+ 12_


_HκNT,s ≡Hks−_ 12

_Hκs ≡Hks+ 12_


The formal definition of equivalence (≡) and subset (⊂) relation between two normed spaces is
given in Appendix A.

_Proof Sketch. The proof follows from the eigendecay of the neural kernels given in Table 2, the_
eigendecay of the Mat´ern family of kernels on manifolds proven in Borovitskiy et al. (2020), and
the Mercer’s representation theorem. Details are given in Appendix F.
□

**Remark 3 The observations that when l = 2, we only have HκlNT,s** _[⊂H][k]s−_ 2[1] _[, and not vice versa, is]_

_a result of parity constraints in Table 2. The subset relation can change to equivalence, with a bias_
_term which removes the parity constraints (see, Ronen et al., 2019)._

A special case of Theorem 1, when s = 1, was recently proven in (Geifman et al., 2020; Chen &
Xu, 2021). This special case pertains to the ReLU activation function and the Mat´ern kernel with
smoothness parameter ν = [1]2 [, that is also referred to as the Laplace kernel.]

**Remark 4 It is known that the RKHS of a Mat´ern kernel with smoothness parameter ν is equivalent**
_to a Sobolev space with parameter ν +_ _[d]2_ _[(e.g., see,][ Kanagawa et al.][,][ 2018][, Section][ 4][). This obser-]_

_vation provides an intuitive interpretation for the RKHS of the neural kernels as a consequence of_
_Theorem 1. That is ∥f_ _∥κlNT,s_ _[(][∥][f]_ _[∥][κ]s[l]_ _[) is proportional to the cumulative][ L][2][ norm of the weak deriva-]_

_tives of f up to s +_ _[d][−]2_ [1] _(s +_ _[d][+1]2_ _[) order. I.e.,][ f][ ∈H][κ]NT[l]_ _,s_ _[(][f][ ∈H][κ]s[l]_ _[) translates to the existence of]_

_weak derivatives of f up to s_ + _[d][−]2_ [1] _(s_ + _[d][+1]2_ _[) order, which can be understood as a versatile measure]_

_for the smoothness of f controlled by s. For the details on the definition of weak derivatives and_
_Sobolev spaces see, e.g., Hunter & Nachtergaele (2011)._


4 MAXIMAL INFORMATION GAIN OF THE NEURAL KERNELS

In this section, we use the eigendecay of the neural kernels shown in the previous section to derive bounds on their information gain, which will then be used to prove uniform bounds on the
generalization error.


-----

4.1 INFORMATION GAIN: A MEASURE OF COMPLEXITY

To define information gain, we need to introduce a fictitious GP model. Assume F is a zero mean
GP indexed on with kernel k. Information gain then refers to the mutual information (Yn; F )
_X_ _I_
between the data values Yn = [yi][n]i=1 [and][ F] [. From the closed from expression of mutual information]
between two multivariate Gaussian distributions (Cover, 1999), it follows that

(Yn; F ) = [1] **_In + [1]_** _._
_I_ 2 [log det] _λ[2][ K][n]_

 


Here Kn is the kernel matrix [Kn]i,j = k(xi, xj), and λ > 0 is a regularization parameter. We also
define the data independent and kernel specific maximal information gain (MIG) as follows.

_γk(n) = sup_ (Yn; F ). (5)
_Xn⊂X_ _I_

The MIG is also closely related to the effective dimension of the kernel. In particular, although
the feature space of the neural kernels are infinite dimensional, for a finite dataset, the number of
features with a significant impact on the regression model can be finite. The effective dimension of
a kernel is often defined as (Zhang, 2005; Valko et al., 2013)

_d˜k(n) = Tr_ **Kn(Kn + λ[2]In)[−][1][]** _._ (6)
 

It is known that the information gain and the effective dimension are the same up to logarithmic
factors. Specifically, _d[˜]k(n)_ (Yn; F ), and (Yn; F ) = ( d[˜]k(n) log(n)) (Calandriello et al.,
_≤I_ _I_ _O_

2019).

Relating the information gain to the effective dimension provides some intuition into why it can
capture the complexity of the learning problem. Roughly speaking, our bounds on the generalization
error are analogous to the ones for a linear model, which has a feature dimension of _d[˜]k(n)._

4.2 BOUNDING THE INFORMATION GAIN OF THE NEURAL KERNELS

In the following theorem, we provide a bound on the maximal information gain of the neural kernels.

**Theorem 2 For the neural kernels with all l ≥** 2, on a hyperspherical domain X = S[d][−][1], we have

_γκlNT,s_ [(][n][)] = _n_ _d+2d−s1−2 (log(n))_ _d+22s−s−1_ 2 _,_ _in the case of NT kernel,_
_O_

_γκls_ [(][n][)] = _O_ n _dd+2−1s (log(n))_ _d2s+2+1s_ _,_ in the case of RF kernel.
 


A special case of our Theorem 2, for the special case of the ReLU activation function, was recently
proved in Kassraie & Krause (2021).

_Proof Sketch. The main components of the analysis include the eigendecay given in Proposition 1,_
a projection on a finite dimensional RKHS technique proposed in Vakili et al. (2021b), and a bound
on the sum of spherical harmonics of degree i determined by the Legendre polynomials, that is
sometimes referred to as the Legendre addition theorem (Maleˇcek & N´aden´ık, 2001). Details are
given in Appendix G.
□

5 UNIFORM BOUNDS ON THE GENERALIZATION ERROR

In this section, we use the bound on MIG from previous section to establish uniform bounds on the
generalization error.

5.1 IMPLICIT ERROR BOUNDS

We first overview the implicit (data dependent) error bounds for the kernel methods under the following assumptions.


-----

**Assumption 1 Assume the true data generating model f is in the RKHS corresponding to a neural**
_kernel κ. In particular, ∥f_ _∥Hκ ≤_ _B, for some B > 0. In addition, assume that the observation_
_noise ϵi are independent R sub-Gaussian random variables. That is E[exp(ηϵi)]_ exp( _[η][2]2[R][2]_ ),
_≤_

_∀η ∈_ R, ∀i ≥ 1, where yi = f (xi) + ϵi, ∀i ≥ 1.

Under Assumption 1, for a fixed x ∈X, we have, with probability at least 1−δ (Vakili et al., 2021a),

_f_ (x) _fn(x)_ _β(δ)σn(x),_ (7)
_|_ _−_ [ˆ] _| ≤_

where _f[ˆ]n(x) = k[⊤]n_ [(][x][)(][K][n] [+][ λ][2][I][n][)][−][1][Y][n] [is the solution to the kernel ridge regression using][ κ]
and _n, σn[2]_ [(][x][) =][ k][(][x, x][)][ −] **[k]n[⊤][(][x][)(][K][n]** [+][ λ][2][I][n][)][−][1][k][n][(][x][)][,][ k][⊤]n [(][x][) = [][κ][(][x][⊤][x][i][)]]i[n]=1[, and][ β][(][δ][) =]
_D_

_B +_ _[R]λ_ 2 log( [2]δ [)][. Equation][ 7][ provides a high probability bound on][ |][f] [(][x][)][ −] _f[ˆ]n(x)|. The decay rate_

of this bound based onq _n, however, is not explicit._

5.2 EXPLICIT ERROR BOUNDS

In this section, we use the bounds on MIG and the implicit error bounds from the previous subsection
to provide explicit (in n) bounds on the error. It is clear that with no assumption on the distribution
of the data, generalization error cannot be nontrivially bounded. For example, if all the data points
are collected from a small region of the input domain, it is not expected for the error to be small
far from this small region. We here consider a dataset that is distributed over the input space in a
sufficiently informative way. For this purpose, we introduce a data collection module which collects
the data points based on the current uncertainty level of the kernel model. In particular, consider a
dataset [˜]n collected as follows: xi = arg maxx _σi_ 1(x), where σi(.) is defined above. We have
_D_ _∈X_ _−_
the following result.

**Theorem 3 Consider the neural kernels with l ≥** 2, on a hyperspherical domain X = S[d][−][1]. Con_sider a model_ _f[ˆ]n trained on_ _D[˜]n. Under Assumption 1, with probability at least 1 −_ _δ, uniformly in_
_x ∈X_ _,_

_f_ (x) _fn(x)_ = _n_ 2d−+42ss+1−4 (log(n)) 2d2+4s−s1−4 (log( _[n][d][−][1]_ )) 12 _,_ _in the case of NT kernel,_
_|_ _−_ [ˆ] _|_ _O_ _δ_
 


_f_ (x) _fn(x)_ = _n_ _−2d2+4s−s1 (log(n))_ 22ds+4+1s (log( _[n][d][−][1]_
_|_ _−_ [ˆ] _|_ _O_ _δ_



1
)) 2 _,_ _in the case of RF kernel._ (8)



_Proof Sketch._ The proof follows the same steps as in the proof of Theorem 3 of Vakili et al.

(2021a). The key step is bounding the total uncertainty in the model with the information gain
that is _i=1_ _[σ]i[2]−1[(][x][i][)][ ≤]_ log(1+2 _λ[1][2][ )]_ _[I][(][Y][n][;][ F]_ [)][ (][Srinivas et al.][,][ 2010][). This allows us to bound the][ σ][n]

in the right hand side of equation 7 by _γκ(n)/n up to multiplicative absolute constants. Plugging_

in the bounds on[P][n] _γκ(n) from Theorem 2, and using the implicit error bound given in equation 7_

p

(with a union bound on a discretization of the domain with size O(n[d][−][1])), we obtain equation 8. A
detailed proof is given in Appendix H.
□

6 EXPERIMENTS

In this section, we provide experimental results on the error rates. In our experiments, we create
synthetic data from a true model f that belongs to the RKHS of a NT kernel κ. For this purpose, we
create two random vectors _X[ˆ]n0 ∈X_ _[n][0]_ and _Y[ˆ]n0 ∈_ R[n][0], and let f (.) = k[ˆ][⊤]n0 [(][.][)( ˆ]Kn0 + δ[2]In0 )[−][1][ ˆ]Yn0,
where **k[ˆ]n0 and** **K[ˆ]** _n0 are defined similar to kn and Kn in Subsection 5.1. We then generate datasets_
_Dn of various sizes n = 2[i], with i = 1, 2, . . ., 13, according to the underlying model f_ . We train
the model to obtain _f[ˆ]n. Figure 2 shows the error rate versus the size n of the dataset for various d_
and s. The experiments show that the error converges to zero at a rate satisfying the bounds given
in Theorem 3. In addition, as analytically predicted, the absolute value of the error rate exponent
increases with s and decreases with d. Our experiments use the neural-tangents library (Novak et al.,
2019). See Appendix I for more details.


-----

Figure 2: Left: the exponent of the error rate versus s and d. As expected, larger values of s and d
result in, respectively, faster and slower error decays. The bars show the standard deviation. Right:
error rates for s = 1, 2, 3 and d = 2, 3, 4, shown in separate panels. Both axes are in log scale so that
the slope of the line represents the exponent of the error rate. Experimental error rates are consistent
with the analytically predicted results.

7 DISCUSSION

Our bounds on MIG may be of independent interest in other problems. Recent works on RL and
bandit problems, which use overparameterized neural network models, derived an [˜](γk(n)[√]n)
_O_
regret bound (see, e.g., Zhou et al., 2020; Yang et al., 2020; ZHANG et al., 2021; Gu et al., 2021).
These works however did not characterize the bound on γk(n), leaving the regret bounds implicit.
A consequence of our bounds on γk(n) is an explicit (in n) regret bound for these RL and bandit
problems. Our results however have mixed implications by showing that the existing regret bounds
are not necessarily sublinear in n. In particular, plugging in our bound on γκNT,s (n) into the existing

regret bounds, we get _O[˜](n_ 1d.5+2d+ss−−22 ), which is sublinear only when s > _[d]2_ [(that, e.g., excludes the]

ReLU activation function). Our analysis of various activation functions is thus essential in showing
sublinear regret bounds for at least some cases. As recently formalized in Vakili et al. (2021c), it
remains an open problem whether the analysis of RL and bandit algorithms in kernel regimes can
improve to have an always sublinear regret bound. We note that this open problem and the analysis
of RL and bandit problems were not considered in this work. Our remark is mainly concerned with
the consequences of our bound on MIG, which appears in the regret bounds.

The recent work of Wang et al. (2020) proved that the L[2] norm of the error is bounded as _f_
_d_ _∥_ _−_
_fˆn_ _L2 =_ (n[−] 2d−1 ), for a two layer neural network with ReLU activation functions in kernel
_∥_ _O_

regimes. Bordelon et al. (2020) decomposed the L[2] norm of the error into a series corresponding to
eigenfunctions of the NT kernel and provided bounds based on the corresponding eigenvalues. Our
results are stronger as they are given in terms of absolute error instead of L[2] norm. In addition, we
provide explicit error bounds depending on differentiability of the activation functions.

The MIG has been studied for popular GP kernels such as Mat´ern and Squared Exponential (Srinivas
et al., 2010; Vakili et al., 2021b). Our proof technique is similar to that of Vakili et al. (2021b).
Their analysis however does not directly apply to the NT kernel on the hypersphere. The reason
is that Vakili et al. (2021b) assume uniformly bounded eigenfunctions. In our analysis, we use a
Mercer decomposition of the NT kernel in the basis of spherical harmonics. Those are not uniformly
bounded. Technical details are provided in the analysis of Theorem 2. Applying a proof technique
similar to Srinivas et al. (2010) results in suboptimal bounds in our case.

Our work may be relevant to the recent developments in the field of implicit neural representations
(Mildenhall et al., 2020; Sitzmann et al., 2020; Fathony et al., 2020). In this line of work, the input to
a neural network often represents the coordinates of a plane (image), a camera position or angle, and
the function to be approximated is the image or the scene itself. Interestingly, it has been observed
that smooth activation functions perform better than ReLU in these applications, since the models
are also smooth (Sitzmann et al., 2020).


-----

REFERENCES

Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge
university press, 2009.

Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International
_Conference on Machine Learning, pp. 322–332. PMLR, 2019._

Peter L Bartlett. The sample complexity of pattern classification with neural networks: the size of
the weights is more important than the size of the network. IEEE transactions on Information
_Theory, 44(2):525–536, 1998._

Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.

Alberto Bietti and Francis Bach. Deep equals shallow for ReLU networks in kernel regimes. arXiv
_preprint arXiv:2009.14397, 2020._

Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in
kernel regression and wide neural networks. In International Conference on Machine Learning,
pp. 1024–1034. PMLR, 2020.

Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, and Marc Deisenroth. Mat´ern Gaussian processes on Riemannian manifolds. In Advances in Neural Information Processing Systems,
volume 33, pp. 12426–12437, 2020.

James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, and Skye Wanderman-Milne. Jax: composable transformations of python+ numpy
[programs. Version 0.1, 55, 2018. URL https://github.com/google/jax.](https://github.com/google/jax)

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.

Daniele Calandriello, Luigi Carratino, Alessandro Lazaric, Michal Valko, and Lorenzo Rosasco.
Gaussian process optimization with adaptive sketching: scalable and no regret. In Proceedings
_of the Thirty-Second Conference on Learning Theory, volume 99 of Proceedings of Machine_
_Learning Research, Phoenix, USA, 25–28 Jun 2019. PMLR._

Lin Chen and Sheng Xu. Deep neural tangent kernel and Laplace kernel have the same RKHS. In
_International Conference on Learning Representations, 2021._

Louis HY Chen, Larry Goldstein, and Qi-Man Shao. Multivariate normal approximation. In Normal
_Approximation by Stein’s Method, pp. 313–341. Springer, 2011._

L´ena¨ıc Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, volume 32, 2019.

Youngmin Cho and Lawrence Saul. Kernel methods for deep learning. In Advances in Neural
_Information Processing Systems, volume 22, 2009._

Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International
_Conference on Machine Learning, pp. 844–853, 2017._

Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.

Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. Advances In Neural Information
_Processing Systems, 29:2253–2261, 2016._

Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. In International Conference on
_Learning Representations, 2018._


-----

Vincent Dutordoir, Nicolas Durrande, and James Hensman. Sparse gaussian processes with spherical
harmonic features. In International Conference on Machine Learning, pp. 2793–2802. PMLR,
2020.

Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
_arXiv:1703.11008, 2017._

Rizal Fathony, Anit Kumar Sahu, Devin Willmott, and J Zico Kolter. Multiplicative filter networks.
In International Conference on Learning Representations, 2020.

Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Basri Ronen. On
the similarity between the Laplace and neural tangent kernels. In Advances in Neural Information
_Processing Systems, volume 33, pp. 1451–1461, 2020._

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
_processing systems, 27, 2014._

Quanquan Gu, Amin Karbasi, Khashayar Khosravi, Vahab Mirrokni, and Dongruo Zhou. Batched
neural bandits. arXiv preprint arXiv:2102.13028, 2021.

John K. Hunter and Bruno Nachtergaele. Applied Analysis. World Scientific, 2011.

Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems, volume 31,
2018.

Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath K Sriperumbudur. Gaussian
processes and kernel methods: A review on connections and equivalences. Available at Arxiv.,
2018.

Parnian Kassraie and Andreas Krause. Neural contextual bandits without regret. arXiv preprint
_arXiv:2107.03144, 2021._

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25:1097–1105,
2012.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444,
2015.

Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as Gaussian processes. In International Conference on
_Learning Representations, 2018._

Chaoyue Liu, Libin Zhu, and Mikhail Belkin. On the linearity of large non-linear models: when
and why the tangent kernel is constant. Advances in Neural Information Processing Systems, 33,
2020.

Kamil Maleˇcek and Zbynˇek N´aden´ık. On the inductive proof of Legendre addition theorem. Studia
_Geophysica et Geodaetica, 45(1):1–11, 2001._

J Mercer. Functions of positive and negative type and their commection with the theory of integral
equations. Philos. Trinsdictions Rogyal Soc, 209:4–415, 1909.

Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and
Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European
_conference on computer vision, pp. 405–421. Springer, 2020._

Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business
Media, 2012.

Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376–1401. PMLR, 2015.


-----

Ivan Nourdin and Giovanni Peccati. Stein’s method on wiener chaos. _Probability Theory and_
_Related Fields, 145(1-2):75–118, 2009._

Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A Alemi, Jascha Sohl-Dickstein,
and Samuel S Schoenholz. Neural tangents: Fast and easy infinite neural networks in
[python. arXiv preprint arXiv:1912.02803, 2019. URL https://github.com/google/](https://github.com/google/neural-tangents)
[neural-tangents.](https://github.com/google/neural-tangents)

Ali Rahimi, Benjamin Recht, et al. Random features for large-scale kernel machines. In NIPS,
volume 3, pp. 5. Citeseer, 2007.

Basri Ronen, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neural networks for learned functions of different frequencies. Advances in Neural Information Processing
_Systems, 32:4761–4771, 2019._

Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. Advances in Neural Information
_Processing Systems, 33, 2020._

Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias W. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In ICML, pp. 1015–1022,
2010.

Charles Stein. Miscellaneous frontmatter. In Approximate computation of expectations, pp. i–iii.
Institute of Mathematical Statistics, 1986.

Elias M Stein and Guido Weiss. Introduction to Fourier Analysis on Euclidean Spaces (PMS-32),
_Volume 32. Princeton university press, 2016._

Ingo Steinwart and Andreas Christmann. Support vector machines. Springer, 2008.

Sattar Vakili, Nacime Bouziani, Sepehr Jalali, Alberto Bernacchia, and Da-shan Shiu. Optimal order
simple regret for Gaussian process bandits. arXiv preprint arXiv:2108.09262, 2021a.

Sattar Vakili, Kia Khezeli, and Victor Picheny. On information gain and regret bounds in gaussian
process bandits. In International Conference on Artificial Intelligence and Statistics, pp. 82–90.
PMLR, 2021b.

Sattar Vakili, Jonathan Scarlett, and Tara Javidi. Open problem: Tight online confidence intervals
for RKHS elements. In Conference on Learning Theory, pp. 4647–4652. PMLR, 2021c.

Michal Valko, Nathan Korda, R´emi Munos, Ilias Flaounas, and Nello Cristianini. Finite-time analysis of kernelised contextual bandits. In Proceedings of the Twenty-Ninth Conference on Un_certainty in Artificial Intelligence, UAI’13, pp. 654–663, Arlington, Virginia, USA, 2013. AUAI_
Press.

Wenjia Wang, Tianyang Hu, Cong Lin, and Guang Cheng. Regularization matters: A nonparametric
perspective on overparametrized neural network. 2020.

Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael I Jordan. On function approximation in reinforcement learning: Optimism in the face of large state spaces. In Advances in
_Neural Information Processing Systems, 2020._

Tong Zhang. Learning bounds for kernel regression using effective data dimensionality. Neural
_Computation, 17(9):2077–2098, 2005._

Weitong ZHANG, Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural Thompson sampling. In
_International Conference on Learning Representations, 2021._

Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with UCB-based exploration. In International Conference on Machine Learning, pp. 11492–11502. PMLR, 2020.


-----

A GENERAL NOTATION

In this section, we formally define our general notations. We use the notations 0n and In to denote
the zero vector and the square identity matrix of dimension n, respectively. For a matrix M (a vector
_v), M_ _[⊤]_ (v[⊤]) denotes its transpose. In addition, det and log det are used to denote determinant of M
and its logarithm, respectively. The notation _v_ _l2 is used to denote the l[2]_ norm of a vector v. For
_∥_ _∥_
_v ∈_ R[d] and a symmetric positive definite matrix Σ ∈ R[d][×][d], N (v, Σ) denotes a normal distribution
with mean v and covariance Σ. The Kronecker delta is denoted by δi,j. The notation S[d][−][1] denotes
the d dimensional hypersphere in R[d]. For example, S[2] _⊂_ R[3] is the usual sphere. The notations
_o and O denote the standard mathematical orders, while_ _O[˜] is used to denote O up to logarithmic_
_bfactors. For two sequencesn = O(an). For s ∈_ N, we define an, bn : (2 Ns → − 1)!! =R, we use the notationi=1[(2][i][ −] [1)][. For example,] an ∼ _bn, when[ 3!! = 3] an =[ and] O[ 5!! = 15](bn) and[.]_

For a normed space, we use _._ to denote the norm associated with . For two normed spaces
_HSecond, there exist a constant1, H2, we write H H1 ⊂H2, if the following two conditions are satisfied. First, c ∥1 >∥H 0, such that_ [Q]f _[s]_ 2 _c1_ _f_ 1, for all H _f_ _H11. We also write ⊂H2 as sets._
_H1 ≡H2, when both H1 ⊂H2 and H2 ⊂H1._ _∥_ _∥H_ _≤_ _∥_ _∥H_ _∈H_

The derivative of a : R → R is denoted by a[′]. We define 0[0] = 0, so that a0(x) = (max(0, x))[0]
corresponds to the step function: a0(x) = 0, when x 0, and a0(x) = 1, when x > 0.
_≤_

B MERCER’S THEOREM

In this section, we give an overview of Mercer’s theorem, as well as the the reproducing kernel
Hilbert spaces (RKHSs) associated with the kernels. Mercer’s theorem (Mercer, 1909) provides a
spectral decomposition of the kernel in terms of an infinite dimensional feature map (see, e.g. Steinwart & Christmann (2008), Theorem 4.49).

**Theorem 4 (Mercer’s Theorem ) Let X be a compact domain. Let k be a continuous square inte-**
_grable kernel with respect to a finite Borel measure µ. Define a positive definite operator Tk_


(Tkf )(.) =


_k(., x)f_ (x)dµ.


_Then, there exists a sequence of eigenvalue-eigenfunction pairs {(λi, φi)}i[∞]=1_ _[such that][ λ][i][ ∈]_ [R][+][,]
_and Tkφi = λiφi, for i ≥_ 1. Moreover, the kernel function can be represented as


_k(x, x[′]) =_


_λiφi(x)φi(x[′]),_
_i=1_

X


_where the convergence of the series holds uniformly on X × X_ _._

The λi and φi defined in Mercer’s theorem are referred to as Mercer eigenvalues and Mercer eigenfunctions, respectively.

Let Hk denote the RKHS corresponding to k, defined as a Hilbert space equipped with an inner
product ⟨., .⟩Hk satisfying the following: k(., x) ∈Hk, ∀x ∈X, and ⟨f, k(., x)⟩Hk = f (x),
_∀x ∈X_ _, ∀f ∈Hk (reproducing property). As a consequence of Mercer’s theorem, Hk can be_
represented in terms of {(λi, φi)}i[∞]=1[, that is often referred to as Mercer’s representation theorem]
(see, e.g., Steinwart & Christmann (2008), Theorem 4.51).

**Theorem 5 (Mercer’s Representation Theorem) Let {(λi, φi)}i[∞]=1** _[be the Mercer eigenvalue-]_
_eigenfunction pairs. Then, the RKHS corresponding to k is given by_


1

_wiλi2_ _[φ][i][(][·][) :][ w][i][ ∈]_ [R][,][ ||][f] _[||][2]_ _k_ [:=]

_H_

_i=1_

X


_wi[2]_ _[<][ ∞]_
_i=1_

X


_Hk =_


_f_ (·) =


Mercer’s representation theorem indicates that1 1 _{λi2_ _[φ][i][}]i[∞]=1_ [form an orthonormal basis for][ H][k][:]

_⟨λi2_ _[φ][i][, λ]i2[′]_ _[φ][i][′]_ _[⟩][H]k_ [=][ δ][i,i][′] [. It also provides a constructive definition for the RKHS as the span of]

this orthonormal basis, and a constructive definition for the ∥f _∥Hk as the l[2]_ norm of the weights

[wi][∞]i=1 [vector.]


-----

C CLASSICAL APPROACHES TO BOUNDING THE GENERALIZATION ERROR

There are classical statistical learning methods which address the generalization error in machine
learning models. Two notable approaches are VC dimension (Bartlett, 1998; Anthony & Bartlett,
2009) and Rademacher complexity (Bartlett & Mendelson, 2002; Neyshabur et al., 2015). In the
former, the expected generalization loss is bounded by the square root of the VC dimension divided
by the square root of n. In the latter, the expected generalization loss scales with the Rademacher
complexity of the hypothesis class. In the case of a 2 layer neural network of width m, it is shown
that both of these approaches result in a generalization bound of O( _m/n) that is vacuous for_

overparameterized neural networks with a very large m.

p

A more recent approach to studying the generalization error is PAC Bayes. It provides non-vacuous
error bounds that depend on the distribution of parameters (Dziugaite & Roy, 2017). However, the
distribution of parameters must be known or be estimated in order to use those bounds. Taking a
different approach, Arora et al. (2019) provides bounds on expected generalization error of wellconditioned 2 layer neural networks based on the Gram matrix of the input data, which does not
depend on the distribution of model parameters as in PAC Bayes. In contrast to all of these results,
our bounds are explicit and are much stronger in the sense that they hold uniformly in x.

D PROOF OF LEMMA 1

Lemma 1 offers a recursive relation over s for the RF kernel. We prove the lemma by taking the
derivative of κs(.), and applying the Stein’s lemma (given at the end of this section).

Let x = [0, 0, . . ., 0, 1][⊤] and x[′] = [0, 0, . . ., _√1 −_ _u[2], u][⊤], so that x[⊤]x[′]_ = u, and x, x[′] _∈_ S[d][−][1].

We have


_as(w[⊤]x)sas−1(w[⊤]x[′])(wd −_ _√[uw]1_ _[d][−]u[1][2][ )]_

_−_


_∂_

_as(w[⊤]x)as(w[⊤]x[′])_ = Ew (0d,Id)
_∂u_ [E][w][∼N][ (][0][d][,][I][d][)] _∼N_
 


= _sEw_ (0d,Id) _sas_ 1(w[⊤]x)as 1(w[⊤]x[′])
_∼N_ _−_ _−_



(s 1)u

+(s 1)uas(w[⊤]x)as 2(w[⊤]x[′]) _sEw_ (0d,Id) _−_ 1 _u[2]as(w[⊤]x)as_ 2(w[⊤]x[′])
_−_ _−_ _−_ _∼N_ _√1_ _u[2]_ _−_ _−_
  _−_ p 

= _s[2]Ew_ (0d,Id) _as_ 1(w[⊤]x)as 1(w[⊤]x[′]) _._
_∼N_ _−_ _−_
 

The first equation is obtained by taking derivative of the term inside the expectation. The second
equation is obtained by applying Stein’s lemma to E[as(w[⊤]x)sas 1(w[⊤]x[′])wd], where wd is the
_−_
normally distributed random variable, also to E[as(w[⊤]x)sas−1(w[⊤]x[′]) _√[uw]1_ _[d][−]u[1][2][ ]][, where][ w][d][−][1][ is the]_

_−_
normally distributed random variable.


Taking into account the constant normalization of the RF kernel c[2] =


2

(2s 1)!! [, we have]
_−_


_κ[′]s[(][u][)]_

which proves the lemma.


2 _∂_

_as(w[⊤]x)as(w[⊤]x[′])_

(2s − 1)!! _∂u_ [E][w][∼N][ (][0][d][,][I][d][)]

2s[2]  

(2s − 1)!! [E][w][∼N][ (][0][d][,][I][d][)] _as−1(w[⊤]x)as−1(w[⊤]x[′])_

_s[2]_ 

2s 1 _[κ][s][−][1][,]_
_−_


**Lemma 2 (Stein’s Lemma) Suppose X ∼N** (0, 1) is a normally distributed random variable.
_Consider a function g : R →_ R such that both E[Xg(X)] and E[g[′](X)] exist. We then have

E[Xg(X)] = E[g[′](X)].

The proof of Stein’s lemma follows from an integration by parts (see, e.g., Stein, 1986; Nourdin &
Peccati, 2009; Chen et al., 2011).


-----

E PROOF OF PROPOSITION 1

This proposition follows from Theorem 1 of Bietti & Bach (2020), which proved that the eigendecay
of a rotationally invariant kernel κ : [−1, 1] →∞ can be determined based on its asymptotic
expansions around the endpoints ±1. Recall that a rotationally invariant kernel k can be represented
as κ(x[⊤]x[′]) = k(x, x[′]). Their result is formally given in the following lemma.

**Lemma 3 (Theorem 1 in Bietti & Bach (2020)) Assume κ : [−1, 1] →** R is C _[∞]_ _and has the fol-_
_lowing asymptotic expansions around ±1_

_κ(1_ _t)_ = _p+1(t) + c+1t[θ]_ + o(t[θ]),
_−_

_κ(−1 + t)_ = _p−1(t) + c−1t[θ]_ + o(t[θ]),

_for t > 0, where p_ 1 are polynomials, and θ > 0 is not an integer. Also, assume that the derivatives
_±_
_of κ admit similar expansions obtained by differentiating the above ones. Then, there exists Cd,θ_
_such that, when i is even, if c+1_ = _c_ 1: _λ[˜]i_ (c+1 + c 1)Cd,θi[−][d][−][2][θ][+1], and, when i is odd,
_̸_ _−_ _−_ _∼_ _−_
_if c+1_ = c 1: _λ[˜]i_ (c+1 _c_ 1)Cd,θl[−][d][−][2][θ][+1]. In the case _c+1_ = _c_ 1 _, then we have_ _λ[˜]i =_
_̸_ _−_ _∼_ _−_ _−_ _|_ _|_ _|_ _−_ _|_
_o(l[−][d][−][2][θ][+1]) for one of the two parities. If κ is infinitely differentiable on [−1, 1] so that no such θ_
_exists, the_ _λ[˜]i decays faster than any polynomial._

Building on this result, the eigendecays can be obtained by deriving the endpoint expansions for the
RF and NT kernels. That is derived in Bietti & Bach (2020) for the ReLU activation function. For
completeness, here, we derive the endpoint expansions for RF and NT kernels associated with s − 1
times differentiable activation functions as(.).

Recall that for a 2 layer network, the corresponding RF and NT kernels are given by

_κNT,s(x[⊤]x[′])_ = _c[2](x[⊤]x[′])Ew_ (0d,Id)[a[′]s[(][w][⊤][x][)][a]s[′] [(][w][⊤][x][′][)] +][ κ][s][(][x][⊤][x][′][)][,]
_∼N_

_κs(x[⊤]x[′])_ = _c[2]Ew_ (0d,Id)[as(w[⊤]x)as(w[⊤]x[′])].
_∼N_

For the special cases of s = 0 and s = 1, the following closed form expressions can be derived by
taking the expectations.

_κ0(u)_ := Ew (0d,Id)[a0(w[⊤]x)a0(w[⊤]x[′])] = [1]
_∼N_ _π_ [(][π][ −] [arccos(][u][))][,]


1 − _u[2]_


_κ1(u)_


_u(π −_ arccos(u)) +


where u = x[⊤]x[′]. At the endpoints 1, κ0(u), κ1(u) have the following asymptotic expansions.
_±_


2 1 1

_π [t]_ 2 + o(t 2 ),


_κ0(1_ _t)_ = 1
_−_ _−_


2 1 1

_π [t]_ 2 + o(t 2 ),


_κ0(_ 1 + t)
_−_


_√2_ 3 3
_κ1(1_ _t)_ = 1 _t + [2]_ 2 + o(t 2 ),
_−_ _−_ 3π [t]


2 3 3

2 + o(t 2 ). (9)
3π [t]


_κ1(_ 1 + t)
_−_


These endpoint expansions where used in Bietti & Bach (2020) to obtain the eigendecay of the RF
and NT kernels with ReLU activation functions.

We first extend the results on the endpoint expansions and the eigendecay to a 2 layer neural network
with s > 1, in Subsection E.1. Then, we extend the derivation to l > 2 layer neural networks, in
Subsection E.2. In Subsection E.3, we show how the eigendecays in terms of _λ[˜]i can be translated to_
the ones in terms of λi.


-----

E.1 ENDPOINT EXPANSIONS FOR 2 LAYER NETWORKS WITH s > 1

We first note that the normalization constant c[2] = (2s 2 1)!! [, suggested in Section][ 2][, ensures][ κ][s][(1) =]

_−_
1, for all s ≥ 1. The reason is that, by the well known values for the even moments of the normal
distribution, we have

Ew (0d,Id)[as(w[⊤]x)as(w[⊤]x[′])] = [(2][s][ −] [1)!!] _,_
_∼N_ 2

when x[⊤]x[′] = 1. Also notice that κs( 1) = 0, for all s 1. The reason is that when x[⊤]x[′] = 1,
_−_ _≥_ _−_
at least one of w[⊤]x and w[⊤]x[′] is non-positive.

We note that the normalization constant is considered only for the convenience of some calculations,
and does not affect the exponent in the eigendecays. Specifically, scaling the kernel with a constant
factor, scales the corresponding Mercer eigenvalues with the same constant. The constant factor
scaling does not affect the Mercer eigenfunctions.

From Lemma 1, recall

_s[2]_
_κ[′]s[(][.][) =]_

2s 1 _[κ][s][−][1][(][.][)][.]_
_−_

This is a key component in deriving the endpoint expansions for s > 1. In particular, this allows us
to recursively obtain the endpoint expansions of κs(.) from those of κs−1(.), by integration. That
results in the following expansions for κs.


2s+1
_κs(1_ _t)_ = _p+1,s(t) + c+1,st_ 2
_−_ 2s+1

_κs(−1 + t)_ = _p−1,s(t) + c−1,st_ 2


2s+1
+ o(t 2 ),

2s+1
+ o(t 2 ), (10)


_s[2]_
Here, p+1,s(t) = − 2s−1 _p+1,s−1(t)dt, subject to p+1,s(0) = 1 (that follows form κs(1) = 1)._

Thus, p+1,s(t) can be obtained recursively, starting from p+1,1(t) = 1 _t (see equation 9). For_
R _−_
example,


_p+1,2(t)_ =
_−_ [4]3 [(][−] [3]4 [+][ t][ −] _[t]2 [2]_ [)][,]

36
_p+1,3(t)_ =
15 [(15]36 _[−]_ 4[3] _[t][ + 1]2_ _[t][2][ −]_ [1]6 _[t][3][)][,]_


and so on.


_s[2]_
Similarly, p−1,s(t) can be expressed in closed form using p−1,s(t) = 2s−1 _p−1,s−1(t)dt subject to_

_p−1,s(0) = 0 (that follows from κs(−1) = 0), and starting from p−1,1(t) = 0R_, ∀t (see equation 9).
That leads to p−1,s(t) = 0, ∀s > 1, t.

The exact expressions of p±1,s(t) however do not affect the eigendecay. Instead, the constants c±1,s
are important for the eigendecay, based on Lemma 3. The constants can also be obtained recursively.
Starting from c+1,1 = [2]3√π2 [(see equation][ 9][),][ c][+1][,s][ =] (2−s2s[2]1)(2c+1s,s+1)−1 [. Also starting from][ c][−][1][,][1][ =][ 2]3√π2

_−_

(see equation 9), c−1,s = (22ss−[2]c1)(2+1,ss−+1)1 [. This recursive relation leads to]

_s_

2 _r[2]_

_c_ 1,s = [2][s][√] (11)
_−_ _π_ (4r[2] 1) _[,]_

_r=1_

_−_

Y

and c+1,s = ( 1)[s][−][1]c 1,s.
_−_ _−_

In the case of RF kernel, applying Lemma 3, we have _λ[˜]i_ _i[−][d][−][2][s], for i having the opposite parity_
_∼_
of s, and _λ[˜]i = o(i[−][d][−][2][s]) for i having the same parity of s._

**For the NT kernel, using 2, we have**

_κNT,s(1 −_ _t)_ = _c[2](1 −_ _t)s[2]κs−1(1 −_ _t) + κs(1 −_ _t)_

_κNT,s(−1 + t)_ = _c[2](−1 + t)s[2]κs−1(−1 + t) + κs(−1 + t)_ (12)


-----

Using the expansion of the RF kernel, we get


_κNTκNT,s(,s(11 + − tt))_ == _pp[′′]+1[′′]_ 1,s,s[(][(][t][t][) +][) +][ c][ c][′′]+1[′′] 1,s,s[t][t] 22ss22−−11
_−_ _−_ _−_


+ o(t 2s2− )1 _,_

+ o(t 2s2− )1 _,_ (13)


where

and


_p[′′]+1,s[(][t][)]_ = _c[2](1_ _t)s[2]p+1,s_ 1(t) + ps(t),
_−_ _−_

_p[′′]_ 1,s[(][t][)] = _c[2](_ 1 + t)s[2]p 1,s(t) + p 1,s( 1 + t),
_−_ _−_ _−_ _−_ _−_


_c[′′]+1,s_ = _c+1,s−1,_

_c[′′]−1,s_ = _c−1,s−1._

Thus, in the case of NT kernel, applying Lemma 3, we have _λ[˜]i_ _i[−][d][−][2][s][+2], for i having the same_
_∼_
parity of s, and _λ[˜]i = o(i[−][d][−][2][s][+2]) for i having the opposite parity of s._

E.2 ENDPOINT EXPANSIONS FOR l > 2 LAYER NETWORKS

We here extend the endpoint expansions and the eigendecays to l > 2 later networks. Recall κ[l]s[(][u][) =]
_κs(κ[l]s[−][1](u)). Using this recursive relation over l, we prove the following expressions_


2s+1 2s+1
_κ[l]s[(1][ −]_ _[t][)]_ = _p[l]+1,s[(][t][) +][ c]+1[l]_ _,s[t]_ 2 + o(t 2 )

2s+1 2s+1
_κ[l]s[(][−][1 +][ t][)]_ = _p[l]_ 1,s[(][t][) +][ c][l] 1,s[t] 2 + o(t 2 )
_−_ _−_

where p[l] 1,s [are polynomials and][ c][l] 1,s [are constants.]
_±_ _±_


The notations p+1,s and c+1,s in Subsection E.1 correspond to p[2]+1,s _[c]+1[2]_ _,s[, where we drop the]_
superscript specifying the number l of layers, for 2 layer networks.

For the endpoint expansion at 1, we have

_κ[l]s[(1][ −]_ _[t][)]_ = _κs(κ[l]s[−][1](1 −_ _t))_ 2s+1 2s+1

= _κs(1 −_ 1 + p[l]+1[−][1],s[(][t][) +][ c][l]+1[−][1],s[t] 2 + o(t 2 ))

2s+1 2s+1
= _p+1,s(1 −_ _p[l]+1[−][1],s[(][t][)][ −]_ _[c][l]+1[−][1],s[t]_ 2 _−_ _o(t_ 2 ))

2s+1 2s+1 2s+1
+c+1,s(1 − _p[l]+1[−][1],s[(][t][)][ −]_ _[c]+1[l][−][1],s[t]_ 2 _−_ _o(t_ 2 )) 2

2s+1 2s+1 2s+1
+o (1 − _p[l]+1[−][1],s[(][t][)][ −]_ _[c][l]+1[−][1],s[t]_ 2 _−_ _o(t_ 2 )) 2
 


Thus, we have


2s+1
_c[l]+1,s_ [= (][−][q]+1[l][−][1],s[,][1][)] 2 c+1,s − _q+1[2][,][1],s[c]+1[l][−][1],s[,]_ (14)

where q+1[l,i] _,s_ [is the coefficient of][ t][i][ in][ p]+1[l] _,s[(][t][)][. For these coefficients, we have]_

_q+1[l,][1]_ _,s_ [=][ −][q]+1[2][,][1],s[q]+1[l][−][1],s[,][1][.] (15)

_s[2]_
Starting from q+1[2][,][1],s [=][ −] 2s 1 [(which can be seen from the recursive expression of][ p][+1][,s][ given in]

_−_

_s[2(][l][−][1)]_
Section E.1), we get q+1[l,][1] _,s_ [=][ −] (2s 1)[l][−][1][ . We thus have]

_−_

_l−2_

_s[(2][s][+1)]_ _s[2]_

_c[l]+1,s_ [=] (2s+1) _c+1,s +_ +1,s[.] (16)

(2s 1) 2 ! 2s 1 _[c][l][−][1]_
_−_ _−_


That implies


_l−2_

_c+1,s,_ (17)

!


_s[(2][s][+1)]_

(2s+1)
(2s − 1) 2


_c[l]+1,s_

_[∼]_


-----

when s > 1.

The characterization of c[l]+1,s [shows that our results do not apply to deep neural networks when]
_s > 1, in the sense that the constants grow exponentially in l (as stated in Remark 2)._

For the endpoint expansion at −1, we have

_κ[l]s[(][−][1 +][ t][)]_ = _κs(κ[l]s[−][1](_ 1 + t))
_−_ 2s+1 2s+1

= _κs(p[l][−]1[1],s[(][t][) +][ c][l][−]1[1],s[t]_ 2 + o(t 2 ))
_−_ _−_

2s+1 2s+1
= _κs(q[l][−]1[1],s[,][0][) +][ κ]s[′]_ [(][q][l][−]1[1],s[,][0][)(][p][l][−]1[1],s[(][t][)][ −] _[q][l][−]1[1],s[,][0]_ [+][ c][l][−]1[1],s[t] 2 ) + o(t 2 ).
_−_ _−_ _−_ _−_ _−_

where q[l,i]1,s [is the coefficient of][ t][i][ in][ p][l] 1,s[(][t][)][. From the expression above we can see that]
_−_ _−_


_q[l,][0]1,s_ [=][ κ][s][(][q][l][−]1[1],s[,][0][)] (18)
_−_ _−_

Thus, 0 ≤ _q−[l,][0]1,s_ _[≤]_ [1][. In addition, the expression above implies]

_c[l]_ 1,s [=][ c][l][−]1[1],s[κ]s[′] [(][q][l][−]1[1],s[,][0][)][.] (19)
_−_ _−_ _−_

_s[2]_
From Lemma 1, κ[′]s[(][u][)][ ≤] 2s 1 [, for all][ u][. Therefore,]

_−_


_s[2]_ _l−2_
_c[l]−1,s_ _≤_ 2s 1 _c−1,s_
 _−_ 

= _o(c[l]+1,s[)][,]_

when, s > 1.

Comparing c[l] 1,a[, we can see that for][ l >][ 2][, we have][ |][c][l]+1,s[| ̸][=][ |][c][l] 1,s[|][. Thus, for the RF kernel with]
_±_ _−_
_l > 2, applying Lemma 3, we have_ _λ[˜]i_ _i[−][d][−][2][s]._
_∼_

**For the NT kernel,** recall

_κ[l]NT,s[(][u][) =][ c][2][κ][l]NT[−][1],s[(][u][)][κ]s[′]_ [(][κ]s[l][−][1](u)) + κ[l]s[(][u][)][.]

The second term is exactly the same as the RF kernel. Recall the expression of κ[′]s [based on][ κ][s][−][1]
from Lemma 1. To find the endpoint expansions of the first term, we prove the following expressions
for κs−1(κ[l]s[−][1](u)).

_κs−1(κ[l]s[−][1](1 −_ _t))_ = _p[′]+1[l]_ _,s[(][t][) +][ c]+1[′][l]_ _,s[t]_ 22ss2−+11 + o(t 22ss2+1− )1

_κs_ 1(κ[l]s[−][1]( 1 + t)) = _p[′][l]_ 1,s[(][t][) +][ c][′][l] 1,s[t] 2 + o(t 2 )
_−_ _−_ _−_ _−_


We use the notation used for the RF kernel to write the following expansion around 1

2s+1 2s+1
_κs_ 1(κ[l]s[−][1](1 _t))_ = _κs_ 1(1 1 + p[l]+1[−][1],s[(][t][) +][ c][l]+1[−][1],s[t] 2 + o(t 2 ))
_−_ _−_ _−_ _−_

2s+1 2s+1
= _p+1,s_ 1(1 _p[l]+1[−][1],s[(][t][)][ −]_ _[c][l]+1[−][1],s[t]_ 2 _o(t_ 2 ))
_−_ _−_ _−_

+c+1,s 1(1 _p+1[l][−][1],s[(][t][)][ −]_ _[c]+1[l][−][1],s[t]_ 2s2+1 _o(t_ 2s2+1 )) 2s2−1
_−_ _−_ _−_

+o (1 − _p[l]+1[−][1],s[(][t][)][ −]_ _[c][l]+1[−][1],s[t]_ 2s2+1 _−_ _o(t_ 2s2+1 )) 2s2−1 _._
 


Thus, we have


_c[′]+1[l]_ _,s_ [= (][−][q]+1[l][−][1],s[,][1][)] 2s2− c1 +1,s−1 − _q+1[2][,][1],s−1[c]+1[l][−][1],s_ (20)

Recall, form the analysis of the RF kernel that q+1[l,][1] _,s_ [=][ −] (2ss[2](l1)−1)[l][−][1][ . We thus have]

_−_

_l−2_

_s[(2][s][−][1)]_

_c[′]+1[l]_ _,s_ [=] (2s 1) (2s2−1) ! _c+1,s−1 + [(]2[s][ −]s_ [1)]3[2] _[c]+1[l][−][1],s[.]_ (21)

_−_ _−_


-----

That implies


_l−2_
!


_s[(2][s][−][1)]_

(2s − 1) (2s2−1)


_c[′]+1[l]_ _,s_

_[∼]_


_c+1,s−1._ (22)


For the endpoint expansion at −1, we have

2s+1 2s+1
_κs_ 1(κ[l]s[−][1]( 1 + t)) = _κs_ 1(p[l][−]1[1],s[(][t][) +][ c][l][−]1[1],s[t] 2 + o(t 2 ))
_−_ _−_ _−_ _−_ _−_

2s+1 2s+1
= _κs−1(q−[l][−]1[1],s[,][0][) +][ κ]s[′]_ _−1[(][q]−[l][−]1[1],s[,][0][)(][p]−[l][−]1[1],s[(][t][)][ −]_ _[q]−[l][−]1[1],s[,][0]_ [+][ c][l]−[−]1[1],s[t] 2 ) + o(t 2 ).

We thus have


_c[′][l]_ 1,s [=][ c][l][−]1[1],s 1[κ]s[′] 1[(][q][l][−]1[1],s[,][0][)][.] (23)
_−_ _−_ _−_ _−_ _−_


Since κ[′]s−1 _[≤]_ [(]2[s][−]s−[1)]3[2] [, we have]


_l−2_
_c−1,s−1._



_s[2]_
_c[′]−[l]_ 1,s _[≤]_ 2s 3
 _−_


Now we use the end point expansions of κ[l]s [and][ κ][s][−][1][(][κ]s[l][−][1]) to obtain the following endpoint expansions for κ[l]NT,s[.]


_κ[l]NT,s[(1][ −]_ _[t][)]_ = _p[′′]+1[l]_ _,s[(][t][) +][ c]+1[′′][l]_ _,s[t]_ 2s2−1

_κ[l]NT,s[(][−][1 +][ t][)]_ = _p[′′]−[l]1,s[(][t][) +][ c][′′]−[l]1,s[t]_ 2s2−1


+ o(t 2s2− )1 _,_

+ o(t 2s2− )1 _._


We have

and,


_c[2]s[2]_
_c[′′]+1[l]_ _,s_ [=] +1,s[c]+1[′′][l][−],s[1] [+][ c][2][s][2] +1,s _c[′]+1[l]_ _,s[,]_

2s 1 _[q][′][l,][0]_ 2s 1 _[q][′′][l][−][1][,][0]_
_−_ _−_

_c[2]s[2]_
_q+1[′′][l,][0],s_ [=] +1,s _q+1[′][l,][0],s_ [+][ q]+1[l,][0] _,s[.]_ (24)

2s 1 _[q][′′][l][−][1][,][0]_
_−_


Since κs(1) = 1, by induction we have κ[l]s[(1) = 1][,][ ∀][l][ ≥] [2][. In addition,][ κ][s][−][1][(][κ][l]s[−][1](1)) =
_κs−1(1) = 1, ∀l ≥_ 3. Therefore q+1[′][l,][0],s[, q]+1[l,][0] _,s_ [= 1][. Thus,]

_c[2]s[2]_
_q+1[′′][l,][0],s_ [=] +1,s + 1. (25)

2s 1 _[q][′′][l][−][1][,][0]_
_−_


_s[2]_
Starting from q+1[′′][2][,],s[0] [=] 2s 1 [+ 1][, we can derive the following expression for][ q]+1[′′][l,][0],s[, when][ s >][ 1][,]

_−_

_c2s2_ _l−1_ 2cs[2]s[2]1 _l−1_ 1

_q+1[′′][l,][0],s_ [= 1]c[2] 2s 1 +  _−c[2]s[2]_ _−_

 _−_  2s 1 1

_−_ _−_

 c2s2 _l−1_

_._

_∼_ _c[1][2]_ 2s 1

 _−_ 

When s = 1, q+1[′′][l,][0],s [=][ l][, that is consistent with the results in][ Bietti & Bach][ (][2020][).]

For c[′′]+1[l] _,s[, we thus have]_


_c[2]s[2]_
_c[′′]+1[l]_ _,s_ [=] +1,s [+][ c][2][s][2] +1,s _c[′]+1[l]_ _,s[.]_

2s 1 _[c][′′][l][−][1]_ 2s 1 _[q][′′][l][−][1][,][0]_
_−_ _−_


-----

Replacing the expressions for q+1[′′][l][−],s[1][,][0] and c[′]+1[l] _,s_ [derived above, we obtain]


2 2 ( [2][s]2[+1]
)(l−2)+1 _c_ _s_

2s 1

 _−_ 


)(l−2)+1
_c+1,s−1._ (26)


_c[′′]+1[l]_ _,s_ _[∼]_ [( 1]c[2][ )][(][ 2][s]2[−][1]


For the constant in the expansion around −1, we have

_c[2]s[2]_
_c[′′][l]1,s_ [=] 1,s[c][′′][l]1[−],s[1][,] (27)
_−_ 2s 1 _[q]−[′][l,][0]_ _−_

_−_

where q[′][l,]1[0],s [=][ κ][s][−][1][(][q][l][−]1[1],s[,][0][)][ is bounded between][ 0][ and][ 1][. Thus, starting from][ c][′′][2]1,s [=][ c][−][1][,s][−][1][, we]
_−_ _−_ _−_
obtain


2 2
_c_ _s_
_c[′′]−[l]1,s_ _[≤]_ 2s 1
 _−_


_l−2_
_c−1,s−1_ (28)



Comparing c[′′][l]1,a[, we can see that for][ l >][ 2][, we have][ |][c][′′]+1[l] _,s[| ̸][=][ |][c][′′][l]1,s[|][. Thus, for the NT kernel with]_
_±_ _−_
_l > 2, applying Lemma 3, we have_ _λ[˜]i_ _i[−][d][−][2][s][+2]._
_∼_

E.3 THE EIGENDECAYS IN TERMS OF λi

Recall the multiplicity Nd,i = [2][i][+]i[d][−][2] _i+d−d−23_ of _λ[˜]i. Here we take into account this multiplicity to_

give the eigendecay expressions in terms of λi.
  

Using ( _[n]k_ [)][k][ ≤] _nk_ _≤_ ( _[ne]k_ [)][k][, for all][ k, n][ ∈] [N][, we have, for all][ i >][ 1][ and][ d >][ 2][,]

  

1 1 1 1
2(i 1)[d][−][2]( (i 1)[d][−][2](
_−_ _d_ 2 [+] _i_ 1 [)][d][−][2][ ≤] _[N][d,i][ ≤]_ [(][d][ + 2)]2 _[e][d][−][2]_ _−_ _d_ 2 [+] _i_ 1 [)][d][−][2][,]

_−_ _−_ _−_ _−_

where we used 2 < [2][i][+]i[d][−][2] _<_ _[d][+2]2_ [. We thus have][ N][d,i][ ∼] [(][i][ −] [1)][d][−][2][, for the scaling of][ N][d,i][ with][ i][,]

with constants given above.

Recall we define λi = λ[˜]i′, for i and i[′] which satisfy _i[′′]=1_ _[N][d,i][′′][ < i][ ≤]_ [P]i[i][′′][′] =1 _[N][d,i][′′]_ [. Using]

_Nd,i1_ _∼_ (i − 1)[d][−][2], we have _i[′′]=1_ _[N][d,i][′′][ ∼]_ _[i][′][d][−][1][. Thus][ λ][i][ ∼]_ _λ[˜]i′ = λ[˜]i_ _d−1_ 1 [. Replacing][ i][ with]

_i_ _d−1, in the expressions derived for ˜λi in this section, we obtain the eigendecay expressions for[P][i][′][−][1]_ _λi_

reported in Table 2. [P][i][′]

F PROOF OF THEOREM 1


The Mat´ern kernel can also be decomposed in the basis of spherical harmonics. In particular,

Borovitskiy et al. (2020) proved the following expression for the Mat´ern kernel with smoothness
parameter ν on the hypersphere S[d][−][1] (see, also Dutordoir et al., 2020, Appendix B)


_Nd,i_

( [2][ν] 2

_κ[2][ +][ i][(][i][ +][ d][ −]_ [2))][−][(][ν][+][ d][−][1]

_j=1_

X


) ˜φi,j(x)φ[˜]i,j(x[′]),


_kν(x, x[′]) =_


_i=1_


where _φ[˜]i,j are the spherical harmonics._

Theorem 5 implies that the RKHS of Met´ern kernel is also constructed as a span of spherical harmonics. Thus, in order to show the equivalence of the RKHSs of the neural kernels with various
activation functions and a Mat´ern kernel with the corresponding smoothness, we show that the ratio
between their norms is bounded by absolute constants.


-----

Let f _kNT,s_ [, with][ l][ ≥] [2][. As a result of Mercer’s representation theorem, we have]
_∈H[l]_

_∞_ _Nd,i_


1

_wi,jλ[˜]i2_ _φ[˜]i,j(·)_
_j=1_

X

_Nd,i_


_f_ (·)


_i=1_

_∞_

_i=1_

X


2 [(][ν][+][ d][−]2 [1]

) [(2]κ[ν][2][ +][ i][(][i][ +][ d][ −] [2))][−] [1]


) ˜φi,j( ).

_·_


_wi,j_
_j=1_

X


( _κ[2][ν][2][ +][ i][(][i][ +][ d][ −]_ [2))][−] 2[1] [(][ν][+][ d][−]2 [1]


Note that
_λ˜i_ _λ˜i_

( _κ[2][ν][2][ +][ i][(][i][ +][ d][ −]_ [2))][−][(][ν][+][ d][−]2 [1] ) [=][ O][(] _i[−][2][ν][−][d][+1][ )][.]_

For the NT kernel, from Proposition 1, _λ[˜]i = O(i[−][d][−][2][s][+2]). Thus, when ν = s −_ [1]2 [,]


_λ˜i_


) [=][ O][(1)][.]


( _κ[2][ν][2][ +][ i][(][i][ +][ d][ −]_ [2))][−][(][ν][+][ d][−]2 [1]

So, with ν = s − 2[1] [we have]


_∞_ _Nd,i_ _λ˜i_

_wi,j[2]_

_i=1_ _j=1_ ( _κ[2][ν][2][ +][ i][(][i][ +][ d][ −]_ [2))][−][(][ν][+][ d][−]2 [1]

X X

_∞_ _Nd,i_

_O(_ _wi,j[2]_ [)]

_i=1_ _j=1_

X X


_f_
_∥_ _∥H[2]_ _kν_


= ( _f_
_O_ _∥_ _∥H[2]_ _κlNT,s_ [)][.]

That proves _kNTl_ _,s_
_H_ _[⊂H][k][ν]_ [.]

A similar proof shows that when ν = s + [1]2 [,][ H]k[l] _s_ _[⊂H][k]ν_ [.]

Now, let f _kν_ . As a result of Mercer’s representation theorem, we have
_∈H_

_∞_ _Nd,i_

_f_ ( ) = _wi,j(_ [2][ν] 2 [(][ν][+][ d][−]2 [1] ) ˜φi,j( )

_·_ _κ[2][ +][ i][(][i][ +][ d][ −]_ [2))][−] [1] _·_

_i=1_ _j=1_

X X

= _∞_ _Nd,i_ _wi,j_ ( _κ[2][ν][2][ +][ i][(][i][ +][ d][ −]1[2))][−]_ 2[1] [(][ν][+][ d][−]2 [1] ) _λ˜i12_ _φ[˜]i,j(_ ).

_i=1_ _j=1_ _λ˜i2_ _·_

X X


For the NT kernel with l > 2, from Proposition 1, _λ[˜]l ∼_ _l[−][d][−][2][s][+2]. Thus, when ν = s −_ 2[1] [,]

( _κ[2][ν][2][ +][ l][(][l][ +][ d][ −]_ [2))][−][(][ν][+][ d][−]2 [1] )

= (1).
_λ˜l_ _O_

So, with ν = s − 2[1] [we have]


_Nd,i_

_∞_ ( _κ[2][ν][2][ +][ i][(][i][ +][ d][ −]_ [2))][−][(][ν][+][ d][−]2 [1]

_i=1_ _j=1_ _wi,j[2]_ _λ˜i_

X X


_f_
_∥_ _∥H[2]_ _κlNT,s_


_Nd,i_

_wi,j[2]_ [)]
_j=1_

X


_O(_

_i=1_

X


( _f_
_O_ _∥_ _∥H[2]_ _kν_ [)][.]


-----

That proves, for l > 2, when ν = s 2 [,][ H][k][ν] _kNT,s_ [. A similar proof shows that for][ l >][ 2][, when]
_−_ [1] _[⊂H][l]_

_ν = s +_ [1]2 [,][ H][k][ν] _ks_ [, which completes the proof of Theorem][ 1][.]

_[⊂H][l]_

G PROOF OF THEOREM 2

Recall the definition of the information gain for a kernel κ,


(Yn; F ) = [1] **_In + [1]_** _._
_I_ 2 [log det] _λ[2][ K][n]_

 

To bound the information gain, we define two new kernels resulting from the partitioning of the
eigenvalues of the neural kernels to the first M eigenvalues and the remainder. In particular, we
define

_M_ _Nd,i_


_κ˜(x[⊤]x)_

_κ˜˜(x[⊤]x)_


_λ˜iφ[˜]i,j(x)φ[˜]i,j(x[′]),_
_j=1_

X

_Nd,i_


_i=1_


_λ˜iφ[˜]i,j(x)φ[˜]i,j(x[′])._ (29)
_j=1_

X


_i=M_ +1


We present the proof for the NT kernel. A similar proof applies to the RF kernel.

The truncated kennel at M eigenvalues, ˜κ(x[⊤]x), corresponds to the projection of the RKHS of κ
onto a finite dimensional space with dimension _i=1_ _Nj=1d,i_ [.]

We use the notations **K[˜]** _n and_ **K˜[˜]** _n to denote the kernel matrices corresponding to[P][M]_ P ˜κ and _κ[˜]˜, respec-_
tively.

As proposed in Vakili et al. (2021b), we write


log det **In + [1]**

_λ[2][ K][n]_




log det **In + [1]** **Kn + K˜[˜]** _n)_

_λ[2][ ( ˜]_




= log det **In + [1]** **Kn**

_λ[2][ ˜]_




+ log det **In + [1]** **Kn)[−][1][ ˜]K˜** _n_

_λ[2][ (][I][n][ + 1]λ[2][ ˜]_




_. (30)_


The analysis in Vakili et al. (2021b) crucially relies on an assumption that the eigenfunctions are
uniformly bounded. In the case of spherical harmonics however (see, e.g., Stein & Weiss, 2016,
Corollary 2.9)

sup _φ˜i,j(x) =_ _._ (31)
_i=1,2,...,_ _∞_
_j=1,2,...,Nd,i,_
_x∈X_

Thus, their analysis does not apply to the case of neural kernels on the hypersphere.

To bound the two terms on the right hand side of equation 30, we first prove in Lemma 4 that _κ[˜]˜(x, x[′])_
approaches 0 as M grows (uniformly in x and x[′]), with a certain rate.

**Lemma 4 For the NT kernel, we have** _κ[˜]˜(x[⊤]x[′]) = O(M_ _[−][2][s][+1]), for all x, x[′]_ _∈X_ _._

_Proof of Lemma 4: Recall that the eigenspace corresponding to_ _λ[˜]i has dimension Nd,i and consists_
of spherical harmonics of degree i. Legendre addition theorem (Maleˇcek & N´aden´ık, 2001) states

_Nd,i_

_φi,j(x)φi,j(x[′]) = ci,dCi[(][d][−][2)][/][2](cos(d(x, x[′]))),_ (32)
_j=1_

X


-----

where d is the geodesic distance on the hypersphere, Ci[(][d][−][2)][/][2] are Gegenbauer polynomials (those
are the same as Legendre polynomials up to a scaling factor), and the constant ci,d is


_Nd,iΓ((d_ 2)/2)
_ci,d =_ _−_ _._ (33)

2π[(][d][−][2)][/][2]Ck[(][d][−][2)][/][2](1)

_Nd,i_


We thus have


_κ˜˜(x[⊤]x[′])_


_φi,j(x)φi,j(x[′])_
_j=1_

X


_λ˜i_
_i=M_ +1

X


_λ˜ici,dCi[(][d][−][2)][/][2](cos(d(x, x[′])))_
_i=M_ +1

X


_∞_ Γ((d 2)/2)

_λ˜iNd,i_ _−_

_≤_ 2π[((][d][−][2)][/][2)]

_i=M_ +1

X

_∞_

_∼_ _i[−][d][−][2][s][+2]i[d][−][2]_

_i=M_ +1

X

_∼_ _M_ _[−][2][s][+1]._

Here, the inequality follows from Ci[(][d][−][2)][/][2](cos(d(x, x[′]))) _Ci[(][d][−][2)][/][2](1), because the Gegenbauer_
_≤_
polynomials attain their maximum at the endpoint 1. For the fourth line we used Nd,i _i[d][−][2]. The_
_∼_
implied constants include the implied constants in Nd,i ∼ _i[d][−][2]_ given in Section E.3, and [Γ((]2π[((][d][d][−][−][2)][2)][/][/][2)][2)][ .]

□

We also introduce a notation NM = _i=1_ _[N][d,i][ for the number of eigenvalues corresponding to]_
the spherical harmonics of degree up to M, taking into account their multiplicities, that satisfies
_NM_ _M_ _[d][−][1]_ (see Section E.3).

[P][M]
_∼_

We are now ready to bound the two terms on the right hand side of equation 30. Let us define
**Φn,NM = [φNM (x1), φNM (x2), . . ., φNM (xn)][⊤], an n × NM matrix which stacks the feature**
vectors φNM (xi) = [φj(xi)][N]j=1[M] [,][ i][ = 1][, . . ., n][, at the observation points, as its rows. Notice that]

**K˜** _n = Φn,NM ΛNM Φn,N[⊤]_ _M_ _[,]_

where ΛNM is the diagonal matrix of the eigenvalues defined as [ΛNM ]i,j = λiδi,j.

Now, consider the Gram matrix


**_G = ΛN2_** _M_ **[Φ]n,N[⊤]** _M_ **[Φ][n,N]M** [Λ]N2 _M_ _[.]_

As it was shown in Vakili et al. (2021b), by matrix determinant lemma, we have

log det(In + [1] **Kn)** = log det(INM + [1]

_λ[2][ ˜]_ _λ[2][ G][)]_

1
_NM log_ tr(INM + [1]
_≤_ _NM_ _λ[2][ G][)]_



_NM log(1 +_


).
_λ[2]NM_


To upper bound the second term on the right hand side of equation 30, we use Lemma 4. In particular,
since

tr (In + [1] **Kn)[−][1][ ˜]K˜** _n_ tr( K˜[˜] _n),_

_λ[2][ ˜]_ _≤_

 

and [ K˜[˜] _n]i,i = O(M_ _[−][2][s][+1]), we have_

tr **In + [1]** **Kn)[−][1][ ˜]K˜** _n_ = _n(1 + [1]_ _._

_λ[2][ (][I][n][ + 1]λ[2][ ˜]_ _O_ _λ[2][ M][ −][2][s][+1][)]_

   


-----

Therefore


log det **In + [1]** **Kn)[−][1][ ˜]K˜** _n_

_λ[2][ (][I][n][ + 1]λ[2][ ˜]_




_n log_ (1 + [1]
_O_ _λ[2][ M][ −][2][s][+1][)]_



_nM_ _[−][2][s][+1]_ + (1),
_≤_ _λ[2]_ _O_

where for the last line we used log(1 + z) ≤ _z which holds for all z ∈_ R.

We thus have γk(n) = (M _[d][−][1]_ log(n) + nM ). Choosing M _n_ _d+21s−2 (log(n))_ _d+2−s1−2, we_
_O_ _[−][2][s][+1]_ _∼_

obtain


_γκlNT,s_ [(][n][) =][ O] _n_ _d+2d−s1−2 (log(n))_ _d+22s−s−1_ 2 _._ (34)
 

For example, with ReLU activation functions, we have


_γκlNT,1_ [(][n][) =][ O] _n_ _d−d (log(1_ _n))_ _d1_ _._
 

H PROOF OF THEOREM 3

As stated in the paper, the proof follows the same steps as in the proof of Theorem 3 of Vakili et al.
(2021a). Their theorem holds for general kernels, provided the bound on MIG. We have adopted
their theorem for the special case of neural kernels, inserting our novel bounds on MIG of the neural
kernels given in Theorem 2. For completeness, we include a detailed proof here.

The proof consists of tow components. First, we bound the uncertainty estimate σn(x) in terms of
_γk(n). Our novel bounds on γk(n) of neural kernels given in Theorem 2 allow us to derive explicit_
bounds on σn(x). Then, we bound the error _f_ (x) _fn(x)_ in terms of σn(x), using implicit error
_|_ _−_ [ˆ] _|_
bounds.

Recall the way that the dataset [˜]n is collected: xi = arg maxx _σi_ 1(x). This ensures that,
_D_ _∈X_ _−_
_∀x ∈X_, σi−1(x) ≤ _σi−1(xi). Due to positive definiteness of the kernel matrix, conditioning on a_
larger dataset reduces the uncertainty. Thus ∀x ∈X and ∀i ≤ _n, σn(x) ≤_ _σi(x). Therefore σn[2]_ [(][x][)]
is upper bounded by the average of σi[2] 1[(][x][i][)][ over][ i][:][ ∀][x][ ∈X]
_−_


_σn[2]_ [(][x][)][ ≤] [1]

_n_


_σi[2]_ 1[(][x][i][)][.] (35)
_−_
_i=1_

X


It is shown that (e.g., see, Srinivas et al., 2010, Lemmas 5.3, 5.4)

_n_

_i=1_ _σi[2]−1[(][x][i][)][ ≤]_ log(1 +[2][I][(][Y][n][;][ F]λ1[2][)][ )] _[.]_ (36)

X


Thus, combining equation 35 and equation 36, and by definition of γk(n), we have, _x_
_∀_ _∈X_


2γk(n)

_n log(1 +_


1 (37)

_λ[2][ )]_ _[.]_


_σn(x)_
_≤_


Now, inserting our bounds on γκlNT,s [(][n][)][ and][ γ][κ]s[l] [(][n][)][ of the neural kernels from Theorem][ 2][, we obtain,]
_∀x ∈X_


2d−+42ss+14 2d2+4s−s1 4

_σn(x)_ _n_ _−_ (log(n)) _−_
_≤_


2

log(1 + _λ1[2][ )]_ _[,]_ in the case of NT kernel,


_σn(x)_ _n_ _−2d2+4s−s1 (log(n))_ 22ds+4+1s
_≤_


1 in the case of RF kernel. (38)

_λ[2][ )]_ _[,]_


log(1 +


The second component of the proof is summarized in the following lemma.


-----

**Lemma 5 Under Assumption 1, we have, with probability at least 1 −** _δ, ∀x ∈X_

1 2

_f_ (x) _fn(x)_ _B + C(log(_ _[n][d][−][1]_ )) 2 _σn(x) +_ (39)
_|_ _−_ [ˆ] _| ≤_ _δ_ _√n,_
 

_where C is an absolute constant, and B is the upper bound on the RKHS norm of f given in As-_
_sumption 1._

Lemma 5 follows from equation 7, and a probability union bound over a discretization of the domain.
A detailed proof of this lemma is given at the end of this section.

Inserting the bounds on σn(x) from equation 38 in Lemma 5, we have, with probability at least
1 − _δ, uniformly in x,_

_f_ (x) _fn(x)_ = _n_ 2d−+42ss+1−4 (log(n)) 2d2+4s−s1−4 (log( _[n][d][−][1]_ )) 12 _,_ in the case of NT kernel,
_|_ _−_ [ˆ] _|_ _O_ _δ_
 


_f_ (x) _fn(x)_ = _n_ _−2d2+4s−s1 (log(n))_ 22ds+4+1s (log( _[n][d][−][1]_
_|_ _−_ [ˆ] _|_ _O_ _δ_


That completes the proof.

_Proof of Lemma 5:_


1
)) 2 _,_ in the case of RF kernel.



Recall the implicit error bound given in equation 7: For a fixed x ∈X, under Assumption 1, we
have, with probability at least 1 − _δ (Vakili et al., 2021a),_

_f_ (x) _fn(x)_ _β(δ)σn(x),_ (40)
_|_ _−_ [ˆ] _| ≤_

where β(δ) = B + _[R]λ_ 2 log( [2]δ [)][. Lemma][ 5][ extends this inequality to a uniform bound in][ x][, using a]

probability union bound over a discretization of the domain.q

For f _k, with_ _f_ _k_ _B, and for n_ N, there exists a fine discretization Xn of with
size |X ∈Hn| such that ∥ f (x∥H) − ≤f ([x]n) ≤ _√1n_, where ∈ [x]n = arg minx′∈Xn ∥x[′] _−_ _x∥l2 is the closest X_

point in Xn to x, and |Xn| ≤ _cB[d][−][1]n[(][d][−][1)][/][2], where c is an absolute constant independent of n and_
_B (Chowdhury & Gopalan, 2017; Vakili et al., 2021a)._

Under Assumption 1, it can be shown that: with probability at least 1 − _δ/2 (Vakili et al., 2021a,_
Lemma 4)

_∥f[ˆ]n∥Hk ≤_ _B +_ _[R][√]λ_ _[n]_ r2 log( [4]δ[n] [)] _._ (41)

_U_ (δ)
| {z }

Note that _f[ˆ]n is a random function, where the randomness comes from the randomness in observation_
noise.

We thus conclude that there is a discretization X[˜] _n with size_ Xn _c(U_ (δ))[d][−][1]n[(][d][−][1)][/][2] such that
1 _|_ [˜] _| ≤_
_f_ (x) _f_ ([x]n) _√n_, and with probability at least 1 _δ/2,_
_|_ _−_ _| ≤_ _−_

1
_f[ˆ]n(x)_ _fn([x]n)_ (42)
_|_ _−_ [ˆ] _| ≤_ _√n._


Let δ[′] = 2c(U (δ))[d][−]δ [1]n[(][d][−][1)][/][2][ . A probability union bound over the discretization][ ˜]Xn implies that:

With probability at least 1 − _δ/2, uniformly in x ∈_ X[˜] _n,_

_f_ (x) _fn(x)_ _β(δ[′])σn(x)._ (43)
_|_ _−_ [ˆ] _| ≤_

Accounting for the discretization error from equation 42, and a probability union bound over equation 41 and equation 43, we have, with probability at least 1 − _δ, uniformly in x ∈X_,


-----

_f_ (x) _fn(x)_ _f_ (x) _f_ ([x]n) + _f_ ([x]n) _fn([x]n)_ + _f[ˆ]n([x]n)_ _fn(x)_
_|_ _−_ [ˆ] _|_ _≤_ _|_ _−_ _|_ _|_ _−_ [ˆ] _|_ _|_ _−_ [ˆ] _|_

2
_f_ ([xn]) _fn([xn])_ +
_≤_ _|_ _−_ [ˆ] _|_ _√n_

2
_β(δ[′])σn(x) +_
_≤_ _√n._

The first line holds by triangle inequality, the second line comes from the discretization error, and
the third line holds by equation 43.


Inserting the value of U (δ) = B + _[R][√]λ_ _[n]_


2 log( [2]δ[n] [)][ in][ δ][′][, and the value of][ δ][′][ =]


2c(U (δ))[d][−][1]n[(][d][−][1)][/][2]


in β( ) = B + _[R]λ_

_·_


2 log( [2] [)][, we arrive at the lemma.]

_·_


I DETAILS ON THE EXPERIMENTS

In this section, we provide further details on the experiments shown in the main text, Section 6. The
code will be made available upon the acceptance of the paper.

We consider NT kernels κNT,s(.), with s = 1, 2, 3, which correspond to wide fully connected
2 layer neural networks with activation functions as(.). In the first step, we create a synthetic
function f belonging to the RKHS of a NT kernel κ. For this purpose, we randomly generate n0 = 100 points on the hypersphere S[d][−][1]. Let _X[ˆ]n0 = [ˆxi][n]i=1[0]_ [denote the vector of]
these points. We also randomly sample _Y[ˆ]n0 = [ˆyi][n]i=1[0]_ [from a multivariate Gaussian distribution]
_N_ (0n0 _, Kn0_ ), where [ K[ˆ] _n0_ ]i,j = κ(ˆx[⊤]i _x[ˆ]j). We define a function g(.) = k[ˆ][⊤]n0_ [(][.][)( ˆ]Kn0 +δ[2]In0 )[−][1][ ˆ]Yn0,
where δ[2] = 0.01 and [ K[ˆ] _n0_ (x)]i = κ(x[⊤]xˆi). We then normalize g with its range to obtain
_f_ (.) = maxx∈X g(xg)−(.)minx∈X g(x) [. For a fixed][ ˆ]Xn0 and _Y[ˆ]n0_, g is a linear combination of partial

applications of the kernel. Thus g is in the RKHS of κ, and its RKHS norm can be bounded as
follows.

_g_ _κ_ = **kˆ[⊤]n0** [(][.][)( ˆ]Kn0 + δ[2]In0 )[−][1][ ˆ]Yn0 _,_ **k[ˆ][⊤]n0** [(][.][)( ˆ]Kn0 + δ[2]In0 )[−][1][ ˆ]Yn0
_∥_ _∥H[2]_ _κ_
 H

= _Yˆn[⊤]0_ [( ˆ]Kn0 + δ[2]In0 )[−][1][ ˆ]Kn0 ( K[ˆ] _n0 + δ[2]In0_ )[−][1][ ˆ]Yn0
= _Yˆn[⊤]0_ [( ˆ]Kn0 + δ[2]In0 )[−][1][ ˆ]Yn0 − _Y[ˆ]n[⊤]0_ [( ˆ]Kn0 + δ[2]In0 )[−][2][ ˆ]Yn0
_≤_ _Yˆn[⊤]0_ [( ˆ]Kn0 + δ[2]In0 )[−][1][ ˆ]Yn0

_∥Y[ˆ]n0_ _∥l[2][2]_ _._
_≤_ _δ[2]_

The second line follows from the reproducing property. We thus can see that for each fixed _X[ˆ]n0 and_
_Yˆn0_, g (and consequently f ) belong to the RKHS of κ.

The values of maxx _g(x) and minx_ _g(x) are numerically approximated by sampling 10, 000_
_∈X_ _∈X_
points on the hypersphere, and choosing the maximum and minimum over the sample.

We then generate the training datasets Dn of the sizes n = 2[i], with i = 1, 2, . . ., 13, by sampling n points Xn = [xi][n]i=1 [on the hypersphere, uniformly at random. The values][ Y][n][ = [][y][i][]]i[n]=1
are generated according to f . We then train the neural network model to obtain _f[ˆ]n(.). The error_
maxthe hypersphere and choosing the maximum of the sample.x∈X |f (x) − _f[ˆ]n(x)| is then numerically approximated by sampling 10, 000 random points on_

We have considered 9 different cases for the pairs of the kernel and the input domain. In particular,
the experiments are run for each κNT,s, s = 1, 2, 3 on all S[d][−][1], d = 2, 3, 4. In addition, each one of
these 9 experiments is repeated 20 times (180 experiments in total).


-----

In Figure 2 we plot maxx∈X |f (x) − _f[ˆ]n(x)| versus n, averaged over 20 repetitions, for each one_
of thefˆn(x) 9) = experiments. Note that for α log(n) + constant. Thus, in our log scale plots, the slope of the line represents the maxx∈X |f (x) − _f[ˆ]n(x)| ∼_ _n[α], we have log(maxx∈X |f_ (x) −
_|_
exponent of the error rate. As predicted analytically, we see all the exponents are negative (the error
converges to 0). In addition, the absolute value of the exponent is larger, when s is larger or d is
smaller. The bars in the plot on the left in Figure 2, show the standard deviation of the exponents.

For training of the model, we have used neural-tangents library (Novak et al., 2019) that is based
on JAX (Bradbury et al., 2018). The library is primarily suitable for κNT,1(.) corresponding to the
ReLU activation function. We thus made an amendment by directly feeding the expressions of the
RF kernels, κs, s = 2, 3, to the stax.Elementwise layer provided in the library. Below we give these
expressions


3 sin(θ) cos(θ) + (π − _θ)(1 + 2 cos[2](θ))_ _,_


15 sin(θ) − 11 sin[3](θ) + (π − _θ)(9 cos(θ) + 6 cos[3](θ))_



_κ2(u)_ =

_κ3(u)_ =

where θ = arccos(u).


3π

1

15π


We derived these expressions using Lemma 1 in a recursive way starting from κ1(.). Also, see Cho
& Saul (2009), which provides a similar expression for κ2(.) and a general method to obtain κs(.)
for other values of s. We note that we only need to supply κs(.) to the neural-tangents library. The
NT kernel κNT,s(.) will then be automatically created.

Our experiments run on a single GPU machine (GeForce RTX 2080 Ti) with 11 GB of VRAM
memory. Each one of the 180 experiments described above takes approximately 4 minutes to run.


-----

