# FULLY DECENTRALIZED MODEL-BASED POLICY OP## TIMIZATION WITH NETWORKED AGENTS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Model-based RL is an effective approach for reducing sample complexity. However, when it comes to multi-agent setting where the number of agent is large,
the model estimation can be problematic due to the exponential increased interactions. In this paper, we propose a decentralized model-based reinforcement learning algorithm for networked multi-agent systems, where agents are cooperative
and communicate locally with their neighbors. We analyze our algorithm theoretically and derive an upper bound of performance discrepancy caused by model
usage, and provide a sufficient condition of monotonic policy improvement. In our
experiments, we compare our algorithm against other strong multi-agent baselines
and demonstrate that our algorithm not only matches the asymptotic performance
of model-free methods but also largely increases its sample efficiency.

1 INTRODUCTION

Many real world problems, such as autonomous driving, wireless communications, multi-player
games can be modeled as multi-agent RL problems, where multiple autonomous agents coexist in a
common environment, aiming to maximize its individual or team reward in the long term by interacting with the environment and other agents. Unlike single-agent tasks, multi-agent tasks are more
challenging, due to partial observations and unstable environments when agents update their policies
simultaneously. Therefore, there are hardly any one-fits-all solutions for MARL problems. Examples include networked systems control (NSC) (Chu et al., 2020), in which agents are connected via
a stationary network. They perform decentralized control based on its local observations and messages from connected neighbors. Examples of networked systems include connected vehicle control
(Jin & Orosz, 2014), traffic signal control (Chu et al., 2020), etc.

Despite the success of multi-agent reinforcement (RL) algorithms, their performance relies on a
massive amount of model usage. Typically, a multi-agent RL algorithm needs millions of interaction with the environment to converge. On the other hand, model-based reinforcement learning
(MBRL) algorithms, which utilize predictive models of the environment to help data collection, are
empirically more data-efficient than model-free approaches. Although model inaccuracy performs
as a bottleneck of policy quality in model-based algorithms, we can still learn a good policy with an
imperfect model (Luo et al., 2019), especially combined with the trick of branched rollout (Janner
et al., 2019) to limit model usage. Experimentally, MuZero (Schrittwieser et al., 2020), a modelbased RL algorithm, succeeded in matching the performance of AlphaZero on Go, chess and shogi,
and becomes state-of-the-art on Atari games. Model-based MARL is not fully investigated. Existing MB-MARL algorithms either limit their field of research on specific scenario, e.g. two-player
zero-sum Markov game (Zhang et al., 2020) or pursuit evasion game (Bouzy & M´etivier, 2007),
or use tabular RL method (Bargiacchi et al., 2021). MB-MARL for multi-agent MDPs is still an
open problem to be solved (Zhang et al., 2019), with profound challenges such as scalability issues
caused by large state-action space and incomplete information of other agents’ state or actions.

In this paper, we develop decentralized model-based algorithms on networked systems, where agents
are cooperative, and able to communicate with each other. We use localized models to predict future
states, and use communication to broadcast their predictions. To address the issue of model error,
we adopt branched rollout (Janner et al., 2019) to limit the rollout length of model trajectories. In
the policy optimization part, we use decentralizd PPO (Schulman et al., 2017) with a extended value
function. At last, we analyze these algorithms theoretically to bound the performance discrepancy


-----

between our method and its model-free, centralized counterpart. At last, we run these algorithms
in traffic control environments (Chu et al., 2020; Vinitsky et al., 2018) to test the performance of
our algorithm. We show that our algorithm increases sample efficiency, and matches the asymptotic
performance of model-free methods.

In summary, our contributions are three-fold. Firstly, we propose an algorithmic framework,
which is a fully decentralized model-based reinforcement learning algorithm, which is named as
**Decentralized Model-based Policy Optimization (DMPO). Secondly, we analyze the theoretical**
performance of our algorithm. Lastly, empirical results on traffic control environments demonstrate
the effectiveness of DMPO in reducing sample complexities and achieving similar asymptotic performance of model-free methods.

2 RELATED WORK

Model-based methods are known for their data efficiency (Kaelbling et al., 1996), especially compared with model-free algorithms. There is a vast literature on the theoretical analysis of modelbased reinforcement learning. In a single-agent scenario, monotonic improvement of policy optimization has been achieved (Luo et al., 2019; Sun et al., 2018), and a later work improved the
performance of model-based algorithms by limiting model usage (Janner et al., 2019). But these
analysis is restricted to single-agent scenarios, whereas ours addresses multi-agent problems.

On the other hand, Networked System Control (NSC) (Chu et al., 2020) is a challenging setting for
MARL algorithm to take effect. Some multi-agent algorithms falls into centralized training decentralized execution (CTDE) framework. For example, QMIX (Rashid et al., 2018) and COMA (Foerster et al., 2018) all use a centralized critic. In a large network, however, centralized training might
not scale. In many scenarios, only fully decentralized algorithms can be used. Zhang et al. (2018)
proposed an algorithm of NSC that can be proven to converge under linear approximation. Qu et al.
(2020a) proposed truncated policy gradient, to optimize local policies with limited communication.
Baking in the idea of truncated Q-learning in (Qu et al., 2020a), we generalize their algorithm to
deep RL, rather than tabular RL. Factoring environmental transition into marginal transitions can
be seen as factored MDP. Guestrin et al. (2001) used Dynamic Bayesian Network to predict system
transition. Simao & Spaan (2019) proposed a tabular RL algorithm to ensure policy improvement at
each step. However, our algorithm is a deep RL algorithm, enabling better performance in general
tasks.

There are some works on applying model-based methods in MARL settings. A line of research
focuses on model-based RL for two-player games. For example, Brafman & Tennenholtz (2000)
solved single-controller-stochastic games, which is a certain type of two-player zero-sum game;
Bouzy & M´etivier (2007) performed MB-MARL in the pursuit evasion game; Zhang et al. (2020)
proved that model-based method can be nearly optimally sample efficient in two-player zero-sum
Markov games. Bargiacchi et al. (2021) extended the concept of prioritized sweeping into a MARL
scenario. However, this is a tabular reinforcement algorithm, thus unable to deal with cases where
state and action spaces are relatively large, or even continuous. In contrast to existing works, our
algorithm is not only applicable to more general multi-agent problems, but is also the first fully
decentralized model-based reinforcement learning algorithm.

3 PROBLEM SETUP

In this section, we introduce multi-agent networked MDP and model-based networked system control.

**Networked MDP** We consider environments with a graph structure. Specifically, n agents coexist
in an underlying undirected and stationary graph G = (V, E). Agents are represented as a node
in the graph, therefore V = {1, ..., n} is the set of agents. E ⊂V × V comprises the edges that
represent the connectivity of agents. Agents are able to communicate along the edges with their
neighbors. Letthe κ-hop neighborhood of Ni denote every neighbor of agent i, and i, i.e. the nodes whose graph distance toN[¯]i = Ni ∪{ ii} is less than or equal to. Furthermore, let Ni[κ] _κ[denote]. For_
the simplicity of notation, we also define N _[κ]i_ [=][ V \][ N][ κ]i [.]
_−_


-----

The corresponding networked MDP is defined as (G, {Si, Ai}i∈V _, p, r). Each agent i have their_
local state si _i, and perform action ai_ _i. The global state is the concatenation of all local_
states: s = (s ∈S1, ..., sn) := 1 _..._ _∈An. Similarly, the global action is a = (a1, ..., an)_ :=
_AN1i, that is, given × ... × An. For the simplicity of notation, we define Ni = ∈S {j1, ..., j Sc ×}, then×S sNi = (sj1_ _, ..., s sNjci) to be the local states of every agent in. aNi_ _, sNi[κ]_ _[, a][N][ κ]i_ [are defined similarly.] ∈A
The transition function is defined as: p(s[′]|s, a) : S × A →S. Each agent possess a localized policy
_πof its neighbors and itself. We usei[θ][i]_ [(][a][i][|][s][ ¯]Ni [)][ that is parameterized by] θ = ([ θ][i] _θ[∈]1, ..., θ[Θ][i][, meaning the local policy is dependent only on states]n) to denote the tuple of localized policy parameters,_
and π[θ](a|s) = _i=1_ _[π]i[θ][i]_ [(][a][i][|][s][ ¯]Ni [)][ denote the joint policy. We also assume that reward functions is]
only dependent on local state and action: ri(si, ai), and the global reward function is defined to be
the average reward[Q][n] r(s, a) = _n[1]_ _ni=1_ _[r][i][(][s][i][, a][i][)][.]_

The goal of reinforcement learning is to maximize the expected sum of discounted rewards, denotedP
by η:

_[∞]_ _n_
_π[θ][∗]_ = arg max _γ[t]_ [1] _ri(st, at)_ _,_ (1)
_π[θ][ η][[][π][θ][] = arg max]π[θ][ E][π][θ]_ _·_ _n_

_t=0_ _i=1_

h X X i

where γ ∈ (0, 1) is the temporal discount factor. We define the stationary distribution under policy
_π to be dπ(s)._

**Independent Networked System** Networked system may have some extent of locality, meaning
in some cases, local states and actions do not affect the states of distant agents. In such systems,
environmental transitions can be factorized, and agents are able to maintain local models to predict
future local states. We define Independent Networked System (INS) as follows:
**Definition 1. An environment is an Independent Networked System (INS) if:**


_p(s[′]|s, a) =_


_pi(s[′]i[|][s][ ¯]Ni_ _[, a][i][)][,][ ∀][s][′][, s][ ∈S][, a][ ∈A][.]_
_i=1_

Y


INS might be an assumption that is too strong to hold. However, for the dynamics that cannot be
factorized, we can still use an INS to approximate it. Let DT V denote the total variation distance
between distributions, we have the following definition:
**Definition 2. (ξ-dependent) Assume there exists an Independent Networked System ¯p such that**
_p¯(s[′]|s, a) =_ _i=1_ _[p][i][(][s]i[′]_ _[|][s][ ¯]Ni_ _[, a][i][)][. An environment is][ ξ][-dependent, if:]_

1
sup _p(s[′]_ _s, a)_ _p¯(s[′]_ _s, a)_ = sup _p(s[′]_ _s, a)_ _p¯(s[′]_ _s, a)_ _ξ._
_s,a[Q][n][D][T V]_ _|_ _∥_ _|_ _s,a_ 2 _|_ _|_ _−_ _|_ _| ≤_

_s[′]_

  X∈S

To explain the intuition behind this definition, we point out that ξ is actually the lower bound of
model error when we use local models ˆp(s ¯Ni _[, a][i][)][. Recall that][ p][(][s][′][|][s, a][)][ is the real environment]_
transition, ¯p = _i=1_ _[p][i][(][s]i[′]_ _[|][s][ ¯]Ni_ _[, a][i][)][ is the product of marginal environment transitions, and][ ˆ]p(s, a) =_
_ni=1_ _p[ˆ]i(s[′]i[|][s][ ¯]Ni_ _[, a][i][)][ is the product of model transitions. Then the universal model error][ D][(][p][∥]p[ˆ]) can_
be divided into two parts: dependency bias D(p _p¯) and model error D(¯p_ _pˆ):_

[Q][n]
Q _∥_ _∥_
_D(p∥pˆ) ≤_ _D(p∥p¯) + D(¯p∥pˆ)._
Then for a ξ-dependent system, when models become very accurate, meaning D(¯p∥pˆ) ≈ 0,
sup D(p∥pˆ) ≈ sup D(p∥p¯) = ξ. While D can be any appropriate distance metric, we use the
TV-distance hereafter for the ease of presentation. In the following, we develop theory under both
INS and ξ-dependent scenarios.

4 DECENTRALIZED MODEL-BASED POLICY OPTIMIZATION

In this section, we formally present Decentralized Model-based Policy Optimization (DMPO),
which is a fully decentralized model-based reinforcement learning algorithm. Compared with independent multi-agent PPO, DMPO is augmented in three ways: localized model, policy with one-step
communication, and extended value function. We introduce the detail of localized model in 4.1. Policy and value functions are introduced in 4.2. The illustration of our algorithm is given in Figure
1. All the components mentioned above are analyzed in Section 5. We argue that under certain
conditions, our algorithm ensures monotonic policy improvement.


-----

(a) Neighborhood (b) Value function


(c) Graph convolutional model


Figure 1: (a) presents the concept of neighborhood. If agent i is the node in purple, then purple and
orange is _N[¯]i, and combination of purple, orange and green is Ni[3][. (b) explains that extended value]_
function takes sNi[κ] [as input, here][ κ][ = 3][. (c) is the illustration of graph convolutional model.]

4.1 DECENTRALIZED PREDICTIVE MODEL

To perform decentralized model-based learning, we let each agent maintain a localized model. The
localized model can observe the state of 1-hop neighbor and the action of itself, and the goal of a
localized model is to predict the information of the next timestep, including state, reward and done.
This process is denoted by ˆpi(s[′]i[, r]i[′][, d][′]i[|][s][ ¯]Ni _[, a][i][)][.]_

We implement a localized model with graph convolutional networks (GCN). Recall that agents are
situated in a graph G = (V, E). In the first step, a node-level encoder encodes local state into node
embedding,

_h[0]i_ [=][ f][ encode]i (si). (2)
Then we perform one step of graph convolution as follows,


_h(i,j) = f([edge]i,j)_ [(][h]i[0][, h]j[0][)][,] _h[1]i_ [=][ f][ node]i


_h(i,j), ai)._ (3)
_e=(i,j)_

X


In this way, h[1]i [is dependent only on][ s][ ¯]Ni [and][ a][i][. Finally, a node-level decoder generates the predic-]
tion of state, reward and done from h[1]i [as follows:]

_s[′]i_ [=][ f][ state]i (h[1]i [) +][ s][i][,] _ri[′]_ [=][ f][ reward]i (h[1]i [)][,d]i[′] [=][ f][ done]i (h[1]i [)][.] (4)

Note that we predict the next state with a skip connection, because empirically, it’s more efficient to
predict the change of the state rather than the state itself.

In practice, the data are all stored locally by each agent. Data that are collected in the environment
by agent i is denoted as _i_, and those generated by predictive model is denoted as _i_ .
_D[env]_ _D[model]_

Scaling model-based methods into real tasks can result in decreased performance, even if the model
is relatively accurate. One main reason is the compound modeling error when long model rollouts
are used, and model error compound along the rollout trajectory, making the trajectory ultimately inaccurate. To reduce the negative effect of model error, we adopt a branched rollout scheme proposed
in (Janner et al., 2019). In branched rollout, model rollout starts not from an initial state, but from a
state that was randomly selected from the most recent environmental trajectory τ . Additionally, the
model rollout length is fixed to be T . This scheme is shown to be effective in reducing the negative
influence of model error both theoretically and empirically.

To deal with the bias of model trajectories, at each model rollout, we allow the algorithm to fall back
to the real trajectory with probability 1 − _q0, where q0 is a hyperparameter. We describe the detailed_
framework of model usage and experiment storage in Algorithm 1.

4.2 PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION

To optimize the policies, we need to adopt an algorithm that can exploit network structure, whilst
remaining decentralized. Independent RL algorithms that observes only local state are fully decentralized, but they often fail to learn an optimal policy. Centralized algorithms that utilize centralized


-----

**Algorithm 1: Decentralized Model-based Policy Optimization (DMPO) for MARL**
**Input: hyperparameters: rollout length T**, truncation radius κ

1: Initialize the model p[ψ]i _[i]_ [, actor][ π]i[θ][i] [and critic][ V][ φ]i _[i]_ [.]

2: Initialize replay buffers _i_ and _i_ .
_D[env]_ _D[model]_

3: for M iterations do
4: Perform environmental rollout together, and each agent i collect trajectory information τi.

5: **for i in N agents do**

6: _Di[env]_ = Di[env] _τi_ .
_∪{_ _}_

7: Train p[ψ]i _[i]_ on Di[env].

8: _Di[model]_ = .
_∅_

9: **for B inner iterations: do**

10: Generate a random number q ∼ _U_ (0, 1).

11: **if q > q0 then**

12: _i_ = τi. Fall back to real trajectory with probability 1 _q0._
_D[model]_ _{_ _−_ _}_

13: **else**

14: **for R rollouts, s ∈** _τ do_

15: Perform T -step model rollout starting from s using policy pψ∗, append to D∗[model].

16: **for G steps, i = 0, ..., n −** 1 do

17: Take a step along the gradient to update πi[θ][i] [and critic][ V][ φ]i _[i]_ on D[model]
_∗_

critics often achieve better performance than decentralized algorithms, but they might not scale to
large environments where communication costs are expensive.

We propose Proximal Policy Optimization with extended value function, which is defined as
_Vi(sNi[κ]_ [) =][ E][s][Nκ]−i _[∼][d][π]_ [[][P]t[∞]=0 _[r]i[t][|][s][0]Ni[κ]_ [=][ s][N][ κ]i []][, i][ ∈V][. The intuition behind extended value func-]
tion comes from (Qu et al., 2020a), where truncated Q-function Q(sNi[κ] _[, a][N][ κ]i_ [)][ is initially proposed.]
In 5.3, we prove that Vi(sNi[κ] [)][ is a good approximation of][ V][i][(][s][)][, with a difference decreasing expo-]
nentially with κ.

To generate the objective for extended value function, or return Ri, we use reward-to-go technique.
However, because model rollout is short, standard reward-to-go returns would get a biased estimation of Vi. To resolve this issue, we add the value estimation of the last state to the return. In this
way, with a local trajectory τi = {(s[t]i[, a][t]i[, r]i[t][,][ (][s][′][)][t]i[, d][t]i[,][ log][ π]i[t][)][, t][ = 0][,][ 1][, ..., T][ −] [1][}][, the objective of]
_Vi[t][(][s][N][ κ]i_ [)][ is]

_T −t−1_

_Ri[t]_ [=] _γ[l]ri[t][+][l]_ + Vi[φ][i] (s[′])N[T][ −]i[κ] [1] _,_ (5)

_l=0_

and the loss of value function is defined asX Li[value] = _m[1]_ m∈Di[model] _Vi[φ][i]_ [(][s]N[m]i[κ] [)][ −] _[R]i[m]_ 2. In policy

training, extended value functions Vi are reduced via communication to their κ-hop neighbors to

P  

generate an estimation of global value function,


_V˜i[t]_ [= 1]


_V˜j[t][,]_ (6)


_j_ _Ni[κ]_
_∈_

and advantages _A[ˆ]i are computed on_ _V[˜]i with generalized advantage estimation (GAE) (Schulman_
et al., 2015) for policy gradient update. The surrogate loss function of a DMPO agent is defined as


_ππiθi[θ]i[k][i]_ [(][(][a][a]i[t]i[t][|][|][s][s][t]N[t]N¯¯ii[)][)] _Aˆi(sVi[κ]_ [)][, g][(][ϵ,][ ˆ]Ai(sVi[κ] [))]


_Li[policy]_ = m[1]

similar to PPO-Clip loss.


(7)


min


_m_ _i_
_∈D[model]_


The communication of κ step might seem costly, yet information of Ni[κ] [is only used in the training]
phase. We argue that in the training phase, algorithms are less sensitive with latency than execution. Furthermore, since model-based learning can effectively increase sample efficiency, we might
tolerate more communication.


-----

5 THEORETICAL ANALYSIS

In this section, we analyze DMPO theoretically. In 5.2, we derive a bound between the true returns
and the returns under a model ˆp in a networked system. In 5.3, we prove that extended value function
_Vi(sNi[κ]_ [)][ is a good approximation of][ V][i][(][s][)][, and with extended value function, the true policy gradient]
can also be approximated.

5.1 BACKGROUND: MONOTONIC MODEL-BASED POLICY OPTIMIZATION

Let η[π] denote the returns of the policy in the true environment, ˆη[π] denote the returns of the
policy under the approximated model. To analyze the difference between η[π] and ˆη[π], we need to
construct a bound
_η[p][π]_ _ηˆp[ˆ][π]_ _C(p, ˆp, π, πD),_ (8)
_≥_ _−_
where C is a non-negative function, and πD is the data-collecting policy. According to equation 8,
if every policy update ensures an improvement of ˆη[π] by at least C, η[π] will improve monotonically. This inequality was first presented in single agent domain (Janner et al., 2019). In this work,
we extend this to the multi-agent networked system, aiming to achieve monotonic team reward improvement.

In this work, we let π indicate a collective policy π = [π1, ..., πn], and the model ˆp be an INS
_pˆ(s[′]|s, a) =_ _i=1_ _p[ˆ]i(s[′]i[|][s][ ¯]Ni_ _[, a][i][)][ that approximating the true MDP. In DMPO, each agent learns a]_
localized model ˆπi, policy πi(|sNk ), critic Vi(sNi[κ] [)][, making it never a trivial extension. We give the]
detailed analysis in 5.2.

[Q][n]

5.2 ANALYSIS OF RETURNS BOUND

In model-based learning, different rollout schemes can be chosen. The vanilla rollout assumes
that models are used in an infinite horizon. The branched rollout performs a rollout from a state
sampled by a state distribution of previous policy πD, and runs T steps in ˆπ according to π. Based
on different rollout schemes, we can construct two lower bounds. Under vanilla rollout, real return
and model return can be bounded by model error and policy divergence. Formal results are presented
in Theorem 1. The detailed proof is deferred to Appendix C.
**Theorem 1. Consider an independent networked system. Denote local model errors as ϵmi =**
maxs ¯Ni _[,a][i]_ _[D][T V][ [][p][i][(][s]i[′]_ _[|][s][ ¯]Ni_ _[, a][i][)][∥]p[ˆ]i(s[′]i[|][s][ ¯]Ni_ _[, a][i][)]][, and divergences between the data-collecting policy]_
_and evaluated policy as ϵπi = maxs ¯Ni_ _[D][T V][ [][π][D][(][a][i][|][s][ ¯]Ni_ [)][∥][π][(][a][i][|][s][ ¯]Ni [)]][. Assume the upper bound of]
_rewards of all agents is rmax. Let η[p][π1, ..., πn] denote the real returns in the environment. Also, let_
_ηp[ˆ][π1, ..., πn] denote the returns estimated in the model trajectories, and the states and actions are_
_collected with πD. Then we have:_


_∞_ _Nik_

_γ[k][+1][ |][ ¯]_ _|_

_n_

_k=0_

X


_ϵπi_

_n_ [+ (][ϵ][m][i][ + 2][ϵ][π][i] [)][ ·]




_η[p][π1, ..., πn]_ _ηp[ˆ][π1, ..., πn]_
_|_ _−_ _| ≤_ [2]1[r][max]γ

_−_


_i=1_


Intuitively, the term _k=0_ _[γ][k][+1][ |][ ¯]Nni_ _k|_ would be in the same magnitude as 1 1 _γ_ [, which might be huge]

_−_
given the choice of γ, making the bound too loose to be effective. To make tighter the discrepancy
bound in Theorem 1, we adopt the branched rollout scheme. The branched rollout enables a effec
[P][∞]

tive combination of model-based and model-free rollouts. For each rollout, we begin from a state
Theorem 2 gives the returns bound.sample from dπD, and run T steps in each localized ˆπi. When branched rollout is applied in an INS,
**Theorem 2. Consider an independent networked system. Denote local model errors as ϵmi =**
maxs ¯Ni _[,a][i]_ _[D][T V][ [][p][i][(][s]i[′]_ _[|][s][ ¯]Ni_ _[, a][i][)][∥]p[ˆ]i(s[′]i[|][s][ ¯]Ni_ _[, a][i][)]][, and divergences between the data-collecting policy]_
_and evaluated policy as ϵπi = maxs ¯Ni_ _[D][T V][ [][π][D][(][a][i][|][s][ ¯]Ni_ [)][∥][π][(][a][i][|][s][ ¯]Ni [)]][. Assume the upper bound of]
_rewards of all agents is rmax. Let η[p][π1, ..., πn] denote the real returns in the environment. Also, let_
_η[branch][π1, ..., πn] denote the returns estimated via T_ _-step branched rollout scheme. Then we have:_



_[T][ −][1]_ _Nik_
_ϵmi_ _·_ _γ[k][+1][ |][ ¯]n_ _|_

_k=0_

  X


[i]


_∞_ _Nik_

_γ[k][+1][ |][ ¯]_ _|_

_n_

_k=T_

X


_η[p][π1, ..., πn]_ _η[branch][π1, ..., πn]_
_|_ _−_ _| ≤_ [2]1[r][max]γ

_−_


+ϵπi

_·_


_i=1_


-----

Comparing the results in Theorem 1 and 2, we can see that branched rollout scheme reduced the coefficient before ϵmi from _k=0_ _[γ][k][+1][ |][ ¯]Nni_ _k|_ _≤_ 1−γ _γ_ [to][ P][T]k=0[ −][1] _[γ][k][+1][ |][ ¯]Nni_ _k|_ _≤_ [P]k[T]=0[ −][1] _[γ][k][+1][ =][ γ][(1]1[−]−[γ]γ[T][ )]_ .

This reduction explains that empirically, branched rollout brings better asymptotic performance.
Also, if we set T = 0, this bound turn into a model-free bound. This indicates that when[P][∞] _ϵmi is_
lower than ϵπi allowed by our algorithm, a model might increase the performance.

In reality, not every system satisfies the definition of INS. Yet we can generalize Theorem 2 into a
_ξ-dependent system._

**Corollary 1. Consider an ξ-dependent networked system. Denote local model errors as ϵmi =**
maxs ¯Ni _[,a][i]_ _[D][T V][ [][p][i][(][s]i[′]_ _[|][s][ ¯]Ni_ _[, a][i][)][∥]p[ˆ]i(s[′]i[|][s][ ¯]Ni_ _[, a][i][)]][, and divergences between the data-collecting policy]_
_and evaluated policy as ϵπi = maxs ¯Ni_ _[D][T V][ [][π][D][(][a][i][|][s][ ¯]Ni_ [)][∥][π][(][a][i][|][s][ ¯]Ni [)]][. Assume the upper bound of]
_rewards of all agents is rmax. Let η[p][π1, ..., πn] denote the real returns in the environment. Also, let_
_η[branch][π1, ..., πn] denote the returns estimated via T_ _-step branched rollout scheme. Then we have:_


_η[p][π1, ..., πn]_ _η[branch][π1, ..., πn]_
_|_ _−_ _|_

_n_ _[T][ −][1]_ _Nik_

_≤_ (1[2][r] −[max]γ)[γ][2][ ξ][ + 2]1[r] −[max]γ Xi=1 hϵmi ·   _kX=0_ _γ[k][+1][ |][ ¯]n_ _|_


[i]


_∞_ _Nik_

_γ[k][+1][ |][ ¯]_ _|_

_n_

_k=T_

X


+ ϵπi
_·_


The proof can also be found in Appendix C. Compared to Theorem 2, Corollary 1 is more general, as
it is applicable to the multi-agent systems that are not fully independent. Intuitively, if a networked
system seems nearly independent, local models will be effective enough. The bound indicates that
when the policy in optimized in a trust region where D(π, πD) _ϵπi_, the bound would also be
_≤_
restricted, making monotonic update more achievable.

5.3 EXTENDED VALUE FUNCTION

In this section, we analyze the effect of extended value function. The idea of extended value function
_Vi(sNi[κ]_ [)][ comes from][ truncated Q-function][ Q][i][(][s][N][ κ]i _[, a][N][ κ]i_ [)][ proposed in (Qu et al., 2020a). We prove]
that extended value function is an approximation of the real value function. The detailed proof of
Theorem 3 is deferred to Appendix C.

**Theorem 3. Define Vi(sNi[κ]** [) =][ E][s][Nκ]−i _[∼][d][π]_ [[][P]t[∞]=0 _[r]i[t][|][s][0]Ni[κ]_ [=][ s][N][ κ]i []][, and][ V][i][(][s][) =][ E][[][P]t[∞]=0 _[r]i[t][|][s][0][ =][ s][]][,]_
_then:_

_Vi(s)_ _Vi(sNi[κ]_ [)][| ≤] _[r][max]_
_|_ _−_ 1 _γ [γ][κ][.]_

_−_

From Theorem 3, it is straightforward that the global value function can be approximated with
the average of all extended value functions: |V (s) − _n[1]_ _ni=1_ _[V][i][(][s][N][ κ]i_ [)][| ≤] _[r]1[max]−γ_ _[γ][κ][.][ In PPO, value]_

functions are used for calculating advantages _A[ˆ][(][t][)]_ = r[(][t][)] + γV (s[(][t][+1)]) _V (s[(][t][)]), and we have_

P _−_ _n_

proven that V (s) can be estimated with the average of extended value functions _n[1]_ _i=1_ _[V][i][(][s][N][ κ]i_ [)][.]

In practice, an agent might not get the value function of distant agents. However, we can prove
P
that _V[˜]i =_ _n[1]_ _j∈Ni[κ]_ _[V][j][(][s][N][ κ]j_ [)][ is already very accurate for calculating the policy gradient for agent]

_i. Theorem 4 justifies that the policy gradients computed based on the sum of the nearby extended_
P
value functions is a close approximation of true policy gradients.

**Theorem 4. Let** _A[ˆ]t = r[(][t][)]+γV (s[(][t][+1)])−V (s[(][t][)]) be the TD residual, and gi = E[ A[ˆ]∇θi log πi(a|s)]_
_be the policy gradient. If_ _A[˜]t and ˜gi are the TD residual and policy gradient when value function_
_V (s) is replaced by_ _V[˜]i(s) =_ _n[1]_ _j∈Ni[κ]_ _[V][j][(][s][N][ κ]i_ [)][, we have:]

P

_gi_ _g˜i_ _i_
_|_ _−_ _| ≤_ 1[γ][κ][−]γ[1] [[1][ −] [(1][ −] _[γ][2][)]_ _[N]n[ κ]_ []][r][max][g][max][,]

_−_


_where rmax and gmax denote the upper bound of the absolute value of reward and gradient, respec-_
_tively._


-----

6 EXPERIMENTS

6.1 ENVIRONMENTS

We test our algorithm in four environments, namely Figure Eight, Ring Attenuation (Wu et al.,
2017a), CACC Catchup, and CACC Slowdown (Chu et al., 2020). Detailed description and visualization of these environments is deferred to Appendix A.

**Cooperative Adaptive Cruise Control** The objective of CACC is to adaptively coordinate a platoon of 8 vehicles to minimize the car-following headway and speed perturbations based on real-time
vehicle-to-vehicle communication. CACC consists of two scenarios: Catch-up and Slow-down. In
CACC Catch-up, vehicles need to catch up to the first car. In CACC Slow-down, every vehicle is
faster than the optimal speed, and they need to slow down without causing any collision. The agents
receives a negative reward if the headway or the speed is not optimal. Also, whenever a collision
happens, a huge negative reward of -1000 is given.

**Flow environments** This task consists of Figure Eight and Ring Attenuation. The objective of
these environments is letting the automated vehicles achieve a target average speed inside the road
network while avoiding collisions. The state of each vehicle is its velocity and position, and the
action is the acceleration of itself. In Ring Attenuation, the objective is to achieve a high speed,
while avoiding stop-and-go loops. Vehicles are rewarded with their speed, but also punished for their
accelerations. In the perspective of a networked system, we assume that the vehicles are connected
with the preceding and succeeding vehicle, thus resulting in a loop-structured graph.

6.2 BASELINES

We describe the following algorithms for performance comparison:

-  CPPO: Centralized PPO learns a centralized critic Vi(s). This baseline aims to analyze the
performance when κ is set to be arbitrarily huge, and is used in (Vinitsky et al., 2018) as a
benchmark algorithm for networked system control.

-  IC3Net (Singh et al., 2018): A communication-based multi-agent RL algorithm. The agents
maintain their local hidden states with a LSTM kernel, and actively determines the communication target. Compared with DPPO, IC3Net uses hidden state and continuous communication, whereas DPPO agents directly observe the states of their neighbors.

-  DPPO: Decentralized PPO learns an independent actor and critic for each agent. We implement it by using neighbor’s state for extended value estimation.

-  DMPO (our method): DMPO is a decentralized and model-based algorithm based on
DPPO. On top of it, we use decentralized graph convolutional kernel as predictive model.

6.3 RESULTS

Figure 2 shows the episode reward v.s. number of training samples curves of the algorithms. We
address that in CACC environments, DMPO uses decentralized SAC as base algorithm. Similar with
DPPO, decentralized SAC uses extended Q-function Qi(sNi[κ] _[, a][N][ κ]i_ [)][ for its policy gradient. From the]
results, we conclude that our algorithm matches the asymptotic performance of model-free methods.
It also learns the policy faster, resulting in increased sample efficiency.

The comparison between DMPO and DPPO can be viewed as an ablation study of model usage. In
figure eight, DMPO increases sample efficiency at the beginning, but as the task becomes difficult,
the sample efficiency of our method decreased. In a relatively easy task, ring attenuation, our method
increased sample efficiency massively, compared with its model-free counterpart.

The comparison between the asymptotic performance of CPPO and DMPO or DPPO can be viewed
as an ablation study of extended value function. From the result in four environments, we observe that the asymptotic performance of CPPO does not exceed that of the algorithms that uses
extended value function. In this way, we conclude that by using extended value function, a centralized algorithm can be decomposed into decentralized algorithm, but the performance would not
drop significantly.


-----

Figure 3 shows the accuracy of our model in predicting the reward and state during training. The error is defined as the ratio of MSE loss to variance. From the figures, we conclude that neighborhood
information is accurate enough for a model to predict the next state in these environments. However,
in CACC Slow-down, local models might fail to learn the reward. We observe that the errors may
increase as the agents explore new regions in the state space.


0

300

1000

250

2000

200

3000

150

100 CPPO 4000 CPPO

50 DPPOIC3Net 5000 DPPOIC3Net

DMPO(ours) DMPO(ours)

0 6000

0 50000 100000 150000 200000 250000 300000 350000 400000 0 50000 100000 150000 200000 250000 300000 350000 400000

(a) Figure Eight (b) Ring Attenuation

500 600

400 500

400

300

300

200

200

100

CPPO 100 CPPO

0 DPPOIC3Net 0 DPPOIC3Net

DMPO(ours) DMPO(ours)

100 100

0 50000 100000 150000 200000 250000 300000 350000 0 200000 400000 600000 800000 1000000

(c) CACC Catch-up (d) CACC Slow-down

Figure 2: Training curves on multi-agent environments. Solid curves depict the mean of five trails,
and shaded region correspond to standard deviation.

10 1 10 1

10 2 10 2

10 3

10 3

10 4

10 4 Catchupslowdown 10 5 Catchupslowdown

ring ring
eight eight

10 5 10 6

0 20000 40000 60000 80000 100000 0 20000 40000 60000 80000 100000


(a) State Error


(b) Reward Error


Figure 3: Figures of state and reward error. Both state error and reward error < 10% in every
environment.

7 CONCLUSIONS

In this paper, we propose algorithm DMPO, a model-based and decentralized multi-agent RL algorithm. We then give a theoretical analysis on the algorithm to analyze its performance discrepancy,
compared with a model-free algorithm. By experiments in several tasks in networked systems,
we show that although our algorithm is decentralized and model-based, it matches the asymptotic
performance of some state-of-art multi-agent algorithms. From the results, we also conclude that
using extended value function instead of centralized value function did not sacrifice performance
massively, yet it makes our algorithm scalable.


-----

REFERENCES

Masako Bando, Katsuya Hasebe, Akihiro Nakayama, Akihiro Shibata, and Yuki Sugiyama. Dynamical model of traffic congestion and numerical simulation. Physical review E, 51(2):1035,
1995.

Eugenio Bargiacchi, Timothy Verstraeten, and Diederik M. Roijers. Cooperative prioritized sweeping. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2021),
pp. 160–168. IFAAMAS, 2021.

Bruno Bouzy and Marc M´etivier. Multi-agent model-based reinforcement learning experiments in
the pursuit evasion game. 2007.

Ronen I Brafman and Moshe Tennenholtz. A near-optimal polynomial time algorithm for learning
in certain classes of stochastic games. Artificial Intelligence, 121(1-2):31–47, 2000.

Tianshu Chu, Sandeep Chinchali, and Sachin Katti. Multi-agent reinforcement learning for networked system control. In International Conference on Learning Representations (ICLR), 2020.
[URL https://openreview.net/forum?id=Syx7A3NFvH.](https://openreview.net/forum?id=Syx7A3NFvH)

Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial
_Intelligence, volume 32, 2018._

Carlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent planning with factored mdps. In
_Advances in Neural Information Processing Systems (NeurIPS), volume 1, pp. 1523–1530, 2001._

Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Modelbased policy optimization. In Advances in Neural Information Processing Systems (NeurIPS),
2019.

I Ge Jin and G´abor Orosz. Dynamics of connected vehicle systems with delayed acceleration feedback. Transportation Research Part C: Emerging Technologies, 46:46–64, 2014.

Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A
survey. Journal of artificial intelligence research, 4:237–285, 1996.

Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic
framework for model-based deep reinforcement learning with theoretical guarantees. In Interna_tional Conference on Learning Representations (ICLR), 2019._

Guannan Qu, Yiheng Lin, Adam Wierman, and Na Li. Scalable multi-agent reinforcement learning
for networked systems with average reward. Advances in Neural Information Processing Systems
_(NeurIPS), 33, 2020a._

Guannan Qu, Adam Wierman, and Na Li. Scalable reinforcement learning of localized policies for
multi-agent networked systems. In Learning for Dynamics and Control (L4DC), pp. 256–266.
PMLR, 2020b.

Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and
Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 4295–4304. PMLR, 2018.

Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020.

John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. _arXiv preprint_
_arXiv:1506.02438, 2015._

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.


-----

Thiago D Simao and Matthijs TJ Spaan. Safe policy improvement with baseline bootstrapping in
factored environments. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),
volume 33, pp. 4967–4974, 2019.

Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Learning when to communicate at scale
in multiagent cooperative and competitive tasks. arXiv preprint arXiv:1812.09755, 2018.

Wen Sun, Geoffrey J Gordon, Byron Boots, and J Bagnell. Dual policy iteration. Advances in Neural
_Information Processing Systems (NeurIPS), 31:7059–7069, 2018._

Eugene Vinitsky, Aboudy Kreidieh, Luc Le Flem, Nishant Kheterpal, Kathy Jang, Cathy Wu,
Fangyu Wu, Richard Liaw, Eric Liang, and Alexandre M Bayen. Benchmarks for reinforcement learning in mixed-autonomy traffic. In Conference on robot learning, pp. 399–409. PMLR,
2018.

Cathy Wu, Aboudy Kreidieh, Kanaad Parvate, Eugene Vinitsky, and Alexandre M Bayen. Flow:
Architecture and benchmarking for reinforcement learning in traffic control. _arXiv preprint_
_arXiv:1710.05465, 10, 2017a._

Cathy Wu, Aboudy Kreidieh, Eugene Vinitsky, and Alexandre M Bayen. Emergent behaviors in
mixed-autonomy traffic. In Conference on Robot Learning, pp. 398–407. PMLR, 2017b.

Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multiagent reinforcement learning with networked agents. In International Conference on Machine
_Learning (ICML), pp. 5872–5881, 2018._

Kaiqing Zhang, Zhuoran Yang, and Tamer Bas¸ar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms, 2019.

Kaiqing Zhang, Sham Kakade, Tamer Basar, and Lin Yang. Model-based multi-agent rl in zero-sum
markov games with near-optimal sample complexity. Advances in Neural Information Processing
_Systems (NeurIPS), 33, 2020._


-----

A EXPERIMENT DETAILS

[The code of our algorithm can be found at https://anonymous.4open.science/r/](https://anonymous.4open.science/r/RL-algorithms-0E72)
[RL-algorithms-0E72.](https://anonymous.4open.science/r/RL-algorithms-0E72)

A.1 ENVIRONMENT DESCRIPTION

The objective of CACC is to adaptively coordinate a line of vehicles to minimize the car-following
headway and speed perturbations based on real-time vehicle-to-vehicle communication. We conduct
experiments on two scenarios: CACC catch-up, CACC slow-down. The local observation of each
agent consists of headway h, velocity v, acceleration a, and is shared to neighbors within two steps.
The action of each agent is to choose appropriate hyper-parameters (α[◦], β[◦]) for each OVM controller Bando et al. (1995), selected from four levels {(0,0),(0.5,0),(0,0.5),(0.5,0.5)}, where α[◦], β[◦]
denotes headway gain and relative gain for OVM controller respectively. The reward function is
state and speed perturbations, where the target headway and velocity profile aredefined as (hi,t − _h[∗])[2]_ + (vi,t − _vt[∗][)][2][ + 0][.][1][a]i,t[2]_ [to punish the gap between the current state to target] h[∗] = 20m and vt[∗][,]
respectively. Whenever a collision happens (hi,t < 1m), a large penalty of -1000 is assigned to each
agent and the state becomes absorbing. An additional cost 5 (2hst − _hi,t)[2]+_ [is provided in training]
for potential collisions. In catch-up scenario, initial headway of the first vehicle is larger than the
target headway, thus the following agents learn how to catch up with the first vehicle, where target
speed vt[∗] [= 15][m/s][ and initial headway][ h][1][,][0] _[> h][i,][0][,][ ∀][i][ ̸][= 1][. In slow-down scenario, target speed][ v]t[∗]_
linearly decreases to 15m/s during the first 30s and then stays at constant, thus agents learn how to
slow down speed cooperatively,where initial headway hi,0 = h[∗].

Flow environments consists of Figure Eight and Ring Attenuation. The objective of these environments is letting the automated vehicles achieve a target average speed inside the road network while
avoiding collisions.

The figure eight network, previously presented in (Wu et al., 2017b), acts as a closed representation
of an intersection. In a figure eight network containing a total of 14 vehicles, we witness the formation of queues resulting from vehicles arriving simultaneously at the intersection and slowing down
to obey right-of-way rules. This behavior significantly reduces the average speed of vehicles in the
network. The state consists of velocity and position for the vehicle. The action is the acceleration
of the vehicle a ∈ R[amin,amax]. The objective of the learning agent is to achieve high speeds while
penalizing collisions. Accordingly, the local reward function is defined aswhere vdes is an arbitrary target velocity. _ri = vdes −|vdes −_ _vi|,_

In Ring Attenuation, the objective is to achieve a high speed, while avoiding accelerationdeceleration loops. To achieve this, vehicles are rewarded with their speed and punished for their
accelerations. The state and action of each vehicle is the same as Figure Eight. In the perspective of
a networked system, we assume that the vehicles are connected with the preceding and succeeding
vehicle, thus resulting in a loop-structured graph.

(a) CACC (b) Figure Eight (c) Ring Attenuation

V2V V2V V2V V2V

Speed, acceleration, Speed, acceleration,

headway headway


Figure 4: Visualization of CACC and Flow environments. (a) A line of vehicles that need to keep
a stable velocity and desired headway. (b) Vehicles travel in a figure eight shaped road section to
learn the behavior at an intersection .(c) Vehicles travel in a ring to reduce stop-and-go waves.


-----

B HYPERPARAMETERS

We list some of the key hyperparameters for DMPO and DPPO in Table 1 and 2.

|Scenario Hyperparameter|Catch Up|Slow Down|Figure Eight|Ring Attenuation|
|---|---|---|---|---|
|Learning rate of critic|3e-4|3e-4|5e-5|5e-4|
|Learning rate of π|3e-4|3e-4|5e-5|5e-4|
|Learning rate of Model|3e-4|3e-4|5e-4|5e-4|
|Clip|-|-|0.15|0.2|
|GAE λ|1|1|0.5|0.5|
|KL target|-|-|7.5e-3|0.01|
|Model usage probability|1|1|0.5|0.5|
|Rollout Length|1|1|25|25|
|κ|1|1|3|3|



Table 1: Hyperparameters for DMPO.

|Scenario Hyperparameter|Catch Up|Slow Down|Figure Eight|Ring Attenuation|
|---|---|---|---|---|
|Learning rate of V|7e-4|5e-4|5e-4|1e-3|
|Learning rate of π|5e-5|5e-5|5e-5|1e-3|
|Clip|0.2|0.2|0.2|0.2|
|GAE λ|0.5|0.5|0.5|0.5|
|KL target|0.01|0.01|0.01|0.01|
|κ|2|2|3|3|



Table 2: Hyperparameters for DPPO.

C PROOF OF THEOREMS

C.1 REMARKS ON PROOFS OF LEMMAS AND THEOREMS

To bound the returns, we use TV distance to assess the distance between si, ai at every timestep,
based on the fact that Esi,ai _p1_ _r(si, ai)_ Esi,ai _p2_ _r(si, ai)_ _rmax_ _DT V (p1_ _p2). To achieve_
_|_ _∼_ _−_ _∼_ _| ≤_ _·_ _∥_
this, we argue that in an INS, s[T]i [only depends on][ s][T]N[ −]i[1] [1][, which is the][ 1][-hop neighbors’ states at]

previous timestep. Then, inductively, s[T]i [is only dependent on][ s]N[T][ −]i[1] [1][, s][T]N[ −]i[2] [2][, ..., s][T]N[ −]i[k] _[k][, and ultimately,]_

_s[0]Ni[T]_ [. Then we consider a chain of states][ x][t][ =][ s][N][ N]i _[−][t]_ _, t = 0, 1, ..., N in Lemma 4 to analyze how_

_s[t]i_ [depends on model errors and policy divergences in a step-by-step manner. Insipred by Janner]
et al., by summing the divergence at each timestep t = 0, 1, ..., we can generate a overall discounted
returns bound of branched rollout in Lemma 5. Lemma 6 is a simple corollary of Lemma 5, which
handles normal rollout scenario.

If the returns are bounded, we can show that if every optimization step increase the return by C,
the real return would also almost monotonically increase. Specifically, if ˆη[π[k][+1]] ≥ _ηˆ[π[k]] + C,_
combined with η[π[k][+1]] ≥ _ηˆ[π[k]] −_ _C, we have η[π[k][+1]] ≥_ _ηˆ[π[k]]. But note that ˆη[π[k]] increases_
monotonically, and it’s a lower bound of η[π[k][+1]]. This means that ˆη[π[k][+1]] would almost monotonically increase.

C.2 PROOF OF THEOREM 1

_Proof. η[p][π1, ..., πn] and ηp[ˆ][π1, ..., πn] differs in two ways. First, they are estimated with different_
transition dynamics. Second, the latter sample their states and actions with another policy, namely
_πD. To deal with these divergences separately, we use an intermediate return η[πD], which is the_


-----

return of πD in the environment. Then the difference can be bounded by:


_|ηi[p][[][π][1][, ..., π][n][]][ −]_ _[η][ ˆ]ip[[][π][1][, ..., π][n][]][| ≤|][η]i[p][[][π][1][, ..., π][n][]][ −]_ _[η]i[p][[][π][D][]][|]_ + |ηi[p][[][π][D][]][ −] _[η][ ˆ]ip[[][π][1][, ..., π][n][]][|]_
_L1_ _L2_
| {z } | {z }

To bound L1, we apply Lemma 6 with ϵmi = 0. Then:


(9)


_∞_

_γ[k][+1][ X]_ _ϵπj_ _._ (10)
_k=0_ _j_ _Ni[k]_

X _∈_ 


_L1_ _ϵπi +_
_≤_ [2]1[r][max]γ

_−_



In L2, both the dynamics and policies and different, therefore:


_∞_

_γ[k][+1][ X]_ (ϵmj + ϵπj ) _._ (11)
_k=0_ _j_ _Ni[k]_

X _∈_ 


_L2_ _ϵπi +_
_≤_ [2]1[r][max]γ

_−_


Putting Equation 10 and 11 yields:


_∞_

_γ[k][+1][ X]_
_k=0_ _j_ _Ni[k]_

X _∈_


_ηi[p][[][π][1][, ..., π][n][]][ −]_ _[η][ ˆ]ip[[][π][1][, ..., π][n][]][| ≤]_ [2][r][max] 2ϵπi +
_|_ 1 _γ_

_−_



(ϵmj + 2ϵπj ) _._ (12)



And because the binary relation of k-hop neighbor is symmetric, we have:


_Vj[k]_
_|_ _[|]_ (13)

_n [ϵ][j][.]_


_ϵj =_


_i=1_


_j_ _Ni[k]_
_∈_


_j=1_


Then, by averaging Equation 12, we have:

_η[p][π1, ..., πn]_ _ηp[ˆ][π1, ..., πn]_
_|_ _−_ _|_

_n_

_ηi[p][[][π][1][, ..., π][n][]][ −]_ _[η][ ˆ]ip[[][π][1][, ..., π][n][]][|]_

_≤_ _n[1]_ _|_

_i=1_

X 1 _n_ _∞_ _n_

_ϵπi +_ _γ[k][+1][ 1]_ (ϵmj + 2ϵπj )

_≤_ [2]1[r][max]γ _n_ _n_

_−_ _i=1_ _k=0_ _i=1_ _j_ _Ni[k]_
 X X X X∈

1 _n_ _∞_ _n_ _Ni[k]_

= [2][r][max] _ϵπi +_ _γ[k][+1]_ _|_ _[|]_ (ϵmi + 2ϵπi )

1 _γ_ _n_ _n_
_−_ _i=1_ _k=0_ _i=1_
 _n_ Xϵπi X X _∞_ _i_ 

= [2][r][max] _γ[k][+1][ |][N][ k][|]_

1 _γ_ _n_ [+ (][ϵ][m][i][ + 2][ϵ][π][i] [)][ ·] _n_
_−_ _i=1_ _k=0_

X  X 


(14)


C.3 PROOF OF THEOREM 2

_Proof. To bound_ _η[p][π1, ..., πn]_ _η[branch][π1, ..., πn]_, we need to analyze how do they differ from
_|_ _−_ _|_
each other. η[p] denote the real returns of these policies in the environment. η[branch] is the returns estimated in the branched rollout scheme. To explicitly illustrate this point, we describe their difference
in Table 3:

By Lemma 5, we have:



_[T][ −][1]_

_ηi[p][[][π][1][, ..., π][n][]][ −]_ _[η]i[branch][π1, ..., πn]_ _γ[k][+1][ X]_
_|_ _| ≤_ [2]1[r][max]γ

_−_ _k=0_ _j_ _Ni[k]_
 X _∈_


_∞_

_ϵmj +_ _γ[k][+1][ X]_

_k=T_ _j_ _Ni[k]_

X _∈_


_ϵπj_ (15)



-----

|branch point|before|Col3|after|Col5|
|---|---|---|---|---|
||dynamics|policies|dynamics|policies|
|ηp|p|π|p|π|
|ηbranch|p|π D|pˆ|π|


Table 3: The difference between η[p] and η[branch]

And because the binary relation of k-hop neighbor is symmetric, we have:


_Vj[k]_
_|_ _[|]_ (16)

_n [ϵ][j][.]_


_ϵj =_


_i=1_


_j_ _Ni[k]_
_∈_


_j=1_


Then, by averaging Equation 15, we have:

_η[p][π1, ..., πn]_ _η[branch][π1, ..., πn]_
_|_ _−_ _|_

_n_

_ηi[p][[][π][1][, ..., π][n][]][ −]_ _[η]i[branch][π1, ..., πn]_

_≤_ _n[1]_ _|_ _|_

_i=1_

X

_[T][ −][1]_ _n_ _∞_ _n_

_γ[k][+1]_ [1] _ϵmj +_ _γ[k][+1]_ [1] _ϵπj_

_≤_ [2]1[r][max]γ _·_ _n_ _·_ _n_

_−_ _k=0_ _i=1_ _j_ _Ni[k]_ _k=T_ _i=1_ _j_ _Ni[k]_
 X X X∈ X X X∈

= [2][r][max] _[T][ −][1]_ _γ[k][+1]_ _n_ _|Ni[k][|]_ _ϵmi +_ _∞_ _γ[k][+1]_ _n_ _|Ni[k][|]_ _ϵπi_

1 _γ_ _·_ _n_ _·_ _n_
_−_ _k=0_ _i=1_ _k=T_ _i=1_
 X X X X 

_n_ _[T][ −][1]_ _∞_

_i_ _i_

= [2][r][max] _ϵmi_ _γ[k][+1][ |][N][ k][|]_ + ϵπi _γ[k][+1][ |][N][ k][|]_

1 − _γ_ _i=1_ _·_ _k=0_ _n_ _·_ _k=T_ _n_

X    X    X 


(17)


C.4 PROOF OF COROLLARY 1

_Proof. Although p(s[′]|s, a) might not satisfy the criteria of an INS, we can construct another_
transition dynamic ˜p = _i=1_ _[p][i][(][s][i][|][s][N][i]_ _[, a][i][)][, which is the product of marginal transitions, thus]_
being an INS. Recall that by definition, if p is a transition dynamic of a ξ-dependent system,
sups,a DT V (p(s[′]|s, a)∥p˜(s[Q][′]|[n]s, a)) ≤ _ξ. Then we divide the difference into two parts:_

_η[p][π1, ..., πn]_ _η[branch][π1, ..., πn]_
_−_

= η[p][π1, ..., πn] _ηp[˜][π1, ..., πn]_ + ηp[˜][π1, ..., πn] _ηbranch[π1, ..., πn]_ (18)
_−_ _−_
_L1_ _L2_

The first part is the difference of policy returns, estimated in two environments. Since there are no| {z } | {z }
policy divergences, and the transition dynamics only differs in a global manner, utilizing Lemma 2
yields that:
_DT V_ _p[t](s, a)_ _p˜[t](s, a)_ _tξ._ (19)
_∥_ _≤_

Then, we have:   


_L1 = η[p][π1, ..., πn] −_ _ηp[˜][π1, ..., πn]_


_γ[t]DT V_ _p[t](s, a)_ _p˜[t](s, a)_
_∥_
_t=0_

X  

_∞_

_γ[t]tξ_

_t=0_

X


2rmax
_≤_

2rmax
_≤_


(20)


= [2][r][max]

1 − _γ_ _[·]_


1 _γ [ξ.]_
_−_


-----

As for L2, because ˜p is an INS, we can directly apply Theorem 2:



_[T][ −][1]_
_ϵmi_ _γ[k][+1][ |][N][ k]i_ _[|]_
_·_ _k=0_ _n_
  X


_∞_

_γ[k][+1][ |][N][ k]i_ _[|]_

_n_

_k=T_

X


(21)



_L2_
_≤_ [2]1[r][max]γ

_−_


+ ϵπi
_·_


_i=1_


Summing Equation 20 and Equation 21 completes the proof.

C.5 PROOF OF THEOREM 3

_Proof. In (Qu et al., 2020b), it was proven that if sNi[κ]_ [and][ a][N][ κ]i [are fixed, then no matter how other]
states and actions changes, Q-function will not change significantly:

_|Q(sNi[κ]_ _[, a][N][ κ]i_ _[, s][N][ κ]−i_ _[, a][N][ κ]−i_ [)][ −] _[Q][(][s][N][ κ]i_ _[, a][N][ κ]i_ _[, s]N[′]_ _−[κ]_ _i_ _[, a]N[′]_ _−[κ]_ _i_ [)][| ≤] 1[r][max]γ [γ][κ][.] (22)

_−_

As value function is the expectation of Q-function

_Vi(s) = Ea_ _πQ(s, a)_
_∼_ (23)
_Vi(sNi[κ]_ [) =][ E][a][∼][π][Q][(][s][N][ κ]i _[, a][N][ κ]i_ [)][,]

we have,
_Vi(s)_ _Vi(sNi[κ]_ [)][|][ =][ |][E][a][∼][π][Q][(][s, a][)][ −] [E][a][∼][π][Q][(][s][N][ κ]i _[, a][N][ κ]i_ [)][|]
_|_ _−_

_≤_ Ea∼π|Q(s, a) − _Q(sNi[κ]_ _[, a][N][ κ]i_ [)][|] (24)

_≤_ 1[r][max]γ [γ][κ][,]

_−_

which concludes the proof.

C.6 PROOF OF THEOREM 4

_Proof. The difference of the gradients can be written as_


_gi −_ _g˜i =E( A[ˆ] −_ _A[˜])∇θi log πi(ai|s ¯Ni_ [)]

= [1] _Aˆj]_ _θi log πi(ai_ _s ¯Ni_ [) + 1] ( A[ˆ]j _Aj)]_ _θi log πi(ai_ _s ¯Ni_ [)]

_n_ [E][[]j /X∈Ni[κ] _∇_ _|_ _n_ [E][[]jX∈Ni[κ] _−_ [˜] _∇_ _|_

= [1] (rj(sj, aj) + γVj(s[′]) _Vj(s))]_ _θi log πi(ai_ _s ¯Ni_ [)]

_n_ [E][[] _−_ _∇_ _|_

_j /Ni[κ]_

X∈

+ n[1] [E] [(rj(sj, aj) + γVj(s[′]) − _Vj(s)) −_ (rj(sj, aj) + γVj(s[′]Ni[κ] [)][ −] _[V][j][(][s][N][ κ]i_ [))]][∇][θ][i][ log][ π][i][(][a][i][|][s][ ¯]Ni [)]

_j_ _Ni[κ]_

X∈

=L1 + L2.
(25)
Because for any function b(s) that depends only on s, E[b(s) log πi[θ][i] [(][a][i][|][s][ ¯]Ni [)] = 0][. Therefore,][ L][2] [in]
Equation 25 becomes:


_|L2| ≤_ _n[1]_ [E] _γ|Vj(s[′]) −_ _Vj(s[′]Ni[κ]_ [)]][||∇][θ][i][ log][ π][i][(][a][i][|][s][ ¯]Ni [)][|]

_j_ _Ni[κ]_

X∈ (26)

_i_ _γ[κ][+1]_

_[|]_
_≤_ _[|][N]n[ κ]_ 1 _γ [r][max][g][max][.]_

_−_

For L1, note that rj(sj, aj) + γVj(s[′]) − _Vj(s) = −Vj(s) + rj(sj, aj) +_ _t=1_ [E][γ][t][r][j][(][s]j[t] _[, a][t]j[) +]_
_γ[κ][−][1]Vj(s[κ][−][1]). And in an INS, s[t]j[, a][t]j[, t][ = 0][,][ 1][, ..., κ][ −]_ [2][ is not affected by policy][ π][i][ if][ j /]∈ _Ni[κ][, we]_
have that [P][κ][−][2]
_L1_ _γ[κ][−][1]Vj(s[κ][−][1])_ _θi log πi(ai_ _s ¯Ni_ [)][|]
_|_ _| ≤_ _n[1]_ [E] _|_ _||∇_ _|_

_j /Ni[κ]_

X∈ (27)

(1 _i_
_≤_ _−_ _[N]n[ κ]_ [)][ γ]1 _[κ][−]γ [1]_ _[r][max][g][max][.]_

_−_


-----

Put Equation 26 and 27 together, we have

_gi_ _g˜i_ _L1_ + _L2_
_|_ _−_ _| ≤|_ _|_ _|_ _|_

_i_ _γ[κ][+1]_ _i_

_[|]_
_≤_ _[|][N]n[ κ]_ 1 _γ [r][max][g][max][ + (1][ −]_ _[N]n[ κ]_ [)][ γ]1 _[κ][−]γ [1]_ _[r][max][g][max]_

_−_ _−_

= _[γ][κ][−][1]_ _i_

1 _γ_ [[1][ −] [(1][ −] _[γ][2][)]_ _[N]n[ κ]_ []][r][max][g][max][.]
_−_


(28)


D USEFUL LEMMAS

**Lemma 1. (TVD of Joint Distribution) Consider two distributions p1(x, y) = p1(x)p1(y|x) and**
_p2(x, y) = p2(x)p2(y_ _x). The total variation distance between them can be bounded as:_
_|_

_DT V_ _p1(x, y)_ _p2(x, y)_ _DT V_ _p1(x)_ _p2(x)_ + Ex _p2_ _DT V_ _p1(y_ _x)_ _p2(y_ _x)_
_∥_ _≤_ _∥_ _∼_ _|_ _∥_ _|_
   _≤_ _DT V_  p1(x)∥p2(x) + maxx _DT V_ _p1 (y|x)∥p2(y|x)_ 

     


_Proof._

_DT V_ _p1(x, y)_ _p2(x, y)_ = [1]
_∥_ 2
  

= [1]

2

= [1]

2

_≤_ [1]2

= [1]


_p1(x, y)_ _p2(x, y)_
_|_ _−_ _|_
_x,y_

X

_p1(x)p1(y_ _x)_ _p2(x)p2(y_ _x)_
_|_ _|_ _−_ _|_ _|_
_x,y_

X

_p1(x)p1(y_ _x)_ _p2(x)p1(y_ _x) + p2(x)p1(y_ _x)_ _p2(x)p2(y_ _x)_
_|_ _|_ _−_ _|_ _|_ _−_ _|_ _|_
_x,y_

X


_p1(x)_ _p2(x)_ _p1(y_ _x) + [1]_
_|_ _−_ _|_ _|_ 2
_x,y_

X


_p2(x)_ _p1(y_ _x)_ _p2(y_ _x)_
_|_ _|_ _−_ _|_ _|_
_x,y_

X


= _p1(x)_ _p2(x)_ + _p2(x)DT V (p1(y_ _x)_ _p2(y_ _x))_

2 _|_ _−_ _|_ _|_ _∥_ _|_

_x_ _x_

X X

= DT V _p1(x)_ _p2(x)_ + Ex _p2_ _DT V_ _p1(y_ _x)_ _p2(y_ _x)_
_∥_ _∼_ _|_ _∥_ _|_

_≤_ _DT V_  p1(x)∥p2(x) + maxx _DT V_ _p1 (y|x)∥p2(y|x)_ 
     


**Lemma 2. Suppose there are two chains of distributions {x[t]1[, t][ ≥]** [0][}][,][ {][x]2[t] _[, t][ ≥]_ [0][}][. At time][ t][, the]
_states of both chains share an identical state space x[t]1[, x][t]2_
_a Markov-like property: p[t][+1](x[t][+1]|x[t], ..., x[1], x[0]) = p[t][+1](x[∈X][t][+1]|[ t]x[. Suppose these two chains satisfy][t]). Then, the TVD of distributions_
_of two chains at time t can be decomposed as:_

_T_

_DT V_ _p[T]1_ [(][x][T][ )][∥][p]2[T] [(][x][T][ )] _DT V_ _p[0]1[(][x][0][)][∥][p][0]2[(][x][0][)]_ + Est−1 _pt2−1_ _DT V_ _p[t]1[(][x][t][|][x][t][−][1][)][∥][p][t]2[(][x][t][|][x][t][−][1][)]_
_≤_ _∼_

_t=1_

      X  

_Proof. We prove this lemma by induction._

When T = 0, it’s easy to see that this lemma is true.


-----

Assume it is true for T = k. We have:

_p[k]1[+1](x[k][+1])_ _p[k]2[+1](x[k][+1])_
_|_ _−_ _|_

= [p[k]1[+1](x[k][+1] _x[k])p[k]1[(][x][k][)][ −]_ _[p][k]2[+1](x[k][+1]_ _x[k])p[k]2[(][x][k][)]][|]_
_|_ _|_ _|_

_x[k]_

X


_p[k]1[+1](x[k][+1]_ _x[k])p[k]1[(][x][k][)][ −]_ _[p][k]2[+1](x[k][+1]_ _x[k])p[k]2[(][x][k][)][|]_
_|_ _|_ _|_
_x[k]_

X

_p[k]1[+1](x[k][+1]_ _x[k])p[k]1[(][x][k][)][ −]_ _[p][k]1[+1](x[k][+1]_ _x[k])p[k]2[(][x][k][)]_
_|_ _|_ _|_
_x[k]_

X

+ p[k]1[+1](x[k][+1] _x[k])p[k]2[(][x][k][)][ −]_ _[p][k]2[+1](x[k][+1]_ _x[k])p[k]2[(][x][k][)][|]_
_|_ _|_

_p[k]2[(][x][k][)][|][p][k]1[+1](x[k][+1]_ _x[k])_ _p[k]2[+1](x[k][+1]_ _x[k])_ + p[k]1[+1](x[k][+1] _x[k])_ _p[k]1[(][x][k][)][ −]_ _[p][k]2[(][x][k][)][|]_
_|_ _−_ _|_ _|_ _|_ _|_

_x[k]_

X 


=Exk _pk2_ [[][|][p]1[k][+1](x[k][+1] _x[k])_ _p[k]2[+1](x[k][+1]_ _x[k])_ ] +
_∼_ _|_ _−_ _|_ _|_

_DT V_ _p[k]1[+1](x[k][+1])_ _p[k]2[+1](x[k][+1])_
_∥_

= [1]2   _|p[k]1[+1](x[k][+1]) −_ _p[k]2[+1](x[k][+1]_ )|

_x[k][+1]_

X


_p1[k][+1](x[k][+1]_ _x[k])_ _p[k]1[(][x][k][)][ −]_ _[p][k]2[(][x][k][)][|]_
_|_ _|_
_x[k]_

X


_≤_ 2[1]


Exk _pk2_ [[][|][p]1[k][+1](x[k][+1] _x[k])_ _p[k]2[+1](x[k][+1]_ _x[k])_ ]
_∼_ _|_ _−_ _|_ _|_


_x[k][+1]_


+ _p[k]1[+1](x[k][+1]_ _x[k])_ _p[k]1[(][x][k][)][ −]_ _[p]2[k][(][x][k][)][|]_

_|_ _|_
_x[k]_

X 

=Esk∼pk2 _[D][T V]_ _p[k]1[+1](x[k][+1]|x[k])∥p2[k][+1](x[k][+1]|x[k])_ + DT V _p[k]1[(][x][k][)][∥][p]2[k][(][x][k][)]_

_k+1_

    


_≤DT V_ _p[0]1[(][x][0][)][∥][p]2[0][(][x][0][)]_ +
  

Then this theorem holds for all n ∈ N.


Est−1 _pt2−1_ _DT V_ _p[t]1[(][x][t][|][x][t][−][1][)][∥][p]2[t]_ [(][x][t][|][x][t][−][1][)]
_∼_
_t=1_

X  


**Lemma 3. Consider two distributions with pdf/pmf p(x) and q(x), where x = (x1, ..., xn) ∈** R[n].
_Suppose p and q can be factorized as: p(x) =_ _i=1_ _[p][i][(][x][i][)][, q][(][x][) =][ Q]i[n]=1_ _[q][i][(][x][i][)][. Then we have:]_

[Q]n[n]


_DT V [p(x)∥q(x)] ≤_


_DT V [pi(xi)∥qi(xi)]._
_i=1_

X


_Also, if the distance is measured by KL-divergence, we have:_


_DKL[p(x)_ _q(x)] =_
_∥_


_DKL[pi(xi)_ _qi(xi)]._
_∥_
_i=1_

X


-----

_Proof. We prove this result for discrete distributions, yet by replacing sum with integration, this_
result stays true in continuous case.


_DT V [p(x)_ _q(x)] = [1]_
_∥_ 2

= [1]

2

= [1]

2

_≤_ [1]2

= [1]


_|p(x) −_ _q(x)|_


_pk(xk)_ _qk(xk)_
_−_ _|_
_k=1_ _k=1_

Y Y


_x1,...,xn_

_x1,...,xn_

X

_x1,...,xn_

X



_[i][−][1]_ _n_

_pk(xk)_ _qk(xk)(pi(xi)_ _qi(xi))_

_−_

_k=1_ _k=i+1_

h Y Y

_[i][−][1]_ _n_

_pk(xk)_ _qk(xk)_ _pi(xi)_ _qi(xi)_

_|_ _−_ _|_

_k=1_ _k=i+1_

h Y Y i


_i=1_


_i=1_


_pi(xi)_ _qi(xi)_
_|_ _−_ _|_
_xi_

X


_i=1_


= _DT V [pi(xi)∥qi(xi)]._

_i=1_

X

In KL-divergence case, we have:


_p(x) log_ _[p][(][x][)]_

_q(x)_

_x_

X _n_

_p(x1, ..., xn)_ log _[p][i][(][x][i][)]_

_qi(xi)_

_x1,...,xn_ _i=1_

X h X


_DKL[p(x)_ _q(x)] =_
_∥_


_n_

_pk(xk) log_ _[p][i][(][x][i][)]_

_qi(xi)_

_k=1_

Y


_i=1_

_n_

_i=1_

X


_x1,...,xn_


_pi(xi) log_ _[p][i][(][x][i][)]_

_qi(xi)_

_xi_

X


= _DKL(pi(xi)_ _qi(xi))._

_∥_
_i=1_

X

**Lemma 4. (N** _-step distribution distance) Suppose the expected TVD between two dynam-_
_ics transitions is bounded as ϵmi_ = maxsNi _,ai DT V_ _p(s[′]i[|][s][N]i_ _[, a][i][)][∥]p[ˆ](s[′]i[|][s][N]i_ _[, a][i][)]_ _and ϵπi_ =
maxs DT V _πi(ai_ _sNi_ ) _πˆi(ai_ _sNi_ ) _. If N_ _κ, the N_ _-step distribution distance is bounded as:_
_|_ _∥_ _|_ _≤_  
 


_N_ _−1_

_t=0_

X


_DT V_ _p[N]i_ [(][s][i][)][∥]p[ˆ][N]i [(][s][i][)]



(ϵπj + ϵmj ).
_j_ _Ni[t]_

X∈


_Thus,_


_N_ _−1_

(ϵπj + ϵmj ).

_t=0_ _j_ _Ni[t]_

X X∈


_DT V_ _p[N]i_ [(][s][i][, a][i][)][∥]p[ˆ][N]i [(][s][i][, a][i][)] _≤_ _ϵπi +_
 


_Proof. Consider a chain of regional state x[t]_ = sN Ni _−t_ . Two chains of distributions p[t](x[t]), ˆp(x[t])
denote the distributions of x[t] under the environment and our model, respectively. First, because of
the property of an INS, these two chains’ transition dynamics can be decomposed as:


-----

_p(x[t]|x[t][−][1]) = p(s[′]Ni[N]_ _[−][t]_ _|sN Ni_ _−t+1_


_pj(s[′]j[|][s][V]j_ [)][.]


_j_ _Ni[N]_ _[−][t]_
_∈_


And because pj(s[′]j[|][s][V]j [) =][ P]aj _[p][j][(][s]j[′]_ _[|][s][V]j_ _[, a][j][)][π][j][(][a][j][|][s][V]j_ [)][, by the property of TVD, we know that:]

_DT V_ _pj(s[′]j[|][s][V]j_ [)][∥]p[ˆ]j(s[′]j[|][s][V]j [)] _≤_ _ϵπj + ϵmj_ _._
 


With Lemma 3, we have:

_DT V_ _p(x[t]_ _x[t][−][1])_ _pˆ(x[t]_ _x[t][−][1])_
_|_ _∥_ _|_


Then, by Lemma 2, we know that


(ϵπj + ϵmj ).

_j∈XNi[N]_ _[−][t]_


_DT V_ _p[N]i_ [(][s][i][)][∥]p[ˆ][N]i [(][s][i][)] = DT V _p[N]i_ [(][x][N] [)][∥]p[ˆ][N]i [(][x][N] [)]

_N_

  

_≤_ (ϵπj + ϵmj )

X X


_t=1_ _j_ _Ni[N]_ _[−][t]_

_∈_

_N_ _−1_

(ϵπj + ϵmj ),

_t=0_ _j_ _Ni[t]_

X X∈


which completes the proof of the first part. And by Lemma 1, the second part holds true.

**Lemma 5. (Returns bound measured in branched rollout) Consider two MDPs p(s) and ˆp(s). Sup-**
_pose they both adopt T_ _-branched rollout scheme. Before the branch, suppose the dynamics dis-_
_tributions are bounded as maxsNi_ _,ai DT V_ _p[pre]i_ (s[′]i[|][s][N]i _[, a][i][)][∥]p[ˆ][pre]i_ (s[′]i[|][s][N]i _[, a][i][)]_ = ϵ[pre]mi _[, and policy]_
_divergences are bounded as maxsNi DT V_  πi[pre](ai|sNi )∥πˆi[pre](ai|sNi ) = ϵ[pre]πi _[. After the branch,]_
_ϵ[post]mi_ _and ϵ[post]πi_ _are defined similarly. Then the (local) T_ _-step returns are bounded as:_
  


_T −1_

_γ[k][+1][ X]_
_k=0_ _j_ _Ni[k]_

X _∈_


_∞_

_γ[k][+1][ X]_ (ϵ[pre]mj [+][ ϵ]π[pre]j [)]
_k=T_ _j_ _Ni[k]_

X _∈_


_|ηi −_ _ηˆi| ≤_ [2]1[r][max]γ _ϵ[post]πi_

_−_



(ϵ[post]mj [+][ ϵ]π[post]j [) +]


_Proof. We prove this by estimating the state-action distribution divergence at each timestep. For_
notation simplicity, we denote ϵ[pre]Ni[k] [=][ ϵ]N[pre]i[k] [=][ P]j∈Ni[k] [(][ϵ]m[pre]j [+][ ϵ]π[pre]j [)][, and][ ϵ]N[post]i[k] analogously. If we

divide the chains into two parts: pre-branch and post-branch, and apply Lemma 4 on both parts, we
have:


For t ≤ _T_ :

and for t > T :


_t−1_

_ϵ[post]Ni[k]_ _[,]_
_k=0_

X


_DT V_ _p[t]i[(][s][i][, a][i][)][∥]p[ˆ][t]i[(][s][i][, a][i][)]_ _ϵ[post]πi_
_≤_
 


_T −1_

_ϵ[post]Ni[k]_
_k=0_

X


_t−1_

_ϵ[pre]Ni[k]_ _[.]_
_k=T_

X


_DT V_ _p[t]i[(][s][i][, a][i][)][∥]p[ˆ][t]i[(][s][i][, a][i][)]_ _ϵ[post]πi_
_≤_
 


-----

Then, the difference of discounted distribution can be bounded as:


_DT V_ _p[t]i[(][s][i][, a][i][)][∥]p[ˆ][t]i[(][s][i][, a][i][)]_
_t=0_

X 


_DT V_ _pi(si, ai)_ _pˆi(si, ai)_ (1 _γ)_
_∥_ _≤_ _−_
 

_≤_ (1 − _γ)_


_t−1_

_ϵ[post]Ni[k]_
_k=0_

X


_T_ _t−1_

_≤_ (1 − _γ)_ _γ[t][ ]ϵ[post]πi_ + _ϵ[post]Ni[k]_ +

_t=0_ _k=0_

X X 

_∞_ _T −1_ _t−1_

(1 − _γ)_ _γ[t][ ]ϵ[post]πi_ + _ϵ[post]Ni[k]_ + _ϵ[pre]Ni[k]_

_t=T +1_ _k=0_ _k=T_

X X X 

_T −1_

= ϵ[post]πi + (1 − _γ)_ _ϵ[post]Ni[k]_ [(][γ][k][+1][ +][ γ][k][+2][ +][ ...][)+]

_k=0_

X


_∞_

_ϵ[pre]Ni[k]_ [(][γ][k][+1][ +][ γ][k][+2][ +][ ...][)]
_k=T_

X


(1 − _γ)_

= ϵ[post]πi +


_T −1_

_ϵ[post]Ni[k]_ _[γ][k][+1][ +]_
_k=0_

X


_ϵ[pre]Ni[k]_ _[γ][k][+1][.]_
_k=T_

X


We can convert this bound into the returns bound:


_γ[t]|ri(s[t]i[, a][i][)][ −]_ _[r][i][(ˆ]s[t]i[,][ ˆ]a[t]i[)][|]_
_t=0_

X


_ηi_ _ηˆi_
_|_ _−_ _| ≤_


_∞_

_DT V_ _p[t]i[(][s][i][, a][i][)][∥]p[ˆ][t]i[(][s][i][, a][i][)]_

_≤_ [2]1[r][max]γ [(1][ −] _[γ][)]_

_t=0_

_−_

X  

= [2][r][max] _pi(si, ai)_ _pˆi(si, ai)_

1 _γ [D][T V]_ _∥_
_−_
 _T_ 1 

_−_ _∞_

= [2]1[r][max]γ _ϵ[post]πi_ + _ϵ[post]Ni[k]_ _[γ][k][+1][ +]_ _ϵ[pre]Ni[k]_ _[γ][k][+1][]_

_−_ _k=0_ _k=T_
 X X

**Lemma 6. (Returns bound measured in full length rollout) Consider two MDPs p(s) and ˆp(s).**
_Suppose they both run their rollouts until the end of every trajectory, and the dynamic distributions_
_are bounded as maxsNi_ _,ai DT V_ _pi(s[′]i[|][s][N]i_ _[, a][i][)][∥]p[ˆ]i(s[′]i[|][s][N]i_ _[, a][i][)]_ = ϵmi _, while policy divergences are_
_bounded as maxsNi DT V_ _πi(ai |sNi_ )∥πˆi(ai|sNi ) = ϵπi _. Then the (local) returns are bounded as:_
  


_∞_

_ηi_ _ηˆi_ _ϵπi +_ _γ[k][+1][ X]_ (ϵmj + ϵπj )
_|_ _−_ _| ≤_ [2]1[r] −[max]γ _k=0_ _j_ _Ni[k]_

 X _∈_


_Proof. We can think it as a special case of branched rollout, where ϵ[post]_ = ϵ[pre] = ϵ for every
subscript, and T = 0. From this perspective, applying the result of Lemma 5 completes the proof.


-----

