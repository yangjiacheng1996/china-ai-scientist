# A CLASS OF SHORT-TERM RECURRENCE ANDERSON MIXING METHODS AND THEIR APPLICATIONS

**Fuchao Wei** [1], Chenglong Bao [3][,][4][∗], Yang Liu [1][,][2]

1Department of Computer Science and Technology, Tsinghua University
2Institute for AI Industry Research, Tsinghua University
3Yau Mathematical Sciences Center, Tsinghua University
4Yanqi Lake Beijing Institute of Mathematical Sciences and Applications
wfc16@mails.tsinghua.edu.cn, {clbao,liuyang2011}@tsinghua.edu.cn

ABSTRACT

Anderson mixing (AM) is a powerful acceleration method for fixed-point iterations, but its computation requires storing many historical iterations. The extra
memory footprint can be prohibitive when solving high-dimensional problems in
a resource-limited machine. To reduce the memory overhead, we propose a novel
class of short-term recurrence AM methods (ST-AM). The ST-AM methods only
store two previous iterations with cheap corrections. We prove that the basic version of ST-AM is equivalent to the full-memory AM in strongly convex quadratic
optimization, and with minor changes it has local linear convergence for solving
general nonlinear fixed-point problems. We further analyze the convergence properties of the regularized ST-AM for nonconvex (stochastic) optimization. Finally,
we apply ST-AM to several applications including solving root-finding problems
and training neural networks. Experimental results show that ST-AM is competitive with the long-memory AM and outperforms many existing optimizers.

1 INTRODUCTION

Anderson mixing (AM) (Anderson, 1965) is a powerful sequence acceleration method (Brezinski
et al., 2018) for fixed-point iterations and has been widely used in scientific computing (Lin et al.,
2019; Fu et al., 2020; An et al., 2017), e.g., the self-consistent field iterations in electronic structure
computations (Garza & Scuseria, 2012; Arora et al., 2017). Specifically, we consider a fixed-point
iteration xk+1 = g(xk), k = 0, 1, . . ., where g : R[d] _7→_ R[d] is the fixed-point map. By using
_m historical iterations, AM(m) aims to extrapolate a new iterate that satisfies certain optimality_
property. When the function evaluation is costly, the reduction of the number of iterations brought
by AM can save a large amount of computation (Fang & Saad, 2009).

AM can be used as a method for solving nonlinear equations (Kelley, 2018) as the fixed-point
problem x = g(x) is equivalent to h(x) := x − _g(x) = 0. In practice, since computing the_
Jacobian of h(x) is commonly difficult or even unavailable (Nocedal & Wright, 2006), AM can be
seen as a practical alternate for Newton’s method (An et al., 2017). Also, compared with the classical
iterative methods such as the nonlinear conjugate gradient (CG) method (Hager & Zhang, 2006), no
line-search or trust-region technique is used in AM, which is preferable for large-scale unconstrained
optimization. Empirically, it is observable that AM can largely accelerate convergence, though its
theoretical analysis is still under-explored. It turns out that in the linear case (Walker & Ni, 2011;
Potra & Engler, 2013), the full-memory AM (m = k) is essentially equivalent to GMRES (Saad
& Schultz, 1986), a powerful Krylov subspace method that can exhibit superlinear convergence
behaviour in solving linear systems (Van der Vorst & Vuik, 1993). For general nonlinear problems,
AM is recognized as a multisecant quasi-Newton method (Fang & Saad, 2009; Brezinski et al.,
2018). As far as we know, only local linear convergence has been obtained for the limited-memory
AM (m < k) in general (Toth & Kelley, 2015; Evans et al., 2020; De Sterck & He, 2021).

For the application of AM, one of the major concerns is the historical length m, a critical factor
related to the efficiency of AM (Walker & Ni, 2011). A larger m can incorporate more historical

_∗Corresponding author._


-----

information into one extrapolation, but it incurs heavier memory overhead since 2m vectors of
dimension d need to be stored in AM(m). The additional memory footprint can be prohibitive for
solving high-dimensional problems in a resource-limited machine (Deng, 2019). Using small m
can alleviate the memory overhead but may deteriorate the efficacy of AM since much historical
information is omitted in the extrapolation (Walker & Ni, 2011; Evans et al., 2020).

To address the memory issue of AM, we deeply investigate the properties of the historical iterations
produced by AM and leverage them to develop the short-term recurrence variant, namely ST-AM.
The basic version of ST-AM imposes some orthogonality property on the historical sequence, which
is inspired by the CG method (Hestenes & Stiefel, 1952) that enjoys a three-term recurrence. Furthermore, to better suit the more difficult nonconvex optimization, a regularized short-term form is
introduced. We highlight the main contributions of our work as follows.

1. We develop a novel class of short-term recurrence AM methods (ST-AM), including the
basic ST-AM, the modified ST-AM (MST-AM), and the regularized ST-AM (RST-AM).
The basic ST-AM is applicable for linear systems; MST-AM can solve general fixed-point
problems; RST-AM aims for solving stochastic optimization. An important feature of STAM is that all methods only need to store two previous iterations with cheap corrections,
which significantly reduces the memory requirement compared with the classical AM.

2. A complete theoretical analysis of the ST-AM methods is given. When solving strongly
convex quadratic optimization, we prove that the basic ST-AM is equivalent to the fullmemory AM and the convergence rate is similar to that of the CG method. We also prove
that MST-AM has improved local linear convergence for solving fixed-point problems.
Besides, we establish the global convergence property and complexity analysis for RSTAM when solving stochastic optimization problems.

3. The numerical results on solving (non)linear equations and cubic-regularized quadratic optimization are consistent with the theoretical results for the basic ST-AM and MST-AM.
Furthermore, extensive experiments on training neural networks for image classification
and language modeling show that RST-AM is competitive with the long-memory AM and
outperforms many existing optimizers such as SGD and Adam.

2 RELATED WORK

AM is also known as an extrapolation algorithm in scientific computing (Anderson, 2019). A parallel method is Shanks transformation (Shanks, 1955) which transforms an existing sequence to a new
sequence for faster convergence. Related classical algorithms include Minimal Polynomial Extrapolation (Cabay & Jackson, 1976) and Reduced Rank Extrapolation (Eddy, 1979), and a framework
of these extrapolation algorithms including AM is given in (Brezinski et al., 2018). Note that an
elegant recursive algorithm named ϵ-algorithm had been discovered for Shanks transformation for
scalar sequence (Wynn, 1956), and was later generalized as the vector ϵ-algorithm (Wynn, 1962) to
handle vector sequences, but this short-term recurrence form is not equivalent to the original Shanks
transformation in general (Brezinski & Redivo-Zaglia, 2017). Since AM is closely related to quasiNewton methods (Fang & Saad, 2009), there are also some works trying to derive equivalent forms
of the full-memory quasi-Newton methods using limited memory (Kolda et al., 1998; Berahas et al.,
2021), while no short-term recurrence is available. To the best of our knowledge, ST-AM is the first
attempt to short-term recurrence quasi-Newton methods.

Recently, there have been growing demands for solving large-scale and high-dimensional fixedpoint problems in scientific computing (Lin et al., 2019) and machine learning (Bottou et al., 2018).
For these applications, Newton-like methods (Byrd et al., 2016; Wang et al., 2017; Mokhtari et al.,
2018) are less appealing due to the heavy memory and computational cost, especially in nonconvex
stochastic optimization, where only sublinear convergence can be expected if only stochastic gradients can be accessed (Nemirovski & Yudin, 1983). On the other side, first-order methods (Necoara
et al., 2019) stand out for their low per-iteration cost, though the convergence can be slow in practice. When training neural networks, SGD with momentum (SGDM) (Qian, 1999), and adaptive
learning rate methods, e.g. AdaGrad (Duchi et al., 2011), RMSprop (Tieleman & Hinton, 2012),
Adam (Kingma & Ba, 2014), are very popular optimizers. Our methods have the nature of quasiNewton methods while the memory footprint is largely reduced to be close to first-order methods.
Thus, ST-AM can be a competitive optimizer from both theoretical and practical perspectives.


-----

3 METHODOLOGY

In this section, we give the details of the proposed ST-AM. We always assume the objective function
as f : R[d] _→_ R, the fixed-point map g : R[d] _7→_ R[d]. Moreover, we do not distinguish rk = −∇f (xk)
and rk = g(xk) − _xk in our discussion as ∇f_ (x) = 0 is equivalent to g(x) = x −∇f (x) = x.

3.1 ANDERSON MIXING

The AM finds the fixed point of g via maintaining two sequences of length m (m ≤ _k):_

_Xk = [∆xk_ _m, ∆xk_ _m+1,_ _, ∆xk_ 1], Rk = [∆rk _m, ∆rk_ _m+1,_ _, ∆rk_ 1] R[d][×][m], (1)
_−_ _−_ _· · ·_ _−_ _−_ _−_ _· · ·_ _−_ _∈_

can be decoupled into two steps, namely thewhere the operator ∆ denotes the forward difference, e.g. projection step ∆ and thexk = x mixing stepk+1 − _xk. Each update of AM:_

_x¯k = xk −_ _XkΓk,_ (Projection step), _xk+1 = ¯xk + βkr¯k,_ (Mixing step), (2)

where ¯rk := rk − _RkΓk and βk > 0 is the mixing parameter. The Γk is determined by_

Γk = arg min (3)
Γ R[m][ ∥][r][k][ −] _[R][k][Γ][∥][2][.]_
_∈_

Thus, the full form of AM (Fang & Saad, 2009; Walker & Ni, 2011) is

_xk+1 = xk + βkrk −_ (Xk + βkRk) Γk. (4)

**Remark 1. To see the rationality of AM, assume g is continuously differentiable, then we have**
_h(xj)−h(xj−1) ≈_ _h[′](xk)(xj_ _−xj−1) around xk, where h[′](xk) is the Jacobian of h(x) := x−g(x)._
_Thus, we can recognize (3) as solvingSo, it is reasonable to assume Rk ≈−h h[′]([′]x(kx)kX)dkk, and we see = h(xk) in a least-squares sense, where ∥rk −_ _RkΓ∥2 ≈∥rk + h[′](xk)X dkΓk =∥2._
_XkΓk. The mixing step incorporates rk into the new update xk+1 if βk > 0. Otherwise, if βk = 0,_
_then xk+1 = ¯xk is an interpolation of the previous iterates, leading to a stagnation._

3.2 THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING

The basic ST-AM is to solve the strongly convex quadratic optimization:

min (5)
_x_ R[d][ f] [(][x][) := 1]2 _[x][T][Ax][ −]_ _[b][T][x,]_
_∈_


where A 0. Let p 1 = q 1 = p0 = q0 = 0 R[d]. At the k-th iteration, given the two matrices
_≻_ _−_ _−_ _∈_
_Pk_ 1 = (pk 2, pk 1) R[d][×][2], Qk 1 = (qk 2, qk 1) R[d][×][2] and defining p = xk _xk_ 1 and
_q =− rk −_ _rk−−1, the basic ST-AM constructs−_ _∈_ _−_ _−_ _−_ _∈_ _−_ _−_

_p˜ = p_ _Pk_ 1(Q[T]k 1[q][)][,] _q˜ = q_ _Qk_ 1(Q[T]k 1[q][)][,] (6a)
_−_ _−_ _−_ _−_ _−_ _−_

_pk = ˜p/∥q˜∥2,_ _qk = ˜q/∥q˜∥2._ (6b)

Then, we update Pk = (pk−1, pk), Qk = (qk−1, qk) ∈ R[d][×][2]. Such construction ensures Q[T]k _[Q][k][ =]_
_I2 for k ≥_ 2 and the storage of Pk and Qk is equal to AM(2). With the corrected Pk and Qk, the
ST-AM method modifies the projection step and the mixing step accordingly, that is,

_x¯k = xk −_ _PkΓk,_ (Projection step), _xk+1 = ¯xk + βkr¯k,_ (Mixing step), (7)

whereand Rk Γ in (1) byk = arg min Pk and ∥r Qk −k respectively and imposes the orthogonality condition onQkΓ∥2 = Q[T]k _[r][k][ and][ ¯]rk = rk −_ _QkΓk. Thus, the ST-AM replaces Qk. The details Xk_
of basic ST-AM are given in Algorithm 2 in Appendix C.1. Define _P[¯]k = (p1, p2, . . ., pk),_ _Q[¯]k =_
(q1, q2, . . ., qk), the Krylov subspace _m(A, v)_ span _v, Av, A[2]v, . . ., A[m][−][1]v_, the range of X
_K_ _≡_ _{_ _}_
as range(X). We give the properties of the basic ST-AM in Theorem 1.
**Theorem 1. Let {xk} be the sequence generated by the basic ST-AM. The following relations hold:**
_(i) ∥q˜∥2 > 0, range( P[¯]k) = range(Xk) = Kk(A, r0), range( Q[¯]k) = range(Rk) = AKk(A, r0);_
_(ii)_ _Q[¯]k = −AP[¯]k,_ _Q[¯][T]k_ _Q[¯]k = Ik;_
_(iii) ¯rk ⊥_ range( Q[¯]k) and ¯xk = x0 + zk, where zk = arg minz∈Kk(A,r0) ∥r0 − _Az∥2._
_If ∥r¯k∥2 = 0, then xk+1 is the exact solution._


-----

The proof is in Appendix C.1. Note that the property (iii) in Theorem 1 exactly describes the relation
_x¯k = x[G]k_ [, where][ x][G]k [is the output of the][ k][-th iteration of GMRES (Saad & Schultz, 1986). Moreover,]
let ¯x[AM]k be the k-th intermediate iterate in the full-memory AM. It holds that ¯x[AM]k = x[G]k [(See]
Proposition 1 in Appendix C.1.), which induces that ¯xk = ¯x[AM]k = x[G]k [. This equivalence indicates]
that ST-AM is more efficient than AM and GMRES since only two historical iterations need to
be stored. Moreover, by directly applying the convergence analysis of GMRES (Corollary 6.33 in
(Saad, 2003)), we obtain the convergence rate of the basic ST-AM for solving (5):
**Corollary 1. Suppose the eigenvalues of A lie in [µ, L] with µ > 0, and let {xk} be the se-**
_quence generated by the basic ST-AM, then thek_ _k-th intermediate residual ¯rk satisfies ∥r¯k∥2 ≤_

_√L/µ_ 1
2 _−_ _r0_ 2. Moreover, the algorithm finds the exact solution in at most (d + 1) iterations.

_√L/µ+1_ _∥_ _∥_

 

**Remark 2. The GMRES can be simplified to an elegant three-term recurrence algorithm called the**
conjugate residual (CR) method (Algorithm 6.20 in (Saad, 2003)) when solving (5). Thus, a similar
_simplification for AM is expected to exist. Like CG and Chebyshev acceleration (Algorithm 12.1 in_
_(Saad, 2003)), the convergence rate of ST-AM has the optimal dependence on the condition number,_
_while ST-AM does not form the Hessian-vector products explicitly._

3.3 THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING

For general nonlinear fixed-point problems, global convergence may be unavailable for the basic STAM, as a counter-example exists for AM (Mai & Johansson, 2020). Thus, we propose a modified
version of the basic ST-AM (MST-AM) and prove the local linear convergence rate under similar
conditions used in (Toth & Kelley, 2015; Evans et al., 2020). Concretely, the MST-AM makes three
main changes to the basic ST-AM.

**Change 1: Instead of applying the normalization (6b), the MST-AM constructs pk and qk via**

_ζk = (Q[T]k_ 1[Q][k][−][1][)][†][Q]k[T] 1[q,] _pk = p_ _Pk_ 1ζk, _qk = q_ _Qk_ 1ζk, (8)
_−_ _−_ _−_ _−_ _−_ _−_

(whereQ[T]k _[Q] “[k][)] †[†][Q] ” is the Moore-Penrose inverse. Accordingly, we choosek[T][r][k][. This change relaxes the orthonormality for][ Q][k][ (][k][ ≥] Γ[2]k[), but keeps the orthogonality] = arg min ∥rk −_ _QkΓ∥2 =_
condition: Q[T]k 1[q][k][ = 0][. In fact,][ ¯]Q[T]k 1[q][k][ = 0][ in the case of solving (5).]
_−_ _−_

**Change 2: MST-AM imposes the boundedness constraints on Pk** 1ζk and Qk 1ζk: If _Pk_ 1ζk 2 >
_−_ _−_ _∥_ _−_ _∥_
_cp_ _p_ 2 or _Qk_ 1ζk 2 > cq _q_ 2, then Pk = Pk 1, Qk = Qk 1, where cp > 0, cq (0, 1) are
predefined constants. It is worth mentioning that adding some boundedness condition is common in∥ _∥_ _∥_ _−_ _∥_ _∥_ _∥_ _−_ _−_ _∈_
the analysis of AM (Toth & Kelley, 2015; Evans et al., 2020).

**Change 3: MST-AM restarts, i.e. setting Pk = Qk = 0 ∈** R[d][×][2] every m iterations. This restart
operation is to limit the number of higher-order terms appeared in the residual expansion in our
analysis and we can set m to be a large number in practice.

The detailed description of MST-AM is given in Appendix C.2. In the next theorem, we establish
the convergence rate analysis for the MST-AM.
**Theorem 2. Let {xk} be the sequence generated by MST-AM, x[∗]** _∈_ R[d] _be a fixed point of g and m_
_be the restarting period for MST-AM. Suppose that in the ball B(ρ) := {x ∈_ R[d]|∥x _−_ _x[∗]∥2 < ρ} for_
_some ρ > 0, g is Lipschitz continuously differentiable and there are constants κ ∈_ (0, 1) and ˆκ > 0
_κfor everywith (i)0 ∈_ (0 ∥, 1) x, yg(. Ify) ∈B − x0g is sufficiently close to(x(ρ))∥, where2 ≤ _κ∥ gy −[′]_ _is the Jacobian ofx∥2 for every x[∗], then for x, y g ∈B r. Assumek :=(ρ g), and (ii)(x |k1) − −_ _xβ ∥kkg, the following bound holds:|[′] +(y κβ) −kg ≤[′](xκ)∥02 for a constant ≤_ _κˆ∥y −_ _x∥2_

_mk_


_O_ _∥rk−j∥2[2]_ _,_ (9)
_j=0_

X   


_∥rk+1∥2 ≤_ _θk(|1 −_ _βk| + κβk)∥rk∥2 + ˆκ_


_and the errorswhere θk = ∥r¯k∥2x/k∥rkx∥2[∗] ≤2_ 1 converge and mk = R k-linearly. mod m. Thus, the residuals {rk} converge Q-linearly,
_{∥_ _−_ _∥_ _}_
**Remark 3. In a local region around x[∗], the convergence rate is determined by the first-order term**
_θk(|1 −_ _βk| + κβk)∥rk∥2. We can choose βk = 1 such that |1 −_ _βk| + κβk = κ < 1. Since_
_r¯k is the orthogonal projection of rk onto the subspace range(Qk)[⊥], θk has the interpretation of_


-----

**Algorithm 1 RST-AM for stochastic programming**

**Input: x0** R[d], βk > 0, αk [0, 1], δk[(1)] _> 0, δk[(2)]_ _> 0._
_∈_ _∈_
**Output: x ∈** R[d]

1: P0, Q0 = 0 ∈ R[d][×][2], p0, q0 = 0 ∈ R[d]

2: for k = 0, 1, . . ., until convergence, do
3: _rk = −∇fSk_ (xk)

4: **if k > 0 then**

5: _p = xk −_ _xk−1, q = rk −_ _rk−1_

6: _ζk = (Q[T]k_ 1[Q][k][−][1][ +][ δ]k[(1)][P]k[ T] 1[P][k][−][1][)][†][Q][T]k 1[q]
_−_ _−_ _−_

7: _qk = q_ _Qk_ 1ζk, pk = p _Pk_ 1ζk
_−_ _−_ _−_ _−_

8: _Pk = [pk_ 1, pk], Qk = [qk 1, qk]
_−_ _−_

9: **end if**

10: Check Condition (13) and use smaller αk if (13) is violated

11: Γk = (Q[T]k _[Q][k][ +][ δ]k[(2)][P][ T]k_ _[P][k][)][†][Q]k[T][r][k]_

13:12: _xx¯kk =+1 = ¯ xk −xk +αk βPkkr¯Γkk, ¯rk = rk −_ _αkQkΓk_

14: Apply learning rate schedule of αk, βk

15: end for
16: return xk


_the direction-sine between rk and the subspace range(Qk). When θk is small, e.g., rk nearly lies in_
range(Qk), the acceleration by MST-AM is significant. Compared to AM(m), MST-AM incorporates
_historical information with orthogonalization. In the SPD linear case and without restart, the global_
_orthogonality property holds, i.e. ¯rk_ _range( Q[¯]k), which means there is no loss of historical_
_information, while AM(m) (Evans et al., 2020) does not have such property in this ideal case. ⊥_

3.4 THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING

Inspired by the recent work on stochastic Anderson mixing (SAM) method (Wei et al., 2021), we
develop a regularized ST-AM (RST-AM) for solving nonconvex stochastic optimization problems.

_T_

Consider the nonconvex optimization problem minx∈Rd f (x) := _T[1]_ _i=1_ _[f][ξ]i_ [(][x][)][, where][ f][ξ]i [:][ R][d][ →]

R is the loss function corresponding to i-th data sample and T is the number of data samples. In
mini-batch training, the gradient is evaluated for fSk (xk) := _n1k_ _i∈PSk_ _[f][ξ]i_ [(][x][k][)][, where][ S][k][ ⊆] [[][T] [] :=]

1, 2, . . ., T is the sampled mini-batch, and nk := _Sk_ is the batch size. In this case, we set
_{_ _}_ _|_ _|_ P
_rk = −∇fSk_ (xk) (Line 3 in Algorithm 1), which is an unbiased estimate of the negative gradient.

Recalling from (8), ζk = (Q[T]k−1[Q][k][−][1][)][†][Q][T]k−1[∆][r][k][−][1][ = arg min][ ∥][∆][r][k][−][1][ −] _[Q][k][−][1][ζ][∥][2][ as][ q][ = ∆][r][k][−][1]_
by definition. Since ST-AM is based on a local quadratic approximation (5) in a small region around
_xk, a large magnitude of_ _Pk_ 1ζk 2 tends to make the change from ∆xk 1 to pk = ∆xk 1
_Pk_ 1ζk too aggressively, which may lead to instability. Consequently, we add a penalty term in the ∥ _−_ _∥_ _−_ _−_ _−_
_−_
above least squares problem, i.e.

_ζk = arg min_ ∆rk 1 _Qk_ 1ζ 2 [+][ δ]k[(1)] 2[,] (10)
_∥_ _−_ _−_ _−_ _∥[2]_ _[∥][P][k][−][1][ζ][∥][2]_

where δk[(1)] _> 0. The same as SAM (Wei et al., 2021), we also add a regularization term for comput-_
ing Γk via
Γk = arg min ∥rk − _QkΓ∥2[2]_ [+][ δ]k[(2)][∥][P][k][Γ][∥]2[2][,] (11)

where δk[(2)] _> 0, and a damping term αk is used as shown in Line 12 in Algorithm 1. In practice, we_
choose the two regularization parameters as

2

_δk[(1)]_ = _∥∆xck1−∥r1k∥∥2[2]_ 2[2][+][ ϵ][0] _, δk[(2)]_ = max  c∥pk2∥∥2[2]rk[+]∥[ ϵ]2 [0] _, Cβk[−][2]_ _,_ (12)

where c1, c2, C > 0 are constants, and ϵ0 > 0 is a small constant to bound the denominators
away from zero. Assuming _pk_ 1 2 _pk_ 2 = ( ∆xk 1 2), the choices of (12) make
_∥_ _−_ _∥_ _≈∥_ _∥_ _O_ _∥_ _−_ _∥_
_δk[(1)][P]k[ T]_ 1[P][k][−][1][∥][2][ ≈O][(][∥][r][k][∥]2[2][)][ and][ ∥][δ]k[(2)][P]k[ T][P][k][∥][2][ ≈O][(][∥][r][k][∥]2[2][)][ aware of the change of the local]
_∥_ _−_
curvature: large (small) ∥rk∥2 tends to lead to a large (small) regularization.


-----

**Remark 4.** _One update of xk given by Line 11-13 in Algorithm 1 can be formulated as xk+1 =_
_xk + Hkrk, where Hk = βkI −_ _αkYkZk[†][Q]k[T][, Y][k][ =][ P][k][ +][ β][k][Q][k][, Z][k][ =][ Q]k[T][Q][k][ +][ δ]k[(2)][P]k[ T][P][k][. To]_
_guarantee the positive definiteness of Hk, we follow the same procedure in SAM. Let λk be the_
_largest eigenvalue of YkZk[†][Q]k[T]_ [+][ Q][k][Z]k[†][Y][ T]k _[. If][ α][k][ satisfies]_
_αkλk_ 2βk(1 _µ),_ (13)
_≤_ _−_

_thenobtained by computing the largest eigenvalue of a matrix of s[T]k_ _[H][k][s][k][ ≥]_ _[β][k][µ][∥][s][k][∥]2[2][,][ ∀][s][k]_ _[∈]_ [R][d][, where][ µ][ ∈] [(0][,][ 1)][ is a constant. Note that] R[4][×][4] _(see Appendix C.3.1).[ λ][k]_ _[can be cheaply]_

We summarize the RST-AM in Algorithm 1 and establish its convergence properties here. First, we
impose the same assumptions on the objective function f as those in (Wei et al., 2021).
**Assumption 1. f : R[d]** _→_ R is continuously differentiable. f (x) ≥ _f_ _[low]_ _> −∞_ _for any x ∈_ R[d].
_∇f is globally L-Lipschitz continuous; namely ∥∇f_ (x)−∇f (y)∥2 ≤ _L∥x−y∥2 for any x, y ∈_ R[d].
**Assumption 2. For any iteration k, the stochastic gradient ∇fξk** (xk) satisfies Eξk [∇fξk (xk)] =
_∇f_ (xk), Eξk [∥∇fξk (xk) −∇f (xk)∥2[2][]][ ≤] _[σ][2][,][ where][ σ >][ 0][, and][ ξ][k][, k][ = 0][,][ 1][, . . .][ are independent]_
_samples that are independent of {xj}j[k]=0[.]_

The diminishing condition about βk is


+∞

_βk = +∞,_
_k=0_

X


+∞

_βk[2]_ _[<][ +][∞][.]_ (14)
_k=0_

X


We give the convergence properties of RST-AM in nonconvex (stochastic) optimization and proofs
are deferred to Appendix C.3.2.
**Theorem 3. Suppose Assumption 1 hold and** _xk_ _is the sequence generated by full-batch RST-AM,_
_µ_ _{_ _}_
_i.e. nk = T_ _. Let βk = β ∈_ (0, 2L(1+C[−][1]) []][ be a constant,][ α][k][ ∈] [[0][,][ 1]][ and satisfies (13), then]

1 _N_ _−1_

_N_ _k=0_ _∥∇f_ (xk)∥2[2] _[≤]_ [2(][f] [(][x]Nµβ[0][)][ −] _[f][ low][)]_ _,_ (15)

X

_in the N iterations. To ensure_ _N[1]_ _kN=0−1_ 2 _[< ϵ][, the number of iterations is][ O][(1][/ϵ][)][ .]_

_[∥∇][f]_ [(][x][k][)][∥][2]

**Theorem 4. Suppose Assumptions 1 and 2 hold and** _xk_ _is the sequence generated by RST-AM_

_with batch size nk = n ≤_ _T_ _. If βPk ∈_ (0, 4L(1+µC[−][1]) []][ and satisfies (14),] { _}_ _[ α][k][ ∈]_ [[0][,][ min][{][1][, β]k12 _[}][]][ and]_

_satisfies (13), then_
lim inf _and_ _Mf > 0_ E[f (xk)] _Mf_ _,_ _k._ (16)
_k_ _∃_ _→_ _≤_ _∀_
_→∞_ _[∥∇][f]_ [(][x][k][)][∥][2][ = 0][ with probability][ 1]

_If Eξk_ [ _fξk_ (xk) 2[]][ ≤] _[M][g][,][ ∀][k,][ where][ M][g]_ _[>][ 0][ is a constant, we have]_
_∥∇_ _∥[2]_
lim (17)
_k→∞_ _[∥∇][f]_ [(][x][k][)][∥][2][ = 0][ with probability][ 1][.]

_RST-AM with fixed batch sizeTheorem 5. Suppose Assumptions 1 and 2 hold and nk = n. Let βk = min {xk4}Lk[N](1+=0[−]µ[1]C[is the first][−][1])_ _[,]_ _σ√D˜N[ N][ iterations generated by]D is a problem-_

1 _{_

_independent constant; αk_ [0, min 1, βk2 _[}][, where][ ˜]_

_ing PR(k) := Prob_ _R = ∈ k_ = 1/N{, then[}][]][ and satisfies (13). Let][ R][ be a random variable follow-]
_{_ _}_

_σ_ 4Df _D_

E[ _f_ (xR) 2[]][ ≤] [16][D][f] _[L][(1 +][ C]_ _[−][1][)]_ + + [4(][L][ +][ µ][−][1][)(1 +][ C] _[−][1][) ˜]_ _,_ (18)
_∥∇_ _∥[2]_ _Nµ[2]_ _µ√N_ _D˜_ _n_ !

_where Df := f_ (x0) − _f_ _[low]_ _and the expectation is taken with respect to R and {Sj}j[N]=0[−][1][. To ensure]_
E[∥∇f (xR)∥2[2][]][ ≤] _[ϵ][, the number of iterations is][ O][(1][/ϵ][2][)][.]_

**Remark 5. The proofs of Theorem 4 and 5 are based on the analysis of SAM (Wei et al., 2021).**
_The theorems show that the convergence of RST-AM is no worse than SGD (Robbins & Monro,_
_1951). There are two key differences between RST-AM and SAM: RST-AM is based on short-term_
_recurrences while SAM usually maintains longer historical sequences to ensure effectiveness; RST-_
_AM uses additional correction and regularization terms (Line 5-8 in Algorithm 1) to incorporate_
_historical information while SAM simply discards the oldest iteration to make space for ∆xk−1_
_and ∆rk−1. The reduced memory requirement in RST-AM makes it applicable for solving more_
_challenging problems in machine learning._


-----

4 EXPERIMENTS

We validated the effectiveness of our proposed ST-AM methods in various applications in fixed-point
iterations and nonconvex optimization, including linear and nonlinear problems, deterministic and
stochastic optimization. Specifically, we first tested ST-AM in linear problems, cubic-regularized
quadratic minimization (Carmon & Duchi, 2020) and a multiscale deep equilibrium (MDEQ) model
(Bai et al., 2020). Then we applied RST-AM to train neural networks and compared them with
several first-order and second-order optimizers. Experimental details are in Appendix D.


4.1 EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM

We verified the properties of ST-AM declared in Theorem 1 and 2 by solving four problems (details
are in Appendix D.1): (I) strongly convex quadratic optimization (corresponding to Theorem 1);
(II) solving a nonsymmetric linear system Ax = b (corresponding to ˆκ = 0 in Theorem 2); (III)

_κ >cubic-regularized quadratic minimizationˆ_ 0 in Theorem 2); (IV) root-finding problems in MDEQ on CIFAR-10 (Krizhevsky et al., 2009). minx∈Rd f (x) := ∥Ax _−_ _b∥2[2]_ [+][ M]3 _[∥][x][∥]2[3]_ [(corresponding to]

The compared methods were gradient descent (GD), fixed-point iteration (FP), conjugate residual
method (CR) (Saad, 2003), BFGS (Nocedal & Wright, 2006), Broyden’s method (Broyden, 1965),
and the full-memory AM (AM). We used the basic ST-AM to solve Problem I and II, and MST-AM
(cp = cq = 1) to solve Problem III and IV.


10 2

10 5

10 8

10 11 GD: ||rk||2/||r0||2

CR: ||rk||2/||r0||2

10 14 AM: ||ST-AM: ||rk||2r/||k||r20/||||2r0||2

0 10 20 30 40 50

iteration


10 1

||/||||||rr202k 101010 1047 FP

CR

10 13 AM

ST-AM

0 10 20 30 40 50

iteration


10 2

||/||||||rr202k 101010 1158 GD

BFGS

10 14 AMMST-AM

0 10 20 30 40 50

iteration


(a) Problem I

0.8 Broyden

2 AM
||z MST-AM
/||)||z(f2 0.60.4

0.2

Forward: ||

0.0

2 4 6 8 10 12 14 16

step


(d) Forward process


(b) Problem II

1.4 Broyden

||z/||)||z(f22 1.21.00.8 AMMST-AM

0.6

0.4

0.2

Backward: ||

0.0

2 4 6 8 10 12 14 16 18

step


(e) Backward process


(c) Problem III (M = 0.1)

85

86

8075 8584 85.3184.9084.52 1.2

70 8325 30 35 40 45 50 1.0

est Accuracy %T 6560 BroydenAMMST-AM 0.8est LossT

0.6

55

0 10 20 30 40 50

epoch


(f) Test accuracy and loss


Figure 1: (a) ∥rk∥2/∥r0∥2 of GD and CR, and ∥r¯k∥2/∥r0∥2 of AM and ST-AM for solving Problem I; (b) ∥rk∥2/∥r0∥2 for solving Problem II; (c) ∥rk∥2/∥r0∥2 for solving Problem III (M = 0.1);
(d)(e) relative residuals of the forward and backward root-finding processes in MDEQ, and shaded
areas correspond to the standard deviations; (f) test accuracy and loss in MDEQ/CIFAR-10.

The numerical results shown in Figure 1 demonstrate the power of ST-AM as a variant of Krylov
subspace methods. It significantly accelerates the slow convergence of the GD or FP method, and
can outperform AM. Figure 1(a) clearly verifies the correctness of Theorem 1: within the machine
precision, the intermediate residual ¯rk of ST-AM coincides with the residual rk of CR. Note that
AM fails to coincide with CR and ST-AM due to the intrinsic numerical weakness to solve (3), as
also pointed out in (Walker & Ni, 2011). Figure 1(b) shows that ST-AM can outperform CR, though
both methods enjoy short-term recurrences and are equivalent for solving SPD linear systems. Figure 1(c) also shows MST-AM surpasses BFGS in solving cubic-regularized problems. The tests in
MDEQ/CIFAR-10 indicate that MST-AM is comparable to the full-memory methods in the forward
root-finding process and converges faster in the backward process. The accuracy is also comparable.


-----

10[0]

10 2

10 4

10 6

10 8


10[2]


10[0]

10 2

10 4

10 6


10

10

10

10 10

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
|SGD Adam|||||
|SAM(2) SAM(5) SAM(10)|||||

|Col1|Col2|
|---|---|
|||
|||
|||
|)||

|Col1|Col2|Col3|
|---|---|---|
||||
|grad prop grad_SAM(2)|||
|prop_SAM(2) grad_RST-AM prop_RST-AM|||


0 50 100 150 200

SGD
Adam
SAM(2)
SAM(5)
SAM(10)
RST-AM

epoch

(a) Train loss (w/o preconditioning)


0 50 100 150 200

SGD
Adam
SAM(2)
SAM(5)
SAM(10)
RST-AM

epoch

(b) SNG (w/o preconditioning)


0 50 100 150 200

Adagrad
RMSprop
Adagrad_SAM(2)
RMSprop_SAM(2)
Adagrad_RST-AM
RMSprop_RST-AM
RST-AM

epoch

(c) Train loss (w/ preconditioning)


Figure 2: Experiments on MNIST. (a)(b) Training loss and the squared norm of gradient (SNG) (w/o
preconditioning for SAM, RST-AM); (c) Training loss (w/ preconditioning for SAM, RST-AM).

4.2 EXPERIMENTS ABOUT RST-AM


We applied RST-AM to train neural networks, with full-batch training on MNIST (LeCun et al.,
1998), and mini-batch training on CIFAR-10/CIFAR-100 and Penn Treebank (Marcus et al., 1993).

**Experiments on MNIST. We trained a convolutional neural network (CNN) on MNIST to see the**
convergence behaviour of RST-AM in nonconvex optimization (cf. Theorem 3), for which we were
only concerned about the training loss. Figure 2(a)(b) show that the short-memory SAMs (m = 2, 5)
hardly show any improvement over the first-order optimizers SGD and Adam, while RST-AM can
close the gap of the long-memory (m = 10) and the short-memory methods. We also considered
the effect of preconditioning on RST-AM (see Appendix A.3). The notation “A B” means B method
preconditioned by A method. Figure 2(c) indicates that preconditioning also works much better for
RST-AM than SAM(2), and RMSprop RST-AM can outperform the non-preconditioned RST-AM.

Table 1: Experiments on CIFAR10/CIFAR100. “-” means failing to complete the test in our device
due to memory limit. “*” indicates numbers published in (Wei et al., 2021).


(a) Final TOP1 test accuracy (mean ± standard deviation) (%) for training 160 epochs.

Test accuracy on CIFAR10 Test accuracy on CIFAR100
Method

ResNet18 ResNet20 ResNet32 ResNet44 ResNet56 WRN16-4 ResNet18 ResNeXt DenseNet

AdamAdaBoundAdaBeliefLookaheadAdaHessianSAM(2)SAM(10)SGDMRST-AM[∗][∗] _[∗][∗][∗][∗]_ 93.0394.6594.9294.3695.1794.8295.2794.2595.07±±±±±±±±±.07.31.13.33.09.04.10.15.04 91.1790.7791.1592.0791.9292.1492.0392.4392.39±±±±±±±±±.16.13.08.21.04.32.33.19.11 92.0391.7392.1592.8692.1893.0492.8693.2293.24±±±±±±±±±.28.06.17.15.18.23.32.15.36 93.1092.2892.0092.7993.2692.7493.4693.5793.52±±±±±±±±±.62.18.24.24.11.09.14.23.02 92.3992.4493.3093.3692.4093.6693.7793.4793.69±±±±±±±±±.23.04.07.13.06.06.12.28.18 92.4593.5094.4694.9094.0495.0795.2394.9095.21±±±±±±±±±.11.12.13.15.12.16.07.09.09 72.4175.0776.2577.6376.5977.5178.1377.2777.91±±±±±±±±±.09.17.14.06.35.42.24.14.22 79.3178.4173.5775.7478.2778.9379.0279.53-±±±±±±±±.54.17.20.16.12.21.27.34 78.4970.8076.0678.8379.3780.0080.0980.36-±±±±±±±±.12.23.13.15.16.23.52.25


(b) The memory and computation cost compared with SGDM. The notations “m”,“t/e” and “t” are abbreviations of memory, per-epoch time and total running time, respectively.

Cost CIFAR10/ResNet18 CIFAR10/WRN16-4 CIFAR100/ResNeXt50 CIFAR100/DenseNet121
(× SGDM) m t/e t m t/e t m t/e t m t/e t

SGDM[∗] 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
SAM(10)[∗] 1.73 1.78 1.00 1.26 1.28 0.80 1.30 1.16 0.58 1.16 1.19 0.60
RST-AM 1.05 1.46 0.82 1.03 1.14 0.71 1.04 1.07 0.54 1.01 1.11 0.55


**Experiments on CIFAR. We trained ResNet18/20/32/44/56 (He et al., 2016), WideResNet16-4**
(Zagoruyko & Komodakis, 2016) (abbr. WRN16-4) on CIFAR-10, and ResNet18, ResNeXt50 (Xie
et al., 2017), DenseNet121 (Huang et al., 2017) on CIFAR-100. The baseline optimizers were
SGDM, Adam, AdaBound (Luo et al., 2018), AdaBelief (Zhuang et al., 2020), Lookahead (Zhang
et al., 2019), AdaHessian (Yao et al., 2021) and SAM. Here, some results of the baselines in (Wei
et al., 2021) were used for reference since the experimental settings were the same. Table 1(a) shows
RST-AM improves SAM(2) and has comparable test accuracy to SAM(10). RST-AM also outperforms other baseline optimizers. Table 1(b) reports the memory and computation cost, where we


-----

used SGDM as the baseline and other optimizers were terminated when achieving a comparable or
better test accuracy than SGDM. It indicates that RST-AM introduces ≤ 5% extra memory overhead compared with SGDM, and significantly reduces the memory footprint of AM. Since RST-AM
needs fewer training epochs, the total running time is less than SGDM.


120

115

110

105

100

95

90

85

80


100

95

90

85

80

75

70

65

60


100

95

90

85

80

75

70

65


|Col1|Col2|Col3|Col4|SGDM|
|---|---|---|---|---|
|||||Adam|
|||||AdaBelief SAM(2)|
|||||SAM(10) RST-AM|
||||||
||||||
||||||
||||||


100 200 300 400 500

epoch

(a) 1-Layer LSTM


|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
|||||SGDM Adam|
|||||AdaBelief SAM(2)|
|||||SAM(10)|
|||||RST-AM|
||||||
||||||
||||||
||||||
||||||


100 200 300 400 500

epoch

(b) 2-Layer LSTM


|Col1|Col2|Col3|Col4|SGDM|
|---|---|---|---|---|
|||||Adam|
|||||AdaBelief SAM(2)|
|||||SAM(10) RST-AM|
||||||
||||||
||||||
||||||


100 200 300 400 500

epoch

(c) 3-Layer LSTM


Figure 3: Validation perplexity of training 1,2,3-layer LSTM on Penn Treebank.


**Experiments on Penn Treebank. We trained**
LSTMs with 1-3 layer(s) on Penn Treebank and
report the validation perplexity in Figure 3 and
test perplexity in Table 2 (lower is better). The
results suggest that RST-AM is comparable to
or even better than SAM(10). The improvement
of RST-AM over other optimizers is also significant. We report the computation and memory cost in Appendix D.2.4. RST-AM can still
surpass Adam while using much fewer epochs,
thus reducing the total running time.


Table 2: Test perplexity of training 1,2,3-layer
LSTM on Penn Treebank. Lower is better.

Method 1-Layer 2-Layer 3-Layer


SGDM 83.48±.03 65.89±.18 61.88±.23
Adam 80.33±.15 64.32±.06 59.72±.13
AdaBelief 81.29±.35 64.68±.10 60.46±.07
SAM(2) 80.79±.19 65.52±.29 61.13±.12
SAM(10) 78.78±.14 62.46±.11 58.93±.09
RST-AM **78.41±.18** **62.46±.08** **58.31±.23**

Table 4: FID score for SN-GAN.

Method Adam AdaBelief RST-AM

Best FIDFinal FID 13.3413.07±±.18.14 12.8013.59±±.09.21 **12.0512.50±±.15.29**


Table 3: Test accuracy (%) for adversarial training.

CIFAR10/ResNet18 CIFAR100/DenseNet121
Optimizer
Clean FGSM PGD-20 C&W∞ Clean FGSM PGD-20 C&W∞

SGD 82.16 63.23 51.91 50.22 59.45 39.76 30.92 29.00
RST-AM **82.53** **63.78** **52.43** **50.52** **60.48** **40.41** **31.20** **29.52**


**Adversarial training. We applied RST-AM to adversarial training (Madry et al., 2018) as the outer-**
optimizer and compared it with SGD by the clean test accuracy and robust test accuracy. The results
on CIFAR10/ResNet18 and CIFAR100/DenseNet121 are reported in Table 3. It can be seen that
RST-AM can achieve both higher clean test accuracy and higher robust test accuracy. More results
can be found in Appendix D.2.5.

**Generative adversarial network (GAN). We tested RST-AM by training a GAN equipped with**
spectral normalization (SN-GAN) (Miyato et al., 2018), where the generator and discriminator networks were ResNets and the dataset was CIFAR-10. Table 4 shows that RST-AM can achieve lower
FID score (better accuracy) than Adam and AdaBelief.

5 CONCLUSION


In this paper, to address the memory issue of Anderson mixing (AM), we develop a novel class
of short-term recurrence AM methods (ST-AM) and test it in various applications, including solving linear and nonlinear problems, deterministic and stochastic optimization. We give a complete
theoretical analysis of the proposed methods. We prove that the basic ST-AM is equivalent to the
full-memory AM in strongly convex quadratic optimization. With some minor changes, it has local
linear convergence for solving general fixed-point problems under some common assumptions. We
also introduce the regularized form of ST-AM and analyze its convergence properties. The numerical
results show that the ST-AM methods are comparable to or even better than the long-memory AM
while consuming less memory. The regularized ST-AM also outperforms many existing optimizers
in training neural networks in various tasks.


-----

ACKNOWLEDGMENTS

This work was supported by the National Key R&D Program of China (No. 2021YFA1001300),
National Natural Science Foundation of China (No.61925601), Tsinghua University Initiative Scientific Research Program, National Natural Science Foundation of China (No.11901338), and Huawei
Noah’s Ark Lab. We thank all anonymous reviewers for their valuable comments and suggestions
on this work.

REFERENCES

Hengbin An, Xiaowei Jia, and Homer F Walker. Anderson acceleration and application to the threetemperature energy equations. Journal of Computational Physics, 347:1–19, 2017.

Donald G Anderson. Iterative procedures for nonlinear integral equations. Journal of the ACM
_(JACM), 12(4):547–560, 1965._

Donald G Anderson. Comments on “Anderson acceleration, mixing and extrapolation”. Numerical
_Algorithms, 80(1):135–234, 2019._

Marcin Andrychowicz, Misha Denil, Sergio G´omez Colmenarejo, Matthew W Hoffman, David
Pfau, Tom Schaul, Brendan Shillingford, and Nando de Freitas. Learning to learn by gradient
descent by gradient descent. In Proceedings of the 30th International Conference on Neural
_Information Processing Systems, pp. 3988–3996, 2016._

Akash Arora, David C Morse, Frank S Bates, and Kevin D Dorfman. Accelerating self-consistent
field theory of block polymers in a variable unit cell. The Journal of Chemical Physics, 146(24):
244902, 2017.

Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. Advances in Neural
_Information Processing Systems, 32:690–701, 2019._

Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Multiscale deep equilibrium models. In Advances
_in Neural Information Processing Systems (NeurIPS), 2020._

Albert S Berahas, Frank E Curtis, and Baoyu Zhou. Limited-memory BFGS with displacement
aggregation. Mathematical Programming, pp. 1–37, 2021.

Lon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223–311, 2018.

Claude Brezinski and Michela Redivo-Zaglia. Shanks function transformations in a vector space.
_Applied Numerical Mathematics, 116:57–63, 2017._

Claude Brezinski, Michela Redivo-Zaglia, and Yousef Saad. Shanks sequence transformations and
Anderson acceleration. SIAM Review, 60(3):646–669, 2018.

Charles G Broyden. A class of methods for solving nonlinear simultaneous equations. Mathematics
_of Computation, 19(92):577–593, 1965._

Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-Newton
method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008–1031, 2016.

Stan Cabay and LW Jackson. A polynomial extrapolation method for finding limits and antilimits
of vector sequences. SIAM Journal on Numerical Analysis, 13(5):734–752, 1976.

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
_IEEE Symposium on Security and Privacy (SP), pp. 39–57. IEEE, 2017._

Yair Carmon and John C Duchi. First-order methods for nonconvex quadratic minimization. SIAM
_Review, 62(2):395–436, 2020._

Mauro Cettolo, Jan Niehues, Sebastian St¨uker, Luisa Bentivogli, and Marcello Federico. Report
on the 11th IWSLT evaluation campaign, IWSLT 2014. In Proceedings of the International
_Workshop on Spoken Language Translation, Hanoi, Vietnam, volume 57, 2014._


-----

Hans De Sterck and Yunhui He. On the asymptotic linear convergence speed of Anderson acceleration, Nesterov acceleration, and nonlinear GMRES. SIAM Journal on Scientific Computing, (0):
S21–S46, 2021.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 248–255. IEEE, 2009.

Yunbin Deng. Deep learning on mobile devices: A review. In Mobile Multimedia/Image Processing,
_Security, and Applications 2019, volume 10993, pp. 109930A. International Society for Optics_
and Photonics, 2019.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(7), 2011.

Iain S Duff, Albert Maurice Erisman, and John Ker Reid. Direct methods for sparse matrices.
Oxford University Press, 2017.

Rick Durrett. Probability: Theory and examples, volume 49. Cambridge University Press, 2019.

R P Eddy. Extrapolating to the limit of a vector sequence. In Peter C.C. Wang, Arthur L. Schoenstadt,
Bert I. Russak, and Craig Comstock (eds.), Information Linkage Between Applied Mathematics
_and Industry, pp. 387–396. Academic Press, 1979._

Claire Evans, Sara Pollock, Leo G Rebholz, and Mengying Xiao. A proof that Anderson acceleration improves the convergence rate in linearly converging fixed-point methods (but not in those
converging quadratically). SIAM Journal on Numerical Analysis, 58(1):788–810, 2020.

Haw-ren Fang and Yousef Saad. Two classes of multisecant methods for nonlinear acceleration.
_Numerical Linear Algebra with Applications, 16(3):197–221, 2009._

Anqi Fu, Junzi Zhang, and Stephen Boyd. Anderson accelerated Douglas–Rachford splitting. SIAM
_Journal on Scientific Computing, 42(6):A3560–A3583, 2020._

Alejandro J Garza and Gustavo E Scuseria. Comparison of self-consistent field convergence acceleration techniques. The Journal of Chemical Physics, 137(5):054110, 2012.

Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013.

Gene H Golub and Charles F Van Loan. Matrix computations, 4th. Johns Hopkins, 2013.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.

William W Hager and Hongchao Zhang. A survey of nonlinear conjugate gradient methods. Pacific
_Journal of Optimization, 2(1):35–58, 2006._

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770–778, 2016.

Magnus R. Hestenes and Eduard Stiefel. Methods of conjugate gradients for solving linear systems.
_Journal of Research of the National Bureau of Standards, 49:409–435, 1952._

Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
_Neural Information Processing Systems, 30, 2017._

Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
_Recognition, pp. 4700–4708, 2017._

Carl T Kelley. Numerical methods for nonlinear equations. Acta Numerica, 27:207–287, 2018.


-----

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
_arXiv:1412.6980, 2014._

Tamara G Kolda, Dianne P O’leary, and Larry Nazareth. BFGS with update skipping and varying
memory. SIAM Journal on Optimization, 8(4):1060–1083, 1998.

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Lin Lin, Jianfeng Lu, and Lexing Ying. Numerical methods for Kohn–Sham density functional
theory. Acta Numerica, 28:405–539, 2019.

Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization.
_Mathematical Programming, 45(1):503–528, 1989._

Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. In International Conference on
_Learning Representations, 2019._

Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. In International Conference on Learning Representations, 2018.

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
_Learning Representations, 2018._

Vien Mai and Mikael Johansson. Anderson acceleration of proximal gradient methods. In Interna_tional Conference on Machine Learning, pp. 6620–6629. PMLR, 2020._

Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of English: The Penn Treebank. 1993.

Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018.

Aryan Mokhtari, Mark Eisen, and Alejandro Ribeiro. IQN: An incremental quasi-Newton method
with local superlinear convergence rate. SIAM Journal on Optimization, 28(2):1670–1698, 2018.

Ion Necoara, Yu Nesterov, and Francois Glineur. Linear convergence of first order methods for
non-strongly convex optimization. Mathematical Programming, 175(1):69–107, 2019.

Arkadij Semenoviˇc Nemirovski and David Borisovich Yudin. Problem complexity and method
efficiency in optimization. 1983.

Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media,
2006.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association
_for Computational Linguistics, pp. 311–318, 2002._

Florian A Potra and Hans Engler. A characterization of the behavior of the Anderson acceleration
on linear problems. Linear Algebra and Its Applications, 438(3):1002–1011, 2013.

Ning Qian. On the momentum term in gradient descent learning algorithms. Neural Networks, 12
(1):145–151, 1999.

Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In
_International Conference on Machine Learning, pp. 8093–8104. PMLR, 2020._


-----

Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat_ical Statistics, pp. 400–407, 1951._

Youcef Saad and Martin H Schultz. GMRES: A generalized minimal residual algorithm for solving
nonsymmetric linear systems. SIAM Journal on Scientific and Statistical Computing, 7(3):856–
869, 1986.

Yousef Saad. Iterative methods for sparse linear systems. SIAM, 2003.

Damien Scieur, Alexandre dAspremont, and Francis Bach. Regularized nonlinear acceleration.
_Mathematical Programming, 179(1):47–83, 2020._

Daniel Shanks. Non-linear transformations of divergent and slowly convergent sequences. Journal
_of Mathematics and Physics, 34(1-4):1–42, 1955._

Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International Conference on Machine Learning, pp.
1139–1147, 2013.

Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–
31, 2012.

Alex Toth and CT Kelley. Convergence analysis for Anderson acceleration. _SIAM Journal on_
_Numerical Analysis, 53(2):805–819, 2015._

Henk A Van der Vorst and C Vuik. The superlinear convergence behaviour of GMRES. Journal of
_Computational and Applied Mathematics, 48(3):327–341, 1993._

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor_mation Processing Systems, pp. 5998–6008, 2017._

Homer F Walker and Peng Ni. Anderson acceleration for fixed-point iterations. SIAM Journal on
_Numerical Analysis, 49(4):1715–1735, 2011._

Xiao Wang, Shiqian Ma, Donald Goldfarb, and Wei Liu. Stochastic quasi-Newton methods for
nonconvex stochastic optimization. SIAM Journal on Optimization, 27(2):927–956, 2017.

Fuchao Wei, Chenglong Bao, and Yang Liu. Stochastic Anderson mixing for nonconvex stochastic
optimization. Advances in Neural Information Processing Systems, 34, 2021.

Peter Wynn. On a device for computing the em(Sn) transformation. Mathematical Tables and Other
_Aids to Computation, pp. 91–96, 1956._

Peter Wynn. Acceleration techniques for iterated vector and matrix problems. Mathematics of
_Computation, 16(79):301–322, 1962._

Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision
_and Pattern Recognition, pp. 1492–1500, 2017._

Yunan Yang. Anderson acceleration for seismic inversion. Geophysics, 86(1):R99–R108, 2021.

Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael Mahoney.
AdaHessian: An adaptive second order optimizer for machine learning. In Proceedings of the
_AAAI Conference on Artificial Intelligence, volume 35, pp. 10665–10673, 2021._

Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision
_Conference 2016. British Machine Vision Association, 2016._

Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps
forward, 1 step back. In Advances in Neural Information Processing Systems, pp. 9597–9608,
2019.

Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan. AdaBelief optimizer: Adapting stepsizes by the belief in observed
gradients. Advances in Neural Information Processing Systems, 33, 2020.


-----

A ADDITIONAL PRELIMINARIES

We provide some additional preliminaries in this section for readers that are not familiar with Anderson mixing, fixed-point iterations and some techniques mentioned in the main paper.

A.1 FIXED-POINT ITERATION

The fixed-point problem and the optimization problem are the main application scenarios of our
methods. It is worth pointing out that there are some minor differences between these two problems that make algorithm designs different. The key difference is that for a fixed-point problem,
the Jacobian (if exists) is generally not symmetric while for optimization, the Hessian is naturally
symmetric. In principle, a fixed-point solver can also be applicable for an optimization problem
since the first-order necessary condition of minx∈Rd f (x), where f : R[d] _→_ R, is ∇f (x) = 0.

_yConsider a contraction mapping∥2, ∀x, y ∈_ R[d]. According to the contraction mapping theorem, a unique fixed point g : R[d] _7→_ R[d], i.e. for some κ < 1, ∥g(x) − _g(y)∥2 ≤ x[∗]κexists∥x −_
for g and the iterates generated by the iteration xk+1 = g(xk) converge to x[∗], starting from any
_xthe update is0 ∈_ R[d]. In practice, the damped fixed-point iteration is also commonly used: given the xk+1 = (1 − _βk)xk + βkg(xk) = xk + βkrk, where rk := g(xk) −_ _xk is called the βk ∈_ (0, 2),
_residual._

The fixed-point iteration, also known as Picard iteration in some areas, can converge very slowly in
practice. Anderson mixing is a method to improve the convergence.

A.2 ANOTHER FORM OF ANDERSON MIXING

The derivation of Anderson mixing (AM) in Section 3.1 explicitly interprets AM as a two-step
procedure. In the literature, there is another equivalent form of AM.

Let the projection coefficients Γk = (γk[(1)][, . . ., γ]k[(][m][)])[T] _∈_ R[m]. Define the auxiliary coefficients
_{θk[(][j][)][}]j[m]=0_ [as][ θ]k[(0)] = γk[(1)][, θ]k[(][j][)] = ∆γk[(][j][)][(][j][ = 1][, . . ., m][ −] [1)][, θ]k[(][m][)] = 1 − _γk[(][m][)], then_ _j=0_ _[θ]k[(][j][)]_ =

1 and ¯rk = _j=0_ _[θ]k[(][j][)][r][k][−][m][+][j][. Hence the least squares problem (3) can be reformulated as a]_

[P][m]

constrained problemas a linear combination of the historical residuals[P][m] min{θk(j)[}]j[m]=0 _[∥]_ [P]j[m]=0 _[θ]k[(][j][)][r][k][−]r[m]j_ [+]j[j]=[∥]k[2][ s.t.]m[,][ P][ ¯]rk is minimal in terms of thej[m]=0 _[θ]k[(][j][)]_ = 1, which indicates that L2-norm.

_{_ _}[m]_ _−_

Also, the projection step and the mixing step (2) can be reformulated as ¯xk = _j=0_ _[θ]k[(][j][)][x][k][−][m][+][j]_
and xk+1 = (1 _βk)_ _j=0_ _[θ]k[(][j][)][x][k][−][m][+][j][ +][β][k]_ _mj=0_ _[θ]k[(][j][)][g][(][x][k][−][m][+][j][)][, respectively. Such formulation]_
_−_
is also adopted in the literature (Toth & Kelley, 2015; Mai & Johansson, 2020; Scieur et al., 2020).[P][m]

Let Hk be the solution to the constrained optimization problem (Fang & Saad, 2009)[P][m] P

min (19)
_Hk_

_[∥][H][k][ −]_ _[β][k][I][∥][F][ subject to][ H][k][R][k][ =][ −][X][k][,]_

then the update (4) is xk+1 = xk +Hkrk. It suggests that AM is a multisecant quasi-Newton method
(Fang & Saad, 2009).

A.3 PRECONDITIONED ANDERSON MIXING

Like preconditioning for Krylov subspace methods (Saad, 2003), preconditioning can also be incorporated into Anderson mixing to mitigate the ill-conditioning of the original problem (Wei et al.,
2021). The idea is to replace the mixing step in (2) via a preconditioned mixing.

Suppose that there is a basic solver preconditioner(xk, sk), which works as a black-box procedure
that updates xk given the residual sk, i.e. xk+1 = preconditioner(xk, sk), then the preconditioned
mixing of RST-AM is
_xk+1 = preconditioner(¯xk, ¯rk)._
which substitutes for the Line 13 in Algorithm 1. The simple mixing (Line 13 in Algorithm 1
can be seen as a special case by defining preconditioner(xk, sk) = xk + βksk, i.e. preconditioned by a damped fixed-point iteration. Moreover, if we write the preconditioning operation as


-----

_preconditioner(xk, sk) := xk + Gksk, where Gk is the matrix to approximate the inverse Jaco-_
bian, then the preconditioned AM is

_xk+1 = xk + Gkrk_ (Xk + GkRk)(Rk[T][R][k][)][†][R]k[T][r][k] (20)
_−_

(cf. the definitions offorms a low-rank updated approximation to the inverse Jacobian: it solves Xk, Rk in Section 3.1). The matrix Hk = Gk − (Xk + GkRk)(Rk[T][R][k][)][†][R]k[T]

min
_Hk_

_[∥][H][k][ −]_ _[G][k][∥][F][ subject to][ H][k][R][k][ =][ −][X][k][,]_

which is a direct extension of (19).

For the stochastic Anderson mixing, using damped projection can be helpful, i.e. ¯xk = (1−αk)xk +
AM with damped projection isαk(xk −XkΓk) = xk −αkXkΓk and ¯rk = rk −αkRkΓk correspondingly. Then the preconditioned

_xk+1 = xk + Gkrk −_ _αk(Xk + GkRk)Γk._ (21)

B ADDITIONAL DISCUSSION

B.1 THE MEMORY AND COMPUTATIONAL EFFICIENCY

Since the ST-AM methods only need to store two previous iterations, the memory and computational
cost can be reduced to be close to first-order methods.

**Memory cost. Assume that the iteration number is k and the model parameter size is d. The full-**
memory AM stores all previous iterations, thus the additional memory is 2kd. To reduce the memory
overhead, the limited-memory AM(m) only maintains the most recent m iterations while discarding
the older historical information (cf. (1)). Hence the additional memory of AM(m) is 2md. Choosing
a suitable m can be problem-dependent (Walker & Ni, 2011), and it is often suggested that m ≥ 5
(Mai & Johansson, 2020; Fu et al., 2020) to avoid too much historical information being discarded.
There is no equivalence between AM(m) and the full-memory AM. On the other side, our ST-AM
methods have the same memory footprint as that of AM(2), i.e. only introducing 4d additional
memory. For a large model that contains millions or even billions of parameters, such reduction in
memory requirement can be significant.

**Computational cost. For ST-AM, besides the gradient evaluations, the main additional computa-**
tional cost comes from vector corrections, the projection and mixing step. These operations are
very cheap: matrix multiplications of R[2][×][d] _× R[d][×][2], R[2][×][d]_ _× R[d][×][1], and R[d][×][2]_ _× R[2][×][1]; the pseudo-_
inverse can be exactly solved as the size of the matrix is R[2][×][2]. Also, the Q[T]k−1[Q][k][−][1][ and][ P][ T]k−1[P][k][−][1]
in Line 6 in Algorithm 1 can reuse the results in the previous iteration (cf. Line 11). So the total
additional computational cost of RST-AM is O(d).

B.2 APPLICABILITY

In the main paper, we develop a class of short-term recurrence Anderson mixing. Among them, the
basic ST-AM can serve as a new linear solver for linear systems; MST-AM can be used as a new
fixed-point solver for nonlinear equations; RST-AM is a new method for optimization. These methods are based on Anderson mixing and have close relationship between each other. The theoretical
analysis and numerical results show the great potential of ST-AM methods for various applications.
Here, we give some comparisons between the ST-AM methods and some other classical methods.

**ST-AM versus current solvers for linear systems.** ST-AM is an iterative solver. It is equivalent
to the full-memory AM and GMRES in strongly convex quadratic optimization (or SPD linear systems), and can also have linear convergence rate for general nonsymmetric linear systems. Since it is
based on short-term recurrences, like the CG method, the memory and per-iteration cost of ST-AM
is economical. On the other side, the LU factorization based methods (Golub & Van Loan, 2013)
are direct solvers that can incur overwhelming memory and computation cost for large sparse linear
systems. Although sparse direct solvers (Duff et al., 2017) can alleviate overhead, they are often
more complicated to implement and difficult for parallel computing. Moreover, iterative solvers can
benefit from the preconditioning technique that improves convergence (Saad, 2003). Also, ST-AM


-----

has advantages over other iterative methods since it does not need to directly access the matrix and
only the residual is required. In the case that explicitly accessing the matrix is difficult, this property
is appealing. Hence, unlike nonlinear CG that relies on line search, it is direct to extend ST-AM
to unconstrained optimization where the gradient is commonly available while the Hessian can be
too costly to obtain. Moreover, ST-AM is very flexible and any iterative solver can be viewed as a
black-box iterative process to be accelerated by ST-AM.

**MST-AM versus other quasi-Newton methods for nonlinear equations.** MST-AM has the nature of quasi-Newton methods since it is built upon AM that is recognized as a multisecant quasiNewton method (Fang & Saad, 2009). In scientific computing, AM has been successfully applied to
solve many difficult nonlinear problems arising from many areas (An et al., 2017; Lin et al., 2019;
Fu et al., 2020; Yang, 2021). Since AM only manipulates residuals and does not use line-search
or trust-region technique, it is efficient to apply AM to accelerate a slowly convergent black-box
iterative process. A comprehensive discussion about the applicability of AM for nonlinear problems
and the relation between AM and Broyden’s methods can be found in (Fang & Saad, 2009).

One of the biggest issues of AM and other quasi-Newton methods is the additional memory overhead, because they need to store historical iterations to form the secant equations. To make a compromise, limited-memory quasi-Newton methods such as L-BFGS (Liu & Nocedal, 1989) are proposed in which only limited number of historical iterations are stored. In each update, the oldest
iteration is discarded to make space for the up-to-date secant pair. As a result, the limited-memory
quasi-Newton methods can lose the local superlinear convergence properties achieved by the fullmemory schemes (Berahas et al., 2021). Also, as stated by Berahas et al. (2021), the choice of the
number of historical iterations (i.e. historical length) is problem-dependent, and one does not know
_a priori the best choice when solving a particular problem._

Unlike the limited-memory quasi-Newton methods whose performance can be sensitive to the historical length, our MST-AM method only needs to store two corrected historical iterations. MSTAM carefully incorporates historical information through orthogonalization. In the ideal case, i.e.
strongly convex quadratic optimization (or SPD linear systems), it is equivalent to the full-memory
AM which means there is no loss of historical information. Our experiments verify the theoretical
properties of MST-AM and indicate MST-AM can be a competitive method for nonlinear equations.

**RST-AM versus first-order methods for stochastic optimization.** For stochastic optimization
such as training deep neural networks in machine learning, the first-order methods have come to
dominate the field due to the low memory and per-iteration cost. The RST-AM is an extension of
ST-AM and MST-AM to tackle this challenging problem. RST-AM has theoretical guarantees in
deterministic/stochastic optimization and also inherits several advantages of AM and ST-AM:

_• Fast convergence in quadratic optimization. It is important to possess such property for an_
optimizer since a smooth function can be approximated by a quadratic function in the local
region around the optima and many techniques such as trust region (Nocedal & Wright,
2006) rely on this local approximation. Adaptive learning rate methods such as AdaGrad
(Duchi et al., 2011), Adam (Kingma & Ba, 2014) use a diagonal approximation of the
_Fisher information matrix that is an approximation of the Hessian. However, in the sim-_
ple quadratic case, these methods can only roughly match the performance of the Jacobi
method for solving linear systems (Saad, 2003), which is better than gradient descent but
far inferior to the powerful Krylov subspace methods. The momentum method mimics
CG method by incorporating a historical iteration into the search direction. However, the
choice of momentum and stepsize can be an art (Sutskever et al., 2013). For RST-AM,
there is no need to determine the stepsize and the fast convergence rate of ST-AM can be
recovered by simply setting αk = 1 and δk[(1)] = δk[(2)] = 0. For more difficult functions, the
damping and regularization in RST-AM can be enabled to improve stability.

_• Theoretical guarantee in stochastic optimization. If only first-order information can be ac-_
cessed, the SGD (Ghadimi & Lan, 2013) achieves an optimal convergence rate O(1/ϵ[2]) to
obtain an ϵ-accurate solution (Nemirovski & Yudin, 1983). In such case, it seems to be a big
mismatch for current second-order methods, because there is no theoretical improvement
albeit with more memory and computation resource. Such mismatch may also account for
the popularity of first-order methods. Since RST-AM has very limited additional memory


-----

overhead, and also achieves the O(1/ϵ[2]) complexity, it can be applied to many applications
that are dominated by first-order methods.

_• Flexibility in use. The application of RST-AM can be very flexible. In principle, RST-AM_
can be applied to improve any slowly convergent black-box iterative process by viewing
the latter as a fixed-point iteration. For example, consider accelerating a solver of a commercial software, where we have no access to the underlying codes to provide our custom
implementation, or some case rewriting the codes is too cumbersome. Moreover, RSTAM can efficiently incorporate the preconditioning technique. Hence any optimizer, even
an optimizer built upon neural networks (Andrychowicz et al., 2016), can be used as a
preconditioner for RST-AM. Preconditioning largely enhances the applicability of RSTAM for various applications. For example, RST-AM can be preconditioned by Adam for
the language task and SGDM for image classification. So any fine-tuned first-order optimizer can be combined with RST-AM to achieve an overall improvement. As RST-AM is
light-weight, this additional cost is marginal and can be largely counteracted by the actual
improvement. So in some sense, the purpose of RST-AM is not to totally replace current
off-the-shelf optimizers but to achieve collaborative effectiveness: RST-AM is aware of the
second-order information while the first-order method can mitigate the ill-conditioning of
the problem.

Overall, the ST-AM methods have wide applicability and can be competent methods from both
theoretical and practical perspectives.

C PROOFS

We give more details about ST-AM and the proofs of the theorems in the main paper.

C.1 THE BASIC ST-AM FOR STRONGLY CONVEX QUADRATIC OPTIMIZATION

Recall that the strongly convex quadratic optimization is formulated as

min (22)
_x_ R[d][ f] [(][x][) := 1]2 _[x][T][Ax][ −]_ _[b][T][x,]_
_∈_

where A ∈ R[d][×][d] is SPD, b ∈ R[d]. Solving (22) is equivalent to solving the SPD linear system

_Ax = b._ (23)

The detail of the basic ST-AM is given in Algorithm 2.

We first state the relationship of AM with GMRES in the following proposition. Similar results can
also be found in (Walker & Ni, 2011; Wei et al., 2021).

Let x[G]k _[, r]k[G]_ [:=][ b][ −] _[Ax]k[G]_ [denote the][ k][-th GMRES iterate and residual, respectively, and][ K][k][(][A, v][) :=]
span{v, Av, . . ., A[k][−][1]v} denotes the k-th Krylov subspace generated by A and v. Define e[j] :=
(1, 1, . . ., 1)[T] _∈_ R[j] for j ≥ 1. Let range(X) denote the linear space spanned by the columns
of X. The main results of the full-memory AM are stated in Proposition 1 and Proposition 2.
The Proposition 1 is the same as the Proposition 2 in (Wei et al., 2021), and we restate it here for
completeness. The Proposition 2 is new as far as we know.

**Proposition 1 (General linear system). For solving a general linear system Ax = b with the full-**
_memory AM (m = k), suppose that βk > 0 and the fixed-point map is g(x) = (I −_ _A)x + b. If_
_the initial point of AM is x0 = x[G]0_ _[and][ rank(][R][k][) =][ m][, then the intermediate iterate][ ¯]xk satisfies_
_x¯k = x[G]k_ _[.]_

_Proof. The definition of the fixed-point map suggests that the residual rk = g(xk)_ _−_ _xk = b_ _−_ _Axk._

Since Rk = −AXk and A is nonsingular, we have rank(Xk) = m. We first show

range(Xk) = Kk(A, r0[G][)] (24)

by induction. We abbreviate Kk(A, r0[G][)][ as][ K][k][ in this proof.]


-----

**Algorithm 2 ST-AM for strongly convex quadratic optimization**

**InputOutput: x: x0 ∈ ∈RR[d][d], βk > 0, 0 < max iter ≤** _d._

1: P0, Q0 = 0 ∈ R[d][×][2], p0, q0 = 0 ∈ R[d]

2: for k = 0, 1, . . ., max iter do
3: _rk = −∇f_ (xk)

4: **if k > 0 then**

6:5: _qp˜ = = q xk −Qxkk−11(, qQ[T]k =1 r[q]k[)] −[,][ ˜]p =rk p−1_ _Pk_ 1(Q[T]k 1[q][)] (Compute(q˜ _Qk_ ∆1) _xk−1, ∆rk−1)_
_−_ _−_ _−_ _−_ _−_ _−_ _⊥_ _−_

7: _pk = ˜p/∥q˜∥2, qk = ˜q/∥q˜∥2_ (∥qk∥2 = 1)

8: _Pk = [pk−1, pk], Qk = [qk−1, qk]_ (Q[T]k _[Q][k][ =][ I][2][)]_

9: **end if**

10: Γk = Q[T]k _[r][k]_

11:12: _xx¯kk =+1 = ¯ xk −xk +Pk βΓkkr¯,k ¯rk = rk −_ _QkΓk_ (Mixing step)(Projection step: ¯rk ⊥ _Qk)_

13: **if ∥r¯k∥2 = 0 then**

14: break

15: **end if**

16: end for
17: return xk


First, ∆x0 = β0r0 = β0r0[G] [since][ x][1][ =][ x][0][ +][ β][0][r][0][. If][ k][ = 1][, then the proof is complete. Then,]
suppose that k > 1 and, as an inductive hypothesis, that range(Xk−1) = Kk−1. With (4) we have

∆xk 1 = xk _xk_ 1
_−_ _−_ _−_
= βk 1rk 1 (Xk 1 + βk 1Rk 1)Γk 1
_−_ _−_ _−_ _−_ _−_ _−_ _−_
= βk−1(b − _Axk−1) −_ (Xk−1 − _βk−1AXk−1)Γk−1_
= βk 1b _βk_ 1A(x0 + ∆x0 + + ∆xk 2) (Xk 1 _βk_ 1AXk 1)Γk 1
_−_ _−_ _−_ _· · ·_ _−_ _−_ _−_ _−_ _−_ _−_ _−_
= βk−1r0 − _βk−1AXk−1e[k][−][1]_ _−_ (Xk−1 − _βk−1AXk−1)Γk−1._ (25)

Since r0 _k_ 1, and by the inductive hypothesis range(Xk 1) _k_ 1 which also implies
rank(range(XAXk ∈K) =k− m1) ⊆K =− _k which impliesk, we know ∆x dim(range(k−1 ∈Kk, which impliesXk)) = dim(_ range(k−), we have ⊆KXk) ⊆K range(− _k. Since we assumeXk) =_ _k, thus_
_K_ _K_
completing the induction. As a result, we also have

range(Rk) = range(AXk) = AKk(A, r0[G][)][.] (26)


Recalling that to determine Γk, we solve the least squares problem (3) and Rk = −AXk. We have

Γk = arg min (27)
Γ R[m][ ∥][r][k][ +][ AX][k][Γ][∥][2][.]
_∈_

Since rank(AXk) = rank(Xk) = m, (27) has a unique solution. Also, since rk = b − _Axk =_
_b −_ _A(x0 + Xke[k]) = r0 −_ _AXke[k], we have rk + AXkΓ = r0 −_ _AXke[k]_ + AXkΓ = r0 − _AXkΓ[˜],_
where Γ =[˜] _e[k]_ _−_ Γ. So Γk solves (27) if and only if Γ[˜]k = e[k] _−_ Γk solves

min Γ 2, (28)
Γ˜ R[m][ ∥][r][0][ −] _[AX][k]_ [˜]∥
_∈_

According to (24), (28) is equal to minz _m(A,r0G[)][ ∥][r][0][ −]_ _[Az][∥][2][ which is the GMRES minimization]_
_∈K_
problem. Since the solution of (28) is also unique, we have

_x¯k = xk −_ _XkΓk = xk −_ _Xk(e[k]_ _−_ Γ[˜]k) = x0 + XkΓ[˜]k = x[G]k _[.]_

In Proposition 1, the assumption that Rk has full column rank is critical to ensure no stagnation
occurs in AM for solving a general linear system. In fact, for SPD linear systems (23) or strongly
convex quadratic optimization (22), when AM breaks down, i.e. Rk is rank deficient, AM obtains
the exact solution, as shown in the next proposition.


-----

**Proposition 2 (SPD). For applying the full-memory AM to minimize a strongly convex quadratic**
_problem (22), or equivalently, solve a SPD linear system (23), suppose that βk > 0 and the fixed-_
_point map is g(x) = (I_ _A)x + b. If rank(Rk) = k holds for 1_ _k < s while failing to hold for_
_−_ _≤_
_k = s, where s_ 1, then the residual of AM satisfies rs = ¯rs 1 = 0.
_≥_ _−_

_Proof. The definition of g suggests that the residual rk = g(xk)−xk = b−Axk. The relation Rk =_
_−AXk holds during the iterations and the nonsingularity of A implies rank(Xk) = rank(Rk)._

For s = 1, since the first step of AM is x1 = x0 + β0r0, the assumption rank(R1) = 0 implies that
rank(r0) = rank(X1) = 0, i.e. r1 = ¯r0 := 0.

For s > 1, because ∆xs 1 = xs _xs_ 1 = _Xs_ 1Γs 1 + βs 1r¯s 1, the rank deficiency of
_Xs implies ∆xs_ 1 range(− _Xs_ 1) −, which further implies− _−_ _−_ _− ¯rs_ 1 _−range(−_ _Xs_ 1). So there exists
_ζ_ R[s][−][1], such that− _∈ ¯rs_ 1 = Xs _−1ζ. Note that according to (3),−_ ¯ ∈rs 1 _Rs_ 1− = _AXs_ 1, so we
have ∈ _−_ _−_ _−_ _⊥_ _−_ _−_ _−_

0 = ¯rs[T] 1[AX][s][−][1] [= (][X][s][−][1][ζ][)][T][AX][s][−][1] [=][ ζ] [T][X]s[T] 1[AX][s][−][1][.] (29)
_−_ _−_

Because rank(Xs 1) = s 1 and A is SPD, we know Xs[T] 1[AX][s][−][1] [is also SPD. So][ ζ][ = 0][, which]
_−_ _−_ _−_
implies ¯rs 1 = 0. Hence xs = ¯xs 1 and rs = ¯rs 1 = 0.
_−_ _−_ _−_

Now we give the proof of Theorem 1.

**_Proof of Theorem 1. Besides relations (i)-(iii), we add an auxiliary relation here:_**
(iv)We prove the relations (i)-(iv) by induction. rk = r0 + Q[¯]kΓ[¯]k ∈Kk+1(A, r0), where Γ[¯]k ∈ R[k].

For k = 1, since ¯r0 = 0, according to Proposition 2, rank(∆x0) = rank(X1) = 1, rank(∆r0) =
rank(R1) = 1, so ˜q ̸= 0, which implies Line 7 in Algorithm 2 is well-defined. The relation (i) holds.
_̸_
Since ˜q = q = ∆x0, ˜p = p = ∆r0, and ∆r0 = −A∆x0, the equality _Q[¯]1 = −AP[¯]1 also holds. Due_
the last equality is due toto the normalization in Line 7,rit is clear that¯1 ⊥ range(Q r11) = range( = r0 _−Q[¯]1 spanQΓ[¯][¯]11 ∈K). Also,Q[¯]{[T]1ArQ2[¯](01A, r} = 1 ¯r = range(1 =0). Since, namely relation (iv). Due to the projection step Line 11, r1 −_ _Q rQ111Γ =) = range( 1 = r0 r −0 −β0βAr0Q[¯]Ar01 and)0. For − range( Q r1Γ1[G]1 =[=]Q[¯][ r] r1) = range(0[0] −[ −]Q[¯][Az]1η[1]1[, where], whereAr0),_
_z1 = arg minz∈K1(A,r0) ∥r0 −_ _Az∥2, it holds r1[G]_ _[⊥]_ _[A][K][1][(][A, r][0][) = range( ¯]Q1). As a result, both ¯r1_
and r1[G] [are the orthogonal projections of][ r][0][ onto the subspace][ range( ¯]Q1)[⊥], which implies ¯r1 = r1[G][.]
So ¯x1 = x[G]1 [=][ x][0][ +][ z][1][ because their residuals are equal and][ A][ is nonsingular. Hence relation (iii)]
holds.

Suppose that k > 1, and as an inductive hypothesis, the relations (i)-(iv) hold for j = 1, . . ., k − 1.
Consider the k-th iteration. From Line 6 in Algorithm 2, ˜q ∈ range(∆rk−1, Qk−1), and ˜p ∈
range(∆xk−1, Pk−1). We first prove that ˜q ̸= 0 by contradiction.

∆If ˜xq = 0k 1, then from Line 6 in Algorithm 2,range(Pk 1) range( P[¯]k 1) as ∆ ∆rkr−k 1 ∈1 =range(A∆Qxkk−11) and ⊆ range( Q[¯]k 1 =Q[¯]k−1A), which impliesP[¯]k 1 and A is
nonsingular. From Line 11 and Line 12, we have− _∈_ _−_ _⊆_ _−_ _−_ _−_ _−_ _−_ _−_ _−_

∆xk 1 = xk _xk_ 1 = _Pk_ 1Γk 1 + βk 1r¯k 1. (30)
_−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_

_ζSo ¯rkR−[k]1[−] ∈[1], such thatrange(P ¯kr−k_ 1)1 ⊆ = P[¯]range( k 1ζ. From the inductive hypothesis, we knowP[¯]k−1) since ∆xk−1 ∈ range(Pk−1). Hence there exists ¯rk 1 _Qk_ 1 =
_− ∈AP[¯]k−1, so we have_ _−_ _−_ _−_ _⊥_ [¯] _−_

0 = ¯rk[T] 1[A][ ¯]Pk 1 = ( P[¯]k 1ζ)[T]AP[¯]k 1 = ζ [T][ ¯]Pk[T] 1[A][ ¯]Pk 1.
_−_ _−_ _−_ _−_ _−_ _−_

Since _Q[¯][T]k−1Q[¯]k−1 = Ik−1, we know rank( Q[¯]k−1) = k −_ 1, which implies rank( P[¯]k−1) = k − 1 due
to _Q[¯]k_ 1 = _AP[¯]k_ 1. Hence _P[¯]k[T]_ 1[A][ ¯]Pk 1 is also SPD. Then ζ = 0 which implies ¯rk 1 = 0. It is
_−_ _−_ _−_ _−_ _−_ _−_
impossible otherwise Algorithm 2 has terminated in the (k − 1)-th iteration. So ˜q ̸= 0 and Line 7 is
well-defined.

Since ¯rk 1 = rk 1 _Qk_ 1Γk 1, and rk 1 _k(A, r0), range(Qk_ 1) range( Q[¯]k 1) =
_AKk−1(A, r−_ 0) as the inductive hypothesis, we have− _−_ _−_ _−_ _−_ _∈K ¯rk−1 ∈Kk(A, r0), which together with (30)−_ _⊆_ _−_


-----

and range(Pk 1) range( P[¯]k 1) = _k_ 1(A, r0) infers ∆xk 1 _k(A, r0). Hence ∆rk_ 1 =
_A∆xk_ 1 _−A_ _⊆k(A, r0). As a result,−_ _K_ _q−k = ˜q/_ _q˜_ 2 range(∆− _∈Krk_ 1, Qk 1) _A_ _k(A, r−_ 0).
So− range( − _Q ∈[¯]k) = range( K_ _Q[¯]k_ 1, qk) _A_ _k(A, r0∥). Moreover,∥_ _∈_ _qk /−_ range( − _Q[¯] ⊆k_ 1), otherwiseK
_−_ _⊆_ _K_ _∈_ _−_
_q˜_ range( Q[¯]k 1) that implies ∆rk 1 = q range( Q[¯]k 1), which is impossible following the
_∈_ _−_ _−_ _∈_ _−_
former proof of ˜q = 0. So we have range( Q[¯]k) = A _k(A, r0)._
_̸_ _K_

Because ∆rk 1 = _A∆xk_ 1 and Qk 1 = _APk_ 1 due to _Q[¯]k_ 1 = _AP[¯]k_ 1, Line 6 in Algo_−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_
rithm 2 infers ˜q = −Ap˜, which implies qk = −Apk. So _Q[¯]k = −AP[¯]k. Since A is nonsingular and_
range( Q[¯]k) = A _k(A, r0), we have range( P[¯]k) =_ _k(A, r0)._
_K_ _K_

As range(Xk) = _k(A, r0) and range(Rk) = A_ _k(A, r0) has been proved in Proposition 1, the_
_K_ _K_
relation (i) holds for the k-th iteration.

To prove _Q[¯][T]k_ _Q[¯]k = Ik, it suffices to show qk ⊥_ _Q[¯]k−1, as the equalities_ _Q[¯][T]k−1Q[¯]k−1 = Ik−1 and_
_qk_ 2 = 1 has already held. It is equivalent to prove ˜q _Qk_ 1. From the construction of ˜q in
_∥_ _∥_ _⊥_ [¯] _−_
Line 6 in Algorithm 2, we know Q[T]k 1q[˜] = 0, so ˜q span(qk 2, qk 1) (for k = 2, ˜q _q0 = 0_
_−_ _⊥_ _−_ _−_ _⊥_
clearly holds). To further prove ˜q ⊥ range( Q[¯]k−3)(k ≥ 4), note that

∆rk 1 = _A∆xk_ 1 = APk 1Γk 1 _βk_ 1Ar¯k 1 = _Qk_ 1Γk 1 _βk_ 1Ar¯k 1,
_−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_

where the second equality is a direct substitution with (30). Therefore,

_Q¯[T]k−3[∆][r][k][−][1]_ [=][ −]Q[¯][T]k−3[Q][k][−][1][Γ][k][−][1] _[−]_ _[β][k][−][1]Q[¯][T]k−3[A]r[¯]k−1 = 0 −_ _βk−1(AQ[¯]k−3)[T]r¯k−1 = 0,_ (31)

where the second equality is due to range(Qk−1) = span(qk−2, qk−1) ⊥ range( Q[¯]k−3) and A is
_ASPD, the third equality is due to[2]Kk−3(A, r0) ⊆_ _AKk−1(A, r0) ¯. As a result, noting thatrk−1 ⊥_ range( Q[¯]k−1) = q A = ∆Kkr−k1−(1A, r, we obtain0) and range(AQ[¯]k−3) =

_Q¯[T]k_ 3q[˜] = Q[¯][T]k 3[q][ −] _Q[¯][T]k_ 3[Q][k][−][1][(][Q]k[T] 1[q][) = 0][,]
_−_ _−_ _−_ _−_

which is due to (31) and range(Qk−1) = span{qk−2, qk−1} ⊥ range( Q[¯]k−3). Therefore, we show
that _Q[¯][T]k_ _Q[¯] = Ik, which along with_ _Q[¯]k = −AP[¯]k proves relation (ii) in the k-th iteration._

Next, we prove the relation (iv). We have

_rk = ¯rk_ 1 _βk_ 1Ar¯k 1
_−_ _−_ _−_ _−_
= rk−1 − _Qk−1Γk−1 −_ _βk−1A(rk−1 −_ _Qk−1Γk−1)_
= rk 1 _Qk_ 1Γk 1 _βk_ 1Ark 1 + βk 1AQk 1Γk 1
_−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_
= r0 + Q[¯]k 1Γ[¯]k 1 _Qk_ 1Γk 1 _βk_ 1(Ar0 + AQ[¯]k 1Γ[¯]k 1) + βk 1AQk 1Γk 1,
_−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_

where the last equality is due to rk 1 = r0 + Q[¯]k 1Γ[¯]k 1 by the inductive hypothesis. Since
_−_ _−_ _−_
range( Q[¯]k−1) = AKk−1(A, r0) ⊆ _AKk(A, r0), range(Qk−1) ⊆_ range( Q[¯]k), span{Ar0} ⊆
_AKk(A, r0), range(AQk−1)_ _⊆_ range(AQ[¯]k−1) _⊆_ _A[2]Kk−1(A, r0)_ _⊆_ _AKk(A, r0), and_
The relation (iv) is proved.range( Q[¯]k) = AKk(A, r0), it is clear that rk = r0 + Q[¯]kΓ[¯]k ∈Kk+1(A, r0) for some Γ[¯]k ∈ R[k].

range(Finally, we prove the relation (iii). For provingQk) already holds due to the projection step (Line 10 and Line 11 in Algorithm 2). It suffices ¯rk ⊥ range( Q[¯]k), note that ¯rk ⊥ span{qk−1, qk} =
to prove ¯rk range( Q[¯]k 2). In fact, since we have ¯rk = rk _QkΓk, we can prove that rk_
range( Q[¯]k− ⊥2) and QkΓk ⊥− range( Q[¯]k−2): _−_ _⊥_

Since range(Qk) = span{qk−1, qk} ⊥ range( Q[¯]k−2) as induced from _Q[¯][T]k_ _Q[¯]k = Ik, it is clear_
that QkΓk range( Q[¯]k 2). For rk, according to Line 12 in Algorithm 2, we have rk = ¯rk 1
_Qβ¯k[T]k−12A[A]r¯r[¯]kk− ⊥11. We have = (AQ[¯]k_ ¯r2−k)[T]−r¯1k ⊥1 = 0range( due toQ[¯]k− range(1) ⊇ range( AQ[¯]k _Q2[¯]) =k−2 A) by the inductive hypothesis. Also,[2]_ _k_ 2(A, r0) _A_ _k_ 1(A, r−0) = −
_−_ _−_ _−_ _−_ _−_ _K_ _−_ _⊆_ _K_ _−_
range( Q[¯]k−1).

range( Therefore, we obtainQ[¯]k). ¯rk ⊥ range( Q[¯]k−2), which along with ¯rk ⊥ span{qk−1, qk} implies ¯rk ⊥

To prove ¯xk = x[G]k [:=][ x][0][ +][ z][k][, where][ z][k][ = arg min]z _k(A,r0)_
_∈K_ _[∥][r][0][ −]_ _[Az][∥][2][, first we have][ r][k][ =]_
_rηOn the other side, for GMRES,0k + ∈Q[¯]Rk[k]Γ[¯]. Sincek, where ¯rkΓ[¯] ⊥k ∈Q[¯]Rk[k], ¯. Hencerk is the orthogonal projection of rk[G]_ ¯[=]rk[ r] =[0] r[ −]k −[Az]Q[k][ ⊥]kΓk[A] =[K] r[k]0[(] + [A, r]Q r[¯][0]k[) = range( ¯]0Γ[¯] onto the subspacek − _QkΓkQ =k) r, so0 − r range( Qk[G][¯]k[is also the]ηk, whereQ[¯]k)[⊥]._


-----

orthogonal projection of r0 onto the subspace range( Q[¯]k)[⊥]. So ¯rk = rk[G][, which further indicates]
_x¯k = x[G]k_ [. Hence, the relation (iii) holds.]

With relations (i)-(iv) being proved in the k-th iteration, we complete the induction.

C.2 MODIFIED ST-AM FOR GENERAL FIXED-POINT ITERATIONS

Algorithm 2 is suitable for analysis and implementation in the linear case. For general nonlinear
fixed-point iterations, we adopt an alternative form as described in Algorithm 3 which discards the
normalization of ˜q in each iteration (Line 7 in Algorithm 2). In Line 7 in Algorithm 3, the orthogonal
projection of ∆rk 1 is checked to ensure ∆rk 1 is “less linearly dependent” on range(Qk), which
_−_ _−_
ensures _qk_ 2 is bounded away from zero; the check of _Pk_ 1ζk 2 ensures that _pk_ ∆xk 1 2
_cp_ ∆xk ∥ 1 _∥2, which is also important since a large deviation from ∥_ _−_ _∥_ ∆xk 1 can make ∥ _−_ _pk_ _−2 > ρ∥_ _≤_
_∥_ _−_ _∥_ _−_ _∥_ _∥_
(ρ is the radius introduced in Theorem 2). When this condition cannot be satisfied, the algorithm
simply reuses the old Pk−1, Qk−1. The main procedure of MST-AM restarts every m iterations, i.e.
_Pk, Qk = 0. Such restart mechanism is to restrict the higher-order terms in the residual expansion,_
as shown in (9). Also, restart can flush out the outdated historical information that may weaken the
quality of Pk and Qk that are used to pursue a local first-order approximation of g in MST-AM.

**Algorithm 3 MST-AM for nonlinear fixed-point problems**

**Input: x0 ∈** R[d], βk ∈ (0, 1], cp > 0, cq ∈ (0, 1), m > 0.
**Output: x ∈** R[d]

1: P0, Q0 = 0 ∈ R[d][×][2], p0, q0 = 0 ∈ R[d]

2: for k = 0, 1, . . ., until convergence do
3: _rk = g(xk) −_ _xk_

4: **if k mod m ̸= 0 then**

5:6: _pζk = = ( xkQ −[T]k_ _x1k[Q]−[k]1[−], q[1] =[)][†][Q] r[T]kk −1[q]rk−1_

_−_ _−_

7: **if** _Pk_ 1ζk 2 _cp_ _p_ 2 and _Qk_ 1ζk 2 _cq_ _q_ 2 then

8: _∥pk =− p_ _∥_ _P ≤k_ 1ζ∥k, q∥k = q ∥ _Q−k_ 1ζ∥k _≤_ _∥_ _∥_
_−_ _−_ _−_ _−_

9: _Pk = [pk_ 1, pk], Qk = [qk 1, qk] (qk _Qk_ 1)

10: **else** _−_ _−_ _⊥_ _−_

11: _Pk = Pk_ 1, Qk = Qk 1
_−_ _−_

12: **end if**

13: **else**

14: _Pk, Qk = 0 ∈_ R[d][×][2], pk, qk = 0 ∈ R[d]

15: **end if**

16: Γk = (Q[T]k _[Q][k][)][†][Q]k[T][r][k]_

17:18: _xx¯kk =+1 = ¯ xk −xk +Pk βΓkkr¯,k ¯rk = rk −_ _QkΓk_ (Mixing step)(Projection step: ¯rk ⊥ _Qk)_

19: end for
20: return xk
(The notation “†” is the Moore-Penrose pseudoinverse.)


In the linear case, Algorithm 2 and Algorithm 3 (with m = ∞) are equivalent. Similar to Algorithm 2, we have the following properties held for MST-AM:

**Claim 1. In the k-th iteration (k > 0) of Algorithm 1 applied to minimize a strongly convex**
_quadratic problem (5), assuming cp = ∞, cq = 1, m = ∞, the following relations hold:_
_(i) ∥qk∥2 > 0, range( P[¯]k) = range(Xk) = Kk(A, r0), range( Q[¯]k) = range(Rk) = AKk(A, r0);_
_(ii)_ _Q[¯]k = −AP[¯]k, qi ⊥_ _qj(1 ≤_ _i ̸= j ≤_ _k);_
_(iii) ¯rk ⊥_ range( Q[¯]k) and ¯xk = x0 + zk, where zk = arg minz∈Kk(A,r0) ∥r0 − _Az∥2._
_If ∥r¯k∥2 = 0, then xk+1 is the exact solution._

The proof of Claim 1 is essentially the same as the proof of Theorem 1, with a special care that
_Q¯[T]k_ _Q[¯]k = Ik is replaced by the relation that columns of_ _Q[¯]k are orthogonal to each other, in other_
words, _Q[¯][T]k_ _Q[¯]k = diag{∥q1∥2[2][, . . .,][ ∥][q][k][∥][2]2[}][.]_


-----

For minimizing nonlinear functions or accelerating nonlinear fixed-point iterations, the long-term
relation that _Q[¯][T]k_ _Q[¯]k = diag{∥q1∥2[2][, . . .,][ ∥][q][k][∥][2]2[}][ generally cannot hold, while the orthogonalization]_
procedure in Line 6 still leads to a short-term orthogonality relation: qi ⊥ _qj for |i −_ _j| ≤_ 2.
Hence, Q[T]k _[Q][k][ = diag][{∥][q][k][−][1][∥]2[2][,][ ∥][q][k][∥][2]2[}][ for][ k][ ≥]_ [1][.] Thus the pseudoinverse (Q[T]k _[Q][k][)][†][ =]_
diag ( _qk_ 1 2 = 0)1/ _qk_ 1 2[,][ I][(][∥][q][k][∥][2] 2[}][, where][ I][(][·][)][ is the indicator function]
that _{I_ _∥_ _−_ _∥_ _̸_ _∥_ _−_ _∥[2]_ _[̸][= 0)1][/][∥][q][k][∥][2]_

1 _x is true,_
(x) =
_I_ 0 _x is false._


Now, we give the proof of Theorem 2.

**_Proof of Theorem 2. For convenience, we restate the main assumptions of g here:_**

(i) _g(y)_ _g(x)_ 2 _κ_ _y_ _x_ 2, κ (0, 1), for _x, y_ (ρ), (32a)
_∥_ _−_ _∥_ _≤_ _∥_ _−_ _∥_ _∈_ _∀_ _∈B_

(ii) _g[′](y)_ _g[′](x)_ 2 _κˆ_ _y_ _x_ 2, ˆκ > 0, for _x, y_ (ρ). (32b)
_∥_ _−_ _∥_ _≤_ _∥_ _−_ _∥_ _∀_ _∈B_

Also, since |1 _−_ _βk|_ + _κβk < 1, we know βk > 0 is bounded, i.e. βk ≤_ _β where β > 0 is a constant._

The proof is based on the two lemmas given in Lemma 1 and Lemma 2. Besides (9), we also prove
that ∥rk∥2 ≤∥r0∥2 by induction.

_xFor[∗]∥ k2 + = 0 β(1 +, ¯x0 κ =)∥ xx00 − ∈Bx[∗](∥ρ2) ≤, and due to (48b),ρ provided ∥x0 − ∥xx[∗]1 −∥2 ≤x[∗]ρ/∥2 ≤∥(1 + βx0(1 + −_ _x κ[∗]))∥2. Since + β0∥r0∥2 ≤∥x0 −_

_r1 = g(x1) −_ _x1 = g(x1) −_ (x0 + β0r0) = g(x1) − (x0 + r0) + (1 − _β0)r0_
= g(x1) _g(x0) + (1_ _β0)r0,_
_−_ _−_

it follows that

_∥r1∥2 ≤∥g(x1) −_ _g(x0)∥2 + |1 −_ _β0|∥r0∥2_
_≤_ _κβ0∥r0∥2 + |1 −_ _β0|∥r0∥2 = (κβ0 + |1 −_ _β0|)∥r0∥2._

Also note that θ0 = ∥r¯0∥2/∥r0∥2 = 1. Thus (9) holds. Because κβ0 + |1 − _β0| ≤_ _κ0 < 1, we have_
_∥r1∥2 < ∥r0∥2._

Now, suppose that (9) and ∥rk∥2 ≤∥r0∥2 hold for k ≥ 0. We establish the results for k + 1.

Let Γk = (γk[(1)][, γ]k[(2)][)][T][ ∈] [R][2][. Since][ Γ][k][ = (][Q]k[T][Q][k][)][†][Q]k[T][r][k][,][ Q]k[T][Q][k][ = diag][{∥][q][k][−][1][∥]2[2][,][ ∥][q][k][∥][2]2[}][, it]
follows that

_k_ _[q][k][−][1]_ _k_ _[q][k]_
_γk[(1)]_ = I(qk−1 ̸= 0) _q[r]k[T]−[T]1[q][k][−][1]_ _, γk[(2)]_ = I(qk ̸= 0) _q[r]k[T][T][q][k]_ _._ (33)

Therefore,


_|γk[(1)][| ≤I][(][q][k][−][1][ ̸][= 0)][ ∥]∥qk[r]−[k][∥]1[2]∥2_ _, |1 −_ _γk[(1)][| ≤]_ [max] I(qk−1 = 0), I(qk−1 ̸= 0) _[∥][r][k]∥[ −]qk−[q]1[k]∥[−]2[1][∥][2]_

_|γk[(2)][| ≤I][(][q][k][ ̸][= 0)]_ _[∥]∥[r]qk[k]∥[∥]2[2]_ _, |1 −_ _γk[(2)][| ≤]_ [max] I(qk = 0), I(qk ̸= 0) _[∥][r][k]∥[ −]qk∥[q]2[k][∥][2]_  _._

Define c = (1−κ1+)(1cp−cq) [. We have]


(34)


_∥PkΓk∥2 = ∥pkγk[(2)]_ + pk−1γk[(1)]−1[∥][2][ ≤∥][p][k][γ]k[(2)][∥][2][ +][ ∥][p][k][−][1][γ]k[(1)]−1[∥][2]

_rk_ 2 _rk_ 2
_≤I(qk ̸= 0)∥pk∥2_ _∥qk∥2_ + I(qk−1 ̸= 0)∥pk−1∥2 _q∥k_ _∥1_ 2 _≤_ 2c∥rk∥2, (35)
_∥_ _∥_ _∥_ _−_ _∥_

where the second inequality is due to (34) and the third inequality is due to (49). Then

_∥x¯k −_ _x[∗]∥2 = ∥xk −_ _PkΓk −_ _x[∗]∥2 ≤∥xk −_ _x[∗]∥2 + ∥PkΓk∥2_


1 _κ_ _[∥][r][k][∥][2][ + 2][c][∥][r][k][∥][2][ = (]_
_−_


1 _κ_ [+ 2][c][)][∥][r][k][∥][2][,]
_−_


-----

and due to _r¯k_ 2 _rk_ 2, it holds that
_∥_ _∥_ _≤∥_ _∥_

_∥xk+1 −_ _x[∗]∥2 = ∥x¯k + βkr¯k −_ _x[∗]∥2 ≤∥x¯k −_ _x[∗]∥2 + βk∥r¯k∥2_

1
_x¯k_ _x[∗]_ 2 + β _rk_ 2 = (
_≤∥_ _−_ _∥_ _∥_ _∥_ 1 _κ_ [+ 2][c][ +][ β][)][∥][r][k][∥][2][.]

_−_

By the inductive hypothesis that _rk_ 2 _r0_ 2, and (48b), it has
_∥_ _∥_ _≤∥_ _∥_


1 1
_x¯k_ _x[∗]_ 2 ( (1 + κ) _x0_ _x[∗]_ 2, (36)
_∥_ _−_ _∥_ _≤_ 1 _κ_ [+ 2][c][)][∥][r][0][∥][2][ ≤] 1 _κ_ [+ 2][c] _∥_ _−_ _∥_

_−_  _−_ 

and

1 1
_xk+1_ _x[∗]_ 2 _r0_ 2 (1 + κ) _x0_ _x[∗]_ 2. (37)
_∥_ _−_ _∥_ _≤_ 1 _κ_ [+ 2][c][ +][ β] _∥_ _∥_ _≤_ 1 _κ_ [+ 2][c][ +][ β] _∥_ _−_ _∥_
 _−_   _−_ 

As a result, we can choosewhich ensure the g(¯xk) and ∥ gx(x0 −k+1x)[∗] are well defined.∥2 sufficiently small to ensure ¯xk ∈B(ρ) and xk+1 ∈B(ρ),

At the end of the k-th iteration of Algorithm 3, we have

_rk+1 = g(xk+1) −_ _xk+1_
= g(xk+1) − _g(¯xk) + g(¯xk) −_ (¯xk + βkr¯k)
= (g(xk+1) _g(¯xk)) + (g(¯xk)_ _x¯k_ _r¯k) + (1_ _βk)¯rk._ (38)
_−_ _−_ _−_ _−_

Let Lk := g(xk+1) − _g(¯xk) + (1 −_ _βk)¯rk, Hk := g(¯xk) −_ _x¯k −_ _r¯k, then_

_∥Lk∥2 ≤_ _κ∥xk+1 −_ _x¯k∥2 + |1 −_ _βk|∥r¯k∥2_
= κβk∥r¯k∥2 + |1 − _βk|∥r¯k∥2_
= θk(κβk + |1 − _βk|)∥rk∥2,_ (39)

which bounds the linear part of the residual rk+1.

For the higher-order terms _k, we have_
_H_

_Hk = g(¯xk) −_ (xk − _PkΓk + rk −_ _QkΓk)_
= g(¯xk) − _g(xk) + (Pk + Qk)Γk._

= g(¯xk) − _g(xk) + (pk + qk)γk[(2)]_ + (pk−1 + qk−1)γk[(1)][.] (40)


1
According to the formula 0 _[g][′][(][x][ +][ t][(][y][ −]_ _[x][))(][y][ −]_ _[x][)][dt][ =][ g][(][y][)][ −]_ _[g][(][x][)][, we have]_
R

_g(¯xk) −_ _g(xk) = g(¯ 1xk) −_ _g(xk −_ _pkγk[(2)][) +][ g][(][x][k][ −]_ _[p][k][γ]k[(2)][)][ −]_ _[g][(][x][k][)]_

= 0 _−g[′](xk −_ _pkγk[(2)]_ _−_ _tpk−1γk[(1)][)][p][k][−][1][γ]k[(1)][dt]_
Z

1
+ 0 _−g[′](xk −_ _tpkγk[(2)][)][p][k][γ]k[(2)][dt.]_ (41)
Z

Also, by Lemma 2, we have

1 _k−1_
_pk + qk =_ _g[′](xk_ _tpk)pkdt + ˆκ_ ∆rπ(k) 1 2 ( ∆rj 2),
Z0 _−_ _∥_ _−_ _∥_ _j=Xk−mk_ _O_ _∥_ _∥_


1
_pk_ 1 + qk 1 =
_−_ _−_ 0
Z


_k−2_

( ∆rj 2),
_O_ _∥_ _∥_
_j=Xk−mk_


_g[′](xk−1 −_ _tpk−1)pk−1dt + ˆκ∥∆rυ(k)−1∥2_


where π(k) denotes that the latest update of qk by Line 8 occurred in the π(k)-th iteration and
_υ(k) = π(π(k)_ 1) marks that qk 1 records qυ(k) that is the penultimate update by Line 8 occurring
_−_ _−_
in the υ(k)-th iteration.


-----

By substituting these relations to (40), it follows that


1 _k−1_

_Hk =_ Z0 _g[′](xk −_ _tpk)pkdt + ˆκ∥∆rπ(k)−1∥2_ _j=Xk−mk_ _O(∥∆rj∥2)_ _γk[(2)]_

 1 _k−2_ 

+ Z0 _g[′](xk−1 −_ _tpk−1)pk−1dt + ˆκ∥∆rυ(k)−1∥2_ _j=Xk−mk_ _O(∥∆rj∥2)_ _γk[(1)]_

 1 1 

_−_ 0 _g[′](xk −_ _pkγk[(2)]_ _−_ _tpk−1γk[(1)][)][p][k][−][1][γ]k[(1)][dt][ −]_ 0 _g[′](xk −_ _tpkγk[(2)][)][p][k][γ]k[(2)][dt]_
Z Z

1
= 0 (g[′](xk − _tpk) −_ _g[′](xk −_ _tpkγk[(2)][))][p][k][γ]k[(2)][dt]_
Z

1
+ 0 (g[′](xk−1 − _tpk−1) −_ _g[′](xk −_ _pkγk[(2)]_ _−_ _tpk−1γk[(1)][))][p][k][−][1][γ]k[(1)][dt]_
Z

_k−1_

+ ˆκ∥∆rπ(k)−1∥2 _O(∥∆rj∥2)γk[(2)]_

_j=Xk−mk_

_k−2_

+ ˆκ∥∆rυ(k)−1∥2 _O(∥∆rj∥2)γk[(1)]_

_j=Xk−mk_

= Ak + Bk + Ck + Dk. (42)
Then we can bound each terms of _k as follows (Here we assume qk_ = 0 and qk 1 = 0 as qk = 0
_H_ _̸_ _−_ _̸_
leads to γk[(2)] = 0 and qk−1 = 0 leads to γk[(1)] = 0 where the result is trivial.):

1

_∥Ak∥2 =_ 0 (g[′](xk − _tpk) −_ _g[′](xk −_ _tpkγk[(2)][))][p][k][γ]k[(2)][dt]_ 2

Z

1
_≤_ 0 _∥g[′](xk −_ _tpk) −_ _g[′](xk −_ _tpkγk[(2)][)][∥][2][∥][p][k][γ]k[(2)][∥][2][dt]_
Z

1
_κˆ_ _tpk_ _tpkγk[(2)]_ _k_
_≤_ 0 _∥_ _−_ _[∥][2][∥][p][k][∥][2][|][γ][(2)][|][dt]_
Z

= κ[ˆ]2 2[|][1][ −] _[γ]k[(2)]_ _k_

_[∥][p][k][∥][2]_ _[||][γ][(2)][|]_ _k_

_κ_ ( _rk_ 2 _rk_ _qk_ 2)
= [ˆ] 2 _O_ _∥_ _∥_ _∥_ _−_ _∥_ = ˆκ ( _rj_ 2[)][,] (43)
2 _[∥][p][k][∥][2]_ _∥qk∥2[2]_ _j=Xk−mk_ _O_ _∥_ _∥[2]_

where the third equality is due to (34), and the last equality is due to (49) and (50b);

1

_∥Bk∥2 =_ 0 (g[′](xk−1 − _tpk−1) −_ _g[′](xk −_ _pkγk[(2)]_ _−_ _tpk−1γk[(1)][))][p][k][−][1][γ]k[(1)][dt]_ 2

Z

1
_≤_ 0 _∥g[′](xk−1 −_ _tpk−1) −_ _g[′](xk −_ _pkγk[(2)]_ _−_ _tpk−1γk[(1)][)][∥][2][∥][p][k][−][1][γ]k[(1)][∥][2][dt]_
Z

1
_≤_ _κˆ_ 0 _∥∆xk−1 −_ _pkγk[(2)]_ _−_ _tpk−1γk[(1)]_ + tpk−1∥2∥pk−1∥2|γk[(1)][|][dt]
Z

_≤_ _κˆ_ ∥∆xk−1∥2 + ∥pk∥2|γk[(2)][|][ + 1]2 _[∥][p][k][−][1][∥][2][|][1][ −]_ _[γ]k[(1)][|]_ _∥pk−1∥2|γk[(1)][|]_


_κˆ_ ∆xk 1 2 + _pk_ 2 _∥rk∥2_ + [1] _∥rk −_ _qk−1∥2_ _pk_ 1 2 _∥rk∥2_
_≤_ ∥ _−_ _∥_ _∥_ _∥_ _∥qk∥2_ 2 _[∥][p][k][−][1][∥][2]_ _∥qk−1∥2_  _∥_ _−_ _∥_ _∥qk−1∥2_

= ˆκ _rk_ 2 ( ( ∆rk 1 2) + ( _rk_ 2) + ( _rk_ _qk_ 1 2)
_∥_ _∥_ _O_ _∥_ _−_ _∥_ _O_ _∥_ _∥_ _O_ _∥_ _−_ _−_ _∥_


_O(∥rj∥2[2][)][,]_ (44)
_j=Xk−mk_


= ˆκ


-----

where the last inequality is due to (34), and the second equality is due to (48a), (49), (50b);


_k−1_

_∥Ck∥2 = ˆκ∥∆rπ(k)−1∥2_ _O(∥∆rj∥2)γk[(2)]_

_j=Xk−mk_

_rk_ 2 _k−1_
_≤_ _κˆ∥∆rπ(k)−1∥2_ _∥qk∥2_ _O(∥∆rj∥2)_
_∥_ _∥_ _j=Xk−mk_

_rk_ 2 _k−1_
= ˆκ∥∆rπ(k)−1∥2 _q∥π(k∥)_ 2 _O(∥∆rj∥2)_

_∥_ _∥_ _j=Xk−mk_


= ˆκ _O(∥rj∥2[2][)][,]_ (45)

_j=Xk−mk_

where the first inequality is from (34), and the second equality is due to (50b);


_k−2_

( ∆rj 2)γk[(1)]
_O_ _∥_ _∥_
_j=Xk−mk_


_Dk_ 2 = ˆκ ∆rυ(k) 1 2
_∥_ _∥_ _∥_ _−_ _∥_


_k−2_

( ∆rj 2)
_O_ _∥_ _∥_
_j=Xk−mk_

_k−2_

( ∆rj 2)
_O_ _∥_ _∥_
_j=Xk−mk_


_rk_ 2
_≤_ _κˆ∥∆rυ(k)−1∥2_ _∥q∥k−∥1∥2_

_rk_ 2
= ˆκ∥∆rυ(k)−1∥2 _q∥υ(k∥)_ 2

_∥_ _∥_


= ˆκ _O(∥rj∥2[2][)][,]_ (46)

_j=Xk−mk_

where the first inequality is from (34), and the second equality is due to (50b). Then with the bounds
(43), (44), (45) and (46), we obtain


_O(∥rj∥2[2][)][.]_ (47)
_j=Xk−mk_


_∥Hk∥2 = ˆκ_

Combining (39) and (47) to (38), we obtain


_∥rk+1∥≤∥Lk∥2 + ∥Hk∥2 ≤_ _θk(κβk + |1 −_ _βk|)∥rk∥2 + ˆκ_ _j=Xk−mk_ _O(∥rj∥2[2][)][,]_

as desired. Since mk _m, the higher-order terms are limited. Note that_
_≤_

_κβk + |1 −_ _βk| ≤_ _κ0 < 1_

by assumption. Then, for ∥x0−x[∗]∥2 sufficiently small, the residuals {rk} are Q-linearly convergent,
which infers _rk_ 2 _r0_ 2. Therefore, we complete the induction.
_∥_ _∥_ _≤∥_ _∥_

**Remark 6. There may be some concern about whether ˆκ can be counteracted by the constant hidden**
_in the Big-O notation. In fact, since ˆκ is the Lipschitz constant of g[′]_ _and the constants in O(·) are_
_composed of κ, cp, cq, it follows that ˆκ is unrelated to_ ( ). Hence a small ˆκ can lead to a small
_O_ _·_
_uniform boundedness of the higher-order terms. In the extreme case where ˆκ = 0, e.g. g is a linear_
_map, the residual only consists of the first-order term_ _k._
_L_
**Lemma 1. Under the same assumptions of Theorem 2, for k ≥** 1, we have the following bounds:

(1 − _κ)∥∆xk−1∥2 ≤∥∆rk−1∥2 ≤_ (1 + κ)∥∆xk−1∥2, (48a)
(1 _κ)_ _xk_ _x[∗]_ 2 _rk_ 2 (1 + κ) _xk_ _x[∗]_ 2, (48b)
_−_ _∥_ _−_ _∥_ _≤∥_ _∥_ _≤_ _∥_ _−_ _∥_

_If qk_ = 0, then
_̸_ _∥pk∥2_ 1 + cp (49)
_qk_ 2 _≤_ (1 _κ)(1_ _cq)_
_∥_ _∥_ _−_ _−_


-----

_If the condition in Line 7 is true, then_

_∥pk∥2 ≤_ (1 + cp)∥∆xk−1∥2, (50a)
_qk ̸= 0, (1 −_ _cq)∥∆rk−1∥≤∥qk∥2 ≤∥∆rk−1∥2_ (50b)

_Proof. From the assumption (32a) of g, we have_

(1 − _κ)∥∆xk−1∥2 ≤∥xk −_ _xk−1∥2 −∥g(xk) −_ _g(xk−1)∥2_
_≤∥g(xk) −_ _g(xk−1) −_ (xk − _xk−1)∥2_
= _rk_ _rk_ 1 2 = ∆rk 1 2
_∥_ _−_ _−_ _∥_ _∥_ _−_ _∥_
_g(xk)_ _g(xk_ 1) 2 + _xk_ _xk_ 1
_≤∥_ _−_ _−_ _∥_ _∥_ _−_ _−_ _∥_
_≤_ (1 + κ)∥xk − _xk−1∥2,_

(1 _κ)_ _xk_ _x[∗]_ 2 _xk_ _x[∗]_ 2 _g(xk)_ _g(x[∗])_ 2
_−_ _∥_ _−_ _∥_ _≤∥_ _−_ _∥_ _−∥_ _−_ _∥_
_≤∥g(xk) −_ _g(x[∗]) −_ (xk − _x[∗])∥2 = ∥rk∥2_
_≤∥g(xk) −_ _g(x[∗])∥2 + ∥xk −_ _x[∗]∥2_
(1 + κ) _xk_ _x[∗]_ 2.
_≤_ _∥_ _−_ _∥_

haveIf the condition in Line 7 is true, i.e., ∥Pk−1ζk∥2 ≤ _cp∥∆xk−1∥2, ∥Qk−1ζk∥2 ≤_ _cq∥∆rk−1∥2, we_

_pk_ 2 = ∆xk 1 _Pk_ 1ζk 2 ∆xk 1 2 + _Pk_ 1ζk 2 (1 + cp) ∆xk 1 2,
_∥_ _∥_ _∥_ _−_ _−_ _−_ _∥_ _≤∥_ _−_ _∥_ _∥_ _−_ _∥_ _≤_ _∥_ _−_ _∥_

_qk_ 2 = ∆rk 1 _Qk_ 1ζk 2 ∆rk 1 2 _Qk_ 1ζk 2 (1 _cq)_ ∆rk 1 2.
_∥_ _∥_ _∥_ _−_ _−_ _−_ _∥_ _≥∥_ _−_ _∥_ _−∥_ _−_ _∥_ _≥_ _−_ _∥_ _−_ _∥_

The inequality _qk_ 2 ∆rk 1 is due to the fact that qk is the orthogonal projection of ∆rk 1
onto range(Qk ∥1)[⊥]∥ . Also, ≤∥ _qk_ _−= 0∥_ must hold otherwise q = Qk 1ζk which violates the condition−
_Qk_ 1ζk 2 _c−q_ ∆rk 1 2 as ̸ cq (0, 1). _−_
_∥_ _−_ _∥_ _≤_ _∥_ _−_ _∥_ _∈_

Ifupdate by Line 8 occurred in the qk ̸= 0, then qk must be updated by Line 8 in some previous iteration. We assume the latest j-th iteration, i.e., qk = qj, pk = pj, hence

_pk_ 2 1 + cp
_∥_ _∥_ =
_∥qk∥2_ _[∥]∥[p]qj[j]∥[∥]2[2]_ _≤_ [(1 +](1 −[ c]c[p]q[)])[∥]∥[∆]∆[x]rj[j]−[−]1[1]∥[∥]2[2] _≤_ (1 − _cq)(1 −_ _κ)_ _[.]_

where the first inequality is due to (50a) and (50b) and the second inequality is due to (48a).

**Lemma 2. Under the same assumptions of Theorem 2, in the k-th iteration (k ≥** 0) of the restarted
_MST-AM (Algorithm 3), we have_


1
_pk + qk =_

0

Z


_k−1_

( ∆rj 2), (51)
_O_ _∥_ _∥_
_j=Xk−mk_


_g[′](xk −_ _tpk)pkdt + ˆκ∥∆rπ(k)−1∥2_


_where mk = k mod m, ∆rk_ _mk_ 1 := rk _mk_ _, π(k) denotes that the latest update of qk by Line 8_
_−_ _−_ _−_
_occurred in the π(k)-th iteration and π(k) = k_ _−mk if Line 8 is never executed up to the k-iteration._

_Proof. We prove (51) by induction. Denote ζk = (ζk[(1)][, ζ]k[(2)][)][T][.]_

For k = 0, the relation trivially holds.

For k = 1, p1 = ∆x0, q1 = ∆r0. It follows that


1
_p1 + q1 = g(x1) −_ _g(x0) =_ 0
Z

So (51) holds for k = 1.


_g[′](x1_ _tp1)p1dt._
_−_


Suppose that (51) holds for 0 ≤ _j ≤_ _k −_ 1, where k ≥ 2. For k ≥ 2, if mk := k mod m = 0 or 1,
(51) holds because it is the same as the case of k = 0 or k = 1. Now consider the nontrivial cases.


-----

For mk = 0 and mk = 1, if the condition in Line 7 in Algorithm 3 is true, then π(k) = k. With the
convention that ̸ ∆rk ̸ _mk_ 1 = rk _mk_, we have
_−_ _−_ _−_

_pk + qk = ∆xk_ 1 _Pk_ 1ζk + ∆rk 1 _Qk_ 1ζk
_−_ _−_ _−_ _−_ _−_ _−_

= g(xk) − _g(xk−1) −_ (pk−1 + qk−1)ζk[(2)] _−_ (pk−2 + qk−2)ζk[(1)]

1
= 0 _g[′](xk −_ _t∆xk−1)∆xk−1dt_
Z

1 _k−2_
_−_ Z0 _g[′](xk−1 −_ _tpk−1)pk−1ζk[(2)][dt][ + ˆ]κ∥∆rπ(k−1)−1∥2_ _j=k−X1−mk−1_ _O(∥∆rj∥2)ζk[(2)]_

1 _k−3_
_−_ Z0 _g[′](xk−2 −_ _tpk−2)pk−2ζk[(1)][dt][ + ˆ]κ∥∆rυ(k−1)−1∥2_ _j=k−X2−mk−2_ _O(∥∆rj∥2)ζk[(1)][,]_

(52)

where υ(k 1) = π(π(k 1) 1) denotes that qk 2 records qυ(k 1) that is the penultimate update
_−_ _−_ _−_ _−_ _−_
by Line 8 occurring in the υ(k − 1)-th iteration. Considering the terms in pk + qk, we know

_g[′](xk −_ _t∆xk−1)∆xk−1 −_ _g[′](xk−1 −_ _tpk−1)pk−1ζk[(2)]_ _−_ _g[′](xk−2 −_ _tpk−2)pk−2ζk[(1)]_
= g[′](xk − _t∆xk−1)(∆xk−1 −_ _pk−1ζk[(2)]_ _−_ _pk−2ζk[(1)][)]_

+ (g[′](xk − _t∆xk−1) −_ _g[′](xk−1 −_ _tpk−1))pk−1ζk[(2)]_
+ (g[′](xk − _t∆xk−1) −_ _g[′](xk−2 −_ _tpk−2))pk−2ζk[(1)][.]_ (53)

For ζk = (Q[T]k 1[Q][k][−][1][)][†][Q][T]k 1[∆][r][k][−][1][, because][ Q][T]k 1[Q][k][−][1][ = diag][{∥][q][k][−][2][∥]2[2][,][ ∥][q][k][−][1][∥][2]2[}][, it follows]
_−_ _−_ _−_
that

_ζk[(1)]_ = I(qk−2 ̸= 0) [∆]q[r]k[T]k−[T]−21[q][q][k][k][−][−][2][2] _, ζk[(2)]_ = I(qk−1 ̸= 0) [∆]q[r]k[T]k−[T]−11[q][q][k][k][−][−][1][1] _._

Therefore,

_|ζk[(1)][| ≤I][(][q][k][−][2][ ̸][= 0)]_ _[∥]∥[∆]q[r]k[k]−[−]2[1]∥[∥]2[2]_ _, |ζk[(2)][| ≤I][(][q][k][−][1][ ̸][= 0)]_ _[∥]∥[∆]q[r]k[k]−[−]1[1]∥[∥]2[2]_ _._ (54)


Now, to bound the terms in (53), we have

_∥(g[′](xk −_ _t∆xk−1) −_ _g[′](xk−1 −_ _tpk−1))pk−1ζk[(2)][∥][2]_

_≤_ _κˆ∥xk −_ _xk−1 −_ _t(∆xk−1 −_ _pk−1)∥2∥pk−1ζk[(2)][∥][2]_

_≤_ _κˆ((1 −_ _t)∥∆xk−1∥2 + t∥pk−1∥2)∥pk−1∥2|ζk[(2)][|]_

_κˆ((1_ _t)_ ∆xk 1 2 + t _pk_ 1 2) _pk_ 1 2 (qk 1 = 0) _[∥][∆][r][k][−][1][∥][2]_
_≤_ _−_ _∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_ _I_ _−_ _̸_ _qk_ 1 2

_∥_ _−_ _∥_

_k−1_ _k−1_

_≤_ _κˆ∥∆rk−1∥2_ _O(∥∆rj∥2) = ˆκ∥∆rk−1∥2_ _O(∥∆rj∥2),_ (55)

_j=k−X1−mk−1_ _j=Xk−mk_

where the third inequality is due to (54), and the last inequality is due to (48a), (50a) and (49), where
the constants are absorbed into the big-O notation. Similarly, it follows that

_∥(g[′](xk −_ _t∆xk−1) −_ _g[′](xk−2 −_ _tpk−2))pk−2ζk[(1)][∥][2]_

_≤_ _κˆ∥∆xk−1 + ∆xk−2 −_ _t∆xk−1 + tpk−2∥2∥pk−2ζk[(1)][∥][2]_

_≤_ _κˆ((1 −_ _t)∥∆xk−1∥2 + ∥∆xk−2∥2 + t∥pk−2∥2)∥pk−2∥2|ζk[(1)][|]_

_κˆ((1_ _t)_ ∆xk 1 2 + ∆xk 2 2 + t _pk_ 2 2) _pk_ 2 2 (qk 2 = 0) _[∥][∆][r][k][−][1][∥][2]_
_≤_ _−_ _∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_ _I_ _−_ _̸_ _qk_ 2 2

_∥_ _−_ _∥_

_k−1_ _k−1_

_≤_ _κˆ∥∆rk−1∥2_ _O(∥∆rj∥2) = ˆκ∥∆rk−1∥2_ _O(∥∆rj∥2)._ (56)

_j=k−X2−mk−2_ _j=Xk−mk_


-----

For the remaining terms in (52), we have


_k−2_

_O(∥∆rj∥2)ζk[(2)]_ + ˆκ∥∆rυ(k−1)−1∥2
_j=k−X1−mk−1_


_k−3_

( ∆rj 2)ζk[(1)]
_O_ _∥_ _∥_
_k=k−X2−mk−2_


_κˆ∥∆rπ(k−1)−1∥2_


_k−2_

_≤_ _κˆ∥∆rπ(k−1)−1∥2I(qk−1 ̸= 0)_ _[∥]∥[∆]q[r]k[k]−[−]1[1]∥[∥]2[2]_ _j=k−X1−mk−1_ _O(∥∆rj∥2)_

_k−3_

+ ˆκ∥∆rυ(k−1)−1∥2I(qk−2 ̸= 0) _[∥]∥[∆]q[r]k[k]−[−]2[1]∥[∥]2[2]_ _k=k−X2−mk−2_ _O(∥∆rj∥2)_

_k−2_

_≤_ _κˆ∥∆rk−1∥2_ _O(∥∆rj∥2),_ (57)

_k=Xk−mk_

where the last inequality is due to (50b) and that qk 1 = qπ(k 1), qk 2 = qυ(k 1).
_−_ _−_ _−_ _−_

With relations (52), (53), (55), (56) and (57), and noting thatand _pk = ∆xk−1 −_ _pk−1ζk[(2)]_ _−_ _pk−2ζk[(1)][,]_

1

0 (g[′](xk − _t∆xk−1)∆xk−1 −_ _g[′](xk−1 −_ _tpk−1)pk−1ζk[(2)]_ _−_ _g[′](xk−2 −_ _tpk−2)pk−2ζk[(1)][)][dt]_ 2

Z

1
_≤_ 0 _∥g[′](xk −_ _t∆xk−1)∆xk−1 −_ _g[′](xk−1 −_ _tpk−1)pk−1ζk[(2)]_ _−_ _g[′](xk−2 −_ _tpk−2)pk−2ζk[(1)][∥][2][dt,]_
Z

we can estimate pk + qk as

1 _k−1_
_pk + qk =_ _g[′](xk_ _t∆xk_ 1)pkdt + ˆκ ∆rk 1 2 ( ∆rj 2). (58)
Z0 _−_ _−_ _∥_ _−_ _∥_ _j=Xk−mk_ _O_ _∥_ _∥_

To further obtain (51), notice that the difference

1 1

0 _g[′](xk −_ _tpk)pkdt −_ 0 _g[′](xk −_ _t∆xk−1)pkdt_ 2

Z Z

1

= 0 (g[′](xk − _tpk) −_ _g[′](xk −_ _t∆xk−1))pkdt_ 2

Z

1
_≤_ 0 _∥g[′](xk −_ _tpk) −_ _g[′](xk −_ _t∆xk−1)∥2∥pk∥2dt_
Z

1
_≤_ _κˆ_ 0 _t∥pk−1ζk[(2)]_ + pk−2ζk[(1)][∥][2][∥][p][k][∥][2][dt]
Z

_κ_
_≤_ [ˆ]2 _[c][p][∥][∆][x][k][−][1][∥][2][∥][p][k][∥][2][ = ˆ]κ∥∆rk−1∥2O(∥∆rk−1∥2),_

where the last inequality is due to the condition _Pk_ 1ζk 2 _cp_ _p_ 2 and inequalities (48a) and
(50a). Hence the difference can be absorbed in the Big- ∥ _−_ _O∥ notation in (58). Thus (51) holds when ≤_ _∥_ _∥_
the condition in Line 7 is true for the k-th iteration.

We consider the case where the condition in Line 7 is false for the k-th iteration. Then Pk =
_Pk_ 1, Qk = Qk 1. In other words, the memory recording pk, qk keeps unchanged from the (π(k)+
_−_ _−_
1)-th to k-th iteration. Since π(k) < k and π(π(k)) = π(k), with the inductive hypothesis, we have

_pk + qk = pπ(k) + qπ(k)_


1

0

Z

1

0

Z


_π(k)−1_

( ∆rj 2)
_O_ _∥_ _∥_
_j=π(k)_ _mπ(k)_

X−


_g[′](xπ(k) −_ _tpπ(k))pπ(k)dt + ˆκ∥∆rπ(k)−1∥2_


_k−1_

( ∆rj 2). (59)
_O_ _∥_ _∥_
_j=Xk−mk_


_g[′](xπ(k) −_ _tpk)pkdt + ˆκ∥∆rπ(k)−1∥_


-----

To further obtain (51), note that the difference

1 1

_g[′](xk_ _tpk)pkdt_ _g[′](xπ(k)_ _tpk)pkdt_
0 _−_ _−_ 0 _−_ 2

Z Z

1

_g[′](xk_ _tpk)_ _g[′](xπ(k)_ _tpk)_ 2 _pk_ 2dt

_≤_ 0 _∥_ _−_ _−_ _−_ _∥_ _∥_ _∥_
Z

_κˆ_ _xk_ _xπ(k)_ 2 _pk_ 2
_≤_ _∥_ _−_ _∥_ _∥_ _∥_

_k−1_ _k−1_

= ˆκ ∆xj 2 _pk_ 2 = ˆκ ∆rπ(k) 1 2 ( ∆rj 2)

_∥_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_ _O_ _∥_ _∥_
_j=π(k)_ _j=π(k)_

X X


can be absorbed into the Big-O notation in (59). Thus (51) also holds when the condition in Line 7
is false for the k-th iteration.

As a result, we complete the induction in the k-th iteration.

C.3 REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING

C.3.1 CHECK OF POSITIVE DEFINITENESS

We describe the check of positive definiteness in RST-AM, which follows the same procedure as
that of SAM (Wei et al., 2021). From Line 11-13 in Algorithm 1, one update of xk RST-AM is
_xk+1 = xk + Hkrk, where Hk = βkI −_ _αkYkZk[†][Q]k[T][,][ Y][k][ =][ P][k][ +][ β][k][Q][k][,][ Z][k][ =][ Q]k[T][Q][k][ +][ δ][k][P]k[ T][P][k][.]_
_Hk is generally not symmetric. For the convergence analysis of RST-AM, a critical condition is the_
_positive definiteness of Hk, i.e._

_s[T]k_ _[H][k][s][k]_ 2[,] _sk_ R[d], (60)

_[≥]_ _[β][k][µ][∥][s][k][∥][2]_ _∀_ _∈_

where µ ∈ (0, 1) is a constant. Next, we show how to guarantee it.

Let λmin( ) denote the smallest eigenvalue, and λmax( ) denote the largest eigenvalue. Since

_·_ _·_ 1
_s[T]k_ _[H][k][s][k][ =][ 1]2_ _[s]k[T][(][H][k][ +][ H]k[T][)][s][k][, Condition (60) is equivalent to][ λ][min]_ 2 _Hk + Hk[T]_ _βkµ. By_

1 _≥_
some simple algebraic operations, we obtain λmin 2 _Hk + Hk[T]_ =  βk  2 _[α][k][λ][max][(][Y][k][Z]k[†][Q]k[T]_ [+]

_−_ [1]

_QkZk[†][Y][ T]k_ [)][. Let][ λ][k][ :=][ λ][max][(][Y][k][Z]k[†][Q]k[T] [+][ Q][k][Z]k[†][Y][ T]k  [)][, then Condition (60) is equivalent to]  

_αkλk_ 2βk(1 _µ),_ (61)
_≤_ _−_

namely, (13) in Remark 4. To check Condition (61), note that

_λk = λmax_ (Yk _Qk)_ 0 _Zk[†]_ _Yk[T]_ = λmax _Yk[T]_ (Yk _Qk)_ 0 _Zk[†]_ _._ (62)

_Zk[†]_ 0 _Q[T]k_ _Q[T]k_ _Zk[†]_ 0
        

Since _Yk[T]_ (Yk _Qk),_ 0 _Zk[†]_ R[4][×][4], λk can be computed cheaply. This cost is negligible
_Q[T]k_ _Zk[†]_ 0 _∈_
   

compared with those to form Pk[T][P][k][, Q]k[T][Q][k][, which need][ O][(][d][)][ flops. Then, to guarantee the positive]
definiteness of Hk, we check whether αk satisfies (61) and use a smaller αk if necessary, e.g. αk =
2βk(1 _µ)/λk._
_−_

C.3.2 PROOFS OF THE THEOREMS

We first give the proof of the boundedness of _Pk_ 1ζk 2 and _Qk_ 1ζk 2.
_∥_ _−_ _∥_ _∥_ _−_ _∥_
**Lemma 3. For P, Q ∈** R[d][×][m](d ≥ _m), δ > 0, and Z = Q[T]Q + δP_ [T]P _, we have_

_PZ_ _[†]Q[T]_ 2 _δ[−]_ [1]2, (63a)
_∥_ _∥_ _≤_

_QZ_ _[†]Q[T]_ 2 1. (63b)
_∥_ _∥_ _≤_

_Proof. We first consider the case that Z is nonsingular, then PZ_ _[†]Q[T]_ = PZ _[−][1]Q[T], QZ_ _[†]Q[T]_ =
_QZ_ _[−][1]Q[T]. It can be seen that P_ [T]P and Q[T]Q are symmetric positive semidefinite, and Z is SPD as
it is assumed to be nonsingular. Also, we have δP [T]P ⪯ _Z and Q[T]Q ⪯_ _Z, where the notation “⪯”_


-----

denotes the Loewner partial order, i.e., A ⪯ _B with A, B ∈_ R[m][×][m] means that B − _A is positive_
semidefinite. Hence, we have


_Z_ _[−]_ 2[1] δP [T]PZ _[−]_ 2[1] ⪯ _I, Z_ _[−]_ 2[1] Q[T]QZ _[−]_ 2[1] ⪯ _I,_


which implies


_Z_ _[−]_ [1]2 _P_ [T]P _Z_ _[−]_ 2[1] 2 _δ[−][1],_
_∥_ _∥_ _≤_

_Z_ _[−]_ [1]2 _Q[T]Q_ _Z_ _[−]_ 2[1] 2 1.
_∥_ [ ] _∥_ _≤_

Let λmax( ) denote the largest eigenvalue, we have

_·_ [ ]

_∥QZ_ _[−][1]Q[T]∥2 = λmax_ _QZ_ _[−][1]Q[T][]_ = λmax _Q[T]QZ_ _[−][1][]_ = λmax _Z_ _[−]_ 2[1] Q[T]QZ _[−]_ 2[1]
    

_PZ_ _[−][1]Q[T]_ 2 [=][ λ][max] _PZ_ _[−][1]Q[T]QZ_ _[−][1]P_ [T][]
_∥_ _∥[2]_

= λmax  P [T]PZ _[−][1]Q[T]QZ_ _[−][1][]_

= λmax   _Z_ _[−]_ 2[1] _P_ [T]P _Z_ _[−]_ 2[1] _Z_ _[−]_ 2[1] _Q[T]Q_ _Z_ _[−]_ 2[1]

_·_

_Z_ _[−]_ 2[1] _P_ [T]P _Z_ _[−]_ [1]2 _Z_ _[−]_ 2[1] _Q[T]Q_ _Z_ _[−]_ 2[1] 2 
_≤∥_ [ ] _·_ [ ] _∥_

_Z_ _[−]_ 2[1] _P_ [T]P  _Z_ _[−]_ [1]2 2 _Z_ _[−]_ 2[1] _Q[T]Q_ _Z_ _[−]_ 2[1] 2 _δ[−][1]._
_≤∥_ [ ] _∥_ _∥_ [ ] _∥_ _≤_

Therefore, _PZ_ _[−][1]Q[T]_ _δ[−]_ 2[1] and _QZ_ _[−][1]Q[T]_ 2 1. 

[ ] [ ]

_∥_ _∥≤_ _∥_ _∥_ _≤_

For the case that Z is singular, it can be proved that


_≤_ 1,


ker(Z) := {x ∈ R[m]|Zx = 0} = {x ∈ R[m]|Px = 0 and Qx = 0}. (64)

In fact, if Px = 0 and Qx = 0, it is obvious that Zx = 0. On the other side, if Zx = 0, then
_x[T]Zx = 0, i.e. x[T]Q[T]Qx + δx[T]P_ [T]Px = 0. Since 0 ⪯ _P_ [T]P, 0 ⪯ _Q[T]Q, δ > 0, it follows that_
_x[T]Q[T]Qx = 0 and δx[T]P_ [T]Px = 0, which further implies Qx = 0 and Px = 0. Hence (64) holds.

Let U1 satisfy U1[T][U][1] [=][ I][ and][ range(][U][1][) = ker(][Z][)][, i.e. the orthonormal basis of][ ker(][Z][)][, and][ U][2]
satisfy U2[T][U][2] [=][ I][ and][ U][ T]2 _[U][1]_ [= 0][, i.e. the orthonormal basis of][ ker(][Z][)][⊥][. With the equality (64),]
we know PU1 = 0, QU1 = 0. Define U = (U1, U2) ∈ R[m][×][m], then U [T]U = Im and by direct
computation, we have

0 0
_U_ [T]ZU = _,_
0 _U2[T][ZU][2]_
 

where U2[T][ZU][2] [= (][QU][2][)][T][QU][2] [+][ δ][(][PU][2][)][T][PU][2] [is nonsingular according to the definition of][ U][2][.]
So

0 0
_Z_ _[†]_ = U _U_ [T].
0 (U2[T][ZU][2][)][−][1]
 

As a result, we can further compute PZ _[†]Q[T]_ and QZ _[†]Q[T]_ as

_PZ_ _[†]Q[T]_ = (PU2)(U2[T][ZU][2][)][−][1][(][QU][2][)][T][, QZ] _[†][Q][T][ = (][QU][2][)(][U][ T]2_ _[ZU][2][)][−][1][(][QU][2][)][T][.]_


Then let _P[ˆ] = PU2,_ _Q[ˆ] = QU2, and_ _Z[ˆ] = Q[ˆ][T][ ˆ]Q + δP[ˆ][T][ ˆ]P_, and noting that _Z[ˆ] is nonsingular, we can_
obtain ∥PZ _[†]Q[T]∥2 = ∥P[ˆ]Z[ˆ][−][1][ ˆ]Q[T]∥2 ≤_ _δ[−]_ [1]2, ∥QZ _[†]Q[T]∥2 = ∥Q[ˆ]Z[ˆ][−][1][ ˆ]Q[T]∥2 ≤_ 1.

As a result of Lemma 3, _Pk_ 1ζk 2, _Qk_ 1ζk 2 in Algorithm 1 are bounded by ( ∆rk 1 2), as
_∥_ _−_ _∥_ _∥_ _−_ _∥_ _O_ _∥_ _−_ _∥_
shown in the following corollary.

**Corollary 2.** _Pk_ 1ζk 2, _Qk_ 1ζ 2 in Algorithm 1 are bounded, i.e.
_∥_ _−_ _∥_ _∥_ _−_ _∥_

_∥Pk−1ζk∥2 ≤_ (δk[(1)][)][−] 2[1] ∥∆rk−1∥2, ∥Qk−1ζk∥2 ≤∥∆rk−1∥2, (65)

_where ζk = (Q[T]k_ 1[Q][k][−][1][ +][ δ]k[(1)][P][k][−][1][P][k][−][1][)][†][Q]k[T] 1[∆][r][k][−][1][.]
_−_ _−_


-----

Now, we turn to the proofs of the theorems about RST-AM in Section 3.4. For brevity, we use δk to
denote δk[(2)][, i.e.][ δ][k][ ≡] _[δ]k[(2)][. The proofs follow those of SAM (Wei et al., 2021). Nonetheless, since]_
RST-AM uses different historical sequences compared with SAM, we give the detailed proofs for
RST-AM for completeness.

From Assumption 2, for the mini-batch gradient fSk (xk) = _n1k_ _i∈Sk_ _[f][ξ]i_ [(][x][k][)][, where][ n][k][ =][ |][S][k][|][,]

the following properties hold:

P

E[∇fSk (x)|xk] = ∇f (xk), (66a)

E[ _fSk_ (xk) _f_ (xk) 2[|][x][k][]][ ≤] _[σ][2]_ _._ (66b)
_∥∇_ _−∇_ _∥[2]_ _nk_


Consider the update of RST-AM. From Line 11-13 in Algorithm 1, it can be written as xk+1 =
_xk + Hkrk, where rk = −∇fSk_ (xk), and for k ≥ 0,

_Hk = βkI −_ _αk (Pk + βkQk)_ _Q[T]k_ _[Q][k]_ [+][ δ][k][P][ T]k _[P][k]_ _† QTk_ _[.]_ (67)
  

To prove the theorems, the critical points are (i) the positive definiteness of the approximate Hessian
_Hk and (ii) an adequate suppression of the noise from the gradient estimates in the stochastic case._

We first give a lemma related to the projection step.

_we haveLemma 4. Suppose that {xk} is generated by RST-AM. If αk ≥_ 0, βk > 0, then for any vk ∈ R[d],

_Hkvk_ 2 _βk[2]_ 1 + 2αk[2] + αk[2][δ]k[−][1] _vk_ 2[.] (68)
_∥_ _∥[2]_ _[≤]_ [2] _[−]_ [2][α][k] _∥_ _∥[2]_
     

_Proof. The result holds when k = 0 as H0 = β0I. For k ≥_ 1,

_Hkvk = βkvk −_ (αkPk + αkβkQk)Γk, (69)

where Γk = (Q[T]k _[Q][k][ +][ δ][k][P]k[ T][P][k][)][†][Q]k[T][v][k][ solves]_

min 2 [+][ δ][k][∥][P][k][Γ][∥]2[2][.] (70)
Γ

_[∥][v][k][ −]_ _[Q][k][Γ][∥][2]_

thusBy direct computation, ∥vk − _QkΓk∥2[2]_ [+] _[δ][k][∥][P][k][Γ][k][∥]2[2]_ [=][ ∥][v][k][∥]2[2] _[−]_ _[v]k[T][Q][k][(][Q]k[T][Q][k][ +]_ _[δ][k][P][ T]k_ _[P][k][)][†][ ·]_ _[Q]k[T][v][k][,]_
_∥vk −_ _QkΓk∥2[2]_ [+][ δ][k][∥][P][k][Γ][k][∥]2[2] _[≤∥][v][k][∥]2[2][.]_ (71)

Therefore,

_Hkvk_ 2
_∥_ _∥[2]_
= _βkvk_ (αkPk + αkβkQk)Γk 2
_∥_ _−_ _∥[2]_
= _βk (vk_ _αkQkΓk)_ _αkPkΓk_ 2
_∥_ _−_ _−_ _∥[2]_ 1

= ∥βk(1 − _αk)vk + βkαk(vk −_ _QkΓk) −_ _αkδk−_ [1]2 _δk2_ _[P][k][Γ][k][∥]2[2]_

_≤_ _βk[2][(1][ −]_ _[α][k][)][2][ +][ β]k[2][α]k[2]_ [+][ α]k[2][δ]k[−][1] _·_ _∥vk∥2[2]_ [+][ ∥][v][k] _[−]_ _[Q][k][Γ][k][∥]2[2]_ [+][ δ][k][∥][P][k][Γ][k][∥]2[2]
 βk[2] 1 + 2αk[2] + αk[2][δ]k[−][1]   vk 2 [+][ ∥][v][k][∥]2[2] 

_≤_ _[−]_ [2][α][k] _∥_ _∥[2]_

= 2  _βk [2]_ 1 + 2αk[2]  + αk[2][δ]k[−]  [1] _vk_ 2[.]  (72)

_[−]_ [2][α][k] _∥_ _∥[2]_

In the above, the first inequality uses the inequality     


2
!


_n_

**xi** 2

! _i=1_ _∥_ _∥[2]_
X


_aixi_ 2
_i=1_ _∥[2]_ _[≤]_

X


_a[2]i_
_i=1_

X


(73)


_ai_ **xi** 2
_|_ _|∥_ _∥_
_i=1_

X


where ai ∈ R, xi ∈ R[d]. The second inequality is based on inequality (71).

With Lemma 4, we can prove the deterministic case of RST-AM.


-----

**_Proof of Theorem 3. Since 1+2αk[2]_** _k_ _C_ _[−][1]β[2], with Lemma 4,_
we have _[−]_ [2][α][k][ ≤] [1][ for][ 0][ ≤] _[α][k][ ≤]_ [1][, and][ δ][−][1] _≤_
_∥Hkrk∥2[2]_ _[≤]_ [2][β][2][(1 +][ C] _[−][1][)][∥][r][k][∥]2[2][.]_

Since αk satisfies (61), we have
_rk[T][H][k][r][k]_ 2[.]

_[≥]_ _[βµ][∥][r][k][∥][2]_

Then, under Assumption 1, we have

_f_ (xk+1) ≤ _f_ (xk) + ∇f (xk)[T](xk+1 − _xk) +_ _[L]2_ _[∥][x][k][+1][ −]_ _[x][k][∥]2[2]_

= f (xk) _rk[T][H][k][r][k]_ [+][ L] 2
_−_ 2 _[∥][H][k][r][k][∥][2]_

_≤_ _f_ (xk) − _βµ∥rk∥2[2]_ [+][ Lβ][2][(1 +][ C] _[−][1][)][∥][r][k][∥]2[2]_
= f (xk) _β_ _µ_ _βL(1 + C_ _[−][1])_ _rk_ 2
_−_ _−_ _∥_ _∥[2]_

_f_ (xk)   2[,]  (74)
_≤_ _−_ 2[1] _[βµ][ · ∥∇][f]_ [(][x][k][)][∥][2]


where the last inequality is due to 0 _<_ _β_ _≤_ 2L(1+µC[−][1]) [.] Thus, f (xk+1) − _f_ (xk) _≤_

_−_ 2[1] _[βµ][∥∇][f]_ [(][x][k][)][∥]2[2][.]

Summing both sides of this inequality for k ∈{0, . . ., N − 1} and recalling f (x) > f _[low]_ in
Assumption 1 gives


_N_ _−1_

_∥∇f_ (xk)∥2[2][.]
_k=0_

X


_f_ _[low]_ _f_ (x0) _f_ (xN ) _f_ (x0)
_−_ _≤_ _−_ _≤−_ 2[1] _[βµ]_

Rearranging and dividing further by N yields (15).

The next lemmas and proofs are about the stochastic case.


**Lemma 5. Suppose that Assumption 2 holds for {xk} generated by RST-AM. In addition, if βk > 0,**
_and αk_ 0 and satisfies (13), then
_≥_

ESk [ _Hkrk_ 2[]][ ≤] [2] _βk[2]_ 1 + 2αk[2] + _[α]k[2]_ _f_ (xk) 2 [+][ σ][2] _,_ (75a)
_∥_ _∥[2]_ _[−]_ [2][α][k] _δk_ _·_ _∥∇_ _∥[2]_ _nk_
   
  


_αk[2][(][δ]k−_ [1]2 + βk)[2]

_[σ][2]_ _,_ (75b)
_βkµ_ _·_ _nk_


_f_ (xk)[T]ESk [Hkrk] 2 [+ 1]
_∇_ _≤−_ [1]2 _[β][k][µ][∥∇][f]_ [(][x][k][)][∥][2] 2

_where µ > 0 is the constant introduced in Remark 4._

_Proof. (i) From Lemma 4, we have_


ESk [∥Hkrk∥2[2][]][ ≤] [2] _βk[2]_ 1 + 2αk[2] _[−]_ [2][α][k] + _[α]δkk[2]_

  

From Assumption 2, we have


ESk [∥rk∥2[2][]][.] (76)


ESk [∥rk∥2[2][] =][ E][S]k [[][∥][r][k] _[−]_ [E][S]k [[][r][k][]][∥][2]2[] +][ ∥][E][S]k [[][r][k][]][∥][2]2 _[≤∥∇][f]_ [(][x][k][)][∥]2[2] [+][ σ][2][/n][k][.] (77)

With (76), (77), we obtain (75a).

(ii) The result holds for k = 0 since H0 = β0I. Consider k ≥ 1. Define ϵk = ∇fSk (xk) −
_∇f_ (xk 1) = −rk −∇f (xk). Then Hkrk = Hk (−ϵk −∇f (xk)) . Since αk satisfies (13), it has
_λmin_ 2 _Hk + Hk[T]_ _βkµ. Thus_

_≥_

    

_f_ (xk)[T]Hk _f_ (xk) = [1] _Hk + Hk[T]_ _f_ (xk) _βkµ_ _f_ (xk) 2[,]
_∇_ _∇_ 2 _[∇][f]_ [(][x][k][)][T][  ] _∇_ _≥_ _∥∇_ _∥[2]_



which implies
ESk [∇f (xk)[T]Hk∇f (xk)] ≥ _βkµ∥∇f_ (xk)∥2[2][.] (78)


-----

Let Mk = αk (Pk + βkQk) _Q[T]k_ _[Q][k][ +][ δ][k][P]k[ T][P][k]_ _† QTk_ [, then][ H][k][ =][ β][k][I][ −] _[M][k][.][ With the assumption]_
(66a), i.e. ESk [ϵk] = 0, we have
  

ESk [∇f (xk)[T]Hkϵk] = ESk [∇f (xk)[T] (βkϵk − _Mkϵk)]_

= βk∇f (xk)[T]ESk [ϵk] − ESk [∇f (xk)[T]Mkϵk] = −ESk [∇f (xk)[T]Mkϵk].
Using the Cauchy-Schwarz inequality with expectations, we obtain


_|ESk_ [∇f (xk)[T]Hkϵk]| = |ESk [∇f (xk)[T]Mkϵk]| ≤


ESk [∥∇f (xk)∥2[2][]]


ESk [∥Mkϵk∥2[2][]]


_≤∥∇f_ (xk)∥2 ESk [∥Mkϵk∥2[2][]][.] (79)
q

We now bound ∥Mkϵk∥2[2][. For brevity, let][ Z][k] [=][ Q][T]k _[Q][k][ +][ δ][k][P]k[ T][P][k][, and][ N][1][ =][ P][k][Z]k[†][Q]k[T][, N][2][ =]_
_βkQkZk[†][Q]k[T][, then]_

_Mk_ 2 = _αk (N1 + N2)_ 2 _αk (_ _N1_ 2 + _N2_ 2) _αk(δk−_ [1]2 + βk), (80)
_∥_ _∥_ _∥_ _∥_ _≤_ _∥_ _∥_ _∥_ _∥_ _≤_

where the last inequality is from Lemma 3.


With (80), we have _Mkϵk_ 2 _αk(δk−_ [1]2 + βk) _ϵk_ 2, which implies
_∥_ _∥_ _≤_ _∥_ _∥_

ESk [ _Mkϵk_ 2[]][ ≤] _[α]k[2][(][δ]k−_ [1]2 + βk)[2]ESk [ _ϵk_ 2[]][ ≤] _[α]k[2][(][δ]k−_ 2[1]
_∥_ _∥[2]_ _∥_ _∥[2]_


+ βk)[2][ σ][2] _,_ (81)

_nk_


where the last inequality is due to (66b). Now we bound |ESk [∇f (xk)[T]Hkϵk]| as follows (cf. (79)):

_|ESk_ [∇f (xk)[T]Hkϵk]|


ESk [∥Mkϵk∥2[2][]]


_f_ (xk) 2
_≤∥∇_ _∥_


_αk(δk−_ 2[1]
_≤_

_αk(δk−_ [1]2
_≤_


+ βk)∥∇f (xk)∥2 ESk [∥ϵk∥2[2][]]
q

+ βk) _[σ]_ _f_ (xk) 2
_√nk_ _∥∇_ _∥_


_βkµ_ _f_ (xk) 2 _[α][k][(][δ]k−_ 2[1] + βk) _σ_
_∥∇_ _∥_ _·_ _√βkµ_ _√nk_


_αk[2][(][δ]k−_ 2[1] + βk)[2]

_[σ][2]_ _._ (82)
_βkµ_ _·_ _nk_


2 [+ 1]

_≤_ 2[1] _[β][k][µ][∥∇][f]_ [(][x][k][)][∥][2] 2


With the inequality (78) and (82), we obtain
_∇f_ (xk)[T]ESk [Hkrk]

= −∇f (xk)[T]ESk [Hk (ϵk + ∇f (xk))]

= −ESk [∇f (xk)[T]Hk∇f (xk)] − ESk [∇f (xk)[T]Hkϵk]

_≤−ESk_ [∇f (xk)[T]Hk∇f (xk)] + |ESk [∇f (xk)[T]Hkϵk]|

_αk[2][(][δ]k−_ 2[1] + βk)[2]

_βkµ_ _f_ (xk) 2 [+ 1] 2 [+ 1] _[σ][2]_
_≤−_ _∥∇_ _∥[2]_ 2 _[β][k][µ][∥∇][f]_ [(][x][k][)][∥][2] 2 _βkµ_ _·_ _nk_


= 2 [+ 1] _αk[2][(][δ]k−_ 2[1] + βk)[2] _[σ][2]_ _._ (83)
_−_ [1]2 _[β][k][µ][∥∇][f]_ [(][x][k][)][∥][2] 2 _βkµ_ _·_ _nk_


By imposing one more restriction on αk, we can obtain a convenient corollary:
**Corollary 3. Suppose that Assumption 2 holds for** _xk_ _generated by RST-AM. C > 0 is the_

1 _{_ _}_

_constant in (12). If βk > 0, 0_ _αk_ min 1, βk2
_≤_ _≤_ _{_ _[}][ and satisfies (61), then]_

ESk [ _Hkrk_ 2[]][ ≤] [2][β]k[2] 1 + C _[−][1][]_ _f_ (xk) 2 [+][ σ][2] _,_ (84a)
_∥_ _∥[2]_ _·_ _∥∇_ _∥[2]_ _nk_
 
 

_f_ (xk)[T]ESk [Hkrk] 2 [+][ β]k[2] 1 + C _[−][1][][ σ][2]_ _._ (84b)
_∇_ _≤−_ [1]2 _[β][k][µ][∥∇][f]_ [(][x][k][)][∥][2] _[·][ µ][−][1][  ]_ _nk_


-----

_Proof. The first result (84a) is clear by considering (75a) and noticing that 1+2αk[2]_

1

haveαk ∈ [0, 1] and δk[−][1] _≤_ _C_ _[−][1]βk[2][.][ Since][ α][k][ ≤]_ _[β]k2_ _[, δ][k][ ≥]_ _[Cβ]k[−][2]_ and (1 + C _[−]_ 2[1] )[2] _≤[−]2(1 +[2][α][k][ ≤] C_ _[−][1][ when][1]) we_


_αk[2][(][δ]k−_ [1]2 + βk)[2]

_[σ][2]_
_βkµ_ _·_ _nk_ _≤_ 2[1]


_βk(C_ _[−]_ 2[1] βk + βk)[2] _[σ][2]_

_βkµ_ _·_ _nk_


= [1]2 _[µ][−][1][(][C]_ _[−]_ 2[1] + 1)[2]βk[2] _[·][ σ]nk[2]_

_βk[2][µ][−][1][(1 +][ C]_ _[−][1][)]_ _[σ][2]_ _._
_≤_ _nk_

Substituting it into (75b), we obtain (84b).

With Corollary 3, we establish the descent property of RST-AM:


**Lemma 6. Suppose that Assumptions 1 and 2 hold for** _xk_ _generated by RST-AM. C > 0 is the_

_constant in (12). If 0 < βk ≤_ 4L(1+µC[−][1]) _[,][ 0][ ≤]_ _[α][k][ ≤]_ [min] {[{][1][, β]}k12 _[}][ and satisfies (61), then]_

_σ[2]_

ESk [f (xk+1)] _f_ (xk) 2 [+][ β]k[2] (L + µ[−][1])(1 + C _[−][1])_ _._ (85)
_≤_ _−_ 4[1] _[β][k][µ][∥∇][f]_ [(][x][k][)][∥][2] _nk_

  

_Proof. According to Assumption 1, we have_

_f_ (xk+1) ≤ _f_ (xk) + ∇f (xk)[T](xk+1 − _xk) +_ _[L]2_ _[∥][x][k][+1][ −]_ _[x][k][∥]2[2]_

= f (xk) + _f_ (xk)[T]Hkrk + _[L]_ 2[.] (86)
_∇_ 2 _[∥][H][k][r][k][∥][2]_

Taking expectation with respect to the mini-batch Sk on both sides of (86) and using Corollary 3 we
obtain

ESk [f (xk+1)]


_≤_ _f_ (xk) + ∇f (xk)[T]ESk [Hkrk] + _[L]2_ [E][S][k] _[∥][H][k][r][k][∥]2[2]_

_f_ (xk) 2 [+][ β]k[2][µ][−][1][(1 +][ C] _[−][1][)]_ _[σ][2]_ + Lβk[2][(1 +][ C] _[−][1][)]_ _f_ (xk) 2 [+][ σ][2]
_≤_ _−_ [1]2 _[β][k][µ][∥∇][f]_ [(][x][k][)][∥][2] _nk_ _∥∇_ _∥[2]_ _nk_

 

1

= f (xk) _βk_ _f_ (xk) 2 [+][ β]k[2][(][L][ +][ µ][−][1][)(1 +][ C] _[−][1][)]_ _[σ][2]_ _._ (87)
_−_ 2 _[µ][ −]_ _[β][k][L][(1 +][ C]_ _[−][1][)]_ _∥∇_ _∥[2]_ _nk_

 

Then (87) combined with the assumption βk ≤ 4L(1+µC[−][1]) [implies (85).]

Lemma 6 suggests that the term related to the noise from gradient estimates is bounded as a secondorder term (i.e. O(βk[2][)][). Thus, with the diminishing stepsize, the effect of noise also diminishes.]
To establish the global convergence, we introduce the definition of a supermartingale following the
proofs in (Wang et al., 2017; Wei et al., 2021).

**Definition 1. Let {Fk} be an increasing sequence of σ-algebras. If {Xk} is a stochastic process**
_is called a supermartingale.satisfying (i) E[|Xk|] < ∞, (ii) Xk ∈Fk for all k, and (iii) E[Xk+1|Fk] ≤_ _Xk for all k, then {Xk}_

**Proposition 3 (Supermartingale convergence theorem, see, e.g., Theorem 4.2.12 in (Durrett, 2019)).**
_If_ _Xk_ _is a nonnegative supermartingale, then limk_ _Xk_ _X almost surely and E[X]_
E {[X0].} _→∞_ _→_ _≤_

Now, we prove the convergence theory of RST-AM in the nonconvex stochastic case.

**_Proof of Theorem 4. (i) Define φk :=_** _βk4µ_ _[∥∇][f]_ [(][x][k][)][∥]2[2] [and][ ˜]L := (L + µ[−][1])(1 + C _[−][1]), γk :=_

_f_ (xk)+ L[˜] _[σ]n[2]_ _∞i=k_ _[β]i[2][. Let][ F][k][ be the][ σ][-algebra measuring][ φ][k][, γ][k][,][ and][ x][k][. From (85) we know that]_

P


-----

for any k,


E[γk+1 _k] = E[f_ (xk+1) _k] + L[˜]_ _[σ][2]_
_|F_ _|F_ _n_


_βi[2]_
_i=k+1_

X


_∞_

_f_ (xk) 2 [+ ˜]L _[σ][2]_ _βi[2]_ [=][ γ][k] (88)
_≤_ _−_ 4[1] _[β][k][µ][∥∇][f]_ [(][x][k][)][∥][2] _n_ _i=k_ _[−]_ _[φ][k][,]_

X

Proposition 3 indicates that there exists afsome constantwhich implies that[low]] ≤ _γ0 −_ _f M[low]f > E<[ + 0γk. According to Definition 1,∞+1. As the diminishing condition (14) holds, we obtain −_ _f_ _[low]|Fk] ≤_ _γγk such that −_ _f_ _[low] {γ−k lim −φk. Sincefk_ _[low]}γ is a supermartingale. Therefore,k φ =k ≥ γ with probability 1, and0, we have E[f_ (xk 0)] ≤ ≤ EM[γf fork −
_→∞_
E[γ] ≤ E[γ0]. Note that from (88) we have E[φk] ≤ E[γk] − E[γk+1]. Thus,

_∞_ _∞_

E "k=0 _φk#_ _≤_ _k=0(E[γk] −_ E[γk+1]) < +∞,

X X


which further yields that
_∞_ _∞_

_φk =_ _[µ]_ _βk_ _f_ (xk) 2 _[<][ +][∞]_ [with probability][ 1][.] (89)

4 _∥∇_ _∥[2]_

_k=0_ _k=0_

X X

Since _k=0_ _[β][k][ = +][∞][, it follows that (16) holds.]_

(ii) If the noisy gradient is bounded, i.e.,

[P][∞]

Eξk [∥∇fξk (xk)∥2[2][]][ ≤] _[M][g][,]_ (90)
where Mg > 0 is a constant, then a stronger result can be obtained.

_ϵFor any give. Then if (17) does not hold, there must exist two infinite sequences of indices ϵ > 0, according to (16), there exist infinitely many iterates xk such that ∥∇si_ _,f_ (xtik) with∥2 ≤
_{_ _}_ _{_ _}_
_ti > si, such that for i = 0, 1, . . ., k = si + 1, . . ., ti −_ 1,
_∥∇f_ (xsi )∥2 ≥ 2ϵ, ∥∇f (xti )∥2 < ϵ, ∥∇f (xk)∥2 ≥ _ϵ._ (91)
Then from (89) it follows that


+∞

_i=0_

X


_ti−1_ +∞

_βk_ _f_ (xk) 2
_k=si_ _∥∇_ _∥[2]_ _[≥]_ _[ϵ][2]_ _i=0_

X X


_ti−1_

_βk with probability 1,_
_k=si_

X


_βk_ _f_ (xk) 2
_k=0_ _∥∇_ _∥[2]_ _[≥]_

X


+∞ _>_


which implies that
_ti−1_

_βk_ 0 with probability 1, as i + _._ (92)
_k=si_ _→_ _→_ _∞_

X

According to (77) and (72), we have
E[∥xk+1 − _xk∥2|xk]_
= E[∥Hkrk∥2|xk]

_≤_ 2 _βk[2]_ [(1 + 2][α]k[2] _[−]_ [2][α][k][) +][ α]k[2][δ]k[−][1] E[∥rk∥2|xk]
q

_≤_ _βk_  2(1 + C _[−][1])E[∥rk∥2|xk]_ 1 

_≤_ _βkp2(1 + C_ _[−][1])(E[1∥rk∥2[2][|][x][k][])]_ 2

2

_≤_ _βkp2(1 + C_ _[−][1])Mg_ _[,]_ (93)

where the last inequalities are due to Cauchy-Schwarz inequality and (90). Then it follows from (93)

p

that


_ti−1_

_βk,_
_k=si_

X


E[∥xti − _xsi_ _∥2] ≤_


2(1 + C _[−][1])M_


which together with (92) implies that _xti_ _xsi_ 2 0 with probability 1, as i + . Hence,
from the Lipschitz continuity of _f_, it follows that ∥ _−_ _∥_ _→_ _f_ (xti ) _f_ (xsi ) 2 0 → with probability∞
1 as i → +∞. However, this contradicts (91). Therefore, the assumption that (17) does not hold is ∇ _∥∇_ _−∇_ _∥_ _→_
not true.


-----

**_Proof of Theorem 5. According to (87) in Lemma 6, we have_**


1

2 _[µ][ −]_ _[β][k][L][(1 +][ C]_ _[−][1][)]_ E∥∇f (xk)∥2[2]

 


_N_ _−1_

_βk_

_k=0_

X


_N_ _−1_

_βk[2][(][L][ +][ µ][−][1][)(1 +][ C]_ _[−][1][)]_ _[σ][2]_ _,_ (94)

_nk_

_k=0_

X


_f_ (x0) _f_ _[low]_ +
_≤_ _−_


where the expectation is taken with respect to {Sj}j[N]=0[−][1][. Define]

1
def _βk_ 2 _[µ][ −]_ _[β][k][L][(1 +][ C]_ _[−][1][)]_
_PR(k)_ = Prob _R = k_ = _N_ 1 1 _, k = 0, . . ., N_ 1, (95)
_{_ _}_ _−_ _−_

_j=0_  [β][j] 2 _[µ][ −]_ _[β][j][L][(1 +][ C]_ _[−]_ [1][)]

then P   

_kN=0−1_ _[β][k]_ 12 _[µ][ −]_ _[β][k][L][(1 +][ C]_ _[−][1][)]_ E _f_ (xk) 2
E _f_ (xR) 2 = _N_ 1 1 _∥∇_ _∥[2]_
_∥∇_ _∥[2]_ _−_
P  j=0 _[β][j]_ 2 _[µ][ −]_ _[β][j][L][(1 +]_ _[ C][−][1][)]_ 
 

P   _k=0_ _[β]k[2][/n][k]_

_N_ 1 1 _._ (96)
_≤_ _[D][f][ +][ σ][2][(][L]−[ +][ µ][−][1][)(1 +][ C]_ _[−][1][)][ P][N]_ _[−][1]_
_j=0_ _[β][j]_ 2 _[µ][ −]_ _[β][j][L][(1 +][ C]_ _[−][1][)]_

_µ_ _D˜_
Let _D[˜] be a problem-independent constant. If we chooseP_   _βk = β := min_ 4L(1+C[−][1]) _[,]_ _σ√N_

_nk = n, then the definition of PR simplifies to PR(k) = 1/N_ . From (96) we have{ _[}][,][ and]_

E[ _f_ (xR) 2[]][ ≤] _[D][f][ +][ σ][2][(][L]N[ +]1[ µ][−][1][)(1 +][ C]_ _[−][1][)][ Nβ]n_ [2]
_∥∇_ _∥[2]_ _−_

_j=0_ _[β][(][ 1]2_ _[µ][ −]_ _[µ]4_ [)]

= _[D][f][ +][ σ][2]P[(][L][ +][ µ][−][1][)(1 +][ C]_ _[−][1][)][ ·][ Nβ]n_ [2]

_Nβ ·_ [1]4 _[µ]_

= Nβµ[4][D][f] [+][ σ][2][(][L][ +][ µ][−][1]1[)(1 +][ C] _[−][1][)][ ·][ β]_

4 _[nµ]_


4L(1 + C _[−][1])_


+ [4][σ][2][(][L][ +][ µ][−][1][)(1 +][ C] _[−][1][)]_

_nµ_

)

+ [4][σ][(][L][ +][ µ][−][1][)(1 +][ C] _[−][1][) ˜]D_

_nµ√N_


_≤_ [4]Nµ[D][f] [max]


_, [σ]_


4L(1 + C _[−][1])_


_≤_ [4]Nµ[D][f]


+ _[σ]_


= [16][D][f] _[L][(1 +][ C]_ _[−][1][)]_

_Nµ[2]_


+ [4(][L][ +][ µ][−][1][)(1 +][ C] _[−][1][) ˜]D_


4Df


Therefore, to ensure E[∥∇f (xR)∥2[2][]][ ≤] _[ϵ][, the number of iterations is][ O][(1][/ϵ][2][)][.]_

D EXPERIMENTAL DETAILS

Our main codes were written based on the PyTorch framework [1] and one GeForce RTX 2080 Ti
GPU was used for the tests in training neural networks. Our methods are ST-AM (MST-AM) and
the regularized version RST-AM.

D.1 EXPERIMENTAL DETAILS ABOUT ST-AM/MST-AM

The experiments about ST-AM were conducted to verify the main theorems, i.e. Theorem 1, Corollary 1 and Theorem 2. Four types of problems were used for the experiments. The conjugate residual
(CR) method is a short-term recurrence version of the full-memory GMRES and needs matrix-vector
products to fulfill the algorithm. We give the pseudocode of the CR method (Algorithm 6.20 in
(Saad, 2003)) here for readers who are not familiar with this numerical algorithm.

[1 Information about this framework can be found in https://pytorch.org.](https://pytorch.org)


-----

**Algorithm 4 CR Algorithm for solving a SPD linear system Ax = b.**

**InputOutput: x: x0 ∈ ∈RR[d][d].**

1: r0 = b − _Ax0, p0 = r0_
2: for k = 0, 1, . . ., until convergence, do

3: _αk =_ (Aprk[T]k)[Ar][T]Ap[k] _k_

4: _xk+1 = xk + αkpk_

6:5: _βrkk+1 = =rk[T] r+1rk[T]k −[Ar][Ar][k][k]α[+1]kApk_

7: _pk+1 = rk+1 + βkpk_

8: _Apk+1 = Ark+1 + βkApk_

9: end for

10: return xk


According to Theorem 1, the CR method is essentially equivalent to ST-AM for solving strongly
convex quadratic optimization. However, it should be pointed out that the CR method needs to
directly assess the matrix A, and the residual update (Line 5) is based on the linear assumption,
which makes it inapplicable for nonlinear problems or the case that A is unavailable. Also, even
though finite difference technique can be used to construct the matrix-vector products, the number
of gradient evaluations is twice of that of ST-AM.

D.1.1 STRONGLY CONVEX QUADRATIC OPTIMIZATION

To construct a case of (5), we considered the least squares problem:

min 2[,] (97)
_x_ R[d][ f] [(][x][) := 1]2 _[∥][Ax][ −]_ _[b][∥][2]_
_∈_


where A ∈ R[ℓ][×][d], b ∈ R[ℓ], which can be reformulated as a form of (5).

We generated a dense random matrix A R[500][×][100] and a dense random vector b R[500] for the
_∈_ 1 _∈_
test. The gradient descent used a fixed stepsize η ≤ _∥A∥2[2]_ [that can guarantee convergence, and the]

same η was used as the βk for the full-memory AM and ST-AM.

We also conducted additional tests about solving the problem (97) with different condition numbers,
where the eigenvalues of A were set to be uniformly distributed. The results with different condition
numbers (cond(A[T]A) = 10[2], 10[4], 10[6]) are shown in Figure 4. It can be observed that the curves
of CR, AM and ST-AM nearly coincide, which verify the correctness of Theorem 1. Also, since
the eigenvalues are uniformly distributed, the superlinear convergence behaviour may not happen.
Nonetheless, CR, AM and ST-AM are still much faster then the GD method.


10 1

10 2

10 3

10 4 GD: ||rk||2/||r0||2

10 5 CR: ||AM: ||rrkk||||22/||/||rr00||||22

10 6 ST-AM: ||rk||2/||r0||2

0 10 20 30 40 50

iteration


GD: ||rk||2/||r0||2

10 1 CR: ||AM: ||rrkk||||22/||/||rr00||||22

ST-AM: ||rk||2/||r0||2

10 2

10 3

10 4

0 10 20 30 40 50

iteration


GD: ||rk||2/||r0||2

10 1 CR: ||AM: ||rrkk||||22/||/||rr00||||22

ST-AM: ||rk||2/||r0||2

10 2

10 3

10 4

0 10 20 30 40 50

iteration


(a) cond(A[T]A) = 1 × 10[2]


(b) cond(A[T]A) = 1 × 10[4]


(c) cond(A[T]A) = 1 × 10[6]


Figure 4: Solving (97) with different condition numbers: cond(A[T]A) = λmax(A[T]A)/λmin(A[T]A).


-----

D.1.2 SOLVING A NONSYMMETRIC LINEAR SYSTEM

For the solution of a nonsymmetric linear system


_Ax = b,_ (98)

where A ∈ R[d][×][d], b ∈ R[d], the fixed-point iteration (FP) is xk+1 = g(xk) := xk + η(b − _Axk)._
Theorem 2 requires g to be a contractive map, i.e. ∥I − _ηA∥2 < 1. We used a test matrix “fidap029”_
from the Matrix Market [2]. This matrix is a banded matrix and not symmetric. We used Jacobi
preconditioner for all the tested iterative methods. Since solving linear systems is not the main
focus of this work, we did not do a thorough test of applying ST-AM to solve various nonsymmetric
linear systems.


D.1.3 CUBIC-REGULARIZED QUADRATIC MINIMIZATION

The concerned cubic-regularized quadratic minimization is


min 2 [+][ M] 2[,] (99)
_x_ R[d][ f] [(][x][) := 1]2 _[∥][Ax][ −]_ _[b][∥][2]_ 3
_∈_ _[∥][x][∥][3]_

where A ∈ R[ℓ][×][d], b ∈ R[ℓ], and M ≥ 0 is the regularization parameter. It can be computed that


_f_ (x) = A[T](Ax _b) + M_ _x_ 2x = (A[T]A + M _x_ 2I)x _A[T]b._
_∇_ _−_ _∥_ _∥_ _∥_ _∥_ _−_

Hence, for the gradient descent xk+1 = g(xk) := xk − _η∇f_ (xk), the Jacobian of g is I − _ηA[T]A −_
_ηM_ (∥x∥2[−][1][xx][T][ +][ ∥][x][∥][2][I][)][, which has][ ˆ]κ > 0 in the local region B(ρ).

We generated a dense random matrix A ∈ R[500][×][100] and a dense vector b ∈ R[500] for the test. The
official implementation of L-BFGS in PyTorch was used for comparison. The historical length m of
L-BFGS was 50, i.e., BFGS was actually used in the test.


10[0]


10[0]


10 1


10 1


|03 06 09 12 15|AM: M= AM: M= MST-AM MST-AM|1 0.01 : M=100 : M=1|
|---|---|---|
||MST-AM|: M=0.01|
||||

|03 06 09 12 15|AM: M AM: M ST-AM ST-AM|=1 =0.01 : M=100 : M=1|
|---|---|---|
||ST-AM|: M=0.01|
||||


10 20 30 40 50

AM: M=100
AM: M=1
AM: M=0.01
MST-AM: M=100
MST-AM: M=1
MST-AM: M=0.01

iteration

(a) MST-AM


10 20 30 40 50

AM: M=100
AM: M=1
AM: M=0.01
ST-AM: M=100
ST-AM: M=1
ST-AM: M=0.01

iteration

(b) ST-AM

|00 P : M = P : M = 1 P : M = Q : M Q : M Q : M 0 10 20 3 iteratio|Col2|
|---|---|
||100 1 0.01|
||= 100 = 1|
||= 0.01 0 40 50 n|

|00 P : M P : M 1 P Q : : M M Q : M Q : M 0 10 20 iterati|Col2|
|---|---|
||= 100 = 1 = 0.01|
||= 100 = 1|
||= 0.01 30 40 50 on|


(c) MST-AM


(d) ST-AM


Figure 5: Solving (99) with different M . (a) ∥rk∥2/∥r0∥2 of MST-AM; (b) ∥rk∥2/∥r0∥2 of ST-AM;
(c) _Pk_ 1ζk 2/ ∆xk 1 2 and _Qk_ 1ζk 2/ ∆rk 1 2 of MST-AM; (d) _Pk_ 1ζk 2/ ∆xk 1 2
_∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_
and _Qk_ 1ζk 2/ ∆rk 1 2 of ST-AM.
_∥_ _−_ _∥_ _∥_ _−_ _∥_

We also conducted the tests related to different regularization parameters M, and the cases that
_M = 0.01, 1, 100 are shown in Figure 5. Figure 5 shows that with the large M = 100, both AM and_
MST-AM converge faster. An ablation study was also conducted about the boundedness restriction
of _Pk_ 1ζk 2, _Qk_ 1ζk 2 in Theorem 2. In Figure 5(b), we show the convergence behaviour of
_∥_ _−_ _∥_ _∥_ _−_ _∥_
ST-AM without the boundedness check. In the case of the rather large regularization M = 100, STAM does not show a monotone decrease of the residual. To further investigate the cause, we plot the
magnitude of _Pk_ 1ζk 2/ ∆xk 1 2, _Qk_ 1ζk 2/ ∆rk 1 2 in Figure5(c) and Figure 5(d). It can
_∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_
be observed that the evolutions of _Pk_ 1ζk 2/ ∆xk 1 2, _Qk_ 1ζk 2/ ∆rk 1 2 are quite oscilla_∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_
tory in ST-AM, while being roughly bounded below 1 in MST-AM. This phenomenon may accounts
for the more stable convergence behaviour of MST-AM and verifies the necessity of the changes of
MST-AM compared with ST-AM. In our experiments, we also found the restarting period can be set
quite large and has little effect on ST-AM.

[2 https://math.nist.gov/MatrixMarket/.](https://math.nist.gov/MatrixMarket/)


-----

D.1.4 ROOT-FINDING PROBLEMS IN THE MULTISCALE DEEP EQUILIBRIUM MODEL

The multiscale deep equilibrium (MDEQ) model is a recent extension of the deep equilibrium (DEQ)
model (Bai et al., 2019) for computer vision. One of the central engines for these DEQ models is
the root-finding problem:

_fθ(z; x) = gθ(z; x)_ _z_ find z[∗] s.t. fθ(z[∗]; x) = 0, (100)
_−_ _⇒_

where θ and x are parameters and the input representation, respectively. In (Bai et al., 2020), the
Broyden’s method is employed to solve this problem. Since MST-AM is suitable for solving nonlinear systems of equations, we can apply MST-AM to solve (100).

We implemented the MST-AM method and integrated it into the MDEQ framework [3] . The task was
image classification on CIFAR-10 and we used the small model for test. We followed the suggested
experimental setting of the framework. The optimizer was Adam with learning rate of 0.001 and the
weight-decay was 2.5 _×_ 10[−][6]. The batch size was 128 and the number of epochs was 50. The tested
fixed-point solver was used for the forward root-finding process and for the backward root-finding
process. The threshold of the steps for the forward process was 18 and the threshold of the steps for
the backward process was 20.

We used the built-in AM method and Broyden’s method as the baseline methods. The m for AM
was 20, i.e. using the full-memory AM.

D.2 EXPERIMENTAL DETAILS ABOUT RST-AM

Our experiments on RST-AM focused on training neural networks. Since ST-AM can be regarded
as a special case of RST-AM with δk[(1)] = δk[(2)] = 0, the basic ST-AM is also covered. In the
Line 10 in Algorithm 1, αk should be adjusted to meet the positive definiteness check (13), which
can be simplified to (∆xk)[T]rk _βkµ_ _rk_ 2 [in practice. The adjustment of][ α][k] [can be (i)][ α][k] [=]
min{αk, 2βk(1 − _µ)/λk}, or (ii) ≥ αk = 0∥. We used the option (ii) since the violation of (13) seldom∥[2]_
happened in our tests and option (ii) is more simple to apply.

In the experiments on MNIST, Penn Treebank, we incorporated preconditioning (described in Appendix A.3) into the baseline method SAM and our method. Preconditioning is found to be effective
for difficult problems, e.g. mini-batch training with very small batch sizes and the scaling of the
model’s parameters being important.

D.2.1 HYPERPARAMETER SETTING OF RST-AM

The hyperparameters of RST-AM are easy to tune. The only hyperparameters that need to be carefully tuned are the regularization parameters c1, c2 in δk[(1)] and δk[(2)][. We found RST-AM is more]
sensitive to c1, possibly due to the fact that it influences the construction of secant equations. c2 can
be quite small in our tests. The hyperparameter C in δk[(2)] was set to be very small (C = 1 10[−][16])

_c2_ _rk_ 2 _×_
so as to ensure δk[(2)] = _pk∥_ 2[+]∥[ϵ][2][0][ almost all the time.][ ϵ][0][ is introduced to prevent the denominators]

_∥_ _∥[2]_
from being zero. We found _pk_ 2 > 0 and ∆xk 1 2 > 0 always held in the tests, so ϵ0 can be
_∥_ _∥_ _∥_ _−_ _∥_
omitted. In the experiments except for deterministic optimization on MNIST, we kept the setting
_c1 = 1, c2 = 1 × 10[−][7]_ unchanged and found such setting is quite robust.

The other hyperparameters are αk, βk, which can be always initially set as 1. So RST-AM has the
same number of hyperparameters to tune as SGDM, since SGDM needs to tune learning rate and
the momentum.

D.2.2 EXPERIMENTS ON MNIST

The experiments on MNIST [4] aimed to validate the effectiveness of RST-AM in deterministic optimization, so we were only concerned about the training loss by regarding it as a nonlinear function
to be optimized. To facilitate the full-batch training, we used a subset of the training dataset by
randomly selecting 10k images from the total 60k images.

[3 https://github.com/locuslab/mdeq.](https://github.com/locuslab/mdeq)
[4Based on the official PyTorch implementation https://github.com/pytorch/examples/blob/master/mnist.](https://github.com/pytorch/examples/blob/master/mnist)


-----

The baselines were SGDM, Adam, Adagrad, RMSprop and SAM. We tried our best to ensure that
the baselines had the best performance in the tests. We tuned the learning rates by log-scale gridsearches from 10[−][3] to 100. For SGDM, Adam, Adagrad, and RMSprop, the learning rates were
0.2, 0.001, 0.01, 0.001, respectively. For SAM, we used the hyperparameter setting recommended
in (Wei et al., 2021).

For RST-AM, we setxk+1 = xk + 0.2rk was used as the new update. c1 = 0.05, c2 = 1 × 10[−][7], αk = βk = 1. When (∆xk)[T]rk ≤ 0 occurs,

For the preconditioned RST-AM, we set c1 = 1 × 10[−][2], c2 = 1 × 10[−][7] for the Adagradpreconditioned RST-AM and c1 = 1 × 10[−][2], c2 = 1 × 10[−][8] for the RMSprop-preconditioned
RST-AM.

For all these tests, we trained the model for 200 epochs.


10[3]

10[0]

10 3

10 6

10 9


10[0]

10 2

10 4

10 6


50 100 150 200

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
|Adam RMSp|rop||||
|Adam RMSp|_SAM(2) rop_SAM(2)||||
|Adam RMSp|_RST-AM rop_RST-AM||||


Adam
RMSprop
Adam_SAM(2)
RMSprop_SAM(2)
Adam_RST-AM
RMSprop_RST-AM

epoch

(a) Train loss


0 50 100 150 200

|Col1|Col2|
|---|---|
|||
|||
|p||
|SAM(2) p_SAM(2)||
|RST-AM p_RST-AM||


Adam
RMSprop
Adam_SAM(2)
RMSprop_SAM(2)
Adam_RST-AM
RMSprop_RST-AM

epoch

(b) Squared norm of gradient


Figure 6: Training on MNIST with the Adam/RMSprop preconditioner.

In Figure 2 in the main paper, we report the RMSprop/Adagrad preconditioned SAM/RST-AM. We
give the result about using Adam as the preconditioner in Figure 6. It suggests that Adam does not
perform as well as the RMSprop method to serve as a preconditioner for SAM/RST-AM in this task.
Nevertheless, Adam RST-AM is still better than Adam and Adam SAM(2), which demonstrates the
effect of our proposed short-term recurrence scheme.


D.2.3 EXPERIMENTS ON CIFAR

This group of experiments were the same as those in (Wei et al., 2021) so that we can have a direct
comparison between RST-AM and SAM. We still give the details here for completeness.

We followed the same way of training ResNet (He et al., 2016). The batchsize was set to be 128.
For N iterations of training, the learning rate of each optimizer was decayed at the ( 2

(⌊ 4[3] _[N]_ _[⌋][)][-th iterations. Here, for SAM and RST-AM, the][ α][k][ and][ β][k][ serve as the learning rates, so]⌊_ _[N]_ _[⌋][)][-th and the]_

the learning rate decay denotes decaying αk, βk simultaneously. The experiments were run with 3
random seeds.

The baseline optimizers were SGDM, Adam, AdaBound (Luo et al., 2018), AdaBelief (Zhuang
et al., 2020), Lookahead (Zhang et al., 2019) and AdaHessian (Yao et al., 2021). Adam and the
recently proposed optimizers AdaBound and AdaBelief are adaptive learning rate methods which
use different learning rates for different model parameters. Lookahead is a k-step method and has
an inner optimizer. In each cycle of Lookahead, the inner-optimizer optim is used to iterate for k
steps and then the first iterate and the last iterate are interpolated to obtain the starting point of the
next cycle. Compared to Adam, AdaHessian uses Hessian-vector products to construct a diagonal
approximation of the Hessian.

For fair comparison, the hyperparameters of all the optimizers (including RST-AM) were tuned
through experiments on CIFAR-10/ResNet20. For each optimizer, the hyperparameter setting that
attained the highest final test accuracy on CIFAR-10/ResNet20 was kept unchanged for training
the other networks on CIFAR. We found the results of hyperparameter tuning were consistent with
those reported in (Wei et al., 2021). For completeness, we list the settings of hyperparameters here
(learning rate is abbreviated as lr, and “*” indicates the same setting as that in (Wei et al., 2021)).



_• SGDM[∗]: lr = 0.1, momentum = 0.9, weight-decay = 5 × 10[−][4], lr-decay = 0.1._


-----

90

80

70

60

50

40

30

|95 94 93 120 140 160 SGDM Adam AdaBound AdaBelief Lookahead Adahessian SAM(2) SAM(10) RST-AM|Col2|
|---|---|
|0 20 40|60 80 100 120 140 160 epochs|


95

94

93

120 140 160

SGDM
Adam
AdaBound
AdaBelief
Lookahead
Adahessian
SAM(2)
SAM(10)
RST-AM


90

80 95

94

70 93

92

120 140 160

60 SGDM

Adam

Test Accuracy % AdaBound

50 AdaBelief

Lookahead
Adahessian

40 SAM(2)

SAM(10)
RST-AM

0 20 40 60 80 100 120 140 160

epochs


(a) CIFAR-10/ResNet18

80

70

80

60

78

50 76

40 74

120 140 160

30 SGDM

Test Accuracy % Adam

AdaBound

20 AdaBelief

Lookahead

10 SAM(2)

SAM(10)
RST-AM

0

0 20 40 60 80 100 120 140 160

epochs


(c) CIFAR-100/ResNeXt50


(b) CIFAR-10/WRN16-4

80

70

60 80

78

50

76

40

120 140 160

SGDM

Test Accuracy %30 Adam

AdaBound
AdaBelief

20 Lookahead

SAM(2)

10 SAM(10)RST-AM

0 20 40 60 80 100 120 140 160

epochs


(d) CIFAR-100/DenseNet121


Figure 7: Test accuracy of training ResNet18 and WideResNet16-4 on CIFAR-10 and training
ResNeXt50 and DenseNet121 on CIFAR-100.

_• Adam[∗]: lr = 0.001, (β1, β2) = (0.9, 0.999), weight-decay = 5 × 10[−][4], lr-decay = 0.1._

_• AdaBound: lr = 0.001, (β1, β2) = (0.9, 0.999), final lr = 0.1, gamma = 0.001, weight-_
decay = 5 × 10[−][4], lr-decay = 0.1.

_• AdaBelief[∗]: lr = 0.001, (β1, β2) = (0.9, 0.999), eps = 1_ _×_ 10[−][8], weight-decay = 5 _×_ 10[−][4],
lr-decay = 0.1.

_• Lookahead[∗]: optim: SGDM (lr = 0.1, momentum = 0.9, weight-decay = 1 × 10[−][3]),_
_α = 0.8, steps = 10, lr-decay = 0.1._

_• AdaHessian[∗]: lr = 0.15, (β1, β2) = (0.9, 0.999), eps=1_ _×_ 10[−][4], hessian-power: 1, weightdecay = 5 × 10[−][4]/0.15, lr-decay = 0.1.

_• SAM(2): optim: SGDM (lr = 0.1, momentum = 0, weight-decay = 1.5 × 10[−][3]), αk =_
1.0, βk = 1.0, c1 = 0.01, p = 1, m = 2, weight-decay = 1.5 × 10[−][3], lr-decay = 0.06.

_• SAM(10)[∗]: optim: SGDM (lr = 0.1, momentum = 0, weight-decay = 1.5 × 10[−][3]), αk =_
1.0, βk = 1.0, c1 = 0.01, p = 1, m = 10, weight-decay = 1.5 × 10[−][3], lr-decay = 0.06.

_• RST-AM: c1 = 1, c2 = 1 × 10[−][7], α0 = β0 = 1, weight-decay = 1 × 10[−][3], lr-decay = 0.1._


For the tests of SGDM, Adam, AdaBelief, Lookahead, AdaHessian and SAM(10), we also had
consistent numerical results with those reported in (Wei et al., 2021), so we reported their results of
these methods in the main paper for reference.

Figure 7 shows the curves of test accuracy for training ResNet18 and WRN16-4 on CIFAR-10 and
training ResNeXt50 and DenseNet121 on CIFAR-100. The numerical results of final test accuracy
can be found in Table 1(a) in the main paper. It can be found in Figure 7 that the convergence
behaviour of RST-AM is less erratic than SAM(10) and SAM(2). In the first 80 epochs, RST-AM


-----

converges faster than SAM, partly due to using a smaller weight decay. The learning rate schedule
has a large impact on the convergence of each optimizer. Similar to SAM(10), RST-AM can always
climb up and stabilize to a higher test accuracy in the final 40 epochs. It is found that RST-AM is
comparable to SAM(10), while improving the short-memory SAM(2).


100

90

80

70

SGDM

60 RST-AM: c2=0.1

RST-AM: c2=0.001

Train Accuracy % 50 RST-AM: c2=10 5

RST-AM: c2=10 7

40 RST-AM: c2=10 9

0 20 40 60 80 100 120 140 160

epochs


90

80

70

SGDM

60 RST-AM: c2=0.1

RST-AM: c2=0.001

Test Accuracy %50 RST-AM: c2=10 5

RST-AM: c2=10 7

40 RST-AM: c2=10 9

0 20 40 60 80 100 120 140 160

epochs


(a) Train accuracy


(b) Test accuracy


Figure 8: Train accuracy and test accuracy of RST-AM with different c2.


100

90

80

70

60 SGDM

Train Accuracy % 50 RST-AM: c1=10RST-AM: c1=1

40 RST-AM: c1=0.1

RST-AM: c1=0.01

0 20 40 60 80 100 120 140 160

epochs


90

80

70

60

SGDM

est Accuracy %T50 RST-AM: c1=10RST-AM: c1=1

RST-AM: c1=0.1

40 RST-AM: c1=0.01

0 20 40 60 80 100 120 140 160

epochs


(a) Train accuracy


(b) Test accuracy


Figure 9: Train accuracy and test accuracy of RST-AM with different c1.


100

90

80

70

60 SGDM

RST-AM: wd=0.0005

Train Accuracy % 50 RST-AM: wd=0.0008

RST-AM: wd=0.001

40 RST-AM: wd=0.0015

0 20 40 60 80 100 120 140 160

epochs


90

80

70

60 SGDM

RST-AM: wd=0.0005

Test Accuracy % 50 RST-AM: wd=0.0008

RST-AM: wd=0.001

40 RST-AM: wd=0.0015

0 20 40 60 80 100 120 140 160

epochs


(a) Train accuracy


(b) Test accuracy


Figure 10: Train accuracy and test accuracy of RST-AM with different weight-decays.

Since our hyperparameter tuning was conducted on CIFAR10/ResNet20, we give some results about
the hyperparameters on this model.

**Effect of c2 in RST-AM. Figure 8 shows the effect of c2, where we kept c1 = 1, weight-decay=5 ×**
10[−][4] fixed, and c2 = 10[−][1], 10[−][3], 10[−][5], 10[−][7], 10[−][9]. It indicates that RST-AM is not sensitive to
_c2._

**Effect of c1 in RST-AM. Figure 9 shows the effect of c1, where we kept c2 = 10[−][7], weight-**
decay=5×10[−][4] fixed, and c1 = 0.01, 0.1, 1, 10. It suggests that with smaller c1, RST-AM converges
faster in terms of train accuracy, but the test accuracy is worse.


-----

**Effect of weight decay in RST-AM. Weight-decay is a common hyperparameter that can affect the**
generalization of each optimizer. In Figure 10, we show the convergence behaviour of RST-AM
with different weight-decays. It suggests that with a too small weight-decay, RST-AM tends to be
overfitting in the test dataset.


90

80

70


90

80


70

60

50


60

50


|95 94.78 94.95 95.21 94.90 94 93 92 60 80 100 120 140 160 SGDM: epoch=80 SGDM: epoch=100 SGDM: epoch=160 SAM: epoch=80 SAM: epoch=100 SAM: epoch=160 RST-AM: epoch=80 RST-AM: epoch=100 RST-AM: epoch=160|Col2|
|---|---|
|0 20 40 6|0 80 100 120 140 160 epochs|


(b) Test accuracy on CIFAR10/WideResNet16-4


40

80

70

60

50

40

30

20

10

|95.27 95 94.93 94.53 94.82 94 93 92 60 80 100 120 140 160 SGDM: epoch=80 SGDM: epoch=100 SGDM: epoch=160 SAM: epoch=80 SAM: epoch=100 SAM: epoch=160 RST-AM: epoch=80 RST-AM: epoch=100 RST-AM: epoch=160|Col2|
|---|---|
|0 20 40 6|0 80 100 120 140 160 epochs|


(a) Test accuracy on CIFAR10/ResNet18


80

70

50

|80.36 80 79.44 79 78.90 78.49 78 77 60 80 100 120 140 160 SGDM: epoch=80 SGDM: epoch=100 SGDM: epoch=160 SAM: epoch=80 SAM: epoch=100 SAM: epoch=160 RST-AM: epoch=80 RST-AM: epoch=100 RST-AM: epoch=160|Col2|
|---|---|
|0 20 40 60|80 100 120 140 160 epochs|



(d) Test accuracy on CIFAR100/DenseNet121

|80 79.53 79 78.96 78.39 78 78.41 77 76 60 80 100 120 140 160 SGDM: epoch=80 SGDM: epoch=100 SGDM: epoch=160 SAM: epoch=80 SAM: epoch=100 SAM: epoch=160 RST-AM: epoch=80 RST-AM: epoch=100 RST-AM: epoch=160|Col2|
|---|---|
|0 20 40 60|80 100 120 140 160 epochs|


(c) Test accuracy on CIFAR100/ResNeXt50


Figure 11: Training deep neural networks for 80,100,160 epochs. The results of final test accuracy
of RST-AM for training 80,100,160 epochs and the final test accuracy of SGDM for training 160
epochs are shown in the nested figures for comparison.

Table 5: The cost and final test accuracy compared with SGDM. The notations “m”,“t/e”, “e”,
“t” and “a” are abbreviations of memory, per-epoch time, training epochs, total running time, and
accuracy, respectively. “*” indicates numbers published in (Wei et al., 2021).


Cost (× SGDM) CIFAR10/ResNet18 CIFAR10/WRN16-4
& accuracy m t/e e t a(%) m t/e e t a(%)

SGDM[∗] 1.00 1.00 1.00 1.00 94.82 1.00 1.00 1.00 1.00 94.90
SAM(10)[∗] 1.73 1.78 0.56 1.00 94.81 1.26 1.28 0.63 0.80 94.94
RST-AM 1.05 1.46 0.56 0.82 94.84 1.03 1.14 0.63 0.71 94.95

Cost (× SGDM) CIFAR100/ResNeXt50 CIFAR100/DenseNet121
& accuracy m t/e e t a(%) m t/e e t a(%)

SGDM[∗] 1.00 1.00 1.00 1.00 78.41 1.00 1.00 1.00 1.00 78.49
SAM(10)[∗] 1.30 1.16 0.50 0.58 78.37 1.16 1.19 0.50 0.60 78.84
RST-AM 1.04 1.07 0.50 0.54 78.39 1.01 1.11 0.50 0.55 78.90

Table 1(a) in the main paper reports the test accuracy of each optimizer when training for the same
epochs. In fact, as shown in Figure 11, within 100 epochs, RST-AM can achieve a better test


-----

accuracy than SGD. So if the running time of the training process matters, it is expected that RSTAM can use less total running time due to the large number of reduction in training epochs. In
Table 1(b), we set SGD as the baseline, and compare the computation and memory cost with SGD.
In Table 5, we give more details about the saving in training epochs and the final test accuracy. It
can be seen that RST-AM can achieve a comparable or better test accuracy than SGDM with less
computation time, while reducing the memory footprint of SAM.

We also tried using Adam as a preconditioner for RST-AM but found the final test accuracy was
often worse. For example, the test accuracy of Adam RST-AM for CIFAR-10/ResNet20 is only
90.79%. We suppose it is the worse generalization ability of Adam (Luo et al., 2018) that makes
Adam not suitable as a preconditioner for RST-AM in the image classification task.

D.2.4 EXPERIMENTS ON PENN TREEBANK

This group of experiments were the same as those in (Wei et al., 2021) for direct comparison. Results
in Table 2 were measured with 3 random seeds. The parameter settings of the LSTM models were
the same as those in (Zhuang et al., 2020; Wei et al., 2021). The baseline optimizers were SGDM,
Adam, AdaBelief, and SAM. The validation dataset was used for tuning hyperparameters.

For SGDM, the learning rate (abbr. lr) was tuned via grid-search in {1, 10, 30, 100}. We set lr = 10,
momentum = 0.9 for the 2-layer/3-layer LSTM, and lr = 30, momentum = 0 for the 1-layer LSTM.

For Adam, the learning rate was tuned via grid-search in {1×10[−][3], 2×10[−][3], 5×10[−][3], 8×10[−][3], 1×
10[−][2], 2 × 10[−][2]}. We found the setting that lr = 5 × 10[−][3] performs best.

For AdaBelief, we set lr = 5 × 10[−][3] which is better than the recommended settings of the official
implementation[5].

For SAM, we used the recommended setting in (Wei et al., 2021) and used the baseline Adam as the
preconditioner. The cases m = 2 and m = 20 are denoted as SAM(2) and SAM(10), respectively.

For our method RST-AM, we kept c1 = 1, c2 = 1 × 10[−][7] unchanged and used the same preconditioner (Adam) as SAM.

The batch size was 20. We trained the model for 500 epochs and the learning rate was decayed by
0.1 at the 250th epoch and the 375th epoch.

Table 6: The cost to achieve comparable results of Adam. The notations “m”,“t/e” and “t” are
abbreviations of memory, per-epoch time and total running time, respectively.

Cost 1-Layer 2-Layer 3-Layer
(× Adam) m t/e t m t/e t m t/e t

Adam 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
SAM(10) 1.20 1.84 0.74 1.36 1.88 0.75 1.90 1.67 0.67
RST-AM 1.06 1.73 0.69 1.15 1.78 0.71 1.11 1.53 0.61

Table 7: Test perplexity of training 1,2,3-layer LSTM on Penn Treebank for 200 epochs. Lower is
better. “*” indicates numbers published in (Wei et al., 2021).

Method 1-Layer 2-Layer 3-Layer

Adam[∗] 80.88±.15 64.54±.18 60.34±.22
SAM(2) 81.82±.09 66.62±.26 61.55±.11
SAM(10) **79.30±.12** **63.21±.02** 59.47±.07
RST-AM 79.49±.11 63.61±.26 **59.34±.23**

In Table 6, we report the memory footprint and per-epoch running time of SAM(10) and RST-AM
compared with Adam. It indicates that RST-AM also largely reduces the memory overhead of the

[5https://github.com/juntang-zhuang/Adabelief-Optimizer/tree/update 0.2.0/PyTorch Experiments/LSTM.](https://github.com/juntang-zhuang/Adabelief-Optimizer/tree/update_0.2.0/PyTorch_Experiments/LSTM)


-----

long-memory SAM(10) in the language modeling task. Since the batch size is very small, the cost of
stochastic gradient evaluation is quite cheap, which makes the additional cost of matrix computation
in RST-AM and SAM(10) become considerable. However, if we consider achieving a comparable
validation/test perplexity to that of Adam, RST-AM does not need to train for the same number of
epochs as Adam. Table 7 shows the test perplexity of Adam, SAM(2), SAM(10), and RST-AM for
training 200 epochs. By comparing the results of Table 2 and Table 7, we see RST-AM is better than
Adam with much fewer training epochs, thus RST-AM can save a large amount of training time, as
shown in Table 6.

D.2.5 EXPERIMENTS ON ADVERSARIAL TRAINING

The adversarial training considers the empirical adversarial risk minimization (EARM) problem:

_T_

1
min max _ξi_ [(][x][)][,] (101)
_x∈R[d]_ _T_ _i=1_ _∥ξ[¯]i−ξi∥2≤ϵ_ _[f]_ [¯]

X

where _ξ[¯]i is the i-th adversarial data within the ϵ-ball centered at ξi. We followed the standard PGD_
adversarial training in (Madry et al., 2018), using projection gradient descent (PGD) to solve the
inner maximization problem and the tested optimizers (SGD and RST-AM) to solve the outer minimization problem. The experiments were conducted on CIFAR10/ResNet18, CIFAR10/WRN16-4,
CIFAR100/ResNet18, and CIFAR100/DenseNet121. We trained the neural networks for 200 epochs
and decayed the learning rate at the 100th and 150th epoch.

Since adversarial training is much more time consuming than the ordinary training in Section D.2.3,
we tuned the hyperparameters in CIFAR10/ResNet20, and applied the same hyperparameters to
other models. We found the setting that c1 = 1, c2 = 1 × 10[−][7] and weight-decay = 0.001 is still
suitable for this task.

The CIFAR-10 (CIFAR-100) dataset contains 50K images for training and 10K images for testing.
Since it is found that the phenomenon of overfitting is severer (Rice et al., 2020) in adversarial
training, we randomly selected 5K images from the total 50K training dataset as the validation
dataset (the other 45K images remained as the training dataset), and chose the best checkpoint
model in the validation set to evaluate on the test dataset. We consider two types of test accuracy:
(i) the clean test accuracy, where clean data was used for model evaluation;
(ii) the robust test accuracy, where corrupted data was used for model evaluation. The attacking
methods are FGSM (Goodfellow et al., 2014), PGD-20 (Madry et al., 2018), and C&W attack
_∞_
(Carlini & Wagner, 2017).

Table 8: Test accuracy (%) for adversarial training.

CIFAR10/ResNet18 CIFAR100/DenseNet121
Optimizer
Clean FGSM PGD-20 C&W Clean FGSM PGD-20 C&W
_∞_ _∞_

SGD 82.16 63.23 51.91 50.22 59.45 39.76 30.92 29.00
RST-AM 82.53 63.78 52.43 50.52 60.48 40.41 31.20 29.52


CIFAR10/WRN16-4 CIFAR100/ResNet18
Optimizer
Clean FGSM PGD-20 C&W Clean FGSM PGD-20 C&W
_∞_ _∞_

SGD 80.84 60.97 49.29 47.62 55.42 36.17 28.18 26.31
RST-AM 81.36 61.38 49.93 47.95 56.49 37.00 28.50 26.66

In Tabel 8, we report the average results of tests with three different random seeds. It shows that
RST-AM can achieve both higher clean test accuracy and higher robust test accuracy across various
models on CIFAR10/CIFAR100. To see the convergence behaviour of SGD and RST-AM, we plot
the curves of the clean validation accuracy and the PGD-10 attacked validation accuracy in Figure 12. We can observe the phenomenon of robust overfitting (Rice et al., 2020) from these curves,
which justifies our experimental setting with validation set for the checkpoint selection. Nonetheless,
the numerical results suggest that RST-AM can still be better than SGDM in adversarial training.

It is also found that due to the heavy cost of gradient evaluations in PGD adversarial training, the
additional computational cost incurred by RST-AM is negligible and the per-epoch running time of
SGD and RST-AM is roughly the same. So we do not report it here.


-----

SGDM

80 RST-AM

70

60

50

40

Validation Accuracy %

30

0 25 50 75 100 125 150 175 200

epochs


55

SGDM

50 RST-AM

45

40

35

30

25

Adv-Validation Accuracy %

20

0 25 50 75 100 125 150 175 200

epochs


(a) CIFAR10/ResNet18

SGDM

80

RST-AM

70

60

50

40

Validation Accuracy %

30

0 25 50 75 100 125 150 175 200

epochs


(c) CIFAR10/WRN16-4


(b) CIFAR10/ResNet18

50 SGDM

RST-AM

45

40

35

30

25

Adv-Validation Accuracy %

0 25 50 75 100 125 150 175 200

epochs


(d) CIFAR10/WRN16-4


SGDM

50 RST-AM

40

30

20

Validation Accuracy %

10

0 25 50 75 100 125 150 175 200

epochs


SGDM

25 RST-AM

20

15

10

Adv-Validation Accuracy %

5

0 25 50 75 100 125 150 175 200

epochs


(e) CIFAR100/ResNet18

60

SGDM
RST-AM

50

40

30

20

Validation Accuracy %

10

0 25 50 75 100 125 150 175 200

epochs


(g) CIFAR100/DenseNet121


(f) CIFAR100/ResNet18

30 SGDM

RST-AM

25

20

15

10

Adv-Validation Accuracy %

5

0 25 50 75 100 125 150 175 200

epochs


(h) CIFAR100/DenseNet121


Figure 12: Clean accuracy and PGD-10 attacked accuracy on the validation set in training different
neural networks.


-----

D.2.6 EXPERIMENTS ON TRAINING A GENERATIVE ADVERSARIAL NETWORK

We describe our setting of training a generative adversarial network (GAN) here. Like the adversarial training, the GAN training process is also a min-max problem. The stability of an optimizer
is critical for the training process. To demonstrate the applicability of RST-AM, we conducted
experiments on a GAN which was equipped with spectral normalization (Miyato et al., 2018) (SNGAN). The experimental setting was the same as that of AdaBelief (Zhuang et al., 2020): the dataset
was CIFAR-10; ResNets were used as the generator and the discriminator networks; the steps for
optimization in the discriminator and the generator per iteration were 5 and 1, respectively; the minbatch size was 64 and the total iteration number was 100000. The Frechet Inception Distance (FID)
(Heusel et al., 2017) was used as the evaluation metric: lower FID score means better accuracy.

The baseline optimizer were Adam and AdaBelief as they perform well in this task (Zhuang et al.,
2020). We also used the recommended hyperparameter settings for the two optimizers. For our
RST-AM method, due to the ill-conditioning of the min-max problem, we used the AdaBelief as the
preconditioner and set the damping parameter αk = 0.6. The regularization parameters c1 = 1 and
_c2 = 1 × 10[−][7]_ were still kept unchanged just as the previous experiments.

45 Adam

AdaBelief

40 RST-AM

35

30

FID score 25

20

15

0 20000 40000 60000 80000 100000

iteration


Figure 13: FID score for training SN-GAN on CIFAR-10.

In Figure 13, we show the curve of FID score for each optimizer, which is the average of three
independent runs. It indicates that the RST-AM method is stable for this min-max optimization
problem.

Table 9: The effect of αk for RST-AM.

Method Adam AdaBelief _α = 0.8_ _α = 0.6_ _α = 0.5_ _α = 0.4_ _α = 0.2_ _α = 0.1_

FID score 13.07 12.80 12.48 **12.05** 12.75 13.13 13.07 12.59

Since we only tuned the damping parameter αk for RST-AM, we report the FID scores of other
choices of αk in Table 9 during our experiment. It shows that even with the suboptimal choices of
_αk, e.g. αk = 0.1, 0.5, 0.8, RST-AM can still outperform the baselines._

D.3 ADDITIONAL EXPERIMENTS

To further test the performance of our method in training neural networks on larger datasets or
different models, we conducted additional experiments of the image classification task in ImageNet
(Deng et al., 2009) and the Transformer (Vaswani et al., 2017) based neural machine translation task
in the IWSTL14 DE-EN (Cettolo et al., 2014) dataset.

D.3.1 EXPERIMENTS ON IMAGENET

We trained ResNet50 on ImageNet with SGDM and RST-AM. We used the built-in ResNet50 model
in PyTorch. We ran the tests of each optimizer with three random seeds and four GeForce RTX 2080


-----

Ti GPUs were used for each test. The hyperparameters of SGDM were set as the recommended
setting in PyTorch. For RST-AM, the hyperparameters were kept the same as those in the CIFAR
experiments. The weight-decay was 1 × 10[−][4]. The number of the total training epochs is 90. The
learning rate decay of SGD was at the 30th and 60th epochs. For RST-AM, since the experiments
on CIFAR show it can often converge faster to an acceptable solution than SGDM, we adopted the
early learning rate decay strategy recommended in (Zhang et al., 2019): decay the αk and βk for
RST-AM at the 30th, 50th and 70th epochs.

Table 10: TOP 1 test accuracy (%) w.r.t. epoch, the best TOP1 test accuracy (%), and the cost. The
memory, per-epoch time and total time of SGDM are set as the units. The total time is the time to
first achieve the accuracy ≥ 75.90%.

Method epoch = 72 epoch = 88 epoch = 90 best memory per-epoch time total time

SGDM 75.75 75.90 75.81 75.93±.15 1.00 1.00 1.00
RST-AM 75.90 75.95 75.98 76.04±.06 1.06 1.01 0.83

In Table 10, we report the TOP1 accuracy in the validation dataset during training. Note that we also
report the epoch number for each optimizer to achieve the accuracy equal or exceeding 75.90%. It
shows RST-AM needs 72 epochs while SGDM needs 88 epochs. The curves of the training accuracy
and test accuracy are shown in Figure 14. The results suggest that RST-AM is still a competitive
optimizer in training a larger model in a larger dataset.

|Col1|75|
|---|---|
||75|
|||

|20 40 epochs|60 80|
|---|---|


80

SGDM

70 RST-AM

60

50

40

30

Train Accuracy %

20

10

0 20 40 60 80

epochs


SGDM

70 RST-AM

77

60 76 75.98

75.81

50 75

74

40 50 60 70 80 90

Test Accuracy % 30

20

10

0 20 40 60 80

epochs


(a) Train accuracy


(b) Test accuracy


Figure 14: Train and test accuracy for training ImageNet/ResNet50.

D.3.2 TRAINING TRANSFORMER

We conducted the neural machine translation task with Transformer. We implemented our RST-AM
method and integrated it into the fairseq framework [6]. The basic experimental setting was set as
the recommended setting in (Yao et al., 2021). We trained the model for 50 epochs and the BLEU
(Papineni et al., 2002) score was calculated using the average model of the last five checkpoints. We
added a baseline optimizer RAdam (Liu et al., 2019) which was inspired by the warmup procedure
in training Transformer.

Table 11: The BLEU score of training Transformer on IWSLT14.

Method SGD Adam AdaBelief RAdam RST-AM

BLEU score 28.14±.08 35.71±.03 35.15±.14 35.60±.06 **35.89±.02**

[6 https://github.com/pytorch/fairseq.](https://github.com/pytorch/fairseq)


-----

The numerical results reported in Table 11 show that Adam is well-suited for this neural machine
translation task, though it does not perform well in the image classification task. Also, the results
demonstrate that RST-AM can still outperform Adam in this task.

Table 12: BLEU score evaluated at the 40/45/50-th epoch, and the cost. The memory, per-epoch
time and total time of Adam are set as the units. The total time is the time of RST-AM to achieve
a BLEU score matching the final BLEU of Adam: | BLEU(RST-AM) − BLEU(Adam) | ≤ 0.03,
where 0.03 is the standard deviation of the results of Adam.

Method epoch = 40 epoch = 45 epoch = 50 memory per-epoch time total time

Adam 35.42±.10 35.54±.08 35.71±.03 1.00 1.00 1.00
RST-AM **35.59±.14** **35.69±.06** **35.89±.02** 1.16 1.00 0.90

In Table 12, we report the BLEU scores evaluated in the test dataset at the 40th, 45th and 50th epochs.
The results show the better performance of RST-AM over Adam and the per-epoch computational
cost is nearly the same. To achieve a comparable solution to Adam, RST-AM can save 10% training
time.


-----

