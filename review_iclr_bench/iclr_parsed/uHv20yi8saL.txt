# MONOTONIC IMPROVEMENT GUARANTEES UNDER NON-STATIONARITY FOR DECENTRALIZED PPO

**Anonymous authors**
Paper under double-blind review

ABSTRACT

We present a new monotonic improvement guarantee for optimizing decentralized
policies in cooperative Multi-Agent Reinforcement Learning (MARL), which
holds even when the transition dynamics are non-stationary. This new analysis
provides a theoretical understanding of the strong performance of two recent
actor-critic methods for MARL, i.e., Independent Proximal Policy Optimization
(IPPO) (Schröder de Witt et al., 2020) and Multi-Agent PPO (MAPPO) (Yu et al.,
2021), which both rely on independent ratios, i.e., computing probability ratios
separately for each agent’s policy. We show that, despite the non-stationarity that
independent ratios cause, a monotonic improvement guarantee still arises as a
result of enforcing the trust region constraint over joint policies. We also show
this trust region constraint can be effectively enforced in a principled way by
bounding independent ratios based on the number of agents in training, providing
a theoretical foundation for proximal ratio clipping. Moreover, we show that the
surrogate objectives optimized in IPPO and MAPPO are essentially equivalent
when their critics converge to a fixed point. Finally, our empirical results support
the hypothesis that the strong performance of IPPO and MAPPO is a direct result
of enforcing such a trust region constraint via clipping in centralized training, and
the good values of the hyperparameters for this enforcement are highly sensitive to
the number of agents, as predicted by our theoretical analysis.

1 INTRODUCTION

In cooperative multi-agent reinforcement learning (MARL), a team of agents must coordinate their
behavior to maximize a single cumulative return (Panait & Luke, 2005). In such a setting, partial
observability and/or communication constraints necessitate the learning of decentralized policies that
condition only on the local action-observation history of each agent. In a simulated or laboratory
setting, decentralized policies can often be learned in a centralized fashion, i.e., Centralized Training
with Decentralized Execution (CTDE)(Oliehoek & Amato, 2016), which allows agents to access
each other’s observations and unobservable extra state information during training.

Actor-critic algorithms (Konda & Tsitsiklis, 2000) are a natural approach to CTDE because critics
can exploit centralized training by conditioning on extra information not available to the decentralized
policies (Lowe et al., 2017; Foerster et al., 2017). Unfortunately, such actor-critic methods have long
been outperformed by value-based methods such as QMIX (Rashid et al., 2018) on MARL benchmark
tasks such as Starcraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019). However, two
recent actor-critic algorithms (Schröder de Witt et al., 2020; Yu et al., 2021) have upended this
ranking by outperforming previously dominant MARL methods, such as MADDPG (Lowe et al.,
2017) and value-decomposed Q-learning (Sunehag et al., 2017; Rashid et al., 2018). Both algorithms
are multi-agent extensions of Proximal Policy Optimization (PPO) (Schulman et al., 2017) but one
uses decentralized critics, i.e., independent PPO (IPPO) (Schröder de Witt et al., 2020), and the other
uses centralized critics, i.e., multi-agent PPO (MAPPO) (Yu et al., 2021).

One key feature of PPO-based methods is the use of ratios (between the policy probabilities before
and after updating) in the objective. Both IPPO and MAPPO extend this feature of PPO to the
multi-agent setting by computing ratios separately for each agent’s policy during training, which we
call independent ratios. Unfortunately, until now there has been no theoretical justification for the
use of such independent ratios. In this paper we show that the analysis that underpins the monotonic


-----

policy improvement guarantee for PPO (Schulman et al., 2015) does not carry over to the use of
independent ratios in IPPO and MAPPO. Instead, a direct application of this analysis leads to a joint
policy optimization and suggests the use of joint ratios, i.e., computing ratios between joint policies.

The difference is crucial because, based on the existing trust region analysis for PPO, only a joint
ratios approach enjoys a monotonic policy improvement guarantee. Moreover, as independent ratios
consider only the change in one agent’s policy and ignore the fact that the other agents’ policies also
change, the transition dynamics underlying these independent ratios are non-stationary (Papoudakis
et al., 2019), breaking the assumptions in the monotonic improvement analysis (Schulman et al.,
2015). While some studies attempt to extend the monotonic improvement analysis to MARL (Wen
et al., 2021; Li & He, 2020), they primarily consider optimizing policies with joint ratios, rather than
independent ratios as in our paper, and are thus not applicable to IPPO or MAPPO.

To address this gap, we provide a new monotonic improvement analysis that holds even when the
transition dynamics are non-stationary. We show that, despite this non-stationarity, a monotonic
improvement guarantee still arises as a result of enforcing the trust region constraint over joint
_policies, i.e., a centralized trust region constraint. In other words, constraining the update of joint_
policies in centralized training addresses the non-stationarity of learning decentralized policies. This
new analysis implies that independent ratios can also enjoy the same performance guarantee as joint
ratios if the centralized trust region constraint is properly enforced by bounding independent ratios.
In this way both IPPO and MAPPO can guarantee monotonic policy improvement. We also provide
a theoretical foundation for proximal ratio clipping by showing that centralized trust region can
be enforced in a principled way by bounding independent ratios based on the number of agents in
training. Furthermore, we show that the surrogate objectives optimized in IPPO and MAPPO are
essentially equivalent when their critics converge to a fixed point.

Finally, we provide empirical results that support the hypothesis that the strong performance of IPPO
and MAPPO is a direct result of enforcing such a trust region constraint via clipping in centralized
training. Particularly, we show that good values of the hyperparameters for the clipping range are
highly sensitive to the number of agents, as these hyperparameters, together with the number of
agents, effectively determine the size of the centralized trust region. Moreover, we show that IPPO
and MAPPO have comparable performance on SMAC maps with varied difficulty and numbers of
agents. This comparable performance also implies that the way of training critics could be less crucial
in practice than enforcing a trust region constraint.

2 RELATED WORK

The use of trust region optimization in MARL traces back to parameter-sharing TRPO (PSTRPO) (Gupta et al., 2017), which combines parameter sharing with TRPO for cooperative multiagent continuous control but provides no theoretical support. Our analysis showing that a trust region
constraint is pivotal to guarantee performance improvement in MARL applies to PS-TRPO, among
other algorithms. Multi-agent trust region learning (MATRL) (Wen et al., 2021) uses a trust region
for independent learning with a game-theoretical analysis in the policy space. MATRL considers independent learning and proposes to enforce a trust region constraint by approximating the stable fixed
point via a meta-game. Despite the improvement guarantee for joint policies, solving a meta-game
itself can be challenging because its complexity increases exponentially in the number of agents.
We instead consider centralized learning and enforce the trust region constraint in a centralized and
scalable way. Multi-Agent TRPO (MATRPO) directly extends TRPO to the multi-agent case (Li &
He, 2020) and divides the trust region by the number of agents. However, the analysis assumes a
private reward for each agent, which yields different theoretical results from ours. Non-stationarity
has been discussed in multi-agent mirror descent with trust region decomposition (Li et al., 2021),
which first decomposes the trust region for each decentralized policy and then approximates the
KL divergence through additional training. However, this method needs to learn a fully centralized
action-value function and thus becomes becomes impractical when there are many agents.

3 BACKGROUND

**Dec-MDPs.** We consider a fully cooperative multi-agent task in which a team of cooperative agents
choose sequential actions in a stochastic environment. It can be modeled as a decentralized Markov


-----

_decision process (Dec-MDP), defined by a tuple {N_ _, S, A, P, r, ρ0, γ}, where N ≜_ _{1, . . ., N_ _}_
denotes the set of N agents and s ∈S ≜ _S_ [1]×S [2]×...×S _[N]_ describes the joint state of the environment.
The initial state s0 ∼ _ρ0 is drawn from distribution ρ0, and at each time step t, all agents k ∈N_
_Achoose simultaneously one actionis drawn from transition kernel[1]_ _× A[2]_ _× ... × A[N]_ . After executing the joint action P a and a collaborative reward[k]t _[∈A][k][, yielding a joint action] at in state rt s =[ a]t, the next state r[t]_ ([≜]st[a], at[1] _t[×]) is returned. In a Dec-[ a]t[2]_ _[×] s[ ...]t+1[ ×] ∼[ a]t[N]P_ ([∈A]st, a[ ≜]t)
MDP, each agent k has a local state s[k]t
policy a[k]t _[∼]_ _[π][k][(][·|][s] ∈Nt[k][)][ based only on its local state. The collaborating team of agents aims to][∈S]_ _[k][, and chooses its actions with a decentralized]_
learn a joint policy, π(at|st) ≜ [Q][N]k=1 _[π][k][(][a]t[k][|][s]t[k][)][, that maximizes their expected discounted return,]_
_η(π) ≜_ E(st,at)[[P][∞]t=0 _[γ][t][r][t][]][, where][ γ][ ∈]_ [[0][,][ 1)][ is a discount factor.]

**Policy optimization methods.** For single-agent RL that is modeled as an infinite-horizon discounted Markov decision process (MDP) _,_ _, P, r, ρ0, γ_, the performance for a policy π(a _s) is_
_{S_ _A_ _}_ _|_
defined as: η(π) = E(st,at) _∞t=0_ _[γ][t][r][(][s][t][, a][t][)]_ . The action-value function Qπ and value function
_Vπ are defined as:_
 P 

_[∞]_

_Qπ(st, at) = Est+1_ _p(_ _st,at),_ _γ[l]r(st+l, at+l)_ _,_ _Vπ(st) = Eat_ _π(_ _st)_ _Qπ(st, at)_ _._
_∼_ _·|_ _∼_ _·|_
_at+1∼π(·|st+1)_ h Xl=0 i h i

Let the advantage function be Aπ(s, a) = Qπ(s, a) _Vπ(s); the following useful identity expresses_
_−_
the expected return of another policy ˜π in terms of the advantage over π (Kakade & Langford,
2002): η(˜π) = η(π) + _s_ _[ρ]π[˜][(][s][)][ P]a_ _π[˜](a|s)Aπ(s, a), where ρπ˜[(][s][)][ is the state distribution induced]_

by ˜π. The complex dependency of ρπ˜[(][s][)][ on][ ˜]π makes the righthand side difficult to optimize directly.
Schulman et al. (2015) proposed to consider the following surrogate objective

[P]

˜π(a _s)_

_Lπ(˜π) = η(π) +_ _ρπ(s)_ _π˜(a_ _s)Aπ(s, a) = η(π) + E(s,a)_ _ρπ_ _|_ _,_

_|_ _∼_ _π(a_ _s)_ _[A][π][(][s, a][)]_

_s_ _a_

X X h _|_ i

where the ρπ˜ [is replaced with][ ρ]π[. Define][ D]TV[max][(][π,][ ˜]π) ≜ maxs DTV _π(·|s), ˜π(·|s)_, where DTV is
the total variation (TV) divergence.
  
**Theorem 1. (Theorem 1 in Schulman et al. (2015)) Let α = DTV[max][(][π,][ ˜]π). Then the following bound**
_holds_
4ϵγ
_η(˜π)_ _Lπ(˜π)_
_≥_ _−_ (1 _γ)[2][ α][2][,]_

_−_

_where ϵ = maxs,a_ _Aπ(s, a)_ _._
_|_ _|_

This theorem forms the foundation of policy optimization methods, including Trust Region Policy
Optimization (TRPO) (Schulman et al., 2015) and Proximal Policy Optimization (PPO) (Schulman
et al., 2017). TRPO suggests a robust way to take large update steps by using a constraint, rather than
a penalty, on the TV divergence, and considers the following practical optimization problem,

˜π(a _s)_
TRPO: max E(s,a) _ρπ_ _|_ _,_ s.t. _DTV[max][(][π][(][·|][s][)][,][ ˜]π(_ _s))_ _α._ (1)
_π˜_ _∼_ _π(a_ _s)_ _[A][π][(][s, a][)]_ _·|_ _≤_
h _|_ i

This constrained optimization is complicated as it requires using conjugate gradient algorithms with
a quadratic approximation to the constraint. PPO simplifies the above optimization by clipping
probability ratios λπ˜ [=][ ˜]ππ((aa|ss)) [to form a lower bound of][ L][π][(˜]π):
_|_

PPO: max E(s,a) _ρπ_ min _λπ˜[A]π[(][s, a][)][,][ clip(][λ]π˜[,][ 1][ −]_ _[ϵ,][ 1 +][ ϵ][)][A]π[(][s, a][)]_ _,_ (2)
_π˜_ _∼_
   

where ϵ is a hyperparameter to specify the clipping range.

**Independent PPO and Multi-Agent PPO.** Both IPPO (Schröder de Witt et al., 2020) and
MAPPO (Yu et al., 2021) optimize decentralized policies with independent ratios. In particular, the
main objective IPPO and MAPPO optimize is

max min _λπ˜k_ _[A][k][(][s][k][, a][k][)][,][ clip(][λ]π˜k_ _[,][ 1][ −]_ _[ϵ,][ 1 +][ ϵ][)][A][k][(][s][k][, a][k][)]_ _k_ 1, 2, ..., N _,_
_π˜k_ [E][(][s][k][,a][k][)][∼][ρ][πk] _∀_ _∈{_ _}_
    (3)


-----

where λπ˜k [=][ ˜]ππkk((aa[k][k]|ss[k][k])) [denotes the ratio between the decentralized policy probabilities of agent][ k]
_|_
before and after updating. The difference between IPPO and MAPPO lies in how they estimate the
advantage function: IPPO learns a decentralized advantage function A[k](s[k], a[k]) ≜ [P][∞]t=0[[][r][(][s]t[k][, a]t[k][)]][−]
_V (s[k]) based on the local information (s[k], a[k]) for each agent, while MAPPO uses a centralized critic_
that conditions on centralized state information s: A[k](s[k], a[k]) ≜ Es−k _∞t=0[[][r][(][s]t[k][, a]t[k][)]][ −]_ _[V][ (][s][)]_,
whereagents. Consequently, as all agents share the same actor and critic networks, one can ignore the agent −k refers the set of all agents except agent k. Both methods use parameter sharing between P 
specifier k in the objective and use experience from all agents to update the actor and critic networks:

maxπ˜θ E(sk,ak)∼ρπθ min _λθAφ(s[k], a[k]), clip(λθ, 1 −_ _ϵ, 1 + ϵ)Aφ(s[k], a[k])_ _,_ (4)

_k_

X    

where λθ = _ππ[˜]θθ((aa[k][k]||ss[k][k]))_ [, and][ θ][,][ φ][ are shared parameters for policy and advantage networks. The use]
of independent ratios together with parameter sharing has shown strong empirical results in various
MARL benchmark tasks (Schröder de Witt et al., 2020; Yu et al., 2021).

4 TRUST REGION ANALYSIS FOR MARL

In this section, we first directly apply TRPO’s trust region analysis to cooperative MARL, which
yields joint ratios rather than the independent ratios adopted in IPPO and MAPPO. We then show that
optimization with independent ratios induces non-stationarity in MARL, which breaks the stationarity
assumption in the trust region analysis. Finally, we provide a new analysis that shows how monotonic
policy improvement can still arise from non-stationary transition dynamics with independent ratios.

4.1 OPTIMIZATION WITH JOINT RATIOS

Consider the joint policy π(a **_s) and the centralized advantage function Aπ(s, a) = Qπ(s, a)_**
_|_ _−_
_Vπ(s). Then, the trust region analysis for single-agent RL carries over directly to MARL with the_
surrogate objective as Lπ(˜π) = η(π) + **_s_** _[ρ][π][(][s][)][ P]a_ **_π[˜]_** (a|s)Aπ(s, a). One can consider the same

optimization problem for TRPO shown in equation 1,

˜π(a **_s)_** [P]
max E(s,a) _ρπ_ _|_ _,_ s.t. _DTV[max][(][π][(][·|][s][)][,][ ˜]π(_ **_s))_** _α._ (5)
**_π˜_** _∼_ **_π(a_** **_s)_** _[A][π][(][s][,][ a][)]_ _·|_ _≤_
h _|_ i

The trust region constraint is enforced over joint policies, which we refer as a centralized trust region
_constraint. With joint ratios defined as λπ˜_ [=] **_ππ˜_** ((aa|ss)) [=][ Q]k[N]=1 ˜ππkk((aa[k][k]|ss[k][k])), one can simplify the
_|_ _|_
above optimization as PPO to have the following objective,
 

JR-PPO: max E(s,a) _ρπ_ min **_λπ˜_** _[A]π[(][s][,][ a][)][,][ clip(][λ]π˜_ _[,][ 1][ −]_ _[ϵ,][ 1 +][ ϵ][)][A]π[(][s][,][ a][)]_ _._ (6)
**_π˜_** _∼_

We call the resulting algorithm Joint Ratio PPO (JR-PPO) (see Algorithm 1 in the appendix).   

Unlike IPPO and MAPPO, JR-PPO consider joint ratios over the joint policies, rather than independent
ones. This difference is crucial, as joint ratios naturally enjoy the monotonic improvement guarantee
carried over from the single-agent trust region analysis:, i.e., Theorem 1. Furthermore, the objective
used in IPPO and MAPPO is not equivalent to the above objective as they are lower bounds of
different objectives. Thus, Theorem 1 does not imply any guarantees for IPPO and MAPPO.

4.2 OPTIMIZATION WITH INDEPENDENT RATIOS

Optimization with independent ratios, however, induces non-stationarity in MARL. When optimizing
decentralized policies, the environment is non-stationary from the perspective of a single agent
since the other agents also change their policies during training. To analyze the non-stationarity in
decentralized policy optimization, we first consider the Markov chain for the local state s[k] induced by
the underlying MDP for agent k. When all agents’ policies are updated from π1, ..., πN to ˜π1, ..., ˜πN,
the state transition distribution of this Markov chain also shifts. We denote such transition shift from
_s[k]_ to ˜s[k] for agent k as


∆ππ[˜]11,...,,...,ππ˜NN [(˜]s[k]|s[k]) ≜


_pπ˜1,...,π˜N_ [(˜]s[k]|s[k], a[k])˜πk(a[k]|s[k]) − _pπ1,...,πN (˜s[k]|s[k], a[k])πk(a[k]|s[k])_


_a[k]_


-----

where pπ1,...,πN and pπ˜1,...,π˜N [refer to the transition dynamics before and update][ π][ is updated. The]
state transition shift consists of two parts: an exogenous part, which is caused by the update of other
agents’ policies (i.e., the change of transition dynamics from pπk to pπ˜k [), and an][ endogenous part][,]
which is contributed by the update of the agent’s own policy (i.e., the change of agent k’s policy from
_πk to ˜πk). See the appendix A.2 for detailed analysis. The exogenous shift breaks the assumption in_
the monotonic improvement guarantee (Schulman et al., 2015) that the MDP is stationary, i.e., the
state transition shift arises only endogenously from the agent’s policy changes. As a result, Theorem 1
no longer holds if one optimizes with independent ratios as in IPPO and MAPPO.

4.3 MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS

We now provide a new analysis for optimization with independent ratios. As the above analysis
suggests that the exogenous transition shift breaks the trust region analysis in TRPO, we instead
consider how to handle this exogenous shift in training. Specifically, since the exogenous shift is
caused by the changes of other agents’ policies, it can be controlled by constraining the update of
other agents’ policies in centralized training.
**Proposition 1. In a Dec-MDP, the transition shift ∆ππ[˜]11,...,,...,ππ˜NN** [(˜]s[k]|s[k]) decomposes as follows:

∆ππ[˜]11,...,π,...,π˜NN [(˜]s[k]|s[k]) = ∆ππ[˜]11,π,π22,...,π,...,πNN [(˜]s[k]|s[k]) + ∆ππ˜[˜]11,π,π˜22,π,π33,...,π,...,πNN [(˜]s[k]|s[k]) + ... + ∆ππ[˜]˜11,...,,...,ππ˜˜NN _−−11,,ππ˜NN_ [(˜]s[k]|s[k]).

The proof is given in the appendix A.3.1. This proposition implies that the state transition shift at
local observation s[k] is caused by the shifts arising from all decentralized policies. This decomposition inspires the derivation of a new monotonic improvement guarantee for decentralized policy
optimization by enforcing the trust region over joint policies.

Define the surrogate objective for decentralized policy πk as


_L[(]π[k]1[)],π2,...,πN_ [(˜]πk′ ) =


_ρπ1,π2,...,πN (s[k])_
_s[k]_

X


_π˜k′_ (a[k] _s[k])Aπk′ (s[k], a[k]),_
_|_
_a[k]_

X


and the expected return of decentralized policy πk as

_ηπ1,...,πN (πk) = E(sk,ak)∼ρπ1_ _,...,πN (sk,ak)_ _rk(s[k], a[k])_ _._

In practice, rk(s[k], a[k]) is usually unknown. As the state transition shift decomposes into the sum of 
transition shifts caused by each decentralized policy, we can bound this state transition shift with a
centralized trust region constraint as in equation 5.
**Theorem 2. Let α = DTV[max][(][π][,][ ˜]π). Then the following bound holds :**


_N_

4ϵγ
_L[(]π[k]1[)],...,πN_ [(˜]πk′ ) _k,_
_−_ (1 _γ)[2][ α][2]_ _∀_
_k[′]=1_ _−_

X


_ηπ˜1,...,π˜N_ [(˜]πk) ≥ _ηπ1,...,πN (πk) +_

_where ϵ = maxk_ maxsk,ak _Aπk_ (s[k], a[k]) _._
_∈N_ _|_ _|_


The proof is given in the appendix A.3.2. This theorem implies that, for sufficiently small α, the
performance increase of a decentralized policy πk is lower bounded by the sum of surrogate objectives
for each decentralized policy with respect to the samples generated by πk. In other words, if the
trust region is enforced, the sum of surrogate objectives yields an approximate lower bound for
_ηπ˜1,π˜2,...,π˜N_ [(˜]πk), which holds for any decentralized policy ˜πk.

Theorem 2 differs from Theorem 1 in three respects. First, the lower bound for one decentralized policy effectively relies on surrogate objectives for all agents, since the update of one agent’s
policy affects all other agents’ transition probability. Therefore, to improve the performance for
policy πk, we can simultaneously maximize L[(]π[k]1[)],...,πN [(˜]π1) + L[(]π[k]1[)],...,πN [(˜]π2) + ... + L[(]π[k]1[)],...,πN [(˜]πN ).
Second, unlike the surrogate objective in Theorem 1, the new surrogate objective implicitly contains an independent ratio λπ˜k [≜] _ππ[˜]kk((aa[k][k]|ss[k][k]))_ [as it can be rewritten as follows:][ L]π[(][k]1[)],π2,...,πN [(˜]πk′ ) =
_|_

E(sk,ak) _ππ˜kk′′_ ((aa[k][k]||ss[k][k])) _[A][π][k]_ [(][s][k][, a][k][)] . Third, the additional term (14−ϵγγ)[2][ α][2][ requires computing the total]

variation between joint policiesh i π(a **_s) and ˜π(a_** **_s), rather than the policies that are directly opti-_**
_|_ _|_
mized. We show in the next section that, in centralized training, this requirement is easily satisfied as
the magnitude of the constraint on the update is proportional to the number of agents.


-----

5 REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS

Theorem 2 indicates that the centralized trust region is crucial to guarantee monotonic improvement.
In this section, we show that bounding independent ratios is an effective way to enforce such a
centralized trust region constraint, and this enforcement requires taking into account the number of
agents. To achieve this, we first present two lemmas about DTV divergence.

**Proposition 2. In a Dec-MDP, DTV[max][(][π][,][ ˜]π) ≤** [P]k[N]=1 _[D]TV[max][(][π][k][,][ ˜]πk)._

This proposition is a direct result of the fact that the joint policy in a Dec-MDP factors as a product
of decentralized polices, i.e., π = _k=1_ _[π][k][ .]_

**Proposition 3. DTV[max][(][π][k][,][ ˜]πk) = maxs∈S** _π˜k(a[k]|s[k])≥πk(a[k]|s[k])_ _π˜k(a[k]|s[k]) −_ _πk(a[k]|s[k])_ _._

[Q][N]

This useful identity follows from a property ofP DTV: DTV(µ(x), ν (x)) = _µ(x)>ν(x)[[][µ][(][x][)][ −]_ _[ν][(][x][)]]_
where µ and ν are two distributions. This proposition indicates that a decentralized trust region is
also defined by the sum of probability differences over a special subset. We use this to upper bound

[P]
the trust region in the following analysis.
**Assumption 1. Assume the advantage function Aπk** (sk, ak) converges to a fixed point for _k._
_∀_

**Theorem 3. If independent ratios λk ≜** _ππ[˜]kk((aa[k][k]|ss[k][k]))_ _[are within the range][ [1][ −]_ _[ϵ][k][,][ 1 +][ ϵ][k][]][ for][ ∀][k][ ∈N]_ _[,]_
_|_

_then the following bound holds: DTV[max][(][π][k][(][·|][s][)][,][ ˜]πk(·|s)) < ϵk; DTV[max][(][π][(][·|][s][)][,][ ˜]π(·|s)) <_ _k=1_ _[ϵ][k][.]_

This theorem comes from that fact that optimizing ˜π(s, a) with respect to a converged A(s, a) leads

[P][N]

to ˜π(a|s) > π(a|s) if A(s, a) > 0, ∀s, a in actor-critic algorithms (Konda & Tsitsiklis, 2000). Thus,
based on Proposition 3, we have the following


_DTV[max][(][π][k][,][ ˜]πk) = max_ [˜πk(a[k] _s[k])_ _πk(a[k]_ _s[k])]_ max
_s[k]_ _|_ _−_ _|_ _≤_ _s[k]_

_a[k]_

X∈A[k]

_Ak(s[k],a[k])>0_



[ϵkπk(a[k] _s[k])] < ϵk._
_|_

_a[∈]_

_Ak(sX[k],aA[k][k])>0_


The equation is from Proposition 3 by considering A(s, a) > 0 such that ˜π(a|s) > π(a|s). The
first inequality is a result of bounded ratios and the second is from _a∈{a:A(s,a)>0}[[][π][(][a][|][s][)]][ <][ 1]_

(considering the lower bound yields the same analysis.) Furthermore, the trust region constraint over
joint policies is a direct result of Proposition 2. The detailed proof is given in the appendix A.3.3. As

[P]

_DTV is a bounded divergence between [0, 1], the ratio guarantee makes sense when ϵk ≤_ 1.0.

Theorem 3 implies that bounding independent ratios _ππ[˜]kk((aa[k][k]|ss[k][k]))_ [with][ [1][ −] _[ϵ][k][,][ 1 +][ ϵ][k][]][ amounts to]_
_|_
enforcing a trust region constraint with size ϵ over decentralized policies. Thus, to enforce the
centralized trust region constraint over joint policies, i.e.,can consider bounding independent ratios according to the number of agents, e.g., DTV[max][(][π][,][ ˜]π) ≤ _α as in Theorem 2, one λk ≜_ _ππ[˜]kk((aa[k][k]|ss[k][k]))_

_|_ _[∈]_

[1 − _N[α]_ _[,][ 1 +][ α]N_ []][. In the next section, we present practical implementations of these ratio constraints.]

6 INSTANTIATING RATIO CONSTRAINTS

In this section, we show that IPPO and MAPPO effectively satisfy the conditions of Theorem 2 and
Theorem 3. Specifically, the independent clipping and parameter sharing used by IPPO and MAPPO
are useful ways to approximate the ratio constraints in Theorem 3 and to optimize the surrogate
objective in Theorem 2. Furthermore, we show that the surrogate objectives optimized in IPPO and
MAPPO are essentially equivalent when their critics converge to a fixed point.

6.1 OPTIMIZING SURROGATE OBJECTIVES

The objective is to update all agents’ policies simultaneously with the experience from all agents,
which can be further simplified with parameter sharing (Gupta et al., 2017):


_ρπ1,π2,...,πN (s[k])_
_s[k]_

X


_π˜θ(u[k]_ _s[k])Aφ(s[k], u[k]),_ (7)
_|_
_u[k]_

X


max


s.t. _DT V[max][(][π][(][·|][s][)][,][ ˜]π(·|s)) ≤_ _α,_ (8)


-----

where θ and φ are the shared parameters for decentralized policies and critics. Furthermore, to
effectively optimize the surrogate objective, we can clip the probability ratios of each decentralized
policies to form a lower bound of the objective in equation 7, similar to PPO (Schulman et al., 2017).
Namely, with independent ratios λ[k] = _ππ[˜]θθ((uu[k][k]|ss[k][k]))_ _k_, we can optimize the following objective:
_|_ _∀_ _∈N_

max E(sk,uk) _ρ(sk,uk)_ min _λ[k]A[k], clip(λ[k], 1_ _ϵ, 1 + ϵ)A[k][],_
_θ_ _∼_ _−_

_k_

X   

which is exactly the objective used by IPPO and MAPPO.

6.2 ENFORCING THE TRUST REGION CONSTRAINT

Proposition 2 implies that, in centralized training, one way to enforce trust region constraint is to
delegate the centralized trust region constraint to each agent, such that the update of each decentralized
policy πk(a[k] _s[k]) is bounded. Therefore, to enforce the centralized trust region constraint, one can_
_|_
impose a sufficient condition as follows,


_DTV[max] πθ(·|s[k]), ˜πθ(·|s[k])_ _≤_ _N[α]_


_∀k ∈N_ _,_ (9)


which suggests that enforcement of the centralized trust region constraint translates to a decentralized
trust region constraint if the trust region is specified properly according to the number of agents.
Furthermore, based on Theorem 3, bounding independent ratios is an effective way to enforce the
trust region constraint. One can thus impose a sufficient condition to constrain independent ratios
_λ[k]_ such that λ[k] _∈_ [1 − _N[α]_ _[,][ 1 +][ α]N_ []][, where][ N][ is the number of agents in training. Clipping is one of]

many ways to achieve this sufficient condition but itself is a heuristic approximation so often fails to
bound ratios exactly within the ranges. In practice, one would need to tune the the clipping range
and the number of epochs so the ratios can be properly bounded. We show in the experiment section
that good values of the hyperparameters for the clipping range are highly sensitive to the number of
agents, as these hyperparameters, together with the number of agents, effectively determine the size
of the centralized trust region.

6.3 LEARNING ADVANTAGE FUNCTIONS

We now look at the training of the advantage function, where IPPO and MAPPO differ. IPPO trains a
decentralized advantage function, while MAPPO trains a centralized one that incorporates centralized
state information. Assume a stationary distribution of (s[k], a[k]) exists. From Lyu et al. (2021), we
have the following:

**Proposition 4. (Lemma 1 & 2 in Lyu et al. (2021)) Training of centralized critic and k-th decen-**
_tralized critic admits unique fixed points Q[π](s[k], s[−][k], a[k], a[−][k]) and Es−k,a−k_ [Q[π](s[k], s[−][k], a[k], a[−][k])]
_respectively, where Q[π]_ _is the true expected return under the joint policy π._

Accordingly, based on the definition, the centralized value function is _V (s)_ =
_V (s[k], s[−][k])_ = Eak,a−k [Q[π](s[k], s[−][k], a[k], a[−][k])] and the decentralized one is V (s[k]) =
Es−k,ak,a−k [Q[π](s[k], s[−][k], u[k], a[−][k])] = Es−k [V (s[k], s[−][k])] = Es−k [V (s)]. Thus, we have
_A[IPPO](s[k], a[k]) = A[MAPPO](s[k], a[k]) (and so IPPO and MAPPO objectives are equivalent) given that the_
underlying critics converge to a fixed point.

7 EXPERIMENTS

We consider the StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019) for our empirical
analysis as it provides a wide range of multi-agent tasks with varied difficulty and numbers of agents.
We first show that clipping is an effective way to constraint ratios when the number of optimization
epochs and the learning rate are properly specified. Furthermore, we show that clipping also requires
taking into account the number of agents such that the centralized trust region can be properly
enforced. We then empirically demonstrate that bounding independent ratios in effect enforces the
trust region over joint policies. Finally, we present results showing that IPPO and MAPPO perform
equivalently on SMAC maps with varied difficulty and numbers of agents.


-----

1.0

Cliping range

No clipping
0.5
0.4
0.3
0.2
0.1


10 Number of optimization epochs20 30 40 0.0 50 0.2 0.4DTV[max][(] k[,] k[)]0.6 0.8 1.0


1.0

0.8


1.0

0.8


0.6

0.4


0.6

0.4


|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||Numb|er of agents 10 9 8 6|


0.0

Number of agents

10
9
8
6
5
3
2

0.0 0.2 0.4 0.6 0.8 1.0

DTV[max][(,] )


0.2

0.0

|Col1|Col2|Col3|
|---|---|---|
||||
||Numb|er of epochs 40 30 25|


Number of epochs

40
30
25
20
15
10


0.0 0.2 0.4 0.6 0.8 1.0

DTV[max][(,] )

(d)

|No clipping Clipping 0.5 Clipping 0.4 Clipping 0.3 Clipping 0.2|0.8 tage|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|Clipping 0.1|0.6 percen ulative|||||
||0.4 Cum 0.2|||Cl|iping range No clipping 0.5 0.4|
|0.3 0.2 0.1 0.0 0 10 20 30 40 0.050 0.2 0.4 max 0.6 0.8 1||||||


(b)


(c)


(a)


Figure 1: Trust region with respect to the clipping range, the number of agents and the number
of optimization epochs: (a) Ratio ranges for 5 agents with the number of optimization epochs; (b)
Cumulative percentage of decentralized trust region as the clipping value varies; (c) Cumulative
percentage of centralized trust region as the number of agents varies (clipping at 0.1); and (d)
Cumulative percentage of centralized trust region with optimization epochs (clipping at 0.1).

**Clipping and ratio ranges.** Theorem 3 indicates that bounding the independent ratios amounts to
enforcing a trust region constraint over joint policies. We empirically show that independent ratio
clipping approximately bounds the independent ratios in the training if some hyperparameters are
properly set. We train decentralized policies on one map, 2s3z, and clip the independent ratios when
optimizing the surrogate objective. Figure 1a shows how the the max and min of the ratios changes
according to the number of optimization epochs with different clipping values. Independent ratio
clipping can effectively constrain the range of ratios only when the number of optimization epochs and
the clipping range are properly specified. In particular, the range of independent ratios grows as the
number of optimization epochs increases. This growth is slower when the clipping range is smaller,
e.g., ϵ = 0.1. Furthermore, the clipping range may not strictly bound ratios between [1 − _ϵ, 1 + ϵ]:_
when the clipping range is 0.1, the independent ratios can exceed 1.2; and the independent ratios can
even grow up to 1.6 when the clipping range is 0.3.


**Ratio clipping and trust region constraint.** Next, we show that the trust region defined by the
total variation is empirically bounded by independent ratio clipping, and this bound is also proportional to the number of agents. Specifically, we compute the maximum total variation divergence
_DTV[max]_ [over empirical samples collected by the behavior policy during the first round of actor update,]
which contains 100 optimization epochs, and report the distribution of DTV[max][. Figure 1b shows the]
distribution of DTV[max] [over decentralized policies when clipping range varies. For clipping at][ 0][.][1][,]
all DTV[max] [values are smaller than][ 0][.][2][, meaning that the trust region is effectively enforced to be]
small. As the clipping range increases, more DTV[max] [values exceed][ 0][.][3][. For the case without clipping,]
_DTV[max]_ [almost uniformly distributes among][ [0][.][0][,][ 0][.][8]][, implying trust region is no longer enforced.]
Figure 1c presents the distribution of DTV[max] [over joint polices for clipping at][ 0][.][1][, on maps with]
different number of agents. See appendix Table 1 for more details. The DTV[max][(][π][,][ ˜]π) is estimated by
summing up the empirical total variation distances DTV[max][(][π][k][,][ ˜]πk) over all agents. The DTV[max][(][π][,][ ˜]π)
grows almost proportionally with the number of agents, indicating that enforcing the centralized trust
region with independent ratios clipping also requires considering the number of agents. Figure 1d
presents the distribution of DTV[max] [over joint polices with different numbers of epochs for clipping at]
0.1. Compared to the number of agents, the number of epochs has less impact on the trust region.

**Independent ratios clipping on SMAC** Figure 2 shows the empirical returns and trust region
estimates with different ratio clipping values across different maps in SMAC. [1] Notably, when the
clipping value is small, e.g., ϵ = 0.1, the joint total variation distance, i.e., the centralized trust region,
can be effectively bounded, as in the second row in Figure 2. The empirical returns corresponding to
_ϵ = 0.1 are thus improved monotonically, outperforming all other returns consistently in four maps._
Moreover, as the number of agents increases, the trust region enforced by clipping value ϵ = 0.1 in
the initial training phase also grows from less than 0.3 to more than 0.5. In contrast, for clipping at

1Trained via decentralized advantage, i.e., IPPO. Results with centralized advantage are similar, as presented
in the appendix A.4. Unlike Yu et al. (2021), the value function is not clipped in our experiments.


-----

2s3z 3s5z 10m_vs_11m 27m_vs_30m

20.0

18 17.5 17.5

17.5

16 15.0 15.0

15.0

14 12.5 12.5

12.5 12

10.0 10.0

10.0 10

Test Return Mean 7.5 Test Return Mean 8 Test Return Mean 7.5 Test Return Mean 7.5

5.0

5.0 0.1 (5) 6 0.1 (5) 0.1 (5) 5.0 0.1 (5)

0.3 (5) 0.3 (5) 0.3 (5) 0.3 (5)

2.5 0.5 (5)1.0 (5) 4 0.5 (5)1.0 (5) 2.5 0.5 (5)1.0 (5) 2.5 0.5 (5)1.0 (5)

No clipping (5) No clipping (5) No clipping (5) No clipping (5)

0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000 0.0 0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000

Number of timesteps (K) Number of timesteps (K) Number of timesteps (K) Number of timesteps (K)

2s3z 0.1 (5) 1.4 3s5z 0.1 (5) 10m_vs_11m 0.1 (5) 3.0 27m_vs_30m 0.1 (5)

1.0 0.3 (5)0.5 (5)1.0 (5) 1.2 0.3 (5)0.5 (5)1.0 (5) 1.0 0.3 (5)0.5 (5)1.0 (5) 0.3 (5)0.5 (5)1.0 (5)

No clipping (5) No clipping (5) No clipping (5) 2.5 No clipping (5)

0.8 1.0 0.8

2.0

0.6 0.8

Joint Approx TV Joint Approx TV Joint Approx TV 0.6 Joint Approx TV

0.6 1.5

0.4

0.4 0.4 1.0

0.2

0.2

0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000

Number of timesteps (K) Number of timesteps (K) Number of timesteps (K) Number of timesteps (K)


Figure 2: Empirical returns and trust region estimates for independent ratios clipping.

0.8 3s5z IPPO (4) 1.0 IPPO (4) 1c3s5z IPPO (4) 10m_vs_11m 1.0 bane_vs_bane

MAPPO (4) MAPPO (4) MAPPO (4)

0.7

0.6 0.8 0.8 0.9

0.5 0.6 0.6 0.8

0.4 0.7

Test Battle Win Mean 0.3 Test Battle Win Mean 0.4 Test Battle Win Mean 0.4 Test Battle Win Mean 0.6

0.2

0.2 0.2 0.5

0.1

0.0 0.0 0.0 0.4 IPPO (4)MAPPO (4)

0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000

Number of timesteps (K) Number of timesteps (K) Number of timesteps (K) Number of timesteps (K)


Figure 3: Contrasting IPPO and MAPPO across different maps.

0.5 and 1.0, the learning quickly plateaus at local optima, especially on maps with many agents, e.g.,
10m_vs_11m and 27m_vs_30m, which shows that the policy performance η(πk) is closely related
to the enforcement of trust region. We also apply the same clipping values to joint clipping and
independent clipping, see Appendix A.6 for more analysis.

**IPPO and MAPPO** We show that the empirical performance of IPPO and MAPPO are very similar
despite the fact that the advantage functions are learned differently. We evaluate IPPO and MAPPO
on maps of varied difficulty and numbers of agents. We heuristically set the clipping range based
on the number of agents. Specifically, we set the clipping range ϵ for 3s5z, 1c3s5z, 10m_vs_11m,
and bane_vs_bane, as 0.1, 0.1, 0.1, and 0.05, respectively. The results are presented in Figure 3. On
the four maps considered, IPPO and MAPPO perform similarly. This phenomenon can be observed
in Yu et al. (2021), which provides more empirical comparisons between IPPO and MAPPO. Such
comparable performance also implies that, for actor-critic methods in MARL, the way of training
critics could be less crucial than enforcing the trust region constraint.

8 CONCLUSION

In this paper, we present a new monotonic improvement guarantee for optimizing decentralized
policies in cooperative MARL. We show that, despite the non-stationarity in IPPO and MAPPO, a
monotonic improvement guarantee still arises from enforcing the trust region constraint over joint
policies. This guarantee provides a theoretical understanding of the strong performance of IPPO and
MAPPO. Furthermore, we provide a theoretical foundation for proximal ratio clipping by showing
that a trust region constraint can be effectively enforced in a principled way by bounding independent
ratios based on the number of agents in training. Finally, our empirical results support the hypothesis


-----

that the strong performance of IPPO and MAPPO is a direct result of enforcing such a trust region
constraint via clipping in centralized training.

REFERENCES

Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual Multi-Agent Policy Gradients. arXiv:1705.08926 [cs], December 2017. URL
[http://arxiv.org/abs/1705.08926. arXiv: 1705.08926.](http://arxiv.org/abs/1705.08926)

Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using
deep reinforcement learning. In International Conference on Autonomous Agents and Multiagent
_Systems, pp. 66–83. Springer, 2017._

Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In In
_Proc. 19th International Conference on Machine Learning. Citeseer, 2002._

Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
_processing systems, pp. 1008–1014, 2000._

Hepeng Li and Haibo He. Multi-agent trust region policy optimization. _arXiv preprint_
_arXiv:2010.07916, 2020._

Wenhao Li, Xiangfeng Wang, Bo Jin, Junjie Sheng, and Hongyuan Zha. Dealing with nonstationarity in multi-agent reinforcement learning via trust region decomposition. arXiv preprint
_arXiv:2102.10616, 2021._

Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017.

Xueguang Lyu, Yuchen Xiao, Brett Daley, and Christopher Amato. Contrasting centralized and
decentralized critics in multi-agent reinforcement learning. arXiv preprint arXiv:2102.04402,
2021.

Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs.
SpringerBriefs in Intelligent Systems. Springer International Publishing, 2016. ISBN 978-3[319-28927-4. doi: 10.1007/978-3-319-28929-8. URL https://www.springer.com/gp/](https://www.springer.com/gp/book/9783319289274)
[book/9783319289274.](https://www.springer.com/gp/book/9783319289274)

Liviu Panait and Sean Luke. Cooperative Multi-Agent Learning: The State of the Art. Autonomous
_Agents and Multi-Agent Systems, 11:387–434, November 2005. doi: 10.1007/s10458-005-2631-2._

Georgios Papoudakis, Filippos Christianos, Arrasy Rahman, and Stefano V Albrecht. Dealing with
non-stationarity in multi-agent deep reinforcement learning. arXiv preprint arXiv:1906.04737,
2019.

Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement
learning. In International Conference on Machine Learning, pp. 4295–4304. PMLR, 2018.

Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.

Christian Schröder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip H. S.
Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft
multi-agent challenge? _[CoRR, abs/2011.09533, 2020. URL https://arxiv.org/abs/](https://arxiv.org/abs/2011.09533)_
[2011.09533.](https://arxiv.org/abs/2011.09533)

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889–1897. PMLR,
2015.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.


-----

Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi,
Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel.
Value-Decomposition Networks For Cooperative Multi-Agent Learning. arXiv:1706.05296 [cs],
[June 2017. URL http://arxiv.org/abs/1706.05296. arXiv: 1706.05296.](http://arxiv.org/abs/1706.05296)

Ying Wen, Hui Chen, Yaodong Yang, Zheng Tian, Minne Li, Xu Chen, and Jun Wang. A gametheoretic approach to multi-agent trust region optimization. arXiv preprint arXiv:2106.06828,
2021.

Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.


-----

A APPENDIX

A.1 JOINT RATIO PPO

**Algorithm 1 Joint Ratio PPO (JR-PPO)**

**for iteration i = 0, 1, 2, . . . do**

Roll out decentralized policies [π1, π2, ..., πN ] in environment;
Compute centralized advantage estimates A(s, a);
Compute joint ratios λπ˜ [=][ ˜]ππ((aa|ss)) [=][ Q]k[N]=1 ˜ππkk((aa[k][k]|ss[k][k])) ;
_|_ _|_

Optimize the surrogate objective maxπ˜ [E] min **_λπ˜[A][(][s][,][ a][)][,][ clip(][λ]π˜[,][ 1][ −]_** _[ϵ,][ 1 +][ ϵ][)][A][(][s][,][ a][)]_ .

**end for**

   


A.2 STATIONARITY ASSUMPTION IN TRPO

The single-agent TRPO relies on the following analysis:


_Lπ(˜π)_ _Lπ(π) =_
_−_


_ρ(s)_

_ρ(s)_

_ρ(s)_

_ρ(s)_

_ρ(s)_


_π˜(a_ _s)_ _π(a_ _s)_ _Aπ(s, a)_
_|_ _−_ _|_


_π˜(a|s) −_ _π(a|s)_ _r(s) +_ _p(s[′]|s, a)γv(s[′]) −_ _v(s)_

_s[′]_

 X

_π˜(a|s) −_ _π(a|s)_ _p(s[′]|s, a)γv(s[′])_

X   


_s[′]_

_s[′]_

X

_s[′]_

X


_π˜(a|s)p(s[′]|s, a) −_ _π(a|s)p(s[′]|s, a)_ _γv(s[′])_

_a_

X   

_pπ˜[(][s][′][|][s][)][ −]_ _[p]π[(][s][′][|][s][)]_ _γv(s[′])._



This analysis is based on the assumption that p(s[′]|s, a) remains the same before and after π is
updated, such that transition shift pπ˜[(][s][′][|][s][)][ −] _[p]π[(][s][′][|][s][)][ is only caused by the agent’s policy update,]_
i.e., endogenously. Such analysis no longer holds when the transition dynamics p(s[′]|s, a) are
non-stationary: pπ˜[(][s][′][|][s, a][)][ ̸][=][ p]π[(][s][′][|][s, a][)][.]

A.3 PROOFS

A.3.1 PROOF OF PROPOSITION 1

_Proof. Assume agent k’s policy π[k]_ is executed independently of other agents policies π[−][k], we have

_π[1],...,π˜[N]_
∆π[˜][1],...,π[N][ (˜]s[k]|s[k])


_p(˜s[k], ˜s[−][k]|s[k], s[−][k], a[k], a[−][k])_ _π˜[k](a[k]|s[k])˜π[−][k](a[−][k]|s[−][k]) −_ _π[k](a[k]|s[k])π[−][k](a[−][k]|s[−][k])_



_s˜[−][k],s[−][k]_

_a[k],a[−][k]_

_s˜[−][k],s[−][k]_

X

_a[k],a[−][k]_


= _p(˜s[k], ˜s[−][k]|s[k], s[−][k], a[k], a[−][k]) ·_ _π˜[k](a[k]|s[k])˜π[−][k](a[−][k]|s[−][k]) −_ _π˜[k](a[k]|s[k])π[−][k](a[−][k]|s[−][k])_

_s˜a[−][k]X[k],a,s[−][−][k][k]_  exogenous

| {z }

+ ˜π[k](a[k]|z[k])π[−][k](a[−][k]|s[−][k]) − _π[k](a[k]|s[k])π[−][k](a[−][k]|s[−][k])_ _._
endogenous



| {z }

The above decomposition can be repeated such that the exogenous part can be translated into
endogenous parts that are specific to each agent. Specifically, repeat the decomposition for the


-----

exogenous part by considering agent k[′] (k[′] ≠ _k):_

_π˜[k](a[k]|s[k])˜π[−][k](a[−][k]|s[−][k]) −_ _π˜[k](a[k]|s[k])π[−][k](a[−][k]|s[−][k])_

=˜π[k](a[k]|s[k]) _π˜[k][′]_ (a[k][′] _|s[k][′]_ )˜π[−{][k,k][′][}](a[−{][k,k][′][}]|s[−{][k,k][′][}]) − _π[k][′]_ (a[k][′] _|s[k][′]_ )π[−{][k,k][′][}](a[−{][k,k][′][}]|s[−{][k,k][′][}])

=˜π[k](a[k] _s[k])_ _π˜[k][′]_ (a[k][′] _s[k][′]_ )˜π[−{][k,k][′][}](a[−{][k,k][′][}] _s[−{][k,k][′][}])_ _π˜[k][′]_ (a[k][′] _s[k][′]_ )π[−{][k,k][′][}](a[−{][k,k][′][}] _s[−{][k,k][′][}])_
_|_ _|_ _|_ _−_ _|_ _|_
 _π[k]-exogenous_

+ ˜π[k][′] (a[k][′] _s|[k][′]_ )π[−{][k,k][′][}](a[−{][k,k][′][}] _s[−{][k,k][′][}])_ _π[k][′]_ (a[k]{z[′] _s[k][′]_ )π[−{][k,k][′][}](a[−{][k,k][′][}] _s[−{][k,k][′][}])_ _._ }
_|_ _|_ _−_ _|_ _|_
_π[k]-endogenous_
| {z }

_π[1],...,π˜[N]_
So on and so forth, one can decompose ∆π[˜][1],...,π[N][ (˜]s[k]|s[k]) as follows:

_π[1],...,π˜[N]_ _π[1],π[2],...,π[N]_ _π[1],π˜[2],π[3],...,π[N]_ _π[1],...,π˜[N]_ _[−][1],π˜[N]_
∆π[˜][1],...,π[N][ (˜]s[k]|s[k]) = ∆π[˜][1],π[2],...,π[N][ (˜]s[k]|s[k]) + ∆π[˜]˜[1],π[2],π[3],...,π[N][ (˜]s[k]|s[k]) + ... + ∆π[˜]˜[1],...,π˜[N] _[−][1],π[N][ (˜]s[k]|s[k]),_

which implies that the state transition shift at local observation s[k] is caused by the shifts arising from
all decentralized policies.

A.3.2 PROOF OF THEOREM 2

_Proof. This proof is based on the perturbation theory._ Let G[s][i] = (1 + γPπ[s]1[i] _,π2,...,πN_ [+]
(γPπ[s]1[i] _,π2,...,πN_ [)][2][ +][ ...][ = (1][ −] _[γP]π[ s]1[i]_ _,π2,...,πN_ [)][−][1][ and][ ˜]G[s][i] = (1 + γPπ˜[s]1[i] _,π˜2,...,π˜N_ [+ (][γP]π[ s]˜1[i] _,π˜2,...,π˜N_ [)][2][ +]
_... = (1_ _−_ _γPπ˜[s]1[i]_ _,π˜2,...,π˜N_ [)][−][1][ denote the distribution of state][ s][i][ under][ π][1][, π][2][, ..., π][N][ and][ ˜]π1, ˜π2, ..., ˜πN .
We will use the convention that ρ (a density on state space) is a vector and r (a reward function
on state space) is a dual vector (i.e., linear functional on vectors), thus rρ is a scalar meaning the
expected reward under density ρ. Note that η(π) = rGρ0, and η(˜π) = rGρ[˜] 0. Denote the state shift
of si as ∆(si) = Pπ˜[s]1[i] _,π˜2,...,π˜N_ _[−]_ _[P]π[ s]1[i]_ _,π2,...,πN_ [. Using the perturbation theory, we have the following]

[G[s][i] ][−][1] _−_ [ G[˜][s][i] ][−][1] = γPπ˜[s]1[i] _,π˜2,...,π˜N_ _[−]_ _[γP]π[ s]1[i]_ _,π2,...,πN_ [=][ γ][∆(][s][i][)][.]

Left multiply by G[s][i] and right multiply by _G[˜][s][i]_ :

_G˜[s][i]_ = G[s][i] + γG[s][i] ∆(si) G[˜][s][i] _._

Substituting the right-hand side into _G[˜][s][i]_ gives

_G˜[s][i]_ _G[s][i]_ = γG[s][i] ∆(si)G[s][i] + γ[2]G[s][i] ∆(si)G[s][i] ∆(si) G[˜][s][i] ).
_−_

Consider decentralized policy πi:


_ηπ˜1,π˜2,...,π˜N_ [(˜]πi) − _ηπ1,π2,...,πN (πi)_

=rG[˜][s][i] _ρ0 −_ _rG[s][i]_ _ρ0 = r( G[˜][s][i]_ _−_ _G[s][i]_ )ρ0

=γrG[s][i] ∆(si)G[s][i] _ρ0 + γ[2]rG[s][i]_ ∆(si)G[s][i] ∆(si) G[˜][s][i] _ρ0._

Let us first consider the leading term γrG[s][i] ∆(si)G[s][i] _ρ0,_

_γrG[s][i]_ ∆(si)G[s][i] _ρ0_


(pπ˜[(][s][′]i[|][s][i][)][ −] _[p][π][(][s][′]i[|][s][i][))][γv][(][s][′]i[) =]_
_s[′]i_

X


∆ππ[˜]11,...,,...,ππ˜NN [(][s]i[′] _[|][s][i][)][γv][(][s][′]i[)]_
_s[′]i_

X


_ρ(si)_
_si_

X

_ρ(si)_
_si_

X


_ρ(si)_
_si_

X


∆ππ[˜]11,π,π22,...,π,...,πNN [(][s]i[′] _[|][s][i][) + ∆]ππ˜[˜]11,π,π˜22,π,π33,...,π,...,πNN_ [(][s]i[′] _[|][s][i][) +][ ...][ + ∆]ππ[˜]˜11,...,,...,ππ˜˜NN_ _−−11,,ππ˜NN_ [(][s]i[′] _[|][s][i][)]_ _γv(s[′]i[)][.]_
i


_s[′]i_


-----

For one of these summation terms, we have the following [2]:


∆ππ[˜]˜11,...,,...,ππ˜˜jj−−11,,ππ˜jj _,...,π,...,πNN_ [(][s]i[′] _[|][s][i][)]_ _γv(s[′]i[)]_ (10)
i


_ρ(si)_
_si_

X

_ρ(si)_
_si_

X

_ρ(si)_
_si_

X

_ρ(si)_
_si_

X

_ρ(si)_
_si_

X


_s[′]i_

_s[′]i_

X

_ai_

X

_ai_

X

_ai_

X


_pπ˜1,...,π˜j−1,πj_ _,...,πN_ [(][s]i[′] _[|][s][i][, a][i][)˜]πj(ai|si) −_ _pπ˜1,...,π˜j−1,πj_ _,...,πN_ [(][s]i[′] _[|][s][i][, a][i][)][π][j][(][a][i][|][s][i][)]_ _γv(s[′]i[)]_



_ai_


(11)

_π˜j(ai|si) −_ _πj(ai|si)_ _pπ˜1,...,π˜j−1,πj_ _,...,πN_ [(][s]i[′] _[|][s][i][, a][i][)][γv][(][s][′]i[)]_ (12)

_s[′]i_

 [X]

_π˜j(ai|si) −_ _πj(ai|si)_ _r(si) +_ _pπ˜1,...,π˜j−1,πj_ _,...,πN_ [(][s]i[′] _[|][s][i][, a][i][)][γv][(][s][′]i[)][ −]_ _[v]π[˜]π1,...,j_ _,...,ππ˜j−N1,(si)_

_s[′]i_

 X

(13)


_π˜j(ai_ _si)_ _πj(ai_ _si)_ _Aπj_ (si, ai) (14)
_|_ _−_ _|_



=L[(]π[i]1[)],π2,...,πN [(˜]πj) − _L[(]π[i]1[)],π2,...,πN_ [(][π][j][)][.] (15)
The derivation from line 13 to 14 is based on the following definition:

_Aπj_ (si, ai) ≜ _r(si) +_ _s[′]i_ _pπ˜1,...,π˜j−1,πj_ _,...,πN_ [(][s]i[′] _[|][s][i][, a][i][)][γv][(][s]i[′]_ [)][ −] _[v]π[˜]π1,...,j_ _,...,ππ˜j−N1,(si),_
X

where vπ˜π1,...,j _,...,ππ˜j−N1,(si) ≜_ _r(si) + γ_ _s[′]i_ _[p]π[˜]1,...,π˜j−1,πj_ _,...,πN_ [(][s]i[′] _[|][s][i][, a][i][)][ P]ai_ _[π][j][(][a][i][|][s][i][)][v][(][s]i[′]_ [)][. which]

will result in L[(]π[i]1[)],π2,...,πN [(][π]j[) = 0][ for][P] _[ ∀][j][. In fact, for line 13,][ r][(][s]i[)][ and][ v]π˜π1,...,j_ _,...,ππ˜j−N1,(si) can be_

interpreted as functions over si, which will be zero if integrated with _ai_ _π˜j(ai_ _si)_ _πj(ai_ _si)_

_|_ _−_ _|_
Note that the advantage function also conditions on the πj.

Note that the advantage of πj with respect to si, ai in multi-agent RL is defined differently from the[P]   
advantage function in the single agent case.

We can thus repeat the above decomposition iteratively and have the following:


_γrG[s][i]_ ∆(si)G[s][i] _ρ0 = L[(]π[i]1[)],π2,...,πN_ [(˜]π1) − _L[(]π[i]1[)],π2,...,πN_ [(][π][1][)]

+ L[(]π[i]1[)],π2,...,πN [(˜]π2) − _L[(]π[i]1[)],π2,...,πN_ [(][π][2][)]

+ ... + L[(]π[i]1[)],π2,...,πN [(˜]πN ) − _L[(]π[i]1[)],π2,...,πN_ [(][π][N] [)][.]

Note that Lπ[(][i]1[)],π2,...,πN [(][π]j[) = 0][ for][ ∀][j][. Thus,]

_γrG[s][i]_ ∆(si)G[s][i] _ρ0 = Lπ[(][i]1[)],π2,...,πN_ [(˜]π1) + L[(]π[i]1[)],π2,...,πN [(˜]π2) + ... + L[(]π[i]1[)],π2,...,πN [(˜]πN ).

Next we bound the second term γ[2]rG[s][i] ∆(si)G[s][i] ∆(si) G[˜][s][i] _ρ0._ First we consider the product
_γrG[s][i]_ ∆(si) = γv∆(si). Consider the component of this dual vector:


(γrG[s][i] ∆(si)s =
_|_ _|_ _|_


(pπ˜i [(][s][′]i[|][s][i][)][ −] _[p][π]i_ [(][s][′]i[|][s][i][))][γv][i][(][s][′]i[)][|]
_s[′]i_

X



_[N]_

=| _p(s[′]1[, ..., s][′]N_ _[|][s][1][, ..., s][N]_ _[, a][1][, ..., a][N]_ [)][v][i][(][s][′]i[)] _π˜j(ai|si) −_

_s[′]1X[,...,s][′]N_ _s1,...,siX−1,si+1,sN_ _a1X,...,aN_  _jY=1_

_[N]_ _N_

= _Ai(si, ai)_ _π˜j(ai_ _si)_ _πj(ai_ _si)_ _._
_|_ _|_ _−_ _|_ _|_

_s1,...,siX−1,si+1,sN_ _a1X,...,aN_  _jY=1_ _jY=1_ 

Thus, we have

_N_ _N_

(γrG[s][i] ∆(si)s _Ai(si, ai)_ _π˜j(ai_ _si)_ _πj(ai_ _si)_ 2αϵ.
_∥_ _∥≤_ _|_ _||_ _|_ _−_ _|_ _| ≤_

_s1,...,sN_ _a1,...,aN_ _j=1_ _j=1_

X X Y Y


=|

_s[′]1[,...,s][′]_

X


_πj(ai_ _si)_
_|_
_j=1_

Y


_s1,...,si−1,si+1,sN_


2 Note that Aπj (si, ai) in the analysis is defined with the transition dynamics pπ˜1,...,π˜j−1,πj _,...,πN_ [(][s]i[′] _[|][s][i][, a][i][)][.]_


-----

We bound the other portion G[s][i] ∆(si) G[˜][s][i] _ρ0 using the l1 operator norm:_

_G[s][i]_ ∆(si) G[˜][s][i] _ρ0_ 1
_∥_ _∥_ _≤_ _[∥](1[∆(][s][i]γ[)])[∥][2][1][ .]_

_−_

The ∥∆(si)∥1 can be bound as follows:


_pπ˜i_ [(][s][′]i[|][s][i][)][ −] _[p][π]i_ [(][s][′]i[|][s][i][)][|]
_|_
_s[′]i[,s][i]_

X


_∥∆(si)∥1 =_


_p(s[′]1[, ..., s][′]N_ _[|][s][1][, ..., s][N]_ _[, a][1][, ..., a][N]_ [)][|] _π˜j(ai|si) −_
_a1,...,aN_ _j=1_

X Y


_πj(ai_ _si)_
_|_ _|_
_j=1_

Y


_s[′]1[,...,s][′]N_

=

_s1,...,sN_

X

So we have that


_s1,...,sN_

_a1,...,aN_

X


_π˜j(ai_ _si)_

_|_ _|_ _−_

_j=1_

Y


_πj(ai_ _si)_ 2α.
_|_ _| ≤_
_j=1_

Y


4ϵγ
_γ[2]|rG[s][i]_ ∆(si)G[s][i] ∆(si) G[˜][s][i] _ρ0| ≤_ _γ∥rG[s][i]_ ∆(si)∥∞∥G[s][i] ∆(si) G[˜][s][i] _ρ0∥1 ≤_ (1 _γ)[2][ α][2][.]_

_−_

A.3.3 PROOF OF THEOREM 3

_Proof. Given the assumption that the advantage function A(s, a) converges to a fixed point. we have_
the fact that optimizingConsider independent ratios ˜π(s, a λ)k with respect to ≜ _ππ[˜]kk((aa[k][k]|ss[k][k]))_ [which are within the range] A(s, a) leads to ˜π(a|s) > π[ [1]([ −]a|s[ϵ])[k] if[,][ 1 +] A([ ϵ]s, a[k][]][ for]) >[ ∀] 0[k], ∀[ ∈N]s, a[,].
_|_
Thus, based on Proposition 3, we have the following


_DTV[max][(][π][k][,][ ˜]πk) = max_
_s[k]_

max
_≤_ _s[k]_

max
_≤_ _s[k]_



[˜πk(a[k] _s[k])_ _πk(a[k]_ _s[k])]_
_|_ _−_ _|_

_a[k]_
_π˜(a|sX)∈A>π([k]a|s)_


max [(1 + ϵk)πk(a[k] _s[k])_ _πk(a[k]_ _s[k])]_
_≤_ _s[k]_ _|_ _−_ _|_

_a[k]_

X∈A[k]

_Ak(s[k],a[k])>0_

max [ϵkπk(a[k] _s[k])] < ϵk._
_≤_ _s[k]_ _|_

_a[∈]_

_Ak(sX[k],aA[k][k])>0_

The first inequality is a result of bounded ratios and the second is from _a∈{a:A(s,a)>0}[[][π][(][a][|][s][)]][ <][ 1][.]_

Furthermore, the trust region constraint over joint policies is a direct result of Proposition 2. One can
also consider A(s, a) < 0, which leads to ˜π(a _s) < π(a_ _s) in optimization. Given that independent_

[P]

_|_ _|_
ratios are also lower bounded by 1 − _ϵ, the same conclusion can be reached for DTV[max][.]_

A.4 EXPERIMENT DETAILS AND MORE RESULTS

The number of agents in each is given in Table 1.

Test battle win mean of IPPO on maps with varied difficulty and numbers of agents is presented in
Figure 4.

Empirical test battle win mean, test returns and trust region estimates of MAPPO on maps with varied
difficult and numbers of agents are presented in Figure 5.

A.5 ABLATIONS ON SMALL CLIPPING VALUES

We also present the ablation results for small clipping values, i.e., < 0.1, in Figure 6. It is true that a
small clipping value results in a small trust region, and thus small clipping values, e.g., 0.08, 0.05
and 0.03, would be preferred for maps with a large number of agents, e.g., maps 10m_vs_11m (10


-----

Table 1: Number of agents on maps.

SMAC Map Number of agents

2s_vs_1sc 2

3s_vs_5z 3

2s3z 5

6h_vs_8z 6

1c3s5z 9

10m_vs_11m 10

|0.1 (5) 0.3 (5) 0.5 (5) 1.0 (5) No clipping (5)|Col2|Col3|
|---|---|---|
||||
||||
||||


|5) 5) 5)|Col2|Col3|
|---|---|---|
|5) lipping (5)|||
||||
||||
||||
||||
||||


|0.1 (5) 0.3 (5) 0.5 (5)|Col2|
|---|---|
|1.0 (5) No clipping (5)||
|||
|||
|||
|||


2s3z 3s5z 10m_vs_11m 27m_vs_30m

1.0 0.8 0.1 (5)0.3 (5) 0.8 0.1 (5)0.3 (5) 0.1 (5)0.3 (5)

0.5 (5)1.0 (5) 0.7 0.5 (5)1.0 (5) 0.5 0.5 (5)1.0 (5)

0.8 No clipping (5) 0.6 No clipping (5) No clipping (5)

0.6 0.4

0.6 0.5

0.4 0.4 0.3

Test Battle Win Mean 0.4 Test Battle Win Mean Test Battle Win Mean 0.3 Test Battle Win Mean 0.2

0.2 0.2

0.2 0.1 (5)0.3 (5)0.5 (5) 0.1 0.1

1.0 (5)

0.0 No clipping (5) 0.0 0.0 0.0

0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000

Number of timesteps (K) Number of timesteps (K) Number of timesteps (K) Number of timesteps (K)


Figure 4: Test battle win mean of IPPO on maps with varied difficulty and numbers of agents

agents) and 27m_vs_30m (27 agents). However, when the clip value is too small, e.g., ϵ = 0.01 in
maps with 5 and 8 agents, the resultant trust region is also small and the update step in each iteration
can thus be too small to improve the policy. Thus, one would need to trade off between the trust
region constraint, to ensure monotonic improvement, and the policy update step, to ensure a sufficient
parameter update at each iteration.

A.6 COMPARISON BETWEEN JOINT RATIO CLIPPING AND INDEPENDENT RATIO CLIPPING

We apply the same clipping values to these two types of clipping, and use maps with many agents,
i.e., 10m_vs_11m and 27m_vs_30m, to make the difference more salient (based on the theoretical
results in the paper). The results are presented in Figure 7 and 8.

Compared to joint ratio clipping, the independent ratio clipping is more sensitive to the number of
agents. In particular, for a small clipping value, e.g., ϵ = 0.1, joint ratio clipping consistently produces
better performance than independent ratio clipping, even when the number of agents changes from
10 to 27. As the clipping value increases to 0.5, the performance gap between these two types of
clipping becomes larger, which is also aligned with our theoretical analysis.


-----

3s5z 10m_vs_11m 27m_vs_30m

0.8 0.1 (4)0.3 (4) 0.1 (4)0.3 (4) 0.6 0.1 (3)0.3 (3)

0.7 0.5 (4)1.0 (4) 0.8 0.5 (4)1.0 (4) 0.5 (3)1.0 (3)

No clipping (4) No clipping (4) 0.5 No clipping (3)

0.6

0.5 0.6 0.4

0.4 0.3

0.4

test_battle_won_mean 0.3 test_battle_won_mean test_battle_won_mean 0.2

0.2 0.2

0.1

0.1

0.0 0.0 0.0

0 250000 500000 750000 10000001250000150000017500002000000 0 250000 500000 750000 10000001250000150000017500002000000 0 250000 500000 750000 10000001250000150000017500002000000

0.1 (4) 3s5z 20.0 10m_vs_11m 27m_vs_30m

18 0.3 (4)0.5 (4)1.0 (4) 17.5 17.5

16 No clipping (4) 15.0 15.0

14

12.5 12.5

12

10.0 10.0

10

test_return_mean test_return_mean 7.5 test_return_mean 7.5

8

6 5.0 0.1 (4) 5.0 0.1 (3)

0.3 (4) 0.3 (3)

4 2.5 0.5 (4)1.0 (4) 2.5 0.5 (3)1.0 (3)

No clipping (4) No clipping (3)

0 250000 500000 750000 10000001250000150000017500002000000 0.0 0 250000 500000 750000 10000001250000150000017500002000000 0 250000 500000 750000 10000001250000150000017500002000000

3s5z 10m_vs_11m 27m_vs_30m

1.4 0.1 (4)0.3 (4)0.5 (4)1.0 (4) 0.9 0.1 (4)0.3 (4)0.5 (4)1.0 (4) 2.752.50 0.1 (3)0.3 (3)0.5 (3)1.0 (3)

1.2 No clipping (4) 0.8 No clipping (4) 2.25 No clipping (3)

1.0 0.7 2.00

1.75

0.8 0.6

joint_approx_TV joint_approx_TV joint_approx_TV 1.50

0.5

0.6 1.25

0.4 1.00

0.4

0.3 0.75

0 250000 500000 750000 10000001250000150000017500002000000 0 250000 500000 750000 10000001250000150000017500002000000 0 250000 500000 750000 10000001250000150000017500002000000


Figure 5: Empirical test battle win mean (first row), test returns (second row) and trust region
estimates (third row) of MAPPO on maps with varied difficult and numbers of agents


-----

20.0 2s3z 20.0 0.01 (5) 3s5z 20.0 10m_vs_11m 0.01 (5) 27m_vs_30m

17.5 17.5 0.03 (5)0.05 (5)0.08 (5) 17.5 17.5 0.03 (5)0.05 (5)0.08 (5)

15.0 15.0 15.0 15.0

12.5 12.5 12.5 12.5

10.0 10.0 10.0 10.0

Test Return Mean 7.5 Test Return Mean Test Return Mean 7.5 Test Return Mean 7.5

7.5

5.0 5.0 5.0

0.01 (5) 5.0 0.01 (5)

2.5 0.03 (5)0.05 (5) 2.5 0.03 (5)0.05 (5) 2.5

0.0 0.08 (5) 2.5 0.08 (5)

0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000

Number of timesteps (K) Number of timesteps (K) Number of timesteps (K) Number of timesteps (K)

2s3z 3s5z 10m_vs_11m 27m_vs_30m

0.01 (5)0.03 (5)0.05 (5) 0.40 0.01 (5)0.03 (5)0.05 (5) 0.45 0.01 (5)0.03 (5)0.05 (5) 0.8

0.4 0.08 (5) 0.08 (5) 0.40 0.08 (5)

0.35 0.7

0.35

0.3 0.30 0.30 0.6

0.25 0.25 0.5

Joint Approx TV 0.2 Joint Approx TV 0.20 Joint Approx TV 0.20 Joint Approx TV 0.4

0.15 0.15 0.3

0.1 0.10 0.10 0.2 0.01 (5)0.03 (5)

0.05 0.05 0.1 0.05 (5)0.08 (5)

0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000

Number of timesteps (K) Number of timesteps (K) Number of timesteps (K) Number of timesteps (K)

2s3z 3s5z 10m_vs_11m 27m_vs_30m

1.0 0.01 (5) 0.01 (5) 0.01 (5) 0.01 (5)

0.03 (5)0.05 (5)0.08 (5) 0.8 0.03 (5)0.05 (5)0.08 (5) 0.8 0.03 (5)0.05 (5)0.08 (5) 0.6 0.03 (5)0.05 (5)0.08 (5)

0.8

0.5

0.6

0.6

0.6 0.4

0.4 0.4 0.4 0.3

Test Battle Win Mean Test Battle Win Mean Test Battle Win Mean Test Battle Win Mean

0.2

0.2 0.2 0.2

0.1

0.0 0.0 0.0 0.0

0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000

Number of timesteps (K) Number of timesteps (K) Number of timesteps (K) Number of timesteps (K)


Figure 6: Empirical returns, trust region estimates and test battle win rate for small values of
independent ratios clipping.


-----

|Col1|Col2|Col3|joint (|4)|
|---|---|---|---|---|
||||||
||||||
||||||
||||||

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
||||||independent (4) joint (4)|

|joint (4)|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||

|Col1|Col2|Col3|joint (|4)|
|---|---|---|---|---|
||||||
||||||
||||||
||||||
||||||
||||||
||||||

|joint (4)|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
|||||||
|||||||
|||||||

|joint (4)|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||

|Col1|Col2|Col3|joint (|3)|
|---|---|---|---|---|
||||||
||||||
||||||
||||||

|Col1|Col2|Col3|Col4|Col5|independent (4)|
|---|---|---|---|---|---|
||||||joint (4)|
|||||||
|||||||
|||||||
|||||||
|||||||
|||||||
|||||||

|joint (3)|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||


10m_vs_11m 27m_vs_30m

0.5 independent (4)joint (4) 1.0 independent (4)joint (4)

0.8

0.4

0.6

0.3

Joint Approx TV Joint Approx TV 0.4

0.2

0.2

0.1

0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000

Number of timesteps (K) Number of timesteps (K)

10m_vs_11m 27m_vs_30m

0.7 independent (4)joint (4) 2.00 independent (4)joint (4)

1.75

0.6

1.50

0.5 1.25

0.4 1.00

Joint Approx TV Joint Approx TV

0.75

0.3

0.50

0.2

0.25

0.1

0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000

Number of timesteps (K) Number of timesteps (K)

10m_vs_11m 27m_vs_30m

0.9 independent (4)joint (4) 2.5 independent (3)joint (3)

0.8

0.7 2.0

0.6 1.5

0.5

Joint Approx TV Joint Approx TV

0.4 1.0

0.3

0.5

0.2

0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000

Number of timesteps (K) Number of timesteps (K)


10m_vs_11m 27m_vs_30m

independent (4)

17.5 17.5 joint (4)

15.0 15.0

12.5 12.5

10.0 10.0

Test Return Mean 7.5 Test Return Mean

7.5

5.0

5.0

2.5 independent (4)joint (4) 2.5

0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000

Number of timesteps (K) Number of timesteps (K)

1816 independent (4)joint (4) 10m_vs_11m 18 independent (4)joint (4) 27m_vs_30m

16

14

14

12

12

10

10

8

Test Return Mean Test Return Mean 8

6

6

4

4

2

2

0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000

Number of timesteps (K) Number of timesteps (K)

10m_vs_11m 27m_vs_30m

independent (4) 18 independent (3)

16 joint (4) joint (3)

16

14

14

12

12

10

10

Test Return Mean 8 Test Return Mean 8

6

6

4

4

2

2

0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000

Number of timesteps (K) Number of timesteps (K)


(a) Joint TV divergence


(b) Empirical returns


Figure 7: Joint divergence estimates and empirical returns for two types of ratio clipping at different
clipping values: 0.1 (first row), 0.3 (first row) and 0.5 (first row).


-----

10m_vs_11m 27m_vs_30m

independent (4) 0.7 independent (4)

0.8 joint (4) joint (4)

0.6

0.6 0.5

0.4

0.4 0.3

Test Battle Win Mean Test Battle Win Mean

0.2

0.2

0.1

0.0 0.0

0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000

Number of timesteps (K) Number of timesteps (K)

10m_vs_11m 27m_vs_30m

independent (4) independent (4)

0.6 joint (4) 0.5 joint (4)

0.5

0.4

0.4

0.3

0.3

Test Battle Win Mean 0.2 Test Battle Win Mean 0.2

0.1 0.1

0.0 0.0

0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000

Number of timesteps (K) Number of timesteps (K)

10m_vs_11m 27m_vs_30m

0.6 independent (4) independent (3)

joint (4) joint (3)

0.5 0.4

0.4 0.3

0.3

0.2

Test Battle Win Mean 0.2 Test Battle Win Mean

0.1

0.1

0.0 0.0

0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000

Number of timesteps (K) Number of timesteps (K)


Figure 8: Test battle win rate for two types of ratio clipping at different clipping values: 0.1 (first
row), 0.3 (first row) and 0.5 (first row).


-----

