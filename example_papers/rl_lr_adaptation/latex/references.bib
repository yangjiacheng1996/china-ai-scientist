%% LaTeX2e file `references.bib'
%% generated by the `filecontents' environment
%% from source `template' on 2024/08/08.
%%
@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT Press}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{karpathy2023nanogpt,
  title = {nanoGPT},
  author = {Karpathy, Andrej},
  year = {2023},
  journal = {URL https://github.com/karpathy/nanoGPT/tree/master},
  note = {GitHub repository}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{loshchilov2017adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@misc{gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2024},
  eprint={2303.08774},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2303.08774},
}

@Article{Xu2021ConvergenceOT,
 author = {Dongpo Xu and Shengdong Zhang and Huisheng Zhang and D. Mandic},
 booktitle = {Neural Networks},
 journal = {Neural networks : the official journal of the International Neural Network Society},
 pages = {
          17-23
        },
 title = {Convergence of the RMSProp deep learning method with penalty for nonconvex optimization},
 volume = {139},
 year = {2021}
}


@Article{Loshchilov2016SGDRSG,
 author = {I. Loshchilov and F. Hutter},
 booktitle = {International Conference on Learning Representations},
 journal = {arXiv: Learning},
 title = {SGDR: Stochastic Gradient Descent with Warm Restarts},
 year = {2016}
}


@Article{Loshchilov2016SGDRSG,
 author = {I. Loshchilov and F. Hutter},
 booktitle = {International Conference on Learning Representations},
 journal = {arXiv: Learning},
 title = {SGDR: Stochastic Gradient Descent with Warm Restarts},
 year = {2016}
}


@Misc{None,
 title = {Supplementary Material for " Asynchronous Methods for Deep Reinforcement Learning "}
}


@Article{Loshchilov2016SGDRSG,
 author = {I. Loshchilov and F. Hutter},
 booktitle = {International Conference on Learning Representations},
 journal = {arXiv: Learning},
 title = {SGDR: Stochastic Gradient Descent with Warm Restarts},
 year = {2016}
}


@Misc{None,
 title = {Supplementary Material for " Asynchronous Methods for Deep Reinforcement Learning "}
}


@Article{Traor'e2020SequentialCO,
 author = {Cheik Traor'e and Edouard Pauwels},
 booktitle = {Operations Research Letters},
 journal = {Oper. Res. Lett.},
 pages = {452-458},
 title = {Sequential convergence of AdaGrad algorithm for smooth convex optimization},
 volume = {49},
 year = {2020}
}

